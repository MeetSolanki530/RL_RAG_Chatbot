{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a12c9cd",
   "metadata": {},
   "source": [
    "### Importing Packages which are required..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e206693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_cerebras import ChatCerebras\n",
    "from langchain_community.document_loaders.notebook import NotebookLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from pathlib import Path\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9a607",
   "metadata": {},
   "source": [
    "### Initialize loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0207bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = PyPDFDirectoryLoader(path=\"./PDF_DOC/PDF\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b9a5f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 34 0 (offset 0)\n",
      "Ignoring wrong pointing object 41 0 (offset 0)\n",
      "Ignoring wrong pointing object 49 0 (offset 0)\n",
      "Ignoring wrong pointing object 64 0 (offset 0)\n",
      "Ignoring wrong pointing object 124 0 (offset 0)\n",
      "Ignoring wrong pointing object 151 0 (offset 0)\n",
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 200 0 (offset 0)\n",
      "Ignoring wrong pointing object 400 0 (offset 0)\n",
      "Ignoring wrong pointing object 411 0 (offset 0)\n",
      "Ignoring wrong pointing object 506 0 (offset 0)\n",
      "Ignoring wrong pointing object 655 0 (offset 0)\n",
      "Ignoring wrong pointing object 683 0 (offset 0)\n",
      "Ignoring wrong pointing object 271 0 (offset 0)\n",
      "Ignoring wrong pointing object 334 0 (offset 0)\n",
      "Ignoring wrong pointing object 355 0 (offset 0)\n",
      "Ignoring wrong pointing object 463 0 (offset 0)\n",
      "Ignoring wrong pointing object 549 0 (offset 0)\n",
      "Ignoring wrong pointing object 553 0 (offset 0)\n",
      "Ignoring wrong pointing object 557 0 (offset 0)\n",
      "Ignoring wrong pointing object 561 0 (offset 0)\n",
      "Ignoring wrong pointing object 565 0 (offset 0)\n",
      "Ignoring wrong pointing object 569 0 (offset 0)\n",
      "Ignoring wrong pointing object 573 0 (offset 0)\n",
      "Ignoring wrong pointing object 577 0 (offset 0)\n",
      "Ignoring wrong pointing object 581 0 (offset 0)\n",
      "Ignoring wrong pointing object 585 0 (offset 0)\n",
      "Ignoring wrong pointing object 603 0 (offset 0)\n",
      "Ignoring wrong pointing object 607 0 (offset 0)\n",
      "Ignoring wrong pointing object 611 0 (offset 0)\n",
      "Ignoring wrong pointing object 663 0 (offset 0)\n",
      "Ignoring wrong pointing object 665 0 (offset 0)\n",
      "Ignoring wrong pointing object 670 0 (offset 0)\n",
      "Ignoring wrong pointing object 692 0 (offset 0)\n",
      "Ignoring wrong pointing object 729 0 (offset 0)\n",
      "Ignoring wrong pointing object 751 0 (offset 0)\n",
      "Ignoring wrong pointing object 764 0 (offset 0)\n",
      "Ignoring wrong pointing object 766 0 (offset 0)\n",
      "Ignoring wrong pointing object 768 0 (offset 0)\n",
      "Ignoring wrong pointing object 846 0 (offset 0)\n",
      "Ignoring wrong pointing object 850 0 (offset 0)\n",
      "Ignoring wrong pointing object 855 0 (offset 0)\n",
      "Ignoring wrong pointing object 857 0 (offset 0)\n",
      "Ignoring wrong pointing object 862 0 (offset 0)\n",
      "Ignoring wrong pointing object 864 0 (offset 0)\n",
      "Ignoring wrong pointing object 921 0 (offset 0)\n",
      "Ignoring wrong pointing object 925 0 (offset 0)\n",
      "Ignoring wrong pointing object 945 0 (offset 0)\n",
      "Ignoring wrong pointing object 948 0 (offset 0)\n",
      "Ignoring wrong pointing object 950 0 (offset 0)\n",
      "Ignoring wrong pointing object 952 0 (offset 0)\n",
      "Ignoring wrong pointing object 954 0 (offset 0)\n",
      "Ignoring wrong pointing object 974 0 (offset 0)\n",
      "Ignoring wrong pointing object 976 0 (offset 0)\n",
      "Ignoring wrong pointing object 981 0 (offset 0)\n",
      "Ignoring wrong pointing object 987 0 (offset 0)\n",
      "Ignoring wrong pointing object 991 0 (offset 0)\n",
      "Ignoring wrong pointing object 993 0 (offset 0)\n",
      "Ignoring wrong pointing object 997 0 (offset 0)\n",
      "Ignoring wrong pointing object 1010 0 (offset 0)\n",
      "Ignoring wrong pointing object 1015 0 (offset 0)\n",
      "Ignoring wrong pointing object 1019 0 (offset 0)\n",
      "Ignoring wrong pointing object 1030 0 (offset 0)\n",
      "Ignoring wrong pointing object 1034 0 (offset 0)\n",
      "Ignoring wrong pointing object 1038 0 (offset 0)\n",
      "Ignoring wrong pointing object 1042 0 (offset 0)\n",
      "Ignoring wrong pointing object 1052 0 (offset 0)\n",
      "Ignoring wrong pointing object 1055 0 (offset 0)\n",
      "Ignoring wrong pointing object 1073 0 (offset 0)\n",
      "Ignoring wrong pointing object 1079 0 (offset 0)\n",
      "Ignoring wrong pointing object 1085 0 (offset 0)\n",
      "Ignoring wrong pointing object 1089 0 (offset 0)\n",
      "Ignoring wrong pointing object 1102 0 (offset 0)\n",
      "Ignoring wrong pointing object 1106 0 (offset 0)\n",
      "Ignoring wrong pointing object 1110 0 (offset 0)\n",
      "Ignoring wrong pointing object 1112 0 (offset 0)\n",
      "Ignoring wrong pointing object 1114 0 (offset 0)\n",
      "Ignoring wrong pointing object 1193 0 (offset 0)\n",
      "Ignoring wrong pointing object 1195 0 (offset 0)\n",
      "Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "Ignoring wrong pointing object 1216 0 (offset 0)\n",
      "Ignoring wrong pointing object 1256 0 (offset 0)\n",
      "Ignoring wrong pointing object 1259 0 (offset 0)\n",
      "Ignoring wrong pointing object 1262 0 (offset 0)\n",
      "Ignoring wrong pointing object 1265 0 (offset 0)\n",
      "Ignoring wrong pointing object 1268 0 (offset 0)\n",
      "Ignoring wrong pointing object 1271 0 (offset 0)\n",
      "Ignoring wrong pointing object 1329 0 (offset 0)\n",
      "Ignoring wrong pointing object 1331 0 (offset 0)\n",
      "Ignoring wrong pointing object 1336 0 (offset 0)\n",
      "Ignoring wrong pointing object 1339 0 (offset 0)\n",
      "Ignoring wrong pointing object 1341 0 (offset 0)\n",
      "Ignoring wrong pointing object 1344 0 (offset 0)\n",
      "Ignoring wrong pointing object 1346 0 (offset 0)\n",
      "Ignoring wrong pointing object 1348 0 (offset 0)\n",
      "Ignoring wrong pointing object 1357 0 (offset 0)\n",
      "Ignoring wrong pointing object 1359 0 (offset 0)\n",
      "Ignoring wrong pointing object 1361 0 (offset 0)\n",
      "Ignoring wrong pointing object 1366 0 (offset 0)\n",
      "Ignoring wrong pointing object 1370 0 (offset 0)\n",
      "Ignoring wrong pointing object 1404 0 (offset 0)\n",
      "Ignoring wrong pointing object 1407 0 (offset 0)\n",
      "Ignoring wrong pointing object 1433 0 (offset 0)\n",
      "Ignoring wrong pointing object 1436 0 (offset 0)\n",
      "Ignoring wrong pointing object 1438 0 (offset 0)\n",
      "Ignoring wrong pointing object 1443 0 (offset 0)\n",
      "Ignoring wrong pointing object 1445 0 (offset 0)\n",
      "Ignoring wrong pointing object 1470 0 (offset 0)\n",
      "Ignoring wrong pointing object 1474 0 (offset 0)\n",
      "Ignoring wrong pointing object 1479 0 (offset 0)\n",
      "Ignoring wrong pointing object 1483 0 (offset 0)\n",
      "Ignoring wrong pointing object 1488 0 (offset 0)\n",
      "Ignoring wrong pointing object 1492 0 (offset 0)\n",
      "Ignoring wrong pointing object 1550 0 (offset 0)\n",
      "Ignoring wrong pointing object 1565 0 (offset 0)\n",
      "Ignoring wrong pointing object 1642 0 (offset 0)\n",
      "Ignoring wrong pointing object 1816 0 (offset 0)\n",
      "Ignoring wrong pointing object 1822 0 (offset 0)\n",
      "Ignoring wrong pointing object 1826 0 (offset 0)\n",
      "Ignoring wrong pointing object 1829 0 (offset 0)\n",
      "Ignoring wrong pointing object 1852 0 (offset 0)\n",
      "Ignoring wrong pointing object 1905 0 (offset 0)\n",
      "Ignoring wrong pointing object 1929 0 (offset 0)\n",
      "Ignoring wrong pointing object 1942 0 (offset 0)\n",
      "Ignoring wrong pointing object 1972 0 (offset 0)\n",
      "Ignoring wrong pointing object 2062 0 (offset 0)\n",
      "Ignoring wrong pointing object 2064 0 (offset 0)\n",
      "Ignoring wrong pointing object 2067 0 (offset 0)\n",
      "Ignoring wrong pointing object 2073 0 (offset 0)\n",
      "Ignoring wrong pointing object 2088 0 (offset 0)\n",
      "Ignoring wrong pointing object 2090 0 (offset 0)\n",
      "Ignoring wrong pointing object 2093 0 (offset 0)\n",
      "Ignoring wrong pointing object 2099 0 (offset 0)\n",
      "Ignoring wrong pointing object 2242 0 (offset 0)\n",
      "Ignoring wrong pointing object 2246 0 (offset 0)\n",
      "Ignoring wrong pointing object 2252 0 (offset 0)\n",
      "Ignoring wrong pointing object 2256 0 (offset 0)\n",
      "Ignoring wrong pointing object 2260 0 (offset 0)\n",
      "Ignoring wrong pointing object 2264 0 (offset 0)\n",
      "Ignoring wrong pointing object 2270 0 (offset 0)\n",
      "Ignoring wrong pointing object 2275 0 (offset 0)\n",
      "Ignoring wrong pointing object 2280 0 (offset 0)\n",
      "Ignoring wrong pointing object 2283 0 (offset 0)\n",
      "Ignoring wrong pointing object 2302 0 (offset 0)\n",
      "Ignoring wrong pointing object 2305 0 (offset 0)\n",
      "Ignoring wrong pointing object 2554 0 (offset 0)\n",
      "Ignoring wrong pointing object 2592 0 (offset 0)\n",
      "Ignoring wrong pointing object 2597 0 (offset 0)\n",
      "Ignoring wrong pointing object 2653 0 (offset 0)\n",
      "Ignoring wrong pointing object 2657 0 (offset 0)\n",
      "Ignoring wrong pointing object 2661 0 (offset 0)\n",
      "Ignoring wrong pointing object 2665 0 (offset 0)\n",
      "Ignoring wrong pointing object 2680 0 (offset 0)\n",
      "Ignoring wrong pointing object 2688 0 (offset 0)\n",
      "Ignoring wrong pointing object 2696 0 (offset 0)\n",
      "Ignoring wrong pointing object 2704 0 (offset 0)\n",
      "Ignoring wrong pointing object 2712 0 (offset 0)\n",
      "Ignoring wrong pointing object 2725 0 (offset 0)\n",
      "Ignoring wrong pointing object 2746 0 (offset 0)\n",
      "Ignoring wrong pointing object 2759 0 (offset 0)\n",
      "Ignoring wrong pointing object 2806 0 (offset 0)\n",
      "Ignoring wrong pointing object 2821 0 (offset 0)\n",
      "Ignoring wrong pointing object 2856 0 (offset 0)\n",
      "Ignoring wrong pointing object 2973 0 (offset 0)\n",
      "Ignoring wrong pointing object 2977 0 (offset 0)\n",
      "Ignoring wrong pointing object 2981 0 (offset 0)\n",
      "Ignoring wrong pointing object 2985 0 (offset 0)\n",
      "Ignoring wrong pointing object 3088 0 (offset 0)\n",
      "Ignoring wrong pointing object 3102 0 (offset 0)\n",
      "Ignoring wrong pointing object 3106 0 (offset 0)\n",
      "Ignoring wrong pointing object 3110 0 (offset 0)\n",
      "Ignoring wrong pointing object 3143 0 (offset 0)\n",
      "Ignoring wrong pointing object 3145 0 (offset 0)\n",
      "Ignoring wrong pointing object 3187 0 (offset 0)\n",
      "Ignoring wrong pointing object 3190 0 (offset 0)\n",
      "Ignoring wrong pointing object 3282 0 (offset 0)\n",
      "Ignoring wrong pointing object 3287 0 (offset 0)\n",
      "Ignoring wrong pointing object 3324 0 (offset 0)\n",
      "Ignoring wrong pointing object 3326 0 (offset 0)\n",
      "Ignoring wrong pointing object 3328 0 (offset 0)\n",
      "Ignoring wrong pointing object 3374 0 (offset 0)\n",
      "Ignoring wrong pointing object 3377 0 (offset 0)\n",
      "Ignoring wrong pointing object 3381 0 (offset 0)\n",
      "Ignoring wrong pointing object 3383 0 (offset 0)\n",
      "Ignoring wrong pointing object 3386 0 (offset 0)\n",
      "Ignoring wrong pointing object 3390 0 (offset 0)\n",
      "Ignoring wrong pointing object 3396 0 (offset 0)\n",
      "Ignoring wrong pointing object 3400 0 (offset 0)\n",
      "Ignoring wrong pointing object 3402 0 (offset 0)\n",
      "Ignoring wrong pointing object 3404 0 (offset 0)\n",
      "Ignoring wrong pointing object 3407 0 (offset 0)\n",
      "Ignoring wrong pointing object 3409 0 (offset 0)\n",
      "Ignoring wrong pointing object 3413 0 (offset 0)\n",
      "Ignoring wrong pointing object 3420 0 (offset 0)\n",
      "Ignoring wrong pointing object 3424 0 (offset 0)\n",
      "Ignoring wrong pointing object 3426 0 (offset 0)\n",
      "Ignoring wrong pointing object 3428 0 (offset 0)\n",
      "Ignoring wrong pointing object 3431 0 (offset 0)\n",
      "Ignoring wrong pointing object 3433 0 (offset 0)\n",
      "Ignoring wrong pointing object 3436 0 (offset 0)\n",
      "Ignoring wrong pointing object 3448 0 (offset 0)\n",
      "Ignoring wrong pointing object 3451 0 (offset 0)\n",
      "Ignoring wrong pointing object 3454 0 (offset 0)\n",
      "Ignoring wrong pointing object 3460 0 (offset 0)\n",
      "Ignoring wrong pointing object 3551 0 (offset 0)\n",
      "Ignoring wrong pointing object 3558 0 (offset 0)\n",
      "Ignoring wrong pointing object 3560 0 (offset 0)\n",
      "Ignoring wrong pointing object 3567 0 (offset 0)\n",
      "Ignoring wrong pointing object 3569 0 (offset 0)\n",
      "Ignoring wrong pointing object 3571 0 (offset 0)\n",
      "Ignoring wrong pointing object 3576 0 (offset 0)\n",
      "Ignoring wrong pointing object 3583 0 (offset 0)\n",
      "Ignoring wrong pointing object 3585 0 (offset 0)\n",
      "Ignoring wrong pointing object 3591 0 (offset 0)\n",
      "Ignoring wrong pointing object 3593 0 (offset 0)\n",
      "Ignoring wrong pointing object 3595 0 (offset 0)\n",
      "Ignoring wrong pointing object 3600 0 (offset 0)\n",
      "Ignoring wrong pointing object 3607 0 (offset 0)\n",
      "Ignoring wrong pointing object 3609 0 (offset 0)\n",
      "Ignoring wrong pointing object 3615 0 (offset 0)\n",
      "Ignoring wrong pointing object 3617 0 (offset 0)\n",
      "Ignoring wrong pointing object 3619 0 (offset 0)\n",
      "Ignoring wrong pointing object 3624 0 (offset 0)\n",
      "Ignoring wrong pointing object 3631 0 (offset 0)\n",
      "Ignoring wrong pointing object 3633 0 (offset 0)\n",
      "Ignoring wrong pointing object 3639 0 (offset 0)\n",
      "Ignoring wrong pointing object 3641 0 (offset 0)\n",
      "Ignoring wrong pointing object 3643 0 (offset 0)\n",
      "Ignoring wrong pointing object 3648 0 (offset 0)\n",
      "Ignoring wrong pointing object 3655 0 (offset 0)\n",
      "Ignoring wrong pointing object 3657 0 (offset 0)\n",
      "Ignoring wrong pointing object 3663 0 (offset 0)\n",
      "Ignoring wrong pointing object 3665 0 (offset 0)\n",
      "Ignoring wrong pointing object 3667 0 (offset 0)\n",
      "Ignoring wrong pointing object 3672 0 (offset 0)\n",
      "Ignoring wrong pointing object 3675 0 (offset 0)\n",
      "Ignoring wrong pointing object 3679 0 (offset 0)\n",
      "Ignoring wrong pointing object 3683 0 (offset 0)\n",
      "Ignoring wrong pointing object 3687 0 (offset 0)\n",
      "Ignoring wrong pointing object 3739 0 (offset 0)\n",
      "Ignoring wrong pointing object 3742 0 (offset 0)\n",
      "Ignoring wrong pointing object 3764 0 (offset 0)\n",
      "Ignoring wrong pointing object 3766 0 (offset 0)\n",
      "Ignoring wrong pointing object 3794 0 (offset 0)\n",
      "Ignoring wrong pointing object 3845 0 (offset 0)\n",
      "Ignoring wrong pointing object 3847 0 (offset 0)\n",
      "Ignoring wrong pointing object 3850 0 (offset 0)\n",
      "Ignoring wrong pointing object 3852 0 (offset 0)\n",
      "Ignoring wrong pointing object 3855 0 (offset 0)\n",
      "Ignoring wrong pointing object 3857 0 (offset 0)\n",
      "Ignoring wrong pointing object 3860 0 (offset 0)\n",
      "Ignoring wrong pointing object 3863 0 (offset 0)\n",
      "Ignoring wrong pointing object 3876 0 (offset 0)\n",
      "Ignoring wrong pointing object 3891 0 (offset 0)\n",
      "Ignoring wrong pointing object 3893 0 (offset 0)\n",
      "Ignoring wrong pointing object 3904 0 (offset 0)\n",
      "Ignoring wrong pointing object 3906 0 (offset 0)\n",
      "Ignoring wrong pointing object 3912 0 (offset 0)\n",
      "Ignoring wrong pointing object 3914 0 (offset 0)\n",
      "Ignoring wrong pointing object 3917 0 (offset 0)\n",
      "Ignoring wrong pointing object 3920 0 (offset 0)\n",
      "Ignoring wrong pointing object 3936 0 (offset 0)\n",
      "Ignoring wrong pointing object 3943 0 (offset 0)\n",
      "Ignoring wrong pointing object 3945 0 (offset 0)\n",
      "Ignoring wrong pointing object 3997 0 (offset 0)\n",
      "Ignoring wrong pointing object 4020 0 (offset 0)\n",
      "Ignoring wrong pointing object 4053 0 (offset 0)\n",
      "Ignoring wrong pointing object 4055 0 (offset 0)\n",
      "Ignoring wrong pointing object 4076 0 (offset 0)\n",
      "Ignoring wrong pointing object 4078 0 (offset 0)\n",
      "Ignoring wrong pointing object 4081 0 (offset 0)\n",
      "Ignoring wrong pointing object 4083 0 (offset 0)\n",
      "Ignoring wrong pointing object 4086 0 (offset 0)\n",
      "Ignoring wrong pointing object 4088 0 (offset 0)\n",
      "Ignoring wrong pointing object 4091 0 (offset 0)\n",
      "Ignoring wrong pointing object 4094 0 (offset 0)\n",
      "Ignoring wrong pointing object 4100 0 (offset 0)\n",
      "Ignoring wrong pointing object 4103 0 (offset 0)\n",
      "Ignoring wrong pointing object 4106 0 (offset 0)\n",
      "Ignoring wrong pointing object 4118 0 (offset 0)\n",
      "Ignoring wrong pointing object 4132 0 (offset 0)\n",
      "Ignoring wrong pointing object 4136 0 (offset 0)\n",
      "Ignoring wrong pointing object 4167 0 (offset 0)\n",
      "Ignoring wrong pointing object 4169 0 (offset 0)\n",
      "Ignoring wrong pointing object 4172 0 (offset 0)\n",
      "Ignoring wrong pointing object 4218 0 (offset 0)\n",
      "Ignoring wrong pointing object 4220 0 (offset 0)\n",
      "Ignoring wrong pointing object 4223 0 (offset 0)\n",
      "Ignoring wrong pointing object 4225 0 (offset 0)\n",
      "Ignoring wrong pointing object 4228 0 (offset 0)\n",
      "Ignoring wrong pointing object 4230 0 (offset 0)\n",
      "Ignoring wrong pointing object 4233 0 (offset 0)\n",
      "Ignoring wrong pointing object 4236 0 (offset 0)\n",
      "Ignoring wrong pointing object 4249 0 (offset 0)\n",
      "Ignoring wrong pointing object 4263 0 (offset 0)\n",
      "Ignoring wrong pointing object 4267 0 (offset 0)\n",
      "Ignoring wrong pointing object 4298 0 (offset 0)\n",
      "Ignoring wrong pointing object 4300 0 (offset 0)\n",
      "Ignoring wrong pointing object 4302 0 (offset 0)\n",
      "Ignoring wrong pointing object 4306 0 (offset 0)\n",
      "Ignoring wrong pointing object 4309 0 (offset 0)\n",
      "Ignoring wrong pointing object 4311 0 (offset 0)\n",
      "Ignoring wrong pointing object 4314 0 (offset 0)\n",
      "Ignoring wrong pointing object 4316 0 (offset 0)\n",
      "Ignoring wrong pointing object 4319 0 (offset 0)\n",
      "Ignoring wrong pointing object 4321 0 (offset 0)\n",
      "Ignoring wrong pointing object 4324 0 (offset 0)\n",
      "Ignoring wrong pointing object 4326 0 (offset 0)\n",
      "Ignoring wrong pointing object 4329 0 (offset 0)\n",
      "Ignoring wrong pointing object 4331 0 (offset 0)\n",
      "Ignoring wrong pointing object 4334 0 (offset 0)\n",
      "Ignoring wrong pointing object 4336 0 (offset 0)\n",
      "Ignoring wrong pointing object 4340 0 (offset 0)\n",
      "Ignoring wrong pointing object 4343 0 (offset 0)\n",
      "Ignoring wrong pointing object 4348 0 (offset 0)\n",
      "Ignoring wrong pointing object 4350 0 (offset 0)\n",
      "Ignoring wrong pointing object 4356 0 (offset 0)\n",
      "Ignoring wrong pointing object 4358 0 (offset 0)\n",
      "Ignoring wrong pointing object 4363 0 (offset 0)\n",
      "Ignoring wrong pointing object 4439 0 (offset 0)\n",
      "Ignoring wrong pointing object 4441 0 (offset 0)\n",
      "Ignoring wrong pointing object 4444 0 (offset 0)\n",
      "Ignoring wrong pointing object 4446 0 (offset 0)\n",
      "Ignoring wrong pointing object 4449 0 (offset 0)\n",
      "Ignoring wrong pointing object 4451 0 (offset 0)\n",
      "Ignoring wrong pointing object 4454 0 (offset 0)\n",
      "Ignoring wrong pointing object 4457 0 (offset 0)\n",
      "Ignoring wrong pointing object 4479 0 (offset 0)\n",
      "Ignoring wrong pointing object 4487 0 (offset 0)\n",
      "Ignoring wrong pointing object 4490 0 (offset 0)\n",
      "Ignoring wrong pointing object 4494 0 (offset 0)\n",
      "Ignoring wrong pointing object 4509 0 (offset 0)\n",
      "Ignoring wrong pointing object 4511 0 (offset 0)\n",
      "Ignoring wrong pointing object 4514 0 (offset 0)\n",
      "Ignoring wrong pointing object 4516 0 (offset 0)\n",
      "Ignoring wrong pointing object 4519 0 (offset 0)\n",
      "Ignoring wrong pointing object 4521 0 (offset 0)\n",
      "Ignoring wrong pointing object 4524 0 (offset 0)\n",
      "Ignoring wrong pointing object 4527 0 (offset 0)\n",
      "Ignoring wrong pointing object 4540 0 (offset 0)\n",
      "Ignoring wrong pointing object 4554 0 (offset 0)\n",
      "Ignoring wrong pointing object 4558 0 (offset 0)\n",
      "Ignoring wrong pointing object 4562 0 (offset 0)\n",
      "Ignoring wrong pointing object 4606 0 (offset 0)\n",
      "Ignoring wrong pointing object 4658 0 (offset 0)\n",
      "Ignoring wrong pointing object 4662 0 (offset 0)\n",
      "Ignoring wrong pointing object 4664 0 (offset 0)\n",
      "Ignoring wrong pointing object 4666 0 (offset 0)\n",
      "Ignoring wrong pointing object 4684 0 (offset 0)\n",
      "Ignoring wrong pointing object 4724 0 (offset 0)\n",
      "Ignoring wrong pointing object 4727 0 (offset 0)\n",
      "Ignoring wrong pointing object 4730 0 (offset 0)\n",
      "Ignoring wrong pointing object 4733 0 (offset 0)\n",
      "Ignoring wrong pointing object 4740 0 (offset 0)\n",
      "Ignoring wrong pointing object 4744 0 (offset 0)\n",
      "Ignoring wrong pointing object 4747 0 (offset 0)\n",
      "Ignoring wrong pointing object 4779 0 (offset 0)\n",
      "Ignoring wrong pointing object 4782 0 (offset 0)\n",
      "Ignoring wrong pointing object 4787 0 (offset 0)\n",
      "Ignoring wrong pointing object 4791 0 (offset 0)\n",
      "Ignoring wrong pointing object 4794 0 (offset 0)\n",
      "Ignoring wrong pointing object 4797 0 (offset 0)\n",
      "Ignoring wrong pointing object 4799 0 (offset 0)\n",
      "Ignoring wrong pointing object 4802 0 (offset 0)\n",
      "Ignoring wrong pointing object 4854 0 (offset 0)\n",
      "Ignoring wrong pointing object 4856 0 (offset 0)\n",
      "Ignoring wrong pointing object 4858 0 (offset 0)\n",
      "Ignoring wrong pointing object 4861 0 (offset 0)\n",
      "Ignoring wrong pointing object 4866 0 (offset 0)\n",
      "Ignoring wrong pointing object 4974 0 (offset 0)\n",
      "Ignoring wrong pointing object 4976 0 (offset 0)\n",
      "Ignoring wrong pointing object 5329 0 (offset 0)\n",
      "Ignoring wrong pointing object 5332 0 (offset 0)\n",
      "Ignoring wrong pointing object 5337 0 (offset 0)\n",
      "Ignoring wrong pointing object 5341 0 (offset 0)\n",
      "Ignoring wrong pointing object 5346 0 (offset 0)\n",
      "Ignoring wrong pointing object 5400 0 (offset 0)\n",
      "Ignoring wrong pointing object 5408 0 (offset 0)\n",
      "Ignoring wrong pointing object 5411 0 (offset 0)\n",
      "Ignoring wrong pointing object 5414 0 (offset 0)\n",
      "Ignoring wrong pointing object 5426 0 (offset 0)\n",
      "Ignoring wrong pointing object 5614 0 (offset 0)\n",
      "Ignoring wrong pointing object 5622 0 (offset 0)\n",
      "Ignoring wrong pointing object 5624 0 (offset 0)\n",
      "Ignoring wrong pointing object 5627 0 (offset 0)\n",
      "Ignoring wrong pointing object 5629 0 (offset 0)\n",
      "Ignoring wrong pointing object 5633 0 (offset 0)\n",
      "Ignoring wrong pointing object 5642 0 (offset 0)\n",
      "Ignoring wrong pointing object 5644 0 (offset 0)\n",
      "Ignoring wrong pointing object 5647 0 (offset 0)\n",
      "Ignoring wrong pointing object 5649 0 (offset 0)\n",
      "Ignoring wrong pointing object 5653 0 (offset 0)\n",
      "Ignoring wrong pointing object 5809 0 (offset 0)\n",
      "Ignoring wrong pointing object 5811 0 (offset 0)\n",
      "Ignoring wrong pointing object 5824 0 (offset 0)\n",
      "Ignoring wrong pointing object 5856 0 (offset 0)\n",
      "Ignoring wrong pointing object 5864 0 (offset 0)\n",
      "Ignoring wrong pointing object 5916 0 (offset 0)\n",
      "Ignoring wrong pointing object 5918 0 (offset 0)\n",
      "Ignoring wrong pointing object 5920 0 (offset 0)\n",
      "Ignoring wrong pointing object 5959 0 (offset 0)\n",
      "Ignoring wrong pointing object 5961 0 (offset 0)\n",
      "Ignoring wrong pointing object 5964 0 (offset 0)\n",
      "Ignoring wrong pointing object 5966 0 (offset 0)\n",
      "Ignoring wrong pointing object 5968 0 (offset 0)\n",
      "Ignoring wrong pointing object 5972 0 (offset 0)\n",
      "Ignoring wrong pointing object 5986 0 (offset 0)\n",
      "Ignoring wrong pointing object 5988 0 (offset 0)\n",
      "Ignoring wrong pointing object 5991 0 (offset 0)\n",
      "Ignoring wrong pointing object 5993 0 (offset 0)\n",
      "Ignoring wrong pointing object 5995 0 (offset 0)\n",
      "Ignoring wrong pointing object 5999 0 (offset 0)\n",
      "Ignoring wrong pointing object 6009 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "pdf_documents = pdf_loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63ab5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_notebooks_from_folder(folder_path, include_outputs=False, max_output_length=10, remove_newline=False, traceback=False):\n",
    "    folder = Path(folder_path)\n",
    "    docs = []\n",
    "    for ipynb_path in folder.glob(\"*.ipynb\"):\n",
    "        loader = NotebookLoader(\n",
    "            str(ipynb_path),\n",
    "            include_outputs=include_outputs,\n",
    "            max_output_length=max_output_length,\n",
    "            remove_newline=remove_newline,\n",
    "            traceback=traceback,\n",
    "        )\n",
    "        docs.extend(loader.load())\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92e6ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_notebooks_from_folder(\n",
    "    \"./PDF_DOC/Notebook/\",\n",
    "    include_outputs=True,\n",
    "    max_output_length=1000,\n",
    "    remove_newline=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebc2582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Ch_3_MDP.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Finite Markov Decision Processes\\\\n\\', \\'\\\\n\\', \\'In this chapter,\\\\n\\', \\'- You will learn about the core components of reinforcement learning.\\\\n\\', \\'- You will learn to represent sequential decision-making problems as reinforcement learning environments using a mathematical framework known as Markov decision processes.\\\\n\\', \\'- You will build from scratch environments that reinforcement learning agents learn to solve in later chapters.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## The Agent–Environment Interface\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:de944f75-b6e9-452a-ab51-5c8f57a981df.png)\\\\n\\', \\'\\\\n\\', \\'To better understand the interactions between the agent and the environment, we can unroll the interaction loop as follows, as shown in the following figure.\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:937ed64a-077d-4921-b6ff-f3cea47d9b5d.png)\\\\n\\', \\'\\\\n\\', \\'- The agent observes the state $S_0$ from the environment.\\\\n\\', \\'- The agent takes action $A_0$ in the environment.\\\\n\\', \\'- The environment transition into a new state $S_1$ and also generate a reward signal $R_0$, where $R_0$ is conditioned on $S_0$ and $A_0$.\\\\n\\', \\'- The agent receives reward $R_0$ along with the successor state $S_1$ from the environment.\\\\n\\', \\'- The agent decides to take action $A_1$.\\\\n\\', \\'- The interaction continues to the next stage until the process reaches the terminal state $S_T$. Once it reaches terminal state, no further action is taken and a new episode can be started from the initial state $S_0$.\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'#### Examples of problems, agents, and environments\\\\n\\', \\'\\\\n\\', \\'**PROBLEM:** A trading agent investing in the stock market. **AGENT:** The executing RL code in memory and in the CPU. **ENVIRONMENT:** Your internet connection, the machine the code is running on, the stock prices, the geopolitical uncertainty, other investors, day traders, and so on. **ACTIONS:** Sell *n* stocks of *y* company. Buy *n* stocks of *y* company. Hold. **OBSERVATIONS:** Market is going up. Market is going down. There are economic tensions between two powerful nations. There’s danger of war in the continent. A global pandemic is wreaking havoc in the entire world.\\\\n\\', \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'#### The Markov property\\\\n\\', \\'\\\\n\\', \\'The probability of the next state, given the current state and action, is independent of the history of interactions. This memoryless property of MDPs is known as the *markov property*: the probability of moving from one state *s* to another state *s* on two separate occasions, given the same action *a*, is the same regardless of all previous states or actions encountered before that point.\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:4e9b9e7b-d590-4b26-9371-20bfa0a3fe0b.png)\\\\n\\', \\'\\\\n\\', \\'-  The state captures all relevant information from the history. i.e. The state is a sufficient statistic of the future.\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'**Que:**  Consider each control problem or decision task in the following list and see if it has the *Markov property* or not:\\\\n\\', \\'\\\\n\\', \\'1. Driving a car\\\\n\\', \\'2. Deciding whether to invest in a stock or not\\\\n\\', \\'3. Choosing a medical treatment for a patient\\\\n\\', \\'4. Diagnosing a patient’s illness\\\\n\\', \\'5. Predicting which team will win in a football game\\\\n\\', \\'6. Choosing the shortest route (by distance) to some destination\\\\n\\', \\'7. Aiming a gun to shoot a distant target\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'1. Driving a car can generally be considered to have the Markov property because you don’t need to know what happened 10 minutes ago to be able to optimally drive your car. You just need to know where everything is right now and where you want to go.\\\\n\\', \\'2. Deciding whether to invest in a stock or not does not meet the criteria of the Markov property since you would want to know the past performance of the stock in order to make a decision.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'3. Choosing a medical treatment seems to have the Markov property because you don’t need to know the biography of a person to choose a good treatment for what ails them right now.\\\\n\\', \\'4. In contrast, diagnosing (rather than treating) would definitely require knowledge of past states. It is often very important to know the historical course of a patient’s symptoms in order to make a diagnosis.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'5. Predicting which football team will win does not have the Markov property, since, like the stock example, you need to know the past performance of the football teams to make a good prediction.\\\\n\\', \\'6. Choosing the shortest route to a destination has the Markov property because you just need to know the distance to the destination for various routes, which doesn’t depend on what happened yesterday.\\\\n\\', \\'7. Aiming a gun to shoot a distant target also has the Markov property since all you need to know is where the target is, and perhaps current conditions like wind velocity and the particulars of your gun. You don’t need to know the wind velocity of yesterday.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Markov Process or Markov Chain\\\\n\\', \\'\\\\n\\', \"A Markov chain can be defined as a tuple of $(\\\\\\\\mathcal S, \\\\\\\\mathcal P)$ , where $\\\\\\\\mathcal S$ is a finite set of states called the state space, and $\\\\\\\\mathcal P$ is the dynamics function (or transition model) of the environment, which specifies the probability of transitioning from a current state $s$ to a successor state ${s}\\'$. Since there are no actions in a Markov chain, we omit actions in the dynamics function $\\\\\\\\mathcal P$. The probability of transitioning from state $s$ to state ${s}\\'$ is denoted by $P\\\\\\\\left( s\\\\\\\\left| {{s}\\'} \\\\\\\\right. \\\\\\\\right)$.\\\\n\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Service Dog Example\\\\n\\', \\'\\\\n\\', \\'In this example, we imagine training a service dog to retrieve an object for its owner. The training is conducted in a house with three rooms, one of which contains a personal object that the dog must retrieve and bring to its owner or trainer. The task falls into the **episodic reinforcement learning problem** category, as the task is considered finished once the dog retrieves the object. To simplify the task, we keep placing the object in the same (or almost the same) location and initialize the starting state randomly. Additionally, one of the rooms leads to the front yard, where the dog can play freely. This scenario is illustrated in the following figure.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:2dddbde0-e1a9-48b1-9dff-0748bd3bc2ec.png)\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Transition Matrix for Markov Chain\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:6a2317a1-83b9-4aac-af18-4ae321539c0c.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:426ba86d-a26c-4ddb-9d71-5fe5d6e9ec54.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'With access to the dynamics function of the environment, we can sample some state transition sequences $S_0, S_1, S_2, \\\\\\\\ldots$ from the environment. For example:\\\\n\\', \\'\\\\n\\', \\'- Episode 1: (Room 1, Room 2, Room 3, Found item, End)\\\\n\\', \\'- Episode 2: (Room 3, Found item, End)\\\\n\\', \\'- Episode 3: (Room 2, Outside, Room 2, Room 3, Found item, End)\\\\n\\', \\'- Episode 4: (Outside, Outside, Outside, …)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:bbc92461-3df8-4e30-ac11-f25a8fac0a4c.png)\\\\n\\', \\'\\\\n\\', \\'The function *p* defines the *dynamics* of the MDP.\\\\n\\', \\'\\\\n\\', \\'The Markov decision process is a decision-making process by which it is possible to make the best decisions without reference to a history of prior states.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Markov Reward Process\\\\n\\', \\'\\\\n\\', \\'The Markov reward process (MRP) is an extension of the Markov chain, where rewards are added to the process. In a Markov reward process, the agent not only observes state transitions but also receives a reward signal along the way. **Note**, there are still no actions involved in the MRPs. We can define the Markov reward process as a tuple $(\\\\\\\\mathcal S, \\\\\\\\mathcal P, \\\\\\\\mathcal R)$ , where\\\\n\\', \\'\\\\n\\', \\'- $\\\\\\\\mathcal S$ is a finite set of states called the state space.\\\\n\\', \"- $\\\\\\\\mathcal P$ is the dynamics function (or transition model) of the environment, where $P\\\\\\\\left( {s}\\'\\\\\\\\left| s \\\\\\\\right. \\\\\\\\right)=P\\\\\\\\left[ {{S}_{t}}={s}\\'\\\\\\\\left| {{S}_{t-1}}=s \\\\\\\\right. \\\\\\\\right]$  specify the probability of environment transition into successor state ${s}\\'$ when in current state $s$.\\\\n\", \\'- $\\\\\\\\mathcal R$ is a reward function of the environment. $R\\\\\\\\left( s \\\\\\\\right)=\\\\\\\\mathbb{E}\\\\\\\\left[ {{R}_{t}}\\\\\\\\left| {{S}_{t-1}}=s \\\\\\\\right. \\\\\\\\right]$ is the reward signal provided by the environment when the agent is in state $s$.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'As shown in following figure, we added a reward signal to each state in our service dog example. We want the reward signals to align with our desired goal, which is to find the object, so we decided to use the highest reward signal for the state *Found item*. For the state *Outside*, the reward signal is +1, because being outside playing might be more enjoyable for the agent compared to wandering between different rooms.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:6babcc6a-7a0c-4205-8df9-7b1accbafb1b.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'With reward signals, we can compute the total rewards the agent could get for these different sample sequences, where the total rewards are calculated for the entire sequence:\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'| Episode                                            | Total Reward                |\\\\n\\', \\'|----------------------------------------------------|-----------------------------|\\\\n\\', \\'| (Room 1, Room 2, Room 3, Found item, End)          | $-1 -1 -1 + 10 + 0 = 7$     |\\\\n\\', \\'| (Room 3, Found item, End)                          | $-1 + 10 + 0 = 9$           |\\\\n\\', \\'| (Room 2, Outside, Room 2, Room 3, Found item, End) | $-1 + 1 -1 -1 + 10 + 0 = 8$ |\\\\n\\', \\'| (Outside, Outside, Outside …)                      | $\\\\\\\\quad 1 + 1 + \\\\\\\\cdots = \\\\\\\\infty$     |\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"At each time step, the reward is a simple number, $R_t \\\\\\\\in \\\\\\\\mathbb R$. Informally, the agent\\'s goal is to maximize\\\\n\", \\'the total amount of reward it receives. **This means maximizing not immediate reward, but cumulative reward in the long run.**\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:104be52b-96bc-4006-a04e-21925cb9a374.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Return\\\\n\\', \\'\\\\n\\', \\'To quantify the total rewards the agent can get in a sequence of states, we use a different term called **return**. The return  is simply the sum of rewards from time step $t$ to the end of the sequence and is defined by the mathematical equation given below, where $T$ is the terminal time step for episodic reinforcement learning problems\\\\n\\', \\'\\\\n\\', \\'$$G_t = R_{t+1} + R_{t+2} + \\\\\\\\cdots + R_{T}$$\\\\n\\', \\'\\\\n\\', \\'The additional concept that we need is that of **discounting**.\\\\n\\', \\'\\\\n\\', \\'The return $G_t$ is the total discounted reward from time step $t$,\\\\n\\', \\'$$G_t = R_{t+1} + \\\\\\\\gamma R_{t+2} + \\\\\\\\gamma^2 R_{t+3} + \\\\\\\\cdots = \\\\\\\\sum\\\\\\\\limits_{k=0}^{\\\\\\\\infty }{{{\\\\\\\\gamma }^{k}}{{R}_{t+k+1}}}$$\\\\n\\', \\'\\\\n\\', \\'- The discount $\\\\\\\\gamma \\\\\\\\in [0, 1]$ is the present value of future rewards.\\\\n\\', \\'- The value of receiving reward $R$ after $k + 1$ time-steps is $\\\\\\\\gamma^k R$.\\\\n\\', \\'-  $\\\\\\\\gamma = 0$, the agent only cares about the immediate reward, i.e. leads to \"myopic\" evaluation.\\\\n\\', \\'-  $\\\\\\\\gamma$ gets closer to 1, future rewards become as important as immediate rewards, i.e. leads to \"far-sighted\" evaluation.\\\\n\\', \\'-  Notice that we omit the discount $\\\\\\\\gamma$ for the immediate reward $R_t$, since $\\\\\\\\gamma^0 = 1$.\\\\n\\', \\'\\\\n\\', \\'#### Why discount?\\\\n\\', \\'\\\\n\\', \\'- Mathematically convenient to discount rewards\\\\n\\', \\'- Avoids infinite returns in cyclic Markov processes\\\\n\\', \\'- Uncertainty about the future may not be fully represented\\\\n\\', \\'- If the reward is financial, immediate rewards may earn more interest than delayed rewards\\\\n\\', \\'- It is sometimes possible to use undiscounted Markov reward processes (i.e. $\\\\\\\\gamma = 1$), e.g. if all sequences terminate.\\\\n\\', \\'\\\\n\\', \\'### Useful property of $G_t$\\\\n\\', \\'\\\\n\\', \\'$$\\\\\\\\begin{align}\\\\n\\', \\'  & {{G}_{t}}={{R}_{t+1}}+\\\\\\\\gamma {{R}_{t+2}}+{{\\\\\\\\gamma }^{2}}{{R}_{t+3}}+ \\\\\\\\cdots \\\\\\\\\\\\\\\\ \\\\n\\', \\' & \\\\\\\\quad \\\\\\\\, = {{R}_{t+1}}+\\\\\\\\gamma \\\\\\\\left( {{R}_{t+2}}+\\\\\\\\gamma {{R}_{t+3}}+ \\\\\\\\cdots \\\\\\\\right) \\\\\\\\\\\\\\\\ \\\\n\\', \\' & \\\\\\\\quad \\\\\\\\,={{R}_{t+1}}+\\\\\\\\gamma {{G}_{t+1}} \\\\\\\\\\\\\\\\ \\\\n\\', \\'\\\\\\\\end{align}$$\\\\n\\', \\'\\\\n\\', \\'Note that this works for all time steps $t < T$, even if termination occurs at $t+1$, provided we define $G_T=0$.\\\\n\\', \\'\\\\n\\', \\'The following shows the returns for some sample episodes in our service dog example, where we use discount factor $\\\\\\\\gamma = 0.9$:\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'| Episode                                            | Total Reward                |\\\\n\\', \\'|----------------------------------------------------|-----------------------------|\\\\n\\', \\'| 1. (Room 1, Room 2, Room 3, Found item, End)          | $G_0 = -1 -1 \\\\\\\\times 0.9 -1 \\\\\\\\times 0.9^2 + 10 \\\\\\\\times 0.9^3 + 0 = 4.6$     |\\\\n\\', \\'| 2. (Room 3, Found item, End)                          | $G_0 = -1 + 10 \\\\\\\\times 0.9 + 0 = 8.0$           |\\\\n\\', \\'| 3. (Room 2, Outside, Room 2, Room 3, Found item, End) | $G_0 = -1 + 1 \\\\\\\\times 0.9 -1 \\\\\\\\times 0.9^2 -1 \\\\\\\\times 0.9^3 + 10 \\\\\\\\times 0.9^4 + 0 = 4.9$ |\\\\n\\', \\'| 4. (Outside, Outside, Outside …)                      | $G_0 = 1 + 1 \\\\\\\\times 0.9 + 1 \\\\\\\\times 0.9^2 \\\\\\\\cdots = 10$     |\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'Comparing the return for these different sample sequences, we can see that episode 4 has the highest return value. However, if the agent gets stuck in a loop staying in the same state Outside, it has no way to achieve the goal, which is to find the object in Room 3. This does not necessarily mean that the reward function is flawed. To prove this, we need to use the value function, which we will explain in detail in the upcoming sections. \\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Value Function for MRPs\\\\n\\', \\'\\\\n\\', \\'The value function $\\\\\\\\mathcal v(s)$ gives the long-term value of state $s$.\\\\n\\', \\'\\\\n\\', \\'The **state value function** $\\\\\\\\mathcal v(s)$ of an MRP is the expected return starting from state $s$. \\\\n\\', \\'$$v\\\\\\\\left( s \\\\\\\\right)=\\\\\\\\mathbb{E}\\\\\\\\left[ {{G}_{t}}\\\\\\\\left| {{S}_{t}}=s \\\\\\\\right. \\\\\\\\right]$$\\\\n\\', \\'\\\\n\\', \\'The state value function $v(s)$ for MRPs also shares the recursive property as shown below.\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:d3b011f8-7d73-470d-9145-f1938dea141a.png)\\\\n\\', \\'\\\\n\\', \\'The following called **Bellman expectation equation** or **Bellman Equation**:\\\\n\\', \\'$$v\\\\\\\\left( s \\\\\\\\right)=\\\\\\\\mathbb{E}\\\\\\\\left[ {{R}_{t+1}}+\\\\\\\\gamma v\\\\\\\\left( {{S}_{t+1}} \\\\\\\\right)\\\\\\\\left| {{S}_{t}}=s \\\\\\\\right. \\\\\\\\right]$$\\\\n\\', \\'\\\\n\\', \\'$$ OR $$\\\\n\\', \\'\\\\n\\', \"$$v\\\\\\\\left( s \\\\\\\\right)=R\\\\\\\\left( s \\\\\\\\right)+\\\\\\\\gamma \\\\\\\\sum\\\\\\\\limits_{{s}\\'\\\\\\\\in \\\\\\\\mathcal S}{P\\\\\\\\left( {s}\\'\\\\\\\\left| s \\\\\\\\right. \\\\\\\\right)v\\\\\\\\left( {{s}\\'} \\\\\\\\right)}$$\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'**Worked Example**\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:41bc8429-1d59-432a-97e1-ba3e48600f2c.png)\\\\n\\', \\'\\\\n\\', \\'$$v(Room3) = -1 + 0.8 \\\\\\\\times 10 + 0.2 \\\\\\\\times 5 = 8$$\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:a8bb1667-531e-4d87-9db0-ef2f7f6047db.png)\\\\n\\', \\'\\\\n\\', \\'Why do we want to estimate the state values? Because it can help the agent make better decisions. If the agent knows which state is better (in terms of expected returns), it can choose actions that may lead it to those better states. For example, in above figure, if the current state is Room 2, then the best successor state is Room 3 since it has the highest state value of 7.0 among all the possible successor states for Room 2. By selecting actions that lead to high-value states, the agent can maximize its long-term expected return.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Markov Decision Process\\\\n\\', \\'\\\\n\\', \\'Now we’re ready to discuss the details of the MDP. Similar to how the MRP extends the Markov chain, the Markov decision process (MDP) extends the MRP by including actions and policy into the process. The MDP contains all the necessary components, including states, rewards, actions, and policy. We can define the MDP as a tuple  $(\\\\\\\\mathcal S, \\\\\\\\mathcal A, \\\\\\\\mathcal P, \\\\\\\\mathcal R)$ , where\\\\n\\', \\'\\\\n\\', \\'- $\\\\\\\\mathcal S$ is a finite set of states called the state space.\\\\n\\', \\'- $\\\\\\\\mathcal A$ is a finite set of actions called the action space.\\\\n\\', \"- $\\\\\\\\mathcal P$ is the dynamics function (or transition model) of the environment, where $P\\\\\\\\left( {s}\\'\\\\\\\\left| s, a \\\\\\\\right. \\\\\\\\right)=P\\\\\\\\left[ {{S}_{t}}={s}\\'\\\\\\\\left| {{S}_{t-1}}=s, {{A}_{t-1}}=a \\\\\\\\right. \\\\\\\\right]$  specify the probability of environment transition into successor state ${s}\\'$ when in current state $s$ and take action $a$.\\\\n\", \\'- $\\\\\\\\mathcal R$ is a reward function of the environment. $R\\\\\\\\left( s, a \\\\\\\\right)=\\\\\\\\mathbb{E}\\\\\\\\left[ {{R}_{t}}\\\\\\\\left| {{S}_{t-1}}=s, {{A}_{t-1}}=a \\\\\\\\right. \\\\\\\\right]$ is the reward signal provided by the environment when the agent is in state $s$ and taking action $a$.\\\\n\\', \\'\\\\n\\', \\'We can list all the elements in the set of $\\\\\\\\mathcal S$, $\\\\\\\\mathcal A$, $\\\\\\\\mathcal R$  for our service dog MDP; notice that not all actions are available (or legal) in each state:\\\\n\\', \\'\\\\n\\', \\'- $\\\\\\\\mathcal S$ = \\\\\\\\{Room 1, Room 2, Room 3, Outside, Found item \\\\\\\\}\\\\n\\', \\'- $\\\\\\\\mathcal A$ = \\\\\\\\{Go to room1, Go to room2, Go to room3, Go outside, Go inside, Search\\\\\\\\}\\\\n\\', \\'- $\\\\\\\\mathcal R$ =  \\\\\\\\{ -1 , -2 , +1, 0, +10\\\\\\\\}\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:3cfb7c50-4c22-4833-ae10-0f98e2f044ef.png)\\\\n\\', \\'\\\\n\\', \\'In above Figure, we have modeled our service dog example using an MDP. Before we move on, we want to explain the small changes we’ve made to Figure. First, we have merged states *Found item* and *End* into a single terminal state Found item. This makes sense because the reward is now conditioned on $(s, a)$ , and there are no additional meaningful states after the agent has reached the state Found item. Second, the straight and curly lines in Figure represent valid actions that the agent can choose in a state, rather than state transition probabilities. Finally, the reward now depends on the action chosen by the agent, and the reward values are slightly different.\\\\n\\', \\'\\\\n\\', \"As we have explained before, our service dog MDP is a deterministic (stationary) reinforcement learning environment. The dynamics function is slightly different compared to our previous example. For MDPs, the transition from the current state $s$ to its successor state $s\\'$ depends on the current state $s$ and the action $a$ chosen by the agent. For a deterministic environment, the transition probability is always 1.0 for legal actions $a \\\\\\\\in \\\\\\\\mathcal A (s)$ , and 0 for illegal actions $a \\\\\\\\notin \\\\\\\\mathcal A (s)$ , which are actions not allowed in the environment. Illegal actions should never be chosen by the agent, since most environments have enforced checks at some level. \\\\n\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Refer Example 3.3 Recycling Robot (Text Book, Pg No. 52)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:731729d7-753c-4cbc-9b42-179f7e9aaede.png)\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:724764b4-49bf-4347-9434-277ae4785e21.png)\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:aeed33af-b699-4e5d-9e4b-83bc06de9928.png)\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:61e4b8cf-27e4-4f21-942a-d2b2ae0b49f0.png)\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:bb6d9b7a-0ed0-4830-8d76-2c80c8c8bbe2.png)\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:d61f836a-7b92-4420-9bd1-8eee7999ccb1.png)\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:d81a30a5-4127-421e-b48b-feb0a4f765cc.png)\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:4a85296a-ac15-4829-bdea-1675ea6b30a8.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Policies and Value Functions\\\\n\\', \\'\\\\n\\', \\'In reinforcement learning and MDPs, the agent has the ability to decide how to act in the environment, but it is not always clear how to choose an action in a given state or what rules to use for making decisions. To address this problem, we introduce the concept of a **policy**.\\\\n\\', \\'\\\\n\\', \\'Formally, a **policy** is a mapping from states to probabilities of selecting each possible action.\\\\n\\', \\'$$\\\\\\\\pi \\\\\\\\left( a\\\\\\\\left| s \\\\\\\\right. \\\\\\\\right)=P\\\\\\\\left[ {{A}_{t}}=a\\\\\\\\left| {{S}_{t}}=s \\\\\\\\right. \\\\\\\\right]$$\\\\n\\', \\'\\\\n\\', \\'The number of possible deterministic policies for a finite MDP is $N(\\\\\\\\mathcal A) ^ {N(\\\\\\\\mathcal S)}$\\\\n\\', \\'\\\\n\\', \\'The **state-value function** $\\\\\\\\mathcal v_\\\\\\\\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\\\\\\\pi$\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:077f3ddd-78c4-4160-9b7a-2ef5adc318f2.png\"  width=\"80%\" height=\"30%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:53d6c831-34bc-461d-bc19-03aeeda4a975.png\"  width=\"80%\" height=\"30%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'The **action-value function** ${{q}_{\\\\\\\\pi }}\\\\\\\\left( s,a \\\\\\\\right)$ is the expected return starting from state $s$, taking action $a$, and then following the policy $\\\\\\\\pi$\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:32bf8408-b533-40ed-ad1e-220ec3078566.png\"  width=\"80%\" height=\"30%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'There is a nice relationship between the $\\\\\\\\mathcal v_\\\\\\\\pi(s)$ and ${{q}_{\\\\\\\\pi }}\\\\\\\\left( s,a \\\\\\\\right)$. The value of a state $s$ under a policy $\\\\\\\\pi$  is just the weighted sum over all state-action values ${{q}_{\\\\\\\\pi }}\\\\\\\\left( s,a \\\\\\\\right)$ for all possible actions $a \\\\\\\\in \\\\\\\\mathcal A$ , as shown in the following equation:\\\\n\\', \\'$${{v}_{\\\\\\\\pi }}\\\\\\\\left( s \\\\\\\\right)=\\\\\\\\sum\\\\\\\\limits_{a\\\\\\\\in \\\\\\\\mathcal A}{\\\\\\\\pi \\\\\\\\left( a\\\\\\\\left| s \\\\\\\\right. \\\\\\\\right)}\\\\\\\\,{{q}_{\\\\\\\\pi }}\\\\\\\\left( s,a \\\\\\\\right)$$\\\\n\\', \\'\\\\n\\', \\'Here, ${\\\\\\\\pi \\\\\\\\left( a\\\\\\\\left| s \\\\\\\\right. \\\\\\\\right)}$ is the probability of taking action $a$ in state $s$ when following policy $\\\\\\\\pi$\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Optimal Policies and Optimal Value Functions\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:ff4e581b-4a69-4d80-967d-727d940c3772.png\"  width=\"80%\" height=\"40%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"Backup diagrams are visual tools used in reinforcement learning to depict the process of updating value functions, specifically the state value function ($v$) and the action-value function ($q$). These diagrams illustrate how an agent evaluates and updates its estimates based on the rewards and subsequent states or actions it encounters. Let\\'s delve into the details of these diagrams, referencing the provided image.\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:036051ad-444e-4a44-b980-f9e2824ddfe6.png)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'### Value Function Backup Diagram ($v_*$)\\\\n\\', \\'\\\\n\\', \\'1. **Structure**:\\\\n\\', \\'    - The left part of the image shows the backup diagram for the state value function $v_*$.\\\\n\\', \"    - At the top of the diagram, there\\'s a state $s$.\\\\n\", \\'    - Arrows emanate from $s$ to several possible actions $a$.\\\\n\\', \\'\\\\n\\', \\'2. **Maximization**:\\\\n\\', \"    - Each action $a$ leads to a subsequent reward $r$ and a transition to a new state $s\\'$.\\\\n\", \"    - The diagram shows that from each action $a$, there\\'s a path to multiple future states $s\\'$, reflecting the stochastic nature of the environment.\\\\n\", \"    - The value function $v_*(s)$ is updated by taking the maximum expected value of these future states\\' values, considering all possible actions.\\\\n\", \\'\\\\n\\', \\'3. **Update Rule**:\\\\n\\', \\'    - The Bellman optimality equation for the state value function is:\\\\n\\', \\'      $$\\\\n\\', \"      v_*(s) = \\\\\\\\max_a \\\\\\\\mathbb{E} \\\\\\\\left[ r(s, a, s\\') + \\\\\\\\gamma v_*(s\\') \\\\\\\\right]\\\\n\", \\'      $$\\\\n\\', \"    - Here, $\\\\\\\\mathbb{E}$ denotes the expectation over the possible next states $s\\'$, $r(s, a, s\\')$ is the immediate reward, and $\\\\\\\\gamma$ is the discount factor.\\\\n\", \\'\\\\n\\', \\'### Action-Value Function Backup Diagram ($q_*$)\\\\n\\', \\'\\\\n\\', \\'1. **Structure**:\\\\n\\', \\'    - The right part of the image shows the backup diagram for the action-value function $q_*$.\\\\n\\', \"    - At the top, there\\'s a state-action pair $(s, a)$.\\\\n\", \"    - Arrows from $(s, a)$ lead to subsequent rewards $r$ and transitions to new states $s\\'$.\\\\n\", \\'\\\\n\\', \\'2. **Maximization**:\\\\n\\', \"    - Each new state $s\\'$ has its own possible actions $a\\'$, which determine the future action-values $q_*(s\\', a\\')$.\\\\n\", \"    - The action-value function $q_*(s, a)$ is updated based on the immediate reward and the maximum expected value of the action-values in the next state $s\\'$.\\\\n\", \\'\\\\n\\', \\'3. **Update Rule**:\\\\n\\', \\'    - The Bellman optimality equation for the action-value function is:\\\\n\\', \\'      $$\\\\n\\', \"      q_*(s, a) = \\\\\\\\mathbb{E} \\\\\\\\left[ r(s, a, s\\') + \\\\\\\\gamma \\\\\\\\max_{a\\'} q_*(s\\', a\\') \\\\\\\\right]\\\\n\", \\'      $$\\\\n\\', \"    - Here, $\\\\\\\\mathbb{E}$ denotes the expectation over the possible next states $s\\'$, and $\\\\\\\\max_{a\\'} q_*(s\\', a\\')$ is the maximum value over all possible actions $a\\'$ in the next state $s\\'$.\\\\n\", \\'\\\\n\\', \\'### Detailed Walkthrough of the Diagrams:\\\\n\\', \\'\\\\n\\', \\'#### Value Function Backup ($v_*$):\\\\n\\', \\'\\\\n\\', \\'1. **State $s$**: Start at the current state $s$.\\\\n\\', \\'2. **Actions $a$**: Consider all possible actions $a$ that can be taken from state $s$.\\\\n\\', \"3. **Transition and Reward**: Each action leads to a transition to a new state $s\\'$ and yields an immediate reward $r$.\\\\n\", \"4. **Future Values**: From each new state $s\\'$, evaluate the expected future value $v_*(s\\')$.\\\\n\", \\'5. **Maximize**: Update $v_*(s)$ by taking the maximum over the expected future values from all possible actions.\\\\n\\', \\'\\\\n\\', \\'#### Action-Value Function Backup ($q_*$):\\\\n\\', \\'\\\\n\\', \\'1. **State-Action Pair $(s, a)$**: Start at the current state-action pair $(s, a)$.\\\\n\\', \"2. **Transition and Reward**: The action $a$ leads to a transition to a new state $s\\'$ and yields an immediate reward $r$.\\\\n\", \"3. **Future Actions $a\\'$**: From the new state $s\\'$, consider all possible actions $a\\'$.\\\\n\", \"4. **Future Action-Values**: Evaluate the expected future action-values $q_*(s\\', a\\')$.\\\\n\", \\'5. **Maximize**: Update $q_*(s, a)$ by taking the maximum over the expected future action-values from all possible actions in the next state.\\\\n\\', \\'\\\\n\\', \\'### Intuition Behind Backup Diagrams:\\\\n\\', \\'\\\\n\\', \\'- **Value Function**: The agent updates the value of the current state by considering the best possible action it can take, which leads to the highest expected future rewards. This process iterates over all states until the values converge to the optimal values.\\\\n\\', \\'- **Action-Value Function**: The agent updates the value of taking a particular action in the current state by considering the immediate reward and the best possible future actions it can take from the subsequent states. This process iterates over all state-action pairs until the values converge to the optimal action-values.\\\\n\\', \\'\\\\n\\', \\'Backup diagrams help visualize and understand the recursive nature of the Bellman equations, providing insight into how value functions are iteratively improved in reinforcement learning algorithms.\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'# Solution for Some Exercises:\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:0fef6d47-e9e5-48af-aa19-070c9a076b9e.png)\\\\n\\', \\'![image.png](attachment:65df5547-69e0-41c4-8a46-77f7a5820c1e.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"To create the table for **Exercise 3.4**, we\\'ll break down the information given in the table from Example 3.3 and extend it to include the reward $ r $ as part of the tuples $(s\\', r \\\\\\\\mid s, a)$. We need to list all possible combinations where the probability $ p(s\\', r \\\\\\\\mid s, a) > 0 $.\\\\n\", \\'\\\\n\\', \\'The given table in Example 3.3 shows:\\\\n\\', \\'- States ($s$): high, low\\\\n\\', \\'- Actions ($a$): search, wait, recharge\\\\n\\', \"- Next states ($s\\'$): high, low\\\\n\", \"- Transition probabilities $ p(s\\' \\\\\\\\mid s, a) $\\\\n\", \"- Rewards $ r(s, a, s\\') $\\\\n\", \\'\\\\n\\', \"Here is the analogous table for $ p(s\\', r \\\\\\\\mid s, a) $:\\\\n\", \\'\\\\n\\', \"| $s$  | $a$     | $s\\'$  | $r$     | $p(s\\', r \\\\\\\\mid s, a)$ |\\\\n\", \\'|-------|-----------|--------|----------|-----------------------|\\\\n\\', \\'| high  | search    | high   | $ r_{\\\\\\\\text{search}} $ | $\\\\\\\\alpha$               |\\\\n\\', \\'| high  | search    | high   | $-3$    | $1 - \\\\\\\\alpha$         |\\\\n\\', \\'| high  | search    | low    | $ r_{\\\\\\\\text{search}} $ | $1 - \\\\\\\\beta$            |\\\\n\\', \\'| low   | search    | low    | $ \\\\\\\\beta$             | $\\\\\\\\beta$               |\\\\n\\', \\'| high  | wait      | high   | $ r_{\\\\\\\\text{wait}} $  | 1                       |\\\\n\\', \\'| low   | wait      | low    | 0         | 1                       |\\\\n\\', \\'| low   | wait      | low    | $- $   | 1                       |\\\\n\\', \\'| low   | recharge  | high   | 1         | 1                       |\\\\n\\', \\'| low   | recharge  | low    | 0         | 1                       |\\\\n\\', \\'\\\\n\\', \"To summarize, the table includes the states ($s$), actions ($a$), next states ($s\\'$), rewards ($r$), and the joint probabilities $p(s\\', r \\\\\\\\mid s, a)$ for every combination where the probability is greater than 0.\\\\n\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'**Exercise 3.8**   Suppose  $\\\\\\\\gamma= 0.5$ and the following sequence of rewards is received $R_1$ = 1, $R_2$ = 2, $R_3$ = 6, $R_4$ = 3, and $R_5$ = 2, with $T$ = 5. What are $G_0$, $G_1$, ..., $G_5$? Hint: Work backwards.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Given the discount factor $\\\\\\\\gamma = 0.5$ and the sequence of rewards $R_1 = 1$, $R_2 = 2$, $R_3 = 6$, $R_4 = 3$, and $R_5 = 2$, we need to calculate the return $G_t$ for each time step $t = 0, 1, \\\\\\\\ldots, 5$. The return $G_t$ is the discounted sum of future rewards:\\\\n\\', \\'\\\\n\\', \\'$$ G_t = R_{t+1} + \\\\\\\\gamma R_{t+2} + \\\\\\\\gamma^2 R_{t+3} + \\\\\\\\gamma^3 R_{t+4} + \\\\\\\\gamma^4 R_{t+5} + \\\\\\\\ldots $$\\\\n\\', \\'\\\\n\\', \\'We will work backwards to calculate these returns.\\\\n\\', \\'\\\\n\\', \\'1. **$G_5$**: Since there are no rewards after $R_5$,\\\\n\\', \\'$$ G_5 = 0 $$\\\\n\\', \\'\\\\n\\', \\'2. **$G_4$**: The reward at time step 5 is $R_5 = 2$,\\\\n\\', \\'$$ G_4 = R_5 + \\\\\\\\gamma G_5 = 2 + 0.5 \\\\\\\\times 0 = 2 $$\\\\n\\', \\'\\\\n\\', \\'3. **$G_3$**: The reward at time step 4 is $R_4 = 3$,\\\\n\\', \\'$$ G_3 = R_4 + \\\\\\\\gamma G_4 = 3 + 0.5 \\\\\\\\times 2 = 3 + 1 = 4 $$\\\\n\\', \\'\\\\n\\', \\'4. **$G_2$**: The reward at time step 3 is $R_3 = 6$,\\\\n\\', \\'$$ G_2 = R_3 + \\\\\\\\gamma G_3 = 6 + 0.5 \\\\\\\\times 4 = 6 + 2 = 8 $$\\\\n\\', \\'\\\\n\\', \\'5. **$G_1$**: The reward at time step 2 is $R_2 = 2$,\\\\n\\', \\'$$ G_1 = R_2 + \\\\\\\\gamma G_2 = 2 + 0.5 \\\\\\\\times 8 = 2 + 4 = 6 $$\\\\n\\', \\'\\\\n\\', \\'6. **$G_0$**: The reward at time step 1 is $R_1 = 1$,\\\\n\\', \\'$$ G_0 = R_1 + \\\\\\\\gamma G_1 = 1 + 0.5 \\\\\\\\times 6 = 1 + 3 = 4 $$\\\\n\\', \\'\\\\n\\', \\'So the returns are:\\\\n\\', \\'- $G_0 = 4$\\\\n\\', \\'- $G_1 = 6$\\\\n\\', \\'- $G_2 = 8$\\\\n\\', \\'- $G_3 = 4$\\\\n\\', \\'- $G_4 = 2$\\\\n\\', \\'- $G_5 = 0$\\\\n\\', \\'\\\\n\\', \\'The final result is:\\\\n\\', \\'$$ G_0 = 4, \\\\\\\\, G_1 = 6, \\\\\\\\, G_2 = 8, \\\\\\\\, G_3 = 4, \\\\\\\\, G_4 = 2, \\\\\\\\, G_5 = 0 $$\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:202dd06d-e61a-4b18-a7b1-a6fbb97dc05d.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'To answer **Exercise 3.18**, we need to derive the value of a state under a given policy $\\\\\\\\pi$.\\\\n\\', \\'\\\\n\\', \\'### Value of a State under Policy $\\\\\\\\pi$\\\\n\\', \\'\\\\n\\', \\'The value of a state $ s $ under policy $\\\\\\\\pi$, denoted as $ v_{\\\\\\\\pi}(s) $, is the expected return when starting from state $ s $ and following policy $\\\\\\\\pi$. This can be expressed as:\\\\n\\', \\'\\\\n\\', \\'$$ v_{\\\\\\\\pi}(s) = \\\\\\\\mathbb{E}_{\\\\\\\\pi} [G_t \\\\\\\\mid S_t = s] $$\\\\n\\', \\'\\\\n\\', \\'Where $ G_t $ is the return starting from time $ t $.\\\\n\\', \\'\\\\n\\', \\'### Using the Backup Diagram\\\\n\\', \\'\\\\n\\', \\'From the backup diagram in the exercise, we see that the value of a state $ s $ is calculated by considering all possible actions that can be taken from $ s $, and then weighing the value of each action by the probability of taking that action under policy $\\\\\\\\pi$. This leads to:\\\\n\\', \\'\\\\n\\', \\'$$ v_{\\\\\\\\pi}(s) = \\\\\\\\sum_{a} \\\\\\\\pi(a \\\\\\\\mid s) q_{\\\\\\\\pi}(s, a) $$\\\\n\\', \\'\\\\n\\', \\'Where:\\\\n\\', \\'- $ \\\\\\\\pi(a \\\\\\\\mid s) $ is the probability of taking action $ a $ in state $ s $ under policy $\\\\\\\\pi$.\\\\n\\', \\'- $ q_{\\\\\\\\pi}(s, a) $ is the expected return (or value) of taking action $ a $ in state $ s $ and then following policy $\\\\\\\\pi$.\\\\n\\', \\'\\\\n\\', \\'### Expected Value Equation\\\\n\\', \\'\\\\n\\', \\'Given the state $ S_t = s $, the equation for $ v_{\\\\\\\\pi}(s) $ can be written using the expected value:\\\\n\\', \\'\\\\n\\', \\'$$ v_{\\\\\\\\pi}(s) = \\\\\\\\mathbb{E}_{a \\\\\\\\sim \\\\\\\\pi(\\\\\\\\cdot \\\\\\\\mid s)} [q_{\\\\\\\\pi}(s, a)] $$\\\\n\\', \\'\\\\n\\', \\'This equation captures the idea that $ v_{\\\\\\\\pi}(s) $ is the expected value of $ q_{\\\\\\\\pi}(s, a) $ where $ a $ is drawn according to the policy $\\\\\\\\pi$.\\\\n\\', \\'\\\\n\\', \\'### Explicit Equation without Expected Value Notation\\\\n\\', \\'\\\\n\\', \\'To write the equation explicitly in terms of $\\\\\\\\pi(a \\\\\\\\mid s)$, we can expand the expectation:\\\\n\\', \\'\\\\n\\', \\'$$ v_{\\\\\\\\pi}(s) = \\\\\\\\sum_{a} \\\\\\\\pi(a \\\\\\\\mid s) q_{\\\\\\\\pi}(s, a) $$\\\\n\\', \\'\\\\n\\', \\'This equation shows that $ v_{\\\\\\\\pi}(s) $ is the sum over all actions of the probability of taking each action $ a $ in state $ s $, weighted by the value $ q_{\\\\\\\\pi}(s, a) $ of taking that action.\\\\n\\', \\'\\\\n\\', \\'### Summary\\\\n\\', \\'\\\\n\\', \\'- **Value under Policy $\\\\\\\\pi$ with Expected Value Notation:**\\\\n\\', \\'  $$ v_{\\\\\\\\pi}(s) = \\\\\\\\mathbb{E}_{a \\\\\\\\sim \\\\\\\\pi(\\\\\\\\cdot \\\\\\\\mid s)} [q_{\\\\\\\\pi}(s, a)] $$\\\\n\\', \\'\\\\n\\', \\'- **Explicit Value under Policy $\\\\\\\\pi$:**\\\\n\\', \\'  $$ v_{\\\\\\\\pi}(s) = \\\\\\\\sum_{a} \\\\\\\\pi(a \\\\\\\\mid s) q_{\\\\\\\\pi}(s, a) $$\\\\n\\', \\'\\\\n\\', \\'These equations provide the required formulation for the value at the root node $ v_{\\\\\\\\pi}(s) $ in terms of the values at the expected leaf nodes $ q_{\\\\\\\\pi}(s, a) $ under the policy $\\\\\\\\pi$.\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:a1d65776-a36a-409e-9bb0-d5fb7c177248.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'To answer **Exercise 3.19**, we need to derive the value of an action under a given policy $\\\\\\\\pi$.\\\\n\\', \\'\\\\n\\', \\'### Value of an Action under Policy $\\\\\\\\pi$\\\\n\\', \\'\\\\n\\', \\'The value of an action $ a $ in state $ s $ under policy $\\\\\\\\pi$, denoted as $ q_{\\\\\\\\pi}(s, a) $, is the expected return starting from state $ s $, taking action $ a $, and thereafter following policy $\\\\\\\\pi$. This can be expressed as:\\\\n\\', \\'\\\\n\\', \\'$$ q_{\\\\\\\\pi}(s, a) = \\\\\\\\mathbb{E} [R_{t+1} + \\\\\\\\gamma v_{\\\\\\\\pi}(S_{t+1}) \\\\\\\\mid S_t = s, A_t = a] $$\\\\n\\', \\'\\\\n\\', \\'Where:\\\\n\\', \\'- $ R_{t+1} $ is the reward received after taking action $ a $.\\\\n\\', \\'- $ S_{t+1} $ is the next state following $ S_t = s $ and $ A_t = a $.\\\\n\\', \\'- $ v_{\\\\\\\\pi}(S_{t+1}) $ is the value of the next state $ S_{t+1} $ under policy $\\\\\\\\pi$.\\\\n\\', \\'\\\\n\\', \\'### Expected Value Equation\\\\n\\', \\'\\\\n\\', \\'Given the state $ S_t = s $ and action $ A_t = a $, the equation for $ q_{\\\\\\\\pi}(s, a) $ can be written using the expected value:\\\\n\\', \\'\\\\n\\', \\'$$ q_{\\\\\\\\pi}(s, a) = \\\\\\\\mathbb{E} [R_{t+1} + \\\\\\\\gamma v_{\\\\\\\\pi}(S_{t+1}) \\\\\\\\mid S_t = s, A_t = a] $$\\\\n\\', \\'\\\\n\\', \\'This equation captures the idea that $ q_{\\\\\\\\pi}(s, a) $ is the expected value of the reward $ R_{t+1} $ plus the discounted value of the next state $ v_{\\\\\\\\pi}(S_{t+1}) $.\\\\n\\', \\'\\\\n\\', \\'### Explicit Equation without Expected Value Notation\\\\n\\', \\'\\\\n\\', \"To write the equation explicitly in terms of $ p(s\\', r \\\\\\\\mid s, a) $, we use the fact that the expectation can be expanded over the possible next states $ s\\' $ and rewards $ r $:\\\\n\", \\'\\\\n\\', \"$$ q_{\\\\\\\\pi}(s, a) = \\\\\\\\sum_{s\\', r} p(s\\', r \\\\\\\\mid s, a) [r + \\\\\\\\gamma v_{\\\\\\\\pi}(s\\')] $$\\\\n\", \\'\\\\n\\', \"This equation shows that $ q_{\\\\\\\\pi}(s, a) $ is the sum over all possible next states $ s\\' $ and rewards $ r $, weighted by their probability $ p(s\\', r \\\\\\\\mid s, a) $, of the reward $ r $ plus the discounted value of the next state $ v_{\\\\\\\\pi}(s\\') $.\\\\n\", \\'\\\\n\\', \\'### Summary\\\\n\\', \\'\\\\n\\', \\'- **Value of Action under Policy $\\\\\\\\pi$ with Expected Value Notation:**\\\\n\\', \\'  $$ q_{\\\\\\\\pi}(s, a) = \\\\\\\\mathbb{E} [R_{t+1} + \\\\\\\\gamma v_{\\\\\\\\pi}(S_{t+1}) \\\\\\\\mid S_t = s, A_t = a] $$\\\\n\\', \\'\\\\n\\', \\'- **Explicit Value of Action under Policy $\\\\\\\\pi$:**\\\\n\\', \"  $$ q_{\\\\\\\\pi}(s, a) = \\\\\\\\sum_{s\\', r} p(s\\', r \\\\\\\\mid s, a) [r + \\\\\\\\gamma v_{\\\\\\\\pi}(s\\')] $$\\\\n\", \\'\\\\n\\', \\'These equations provide the required formulation for the value of an action $ q_{\\\\\\\\pi}(s, a) $ in terms of the expected next reward $ R_{t+1} $ and the expected next state value $ v_{\\\\\\\\pi}(S_{t+1}) $.\\\\n\\']\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Ch_4_DP_Algos.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Ch:4 Dynamic Programming \\\\n\\', \\'## Algorithms\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'![image.png](attachment:6049b364-3793-4a8e-88f7-7cdbfad187b6.png)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'<div style=\"\\\\n\\', \\'    background-color: rgba(0, 0, 0, 0.05); \\\\n\\', \\'    padding: 15px 25px; \\\\n\\', \"    font-family: \\'Courier New\\', Courier, monospace; \\\\n\", \\'    font-size: 1.2em; \\\\n\\', \\'    border: 1px solid rgba(0, 0, 0, 0.1); \\\\n\\', \\'    border-radius: 8px; \\\\n\\', \\'    box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1); \\\\n\\', \\'    margin: 20px 0;\">\\\\n\\', \\'    <h1><b>Algorithm 1: Policy Iteration Algorithm</h1></b><br>\\\\n\\', \\'    <p><strong>Data:</strong> $\\\\\\\\theta$: a small number, $\\\\\\\\eta$: another small number and $\\\\\\\\eta > \\\\\\\\theta$</p>\\\\n\\', \\'    <p><strong>Result:</strong> $V$: a value function s.t. $V \\\\\\\\approx v_*$, $\\\\\\\\pi$: a deterministic policy s.t. $\\\\\\\\pi \\\\\\\\approx \\\\\\\\pi_*$</p>    \\\\n\\', \\'    <b>Function</b> <font color=\"red\">PolicyIteration</font> <b>is</b><br><br>\\\\n\\', \\'    &nbsp;&nbsp;/* Initialization */<br>\\\\n\\', \\'    &nbsp;&nbsp;Initialize $V(s)$ arbitrarily;<br>\\\\n\\', \\'    &nbsp;&nbsp;Randomly initialize policy $\\\\\\\\pi(s)$;<br>\\\\n\\', \\'    &nbsp;&nbsp;/* Policy Evaluation */<br>\\\\n\\', \\'    &nbsp;&nbsp;$\\\\\\\\Delta \\\\\\\\leftarrow \\\\\\\\eta$;<br>\\\\n\\', \\'    &nbsp;&nbsp;while $\\\\\\\\Delta > \\\\\\\\theta$ do<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for each $s \\\\\\\\in S$ do<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$v \\\\\\\\leftarrow V(s)$;<br>\\\\n\\', \"    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V(s) \\\\\\\\leftarrow \\\\\\\\sum_{s\\',r} p(s\\',r|s,\\\\\\\\pi(s))[r + \\\\\\\\gamma V(s\\')]$;<br>\\\\n\", \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\\\\\\\Delta \\\\\\\\leftarrow \\\\\\\\max(\\\\\\\\Delta, |v - V(s)|)$;<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end<br>\\\\n\\', \\'    &nbsp;&nbsp;end<br>\\\\n\\', \\'    &nbsp;&nbsp;/* Policy Improvement */<br>\\\\n\\', \\'    &nbsp;&nbsp;$policy\\\\\\\\_stable \\\\\\\\leftarrow true$;<br>\\\\n\\', \\'    &nbsp;&nbsp;for each $s \\\\\\\\in S$ do<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$old\\\\\\\\_action \\\\\\\\leftarrow \\\\\\\\pi(s)$;<br>\\\\n\\', \"    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\\\\\\\pi(s) \\\\\\\\leftarrow \\\\\\\\arg\\\\\\\\max_a \\\\\\\\sum_{s\\',r} p(s\\',r|s,a)[r + \\\\\\\\gamma V(s\\')]$;<br>\\\\n\", \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if $old\\\\\\\\_action \\\\\\\\ne \\\\\\\\pi(s)$ then<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$policy\\\\\\\\_stable \\\\\\\\leftarrow false$;<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;end<br>\\\\n\\', \\'    &nbsp;&nbsp;end<br>\\\\n\\', \\'    &nbsp;&nbsp;if $policy\\\\\\\\_stable$ then<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return $V \\\\\\\\approx v_*$ and $\\\\\\\\pi \\\\\\\\approx \\\\\\\\pi_*$;<br>\\\\n\\', \\'    &nbsp;&nbsp;else<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;go to Policy Evaluation;<br>\\\\n\\', \\'    &nbsp;&nbsp;end<br>\\\\n\\', \\'    <b>end</b><br>\\\\n\\', \\'</div>\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import numpy as np\\']\\'\\n\\n\\'code\\' cell: \\'[\\'class GridWorld:\\\\n\\', \\'    def __init__(self, size=4, terminal_states=[15], success_prob=None, holes=None):\\\\n\\', \\'        self.size = size\\\\n\\', \\'        self.num_states = size * size\\\\n\\', \\'        self.terminal_states = terminal_states  # Bottom-right corner\\\\n\\', \\'        self.holes = holes if holes is not None else []  # Default no holes\\\\n\\', \"        self.actions = [\\'up\\', \\'down\\', \\'left\\', \\'right\\']\\\\n\", \\'        self.num_actions = len(self.actions)\\\\n\\', \\'        \\\\n\\', \\'        # Handle deterministic vs stochastic transitions\\\\n\\', \\'        self.success_prob = 1.0 if success_prob is None else success_prob\\\\n\\', \\'        \\\\n\\', \\'        # Initialize transition probabilities\\\\n\\', \\'        self.P = np.zeros((self.num_states, self.num_actions, self.num_states))\\\\n\\', \\'        self._build_transition_matrix()\\\\n\\', \\'        \\\\n\\', \\'        # Initialize rewards\\\\n\\', \\'        self.R = np.full((self.num_states, self.num_actions, self.num_states), -1)\\\\n\\', \\'        for term_state in self.terminal_states:\\\\n\\', \\'            self.R[:, :, term_state] = 10  # Reward for any terminal state\\\\n\\', \\'        for hole in self.holes:\\\\n\\', \\'            self.R[:, :, hole] = -10  # Large penalty for falling in hole\\\\n\\', \\'    \\\\n\\', \\'    def _build_transition_matrix(self):\\\\n\\', \\'        \"\"\"Build transition matrix with optional slip probability\"\"\"\\\\n\\', \\'        for s in range(self.num_states):\\\\n\\', \\'            if s in self.terminal_states or s in self.holes:\\\\n\\', \\'                # Terminal/hole states transition to themselves with probability 1\\\\n\\', \\'                self.P[s, :, s] = 1\\\\n\\', \\'                continue\\\\n\\', \\'                \\\\n\\', \\'            for a, action in enumerate(self.actions):\\\\n\\', \\'                intended_next = self._get_intended_state(s, action)\\\\n\\', \\'                \\\\n\\', \\'                if self.success_prob == 1.0:\\\\n\\', \\'                    # Deterministic transition\\\\n\\', \\'                    self.P[s, a, intended_next] = 1.0\\\\n\\', \\'                else:\\\\n\\', \\'                    # Stochastic transition with perpendicular slip\\\\n\\', \\'                    perpendicular_states = self._get_perpendicular_states(s, action)\\\\n\\', \\'                    \\\\n\\', \\'                    # Probability distribution\\\\n\\', \\'                    self.P[s, a, intended_next] = self.success_prob\\\\n\\', \\'                    \\\\n\\', \\'                    # Distribute remaining probability equally among perpendicular states\\\\n\\', \\'                    remaining_prob = 1 - self.success_prob\\\\n\\', \\'                    num_perpendicular = len(perpendicular_states)\\\\n\\', \\'                    \\\\n\\', \\'                    if num_perpendicular > 0:\\\\n\\', \\'                        prob_per_state = remaining_prob / num_perpendicular\\\\n\\', \\'                        for perp_s in perpendicular_states:\\\\n\\', \\'                            self.P[s, a, perp_s] += prob_per_state\\\\n\\', \\'    \\\\n\\', \\'      \\\\n\\', \\'    def _get_intended_state(self, s, action):\\\\n\\', \\'        \"\"\"Get intended next state for an action (without slip)\"\"\"\\\\n\\', \\'        if s in self.terminal_states or s in self.holes:\\\\n\\', \\'            return s\\\\n\\', \\'            \\\\n\\', \\'        row, col = s // self.size, s % self.size\\\\n\\', \\'        \\\\n\\', \"        if action == \\'up\\':\\\\n\", \\'            row = max(row - 1, 0)\\\\n\\', \"        elif action == \\'down\\':\\\\n\", \\'            row = min(row + 1, self.size - 1)\\\\n\\', \"        elif action == \\'left\\':\\\\n\", \\'            col = max(col - 1, 0)\\\\n\\', \"        elif action == \\'right\\':\\\\n\", \\'            col = min(col + 1, self.size - 1)\\\\n\\', \\'            \\\\n\\', \\'        return row * self.size + col\\\\n\\', \\'    \\\\n\\', \\'    def _get_perpendicular_states(self, s, action):\\\\n\\', \\'        \"\"\"Get states reachable by perpendicular actions\"\"\"\\\\n\\', \\'        if s in self.terminal_states or s in self.holes:\\\\n\\', \\'            return []\\\\n\\', \\'            \\\\n\\', \\'        row, col = s // self.size, s % self.size\\\\n\\', \\'        perpendicular = []\\\\n\\', \\'        \\\\n\\', \"        if action in [\\'up\\', \\'down\\']:\\\\n\", \\'            # Perpendicular actions are left/right\\\\n\\', \\'            left_col = max(col - 1, 0)\\\\n\\', \\'            right_col = min(col + 1, self.size - 1)\\\\n\\', \\'            perpendicular.append(row * self.size + left_col)\\\\n\\', \\'            perpendicular.append(row * self.size + right_col)\\\\n\\', \\'        else:  # left/right\\\\n\\', \\'            # Perpendicular actions are up/down\\\\n\\', \\'            up_row = max(row - 1, 0)\\\\n\\', \\'            down_row = min(row + 1, self.size - 1)\\\\n\\', \\'            perpendicular.append(up_row * self.size + col)\\\\n\\', \\'            perpendicular.append(down_row * self.size + col)\\\\n\\', \\'        \\\\n\\', \\'        # Remove current state if included (only happens at boundaries)\\\\n\\', \\'        perpendicular = [s_next for s_next in perpendicular if s_next != s]\\\\n\\', \\'        return perpendicular # list(set(perpendicular))  # Remove duplicates\\\\n\\', \\'    \\\\n\\', \\'    def is_terminal(self, s):\\\\n\\', \\'        \"\"\"Check if state is terminal or hole\"\"\"\\\\n\\', \\'        return s in self.terminal_states or s in self.holes\\\\n\\', \\'    \\\\n\\', \\'    def step(self, s, a):\\\\n\\', \\'        \"\"\"Execute action in state s, return (s_next, r, done)\"\"\"\\\\n\\', \\'        a_idx = self.actions.index(a)\\\\n\\', \\'        s_next = np.random.choice(self.num_states, p=self.P[s, a_idx])\\\\n\\', \\'        r = self.R[s, a_idx, s_next]\\\\n\\', \\'        done = self.is_terminal(s_next)\\\\n\\', \\'        return s_next, r, done\\\\n\\', \\'    \\\\n\\', \\'    def get_equiprobable_policy(self):\\\\n\\', \\'        \"\"\"Return an equiprobable random policy\"\"\"\\\\n\\', \\'        policy = np.ones((self.num_states, self.num_actions)) / self.num_actions\\\\n\\', \\'        for s in self.terminal_states + self.holes:\\\\n\\', \\'            policy[s] = 0  # No actions in terminal/hole states\\\\n\\', \\'        return policy\\\\n\\', \\'    \\\\n\\', \\'    def visualize(self):\\\\n\\', \\'        \"\"\"Print a text visualization of the grid\"\"\"\\\\n\\', \\'        grid = []\\\\n\\', \\'        for row in range(self.size):\\\\n\\', \\'            cells = []\\\\n\\', \\'            for col in range(self.size):\\\\n\\', \\'                s = row * self.size + col\\\\n\\', \\'                if s in self.terminal_states:\\\\n\\', \"                    cells.append(\\'T\\' if s == 0 else \\'G\\')  # Start terminal and goal terminal\\\\n\", \\'                elif s in self.holes:\\\\n\\', \"                    cells.append(\\'H\\')\\\\n\", \\'                else:\\\\n\\', \\'                    cells.append(str(s))\\\\n\\', \"            grid.append(\\'\\\\\\\\t\\'.join(cells))\\\\n\", \"        print(\\'\\\\\\\\n\\'.join(grid))\\\\n\", \\'    \\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Deterministic environment with no holes\\\\n\\', \\'env_det = GridWorld(success_prob=None, holes=None)\\\\n\\', \\'s = 1\\\\n\\', \"a = \\'right\\'\\\\n\", \\'print(f\"Deterministic transition from {s}, {a}:\")\\\\n\\', \"print(env_det.P[1, env_det.actions.index(\\'right\\')])\\\\n\", \\'# Will show probability 1.0 to state 2\\\\n\\', \\'\\\\n\\', \\'# Stochastic environment with holes\\\\n\\', \\'env_stoch = GridWorld(success_prob=0.8, holes=[5,7,12])\\\\n\\', \\'s = 0\\\\n\\', \"a = \\'down\\'\\\\n\", \\'print(f\"\\\\\\\\nStochastic transition from {s}, {a}:\")\\\\n\\', \"print(env_stoch.P[s, env_stoch.actions.index(\\'down\\')])\\\\n\", \\'# Will show 0.7 to intended state, 0.15 to perpendicular states\\']\\'\\n with output: \\'[\\'Deterministic transition from 1, right:\\\\n\\', \\'[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\\\\n\\', \\'\\\\n\\', \\'Stochastic transition from 0, down:\\\\n\\', \\'[0.  0.2 0.  0.  0.8 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"To provide some example episodes, we need to simulate the agent\\'s movement through the gridworld according to the policy and state transitions described. We will create a function that simulates episodes by taking actions until a terminal state is reached.\\\\n\", \\'\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'start_state = 0\\\\n\\', \\'done = False\\\\n\\', \\'cnt = 0\\\\n\\', \\'print(\"Starting episode...\")\\\\n\\', \\'\\\\n\\', \\'while not done:\\\\n\\', \\'    # Choose a new random action at each step\\\\n\\', \\'    a_ix = np.random.randint(env_det.num_actions)\\\\n\\', \\'    action = env_det.actions[a_ix]\\\\n\\', \\'    \\\\n\\', \\'    # Take step\\\\n\\', \\'    s_next, r, done = env_det.step(start_state, action)\\\\n\\', \\'    \\\\n\\', \\'    # Print results\\\\n\\', \\'    print(f\"State: {start_state}, Action: {action}, Next_State: {s_next}, Reward: {r}\")\\\\n\\', \\'    \\\\n\\', \\'    # Update state\\\\n\\', \\'    start_state = s_next\\\\n\\', \\'    cnt += 1\\\\n\\', \\'\\\\n\\', \\'print(f\"Episode completed! with Episode length: {cnt}\")\\']\\'\\n with output: \\'[\\'Starting episode...\\\\n\\', \\'State: 0, Action: right, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: down, Next_State: 5, Reward: -1\\\\n\\', \\'State: 5, Action: left, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: left, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: down, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: right, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: right, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: left, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: left, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: down, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: down, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: down, Next_State: 14, Reward: -1\\\\n\\', \\'State: 14, Action: down, Next_State: 14, Reward: -1\\\\n\\', \\'State: 14, Action: left, Next_State: 13, Reward: -1\\\\n\\', \\'State: 13, Action: up, Next_State: 9, Reward: -1\\\\n\\', \\'State: 9, Action: right, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: down, Next_State: 14, Reward: -1\\\\n\\', \\'State: 14, Action: left, Next_State: 13, Reward: -1\\\\n\\', \\'State: 13, Action: up, Next_State: 9, Reward: -1\\\\n\\', \\'State: 9, Action: left, Next_State: 8, Reward: -1\\\\n\\', \\'State: 8, Action: down, Next_State: 12, Reward: -1\\\\n\\', \\'State: 12, Action: left, Next_State: 12, Reward: -1\\\\n\\', \\'State: 12, Action: down, Next_State: 12, Reward: -1\\\\n\\', \\'State: 12, Action: down, Next_State: 12, Reward: -1\\\\n\\', \\'State: 12, Action: up, Next_State: 8, Reward: -1\\\\n\\', \\'State: 8, Action: up, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: right, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: down, Next_State: 5, Reward: -1\\\\n\\', \\'State: 5, Action: left, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: down, Next_State: 8, Reward: -1\\\\n\\', \\'State: 8, Action: up, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: right, Next_State: 5, Reward: -1\\\\n\\', \\'State: 5, Action: right, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: down, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: up, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: left, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: down, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: down, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: up, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: down, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: right, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: left, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: left, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: down, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: left, Next_State: 5, Reward: -1\\\\n\\', \\'State: 5, Action: right, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: down, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: down, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: down, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: right, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: left, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: up, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: down, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: right, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: left, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: down, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: left, Next_State: 5, Reward: -1\\\\n\\', \\'State: 5, Action: left, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: left, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: down, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: left, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: down, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: left, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: right, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: right, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: right, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: up, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: right, Next_State: 3, Reward: -1\\\\n\\', \\'State: 3, Action: down, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: left, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: right, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: right, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: right, Next_State: 7, Reward: -1\\\\n\\', \\'State: 7, Action: down, Next_State: 11, Reward: -1\\\\n\\', \\'State: 11, Action: right, Next_State: 11, Reward: -1\\\\n\\', \\'State: 11, Action: left, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: right, Next_State: 11, Reward: -1\\\\n\\', \\'State: 11, Action: left, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: up, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: down, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: down, Next_State: 14, Reward: -1\\\\n\\', \\'State: 14, Action: left, Next_State: 13, Reward: -1\\\\n\\', \\'State: 13, Action: right, Next_State: 14, Reward: -1\\\\n\\', \\'State: 14, Action: down, Next_State: 14, Reward: -1\\\\n\\', \\'State: 14, Action: right, Next_State: 15, Reward: 10\\\\n\\', \\'Episode completed! with Episode length: 128\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'start_state = 0\\\\n\\', \\'done = False\\\\n\\', \\'cnt = 0\\\\n\\', \\'print(\"Starting episode...\")\\\\n\\', \\'\\\\n\\', \\'while not done:\\\\n\\', \\'    # Choose a new random action at each step\\\\n\\', \\'    a_ix = np.random.randint(env_stoch.num_actions)\\\\n\\', \\'    action = env_stoch.actions[a_ix]\\\\n\\', \\'    \\\\n\\', \\'    # Take step\\\\n\\', \\'    s_next, r, done = env_stoch.step(start_state, action)\\\\n\\', \\'    \\\\n\\', \\'    # Print results\\\\n\\', \\'    print(f\"State: {start_state}, Action: {action}, Next_State: {s_next}, Reward: {r}\")\\\\n\\', \\'    \\\\n\\', \\'    # Update state\\\\n\\', \\'    start_state = s_next\\\\n\\', \\'    cnt += 1\\\\n\\', \\'\\\\n\\', \\'print(f\"Episode completed! with Episode length: {cnt}\")\\']\\'\\n with output: \\'[\\'Starting episode...\\\\n\\', \\'State: 0, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: right, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: left, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: down, Next_State: 4, Reward: -1\\\\n\\', \\'State: 4, Action: up, Next_State: 0, Reward: -1\\\\n\\', \\'State: 0, Action: right, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: up, Next_State: 1, Reward: -1\\\\n\\', \\'State: 1, Action: right, Next_State: 2, Reward: -1\\\\n\\', \\'State: 2, Action: right, Next_State: 6, Reward: -1\\\\n\\', \\'State: 6, Action: down, Next_State: 10, Reward: -1\\\\n\\', \\'State: 10, Action: right, Next_State: 11, Reward: -1\\\\n\\', \\'State: 11, Action: right, Next_State: 11, Reward: -1\\\\n\\', \\'State: 11, Action: right, Next_State: 11, Reward: -1\\\\n\\', \\'State: 11, Action: right, Next_State: 11, Reward: -1\\\\n\\', \\'State: 11, Action: up, Next_State: 7, Reward: -10\\\\n\\', \\'Episode completed! with Episode length: 17\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def initialize_value_function(grid_world):\\\\n\\', \\'    # Initialize value function with zeros (using flat indices)\\\\n\\', \\'    return np.zeros(grid_world.num_states)\\\\n\\', \\'\\\\n\\', \\'def policy_evaluation(grid_world, policy, value_function, gamma=0.9, theta=1e-4):\\\\n\\', \\'    while True:\\\\n\\', \\'        delta = 0\\\\n\\', \\'        for s in range(grid_world.num_states):\\\\n\\', \\'            if grid_world.is_terminal(s):\\\\n\\', \\'                continue\\\\n\\', \\'                \\\\n\\', \\'            old_value = value_function[s]\\\\n\\', \\'            new_value = 0\\\\n\\', \\'            \\\\n\\', \\'            for a in range(grid_world.num_actions):\\\\n\\', \\'                action_prob = policy[s, a]\\\\n\\', \\'                # Sum over possible next states (for stochastic transitions)\\\\n\\', \\'                for s_next in range(grid_world.num_states):\\\\n\\', \\'                    prob = grid_world.P[s, a, s_next]\\\\n\\', \\'                    reward = grid_world.R[s, a, s_next]\\\\n\\', \\'                    new_value += action_prob * prob * (reward + gamma * value_function[s_next])\\\\n\\', \\'            \\\\n\\', \\'            value_function[s] = new_value\\\\n\\', \\'            delta = max(delta, abs(old_value - value_function[s]))\\\\n\\', \\'        \\\\n\\', \\'        if delta < theta:\\\\n\\', \\'            break\\\\n\\', \\'    \\\\n\\', \\'    return value_function\\\\n\\', \\'\\\\n\\', \\'def policy_improvement(grid_world, policy, value_function, gamma=0.9):\\\\n\\', \\'    policy_stable = True\\\\n\\', \\'    \\\\n\\', \\'    for s in range(grid_world.num_states):\\\\n\\', \\'        if grid_world.is_terminal(s):\\\\n\\', \\'            continue\\\\n\\', \\'            \\\\n\\', \\'        old_action = np.argmax(policy[s])\\\\n\\', \\'        action_values = np.zeros(grid_world.num_actions)\\\\n\\', \\'        \\\\n\\', \\'        for a in range(grid_world.num_actions):\\\\n\\', \\'            # Sum over possible next states\\\\n\\', \\'            for s_next in range(grid_world.num_states):\\\\n\\', \\'                prob = grid_world.P[s, a, s_next]\\\\n\\', \\'                reward = grid_world.R[s, a, s_next]\\\\n\\', \\'                action_values[a] += prob * (reward + gamma * value_function[s_next])\\\\n\\', \\'        \\\\n\\', \\'        best_action = np.argmax(action_values)\\\\n\\', \\'        \\\\n\\', \\'        # Update policy to be greedy\\\\n\\', \\'        new_policy = np.zeros(grid_world.num_actions)\\\\n\\', \\'        new_policy[best_action] = 1\\\\n\\', \\'        policy[s] = new_policy\\\\n\\', \\'        \\\\n\\', \\'        if old_action != best_action:\\\\n\\', \\'            policy_stable = False\\\\n\\', \\'    \\\\n\\', \\'    return policy, policy_stable\\\\n\\', \\'\\\\n\\', \\'def policy_iteration(grid_world, gamma=0.9, theta=1e-4):\\\\n\\', \\'    policy = grid_world.get_equiprobable_policy()  # Uses numpy array\\\\n\\', \\'    value_function = initialize_value_function(grid_world)\\\\n\\', \\'    \\\\n\\', \\'    iteration = 0\\\\n\\', \\'    while True:\\\\n\\', \\'        print(f\"Iteration {iteration}\")\\\\n\\', \\'        value_function = policy_evaluation(grid_world, policy, value_function, gamma, theta)\\\\n\\', \\'        policy, policy_stable = policy_improvement(grid_world, policy, value_function, gamma)\\\\n\\', \\'        \\\\n\\', \\'        if policy_stable:\\\\n\\', \\'            break\\\\n\\', \\'        iteration += 1\\\\n\\', \\'    \\\\n\\', \\'    return policy, value_function\\\\n\\', \\'\\\\n\\', \\'def print_policy(grid_world, policy):\\\\n\\', \"    action_symbols = [\\'↑\\', \\'↓\\', \\'←\\', \\'→\\', \\'T\\']\\\\n\", \\'    for i in range(grid_world.size):\\\\n\\', \\'        row = []\\\\n\\', \\'        for j in range(grid_world.size):\\\\n\\', \\'            s = i * grid_world.size + j\\\\n\\', \\'            if grid_world.is_terminal(s):\\\\n\\', \"                row.append(\\'T\\')\\\\n\", \\'            else:\\\\n\\', \\'                best_action = np.argmax(policy[s])\\\\n\\', \\'                row.append(action_symbols[best_action])\\\\n\\', \"        print(\\' \\'.join(row))\\\\n\", \\'\\\\n\\', \\'def print_values(grid_world, value_function):\\\\n\\', \\'    for i in range(grid_world.size):\\\\n\\', \\'        row = []\\\\n\\', \\'        for j in range(grid_world.size):\\\\n\\', \\'            s = i * grid_world.size + j\\\\n\\', \\'            row.append(f\"{value_function[s]:6.2f}\")\\\\n\\', \"        print(\\' \\'.join(row))\"]\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"Running Policy Iteration...\")\\\\n\\', \\'optimal_policy_det, optimal_values_det = policy_iteration(env_det)\\']\\'\\n with output: \\'[\\'Running Policy Iteration...\\\\n\\', \\'Iteration 0\\\\n\\', \\'Iteration 1\\\\n\\', \\'Iteration 2\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"Optimal Policy:\")\\\\n\\', \\'print_policy(env_det, optimal_policy_det)\\']\\'\\n with output: \\'[\\'Optimal Policy:\\\\n\\', \\'↓ ↓ ↓ ↓\\\\n\\', \\'↓ ↓ ↓ ↓\\\\n\\', \\'↓ ↓ ↓ ↓\\\\n\\', \\'→ → → T\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"Value Function:\")\\\\n\\', \\'print_values(env_det, optimal_values_det)\\']\\'\\n with output: \\'[\\'Value Function:\\\\n\\', \\'  1.81   3.12   4.58   6.20\\\\n\\', \\'  3.12   4.58   6.20   8.00\\\\n\\', \\'  4.58   6.20   8.00  10.00\\\\n\\', \\'  6.20   8.00  10.00   0.00\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"Running Policy Iteration...\")\\\\n\\', \\'optimal_policy_stoch, optimal_values_stoch = policy_iteration(env_stoch)\\']\\'\\n with output: \\'[\\'Running Policy Iteration...\\\\n\\', \\'Iteration 0\\\\n\\', \\'Iteration 1\\\\n\\', \\'Iteration 2\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"Optimal Policy:\")\\\\n\\', \\'print_policy(env_stoch, optimal_policy_stoch)\\']\\'\\n with output: \\'[\\'Optimal Policy:\\\\n\\', \\'↓ → ↓ ←\\\\n\\', \\'↓ T ↓ T\\\\n\\', \\'→ ↓ ↓ ↓\\\\n\\', \\'T → → T\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"Value Function:\")\\\\n\\', \\'print_values(env_stoch, optimal_values_stoch)\\']\\'\\n with output: \\'[\\'Value Function:\\\\n\\', \\' -3.06  -2.85  -0.07  -2.85\\\\n\\', \\' -2.15   0.00   2.00   0.00\\\\n\\', \\'  0.91   4.17   6.66   9.00\\\\n\\', \\'  0.00   6.23   9.00   0.00\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def run_episode(grid_world, policy, start_state=0, max_steps=100):\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    Run one episode using the given policy\\\\n\\', \\'    Returns: (success, steps_taken, states_visited, actions_taken)\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    state = start_state\\\\n\\', \\'    states = [state]\\\\n\\', \\'    actions_taken = []\\\\n\\', \\'    done = False\\\\n\\', \\'    steps = 0\\\\n\\', \\'    \\\\n\\', \\'    while not done and steps < max_steps:\\\\n\\', \\'        # Get action from policy (assuming deterministic policy)\\\\n\\', \\'        action_idx = np.argmax(policy[state])\\\\n\\', \\'        action = grid_world.actions[action_idx]\\\\n\\', \\'        \\\\n\\', \\'        # Take step\\\\n\\', \\'        next_state, reward, done = grid_world.step(state, action)\\\\n\\', \\'        \\\\n\\', \\'        # Record trajectory\\\\n\\', \\'        actions_taken.append(action)\\\\n\\', \\'        states.append(next_state)\\\\n\\', \\'        state = next_state\\\\n\\', \\'        steps += 1\\\\n\\', \\'    \\\\n\\', \\'    success = (state in grid_world.terminal_states) and (state not in grid_world.holes)\\\\n\\', \\'    return success, steps, states, actions_taken\\\\n\\', \\'\\\\n\\', \\'# Run multiple episodes\\\\n\\', \\'def run_multiple_episodes(grid_world, policy, num_episodes=100, start_state=0):\\\\n\\', \\'    wins = 0\\\\n\\', \\'    results = []\\\\n\\', \\'    \\\\n\\', \\'    for episode in range(num_episodes):\\\\n\\', \\'        success, steps, states, actions = run_episode(grid_world, policy, start_state)\\\\n\\', \\'        wins += success\\\\n\\', \\'        results.append({\\\\n\\', \"            \\'episode\\': episode + 1,\\\\n\", \"            \\'success\\': success,\\\\n\", \"            \\'steps\\': steps,\\\\n\", \"            \\'states\\': states,\\\\n\", \"            \\'actions\\': actions\\\\n\", \\'        })\\\\n\\', \\'        print(f\"Episode {episode + 1}: {\\\\\\'WIN\\\\\\' if success else \\\\\\'LOSS\\\\\\'} | \"\\\\n\\', \\'              f\"Steps: {steps} | \"\\\\n\\', \\'              f\"Terminal: {states[-1]} | \"\\\\n\\', \\'              f\"Actions: {actions}\")\\\\n\\', \\'    \\\\n\\', \\'    print(f\"\\\\\\\\nSuccess rate: {wins/num_episodes:.2%} ({wins}/{num_episodes})\")\\\\n\\', \\'    return results\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Run 100 episodes\\\\n\\', \\'results = run_multiple_episodes(env_det, optimal_policy_det, num_episodes=100)\\']\\'\\n with output: \\'[\"Episode 1: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 2: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 3: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 4: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 5: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 6: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 7: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 8: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 9: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 10: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 11: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 12: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 13: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 14: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 15: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 16: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 17: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 18: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 19: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 20: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 21: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 22: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 23: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 24: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 25: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 26: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 27: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 28: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 29: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 30: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 31: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 32: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 33: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 34: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 35: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 36: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 37: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 38: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 39: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 40: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 41: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 42: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 43: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 44: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 45: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 46: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 47: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 48: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 49: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 50: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 51: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 52: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 53: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 54: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 55: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 56: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 57: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 58: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 59: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 60: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 61: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 62: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 63: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 64: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 65: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 66: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 67: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 68: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 69: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 70: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 71: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 72: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 73: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 74: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 75: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 76: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 77: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 78: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 79: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 80: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 81: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 82: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 83: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 84: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 85: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 86: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 87: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 88: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 89: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 90: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 91: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 92: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 93: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 94: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 95: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 96: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 97: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 98: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 99: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 100: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\', \\'right\\']\\\\n\", \\'\\\\n\\', \\'Success rate: 100.00% (100/100)\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Run 100 episodes\\\\n\\', \\'results = run_multiple_episodes(env_stoch, optimal_policy_stoch, num_episodes=100)\\']\\'\\n with output: \\'[\"Episode 1: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\']\\\\n\", \"Episode 2: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 3: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 4: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 5: LOSS | Steps: 4 | Terminal: 7 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\']\\\\n\", \"Episode 6: WIN | Steps: 14 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 7: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 8: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 9: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 10: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 11: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 12: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 13: LOSS | Steps: 3 | Terminal: 12 | Actions: [\\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 14: WIN | Steps: 10 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 15: LOSS | Steps: 5 | Terminal: 12 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 16: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 17: LOSS | Steps: 3 | Terminal: 12 | Actions: [\\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 18: LOSS | Steps: 4 | Terminal: 7 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'left\\']\\\\n\", \"Episode 19: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 20: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 21: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 22: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 23: LOSS | Steps: 3 | Terminal: 12 | Actions: [\\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 24: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 25: WIN | Steps: 14 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 26: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 27: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 28: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 29: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 30: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'left\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 31: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 32: WIN | Steps: 10 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 33: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 34: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 35: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 36: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 37: LOSS | Steps: 4 | Terminal: 5 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\']\\\\n\", \"Episode 38: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 39: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 40: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 41: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 42: LOSS | Steps: 3 | Terminal: 12 | Actions: [\\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 43: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 44: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 45: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 46: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 47: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 48: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 49: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 50: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 51: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 52: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 53: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 54: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 55: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 56: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 57: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 58: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 59: LOSS | Steps: 3 | Terminal: 12 | Actions: [\\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 60: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'down\\']\\\\n\", \"Episode 61: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 62: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 63: WIN | Steps: 12 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 64: WIN | Steps: 12 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 65: WIN | Steps: 12 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 66: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 67: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 68: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 69: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 70: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 71: LOSS | Steps: 4 | Terminal: 5 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\']\\\\n\", \"Episode 72: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 73: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 74: WIN | Steps: 10 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 75: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 76: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 77: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 78: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 79: LOSS | Steps: 4 | Terminal: 5 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\']\\\\n\", \"Episode 80: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 81: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 82: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 83: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 84: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'left\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \"Episode 85: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 86: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 87: WIN | Steps: 10 | Terminal: 15 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 88: WIN | Steps: 8 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 89: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 90: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 91: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 92: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'right\\']\\\\n\", \"Episode 93: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 94: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 95: LOSS | Steps: 4 | Terminal: 5 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\']\\\\n\", \"Episode 96: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 97: LOSS | Steps: 2 | Terminal: 5 | Actions: [\\'down\\', \\'down\\']\\\\n\", \"Episode 98: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'right\\', \\'right\\']\\\\n\", \"Episode 99: LOSS | Steps: 4 | Terminal: 7 | Actions: [\\'down\\', \\'right\\', \\'down\\', \\'down\\']\\\\n\", \"Episode 100: WIN | Steps: 6 | Terminal: 15 | Actions: [\\'down\\', \\'down\\', \\'right\\', \\'down\\', \\'down\\', \\'right\\']\\\\n\", \\'\\\\n\\', \\'Success rate: 68.00% (68/100)\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'<div style=\"\\\\n\\', \\'    background-color: rgba(0, 0, 0, 0.05); \\\\n\\', \\'    padding: 15px 25px; \\\\n\\', \"    font-family: \\'Courier New\\', Courier, monospace; \\\\n\", \\'    font-size: 1.2em; \\\\n\\', \\'    border: 1px solid rgba(0, 0, 0, 0.1); \\\\n\\', \\'    border-radius: 8px; \\\\n\\', \\'    box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1); \\\\n\\', \\'    margin: 20px 0;\">    \\\\n\\', \\'    <h1><b>Algorithm 2: Value Iteration Algorithm</b></h1><br><br>\\\\n\\', \\'    <b>Data:</b> <i>$\\\\\\\\theta$</i>: a small number<br>\\\\n\\', \\'    <b>Result:</b> <i>$\\\\\\\\pi$</i>: a deterministic policy such that <i>$\\\\\\\\pi$</i> $\\\\\\\\approx $  <i>$\\\\\\\\pi^*$;</i><br><br>\\\\n\\', \\'    <b>Function</b> <font color=\"red\">ValueIteration</font> <b>is</b><br><br>\\\\n\\', \\'    &nbsp;&nbsp;/* Initialization */<br>\\\\n\\', \\'    &nbsp;&nbsp;Initialize \\\\\\\\( V(s) \\\\\\\\) arbitrarily, except \\\\\\\\( V(\\\\\\\\text{terminal}) \\\\\\\\);<br>\\\\n\\', \\'    &nbsp;&nbsp;\\\\\\\\( V(\\\\\\\\text{terminal}) \\\\\\\\gets 0 \\\\\\\\);<br><br>\\\\n\\', \\'    &nbsp;&nbsp;/* Loop until convergence */<br>\\\\n\\', \\'    &nbsp;&nbsp;\\\\\\\\( \\\\\\\\Delta \\\\\\\\gets 0 \\\\\\\\);<br>\\\\n\\', \\'    &nbsp;&nbsp;<b>while</b> \\\\\\\\( \\\\\\\\Delta < \\\\\\\\theta \\\\\\\\) <b>do</b><br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;<b>for each</b> \\\\\\\\( s \\\\\\\\in S \\\\\\\\) <b>do</b><br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\\\\\\\( v \\\\\\\\gets V(s) \\\\\\\\);<br>\\\\n\\', \"    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\\\\\\\( V(s) \\\\\\\\gets \\\\\\\\max_a \\\\\\\\sum_{s\\', r} p(s\\', r \\\\\\\\mid s, a) [r + \\\\\\\\gamma V(s\\')] \\\\\\\\);<br>\\\\n\", \\'    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\\\\\\\\( \\\\\\\\Delta \\\\\\\\gets \\\\\\\\max(\\\\\\\\Delta, |v - V(s)|) \\\\\\\\);<br>\\\\n\\', \\'    &nbsp;&nbsp;&nbsp;&nbsp;<b>end</b><br>\\\\n\\', \\'    &nbsp;&nbsp;<b>end</b><br><br>\\\\n\\', \\'    &nbsp;&nbsp;/* Return optimal policy */<br>\\\\n\\', \"    &nbsp;&nbsp;<b>return</b> \\\\\\\\( \\\\\\\\pi \\\\\\\\) such that \\\\\\\\( \\\\\\\\pi(s) = \\\\\\\\arg \\\\\\\\max_a \\\\\\\\sum_{s\\', r} p(s\\', r \\\\\\\\mid s, a) [r + \\\\\\\\gamma V(s\\')] \\\\\\\\);<br>\\\\n\", \\'    <b>end</b><br>\\\\n\\', \\'\\\\n\\', \\'</div>\\']\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Ch_5_MCM.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Monte Carlo Methods\\\\n\\', \\'\\\\n\\', \\'In this chapter\\\\n\\', \\'\\\\n\\', \\'- You will learn about estimating policies when learning from feedback that is simultaneously sequential and evaluative.\\\\n\\', \\'- You will develop algorithms for evaluating policies in reinforcement learning environments when the transition and reward functions are unknown.\\\\n\\', \\'- You will write code for estimating the value of policies in environments in which the full reinforcement learning problem is on display.\\\\n\\', \\'\\\\n\\', \"In summary, this chapter will introduce Monte Carlo methods for model-free reinforcement learning. These methods use the agent\\'s experience to estimate value functions without requiring a model of the environment. While they do not assume the Markov property, Monte Carlo methods can only be used to solve episodic reinforcement learning problems.\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'<img src=\"attachment:684b266d-86b6-4bf3-8a4e-41b379c14584.png\"  width=\"70%\" height=\"50%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Monte Carlo Policy Evaluation\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'### 1. **What is Monte Carlo?**\\\\n\\', \\'Monte Carlo methods are a class of algorithms that rely on random sampling to obtain numerical results. In the context of policy evaluation, this involves generating multiple episodes (sequences of states, actions, and rewards) under a policy and then using these episodes to estimate the value functions.\\\\n\\', \\'\\\\n\\', \\'- Monte Carlo policy evaluation is a model-free method that estimates the state value function for a given policy by collecting a large number of sample episodes and averaging the returns for the corresponding environment states.\\\\n\\', \\'- The agent learns from its own experience and does not require a model of the environment or the Bellman equation to estimate the state values. However, the algorithm often requires a large number of episodes to get an accurate estimate of  since the agent may only visit a few states in a single episode.\\\\n\\', \\'- Monte Carlo policy evaluation is typically used to solve episodic reinforcement learning problems, where the process terminates after a finite number of steps.\\\\n\\', \\'\\\\n\\', \\'### 2. **How Does Monte Carlo Policy Evaluation Work?**\\\\n\\', \\'\\\\n\\', \\'   - **Generate Episodes:** Under the policy $ \\\\\\\\pi $, generate multiple episodes. An episode is a complete sequence from the start state to a terminal state, where each step consists of a state, an action taken, and the reward received.\\\\n\\', \\'\\\\n\\', \\'   - **Compute Returns:** For each episode, calculate the return (sum of rewards) from each state or state-action pair. The return is the total reward accumulated from a certain point onwards until the end of the episode.\\\\n\\', \\'\\\\n\\', \\'   - **Average Returns:** For each state $ s $ or state-action pair $ (s, a) $, average the returns across all episodes where the state $ s $ or the state-action pair $ (s, a) $ is visited. This average serves as the estimate for the value function $ V_\\\\\\\\pi(s) $ or $ Q_\\\\\\\\pi(s, a) $.\\\\n\\', \\'\\\\n\\', \\'### 3. **Monte Carlo Estimation Formula**\\\\n\\', \\'\\\\n\\', \\'   - **State Value Function $ V_\\\\\\\\pi(s) $:**\\\\n\\', \\'     $$\\\\n\\', \\'     V_\\\\\\\\pi(s) = \\\\\\\\frac{1}{N(s)} \\\\\\\\sum_{i=1}^{N(s)} G_i\\\\n\\', \\'     $$\\\\n\\', \\'     where $ G_i $ is the return following state $ s $ in the $ i $-th episode, and $ N(s) $ is the number of times state $ s $ was visited.\\\\n\\', \\'\\\\n\\', \\'   - **Action Value Function $ Q_\\\\\\\\pi(s, a) $:**\\\\n\\', \\'     $$\\\\n\\', \\'     Q_\\\\\\\\pi(s, a) = \\\\\\\\frac{1}{N(s, a)} \\\\\\\\sum_{i=1}^{N(s, a)} G_i\\\\n\\', \\'     $$\\\\n\\', \\'     where $ G_i $ is the return following the action $ a $ in state $ s $ in the $ i $-th episode, and $ N(s, a) $ is the number of times action $ a $ was taken in state $ s $.\\\\n\\', \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Reward vs. return vs. value function\\\\n\\', \\'\\\\n\\', \\'**Reward:** Refers to the one-step reward signal the agent gets: the agent observes a state, selects an action, and receives a reward signal. The reward signal is the core of RL, but it is not what the agent is trying to maximize! Again, the agent isn’t trying to maximize the reward! Realize that while your agent maximizes the one-step reward, in the long-term, it’s getting less than it could.\\\\n\\', \\'\\\\n\\', \\'**Return:** Refers to the total discounted rewards. Returns are calculated from any state and usually go until the end of the episode. That is, when a terminal state is reached, the calculation stops. Returns are often referred to as total reward, cumulative reward, sum of rewards, and are commonly discounted: total discounted reward, cumulative discounted reward, sum of discounted reward. But, it’s basically the same: a return tells you how much reward your agent obtained in an episode. As you can see, returns are better indicators of performance because they contain a long-term sequence, a single-episode history of rewards. But the return isn’t what an agent tries to maximize, either! An agent who attempts to obtain the highest possible return may find a policy that takes it through a noisy path; sometimes, this path will provide a high return, but perhaps most of the time a low one.\\\\n\\', \\'\\\\n\\', \\'**Value function:** Refers to the expectation of returns. Sure, we want high returns, but high in expectation (on average). If the agent is in a noisy environment, or if the agent is using a stochastic policy, it’s all fine. The agent is trying to maximize the expected total discounted reward, after all: value functions.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## First-Visit Monte Carlo Policy Evaluation\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:6bec8d34-7b5c-4aed-8b00-0f146059c9ad.png\"  width=\"80%\" height=\"80%\" style=\"margin-left:auto; margin-right:auto\">\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Worked Example\\\\n\\', \\'\\\\n\\', \\'> **The random walk environment**\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:7608c012-5812-4437-ab70-26e99c7d1690.png\"  width=\"70%\" height=\"60%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'This is a walk, single-row grid-world environment, with five non-terminal states in which the agent moves left and right uniformly at random. The goal is to estimate the expected total discounted reward the agent can obtain given these circumstances.\\\\n\\', \\'\\\\n\\', \\'MC is easy to implement. The agent will first interact with the environment using policy $\\\\\\\\pi$ until the agent hits a terminal state $S_T$. The collection of state $S_t$, action  $A_t$, reward  $R_{t+1}$, and next state $S_{t+1}$ is called an *experience tuple*. A sequence of experiences is called a *trajectory*. The first thing you need to do is have your agent generate a trajectory.\\\\n\\', \\'\\\\n\\', \\'Once you have a trajectory, you calculate the returns $G_{t:T}$ for every state $S_t$ encountered.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:031177ac-6822-4e2a-a1d2-2332de40b8c4.png\"  width=\"70%\" height=\"60%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'After generating a trajectory and calculating the returns for all states $S_t$, you can estimate the state-value function $V_\\\\\\\\pi(S)$ at the end of every episode $e$ and final time step $t$ by merely averaging the returns obtained from each state S. In other words, we’re estimating an expectation with an average. As simple as that.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import numpy as np\\\\n\\', \\'import random\\\\n\\', \\'from collections import defaultdict\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import numpy as np\\\\n\\', \\'import random\\\\n\\', \\'from collections import defaultdict\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Define the environment based on the provided diagram\\\\n\\', \\'class RandomWalkEnv:\\\\n\\', \\'    def __init__(self):\\\\n\\', \\'        self.states = list(range(1, 6))\\\\n\\', \\'        self.terminal_states = [0, 6]\\\\n\\', \\'        self.current_state = None\\\\n\\', \\'        self.reset()\\\\n\\', \\'\\\\n\\', \\'    def reset(self):\\\\n\\', \\'        self.current_state = random.choice(self.states)\\\\n\\', \\'        return self.current_state\\\\n\\', \\'\\\\n\\', \\'    def step(self, action):\\\\n\\', \\'        if self.current_state in self.terminal_states:\\\\n\\', \\'            return self.current_state, 0, True\\\\n\\', \\'        \\\\n\\', \\'        next_state = self.current_state + (1 if action == 1 else -1)        \\\\n\\', \\'        reward = 1 if next_state == 6 else 0\\\\n\\', \\'        done = next_state in self.terminal_states\\\\n\\', \\'        self.current_state = next_state\\\\n\\', \\'        \\\\n\\', \\'        return self.current_state, reward, done\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def run_simulation(env, policy, num_steps=10):\\\\n\\', \\'    state = env.reset()\\\\n\\', \\'    print(\"Starting from State 3...\")\\\\n\\', \\'    print(f\"{\\\\\\'Action\\\\\\':<10} {\\\\\\'State\\\\\\':<10} {\\\\\\'Reward\\\\\\':<10} {\\\\\\'Done\\\\\\':<10}\")\\\\n\\', \\'    print(\"-\" * 40)\\\\n\\', \\'    \\\\n\\', \\'    for _ in range(num_steps):\\\\n\\', \\'        action = np.random.choice([0, 1], p=policy[state])\\\\n\\', \\'        state, reward, done = env.step(action)\\\\n\\', \\'        print(f\"{ACTION_NAMES[action]:<10} {state:<10} {reward:<10} {done:<10}\")\\\\n\\', \\'        \\\\n\\', \\'        if done:\\\\n\\', \\'            state = env.reset()  # Reset the environment if done\\\\n\\', \\'            print(\"Restarting from State 3...\")\\\\n\\', \\'    \\\\n\\', \\'    return state, reward, done\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Testing the environment\\\\n\\', \\'ACTION_NAMES = [\"LEFT\", \"RIGHT\"]\\\\n\\', \\'env_rw = RandomWalkEnv()\\\\n\\', \\'policy_b = np.array([[0.4, 0.6], [0.45, 0.55], [0.5, 0.5], [0.7, 0.3], [0.5, 0.5], [0.6, 0.4], [0.9, 0.1]])\\\\n\\', \\'policy_pi = np.array([[0.5, 0.5]] * 7) # uniform probability\\']\\'\\n\\n\\'code\\' cell: \\'[\\'state, reward, done = run_simulation(env_rw, policy_pi)\\']\\'\\n with output: \\'[\\'Starting from State 3...\\\\n\\', \\'Action     State      Reward     Done      \\\\n\\', \\'----------------------------------------\\\\n\\', \\'LEFT       4          0          0         \\\\n\\', \\'RIGHT      5          0          0         \\\\n\\', \\'RIGHT      6          1          1         \\\\n\\', \\'Restarting from State 3...\\\\n\\', \\'RIGHT      3          0          0         \\\\n\\', \\'RIGHT      4          0          0         \\\\n\\', \\'LEFT       3          0          0         \\\\n\\', \\'RIGHT      4          0          0         \\\\n\\', \\'RIGHT      5          0          0         \\\\n\\', \\'LEFT       4          0          0         \\\\n\\', \\'RIGHT      5          0          0         \\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Monte Carlo First-Visit Policy Evaluation\\\\n\\', \\'def first_visit_mc_policy_evaluation(env, policy, num_episodes):\\\\n\\', \\'    returns = defaultdict(list)\\\\n\\', \\'    V = defaultdict(float)\\\\n\\', \\'\\\\n\\', \\'    for _ in range(num_episodes):\\\\n\\', \\'        state = env.reset()\\\\n\\', \\'        episode = []\\\\n\\', \\'        visited_states = set()\\\\n\\', \\'\\\\n\\', \\'        # Generate an episode\\\\n\\', \\'        while True:\\\\n\\', \\'            action = np.random.choice([0, 1], p=policy[state])\\\\n\\', \\'            next_state, reward, done = env.step(action)\\\\n\\', \\'            episode.append((state, reward))\\\\n\\', \\'            state = next_state\\\\n\\', \\'            if done:\\\\n\\', \\'                break\\\\n\\', \\'\\\\n\\', \\'        # Compute returns and update value estimates\\\\n\\', \\'        G = 0\\\\n\\', \\'        for state, reward in reversed(episode):\\\\n\\', \\'            G = reward + G\\\\n\\', \\'            if state not in visited_states:\\\\n\\', \\'                visited_states.add(state)\\\\n\\', \\'                returns[state].append(G)\\\\n\\', \\'                V[state] = np.mean(returns[state])\\\\n\\', \\'\\\\n\\', \\'    return V\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Run the policy evaluation\\\\n\\', \\'env = RandomWalkEnv()\\\\n\\', \\'num_episodes = 10000  # Number of episodes to simulate\\\\n\\', \\'V = first_visit_mc_policy_evaluation(env, policy_pi, num_episodes)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Print the value function\\\\n\\', \\'for state in sorted(V.keys()):\\\\n\\', \\'    print(f\"V({state}) = {V[state]:.4f}\")\\']\\'\\n with output: \\'[\\'V(1) = 0.1695\\\\n\\', \\'V(2) = 0.3405\\\\n\\', \\'V(3) = 0.5019\\\\n\\', \\'V(4) = 0.6634\\\\n\\', \\'V(5) = 0.8305\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'class RandomWalkDPEnv:\\\\n\\', \\'    def __init__(self):\\\\n\\', \\'        self.states = list(range(1, 6))  # States 1 to 5\\\\n\\', \\'        self.terminal_states = [0, 6]\\\\n\\', \\'        self.gamma = 1.0  # Discount factor\\\\n\\', \\'        # # usually set to 1 for episodic tasks like this random walk\\\\n\\', \\'\\\\n\\', \\'    def value_iteration(self, theta=1e-6):\\\\n\\', \\'        V = np.zeros(7)  # There are 7 states from 0 to 6\\\\n\\', \\'\\\\n\\', \\'        while True:\\\\n\\', \\'            delta = 0\\\\n\\', \\'            for state in self.states:\\\\n\\', \\'                v = V[state]\\\\n\\', \\'                # Compute the value for each possible action under the random policy\\\\n\\', \\'                left_value = 0.5 * (0 + self.gamma * V[state - 1])\\\\n\\', \\'                right_value = 0.5 * (1 if state + 1 == 6 else 0 + self.gamma * V[state + 1])\\\\n\\', \\'                \\\\n\\', \\'                # The new value for the state is the average of the two possible transitions\\\\n\\', \\'                V[state] = left_value + right_value\\\\n\\', \\'                delta = max(delta, abs(v - V[state]))\\\\n\\', \\'            \\\\n\\', \\'            if delta < theta:\\\\n\\', \\'                break\\\\n\\', \\'\\\\n\\', \\'        return V\\\\n\\', \\'\\\\n\\', \\'# Run value iteration\\\\n\\', \\'env = RandomWalkDPEnv()\\\\n\\', \\'state_values = env.value_iteration()\\\\n\\', \\'\\\\n\\', \\'# Print the state values\\\\n\\', \\'for state in range(1, 6):\\\\n\\', \\'    print(f\"V({state}) = {state_values[state]:.4f}\")\\\\n\\']\\'\\n with output: \\'[\\'V(1) = 0.1667\\\\n\\', \\'V(2) = 0.3333\\\\n\\', \\'V(3) = 0.5000\\\\n\\', \\'V(4) = 0.6667\\\\n\\', \\'V(5) = 0.8333\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Every-Visit Monte Carlo Policy Evaluation\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"It\\'s fairly easy to extend the first-visit Monte Carlo policy evaluation to the every-visit case; we just need to remove the conditional check inside the for loop, and everything else stays exactly the same, as shown in the following Algorithm.\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:3811f04e-54c9-4749-ba7d-090ae55b4c63.png\"  width=\"70%\" height=\"80%\" style=\"margin-left:auto; margin-right:auto\">\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Comparison between Dynamic Programming (DP) and Monte Carlo (MC) Methods\\\\n\\', \\'\\\\n\\', \\'| **Aspect**                        | **Dynamic Programming (DP)**                                | **Monte Carlo (MC) Methods**                                  |\\\\n\\', \\'|-----------------------------------|-------------------------------------------------------------|---------------------------------------------------------------|\\\\n\\', \\'| **Learning Approach**             | Model-based (requires full knowledge of the environment).   | Model-free (can learn directly from interaction with the environment). |\\\\n\\', \\'| **Type of Update**              | Bootstrapping (update from estimates of other states)     | Averaging (update from complete returns) |\\\\n\\', \\'| **Value Estimation**              | Computes exact value functions using Bellman equations.     | Estimates value functions based on sample returns from episodes. |\\\\n\\', \\'| **Policy Iteration/Improvement**  | Alternates between policy evaluation and policy improvement (e.g., Value Iteration, Policy Iteration). | Uses policy evaluation (often through first-visit or every-visit) and can be used with exploration policies. |\\\\n\\', \\'| **Convergence Speed**             | Typically faster in terms of convergence if the model is fully known. | Slower convergence because it requires many episodes to accurately estimate values. |\\\\n\\', \\'| **Computational Requirements**    | Requires the entire state space and transition dynamics to be stored and computed. | Less demanding on memory but may require a large number of episodes to achieve accurate estimates. |\\\\n\\', \\'| **Exploration**                   | Requires exploration to be managed separately (e.g., ε-greedy policies) to ensure all states are visited. | Naturally explores the state space through random or stochastic policies. |\\\\n\\', \\'| **Handling of Stochasticity**     | Handles stochastic environments well, but requires knowledge of transition probabilities. | Can handle stochastic environments directly through sampling. |\\\\n\\', \\'| **Efficiency**                    | Efficient when the state space is small or manageable and the transition model is known. | More efficient in environments where the model is complex or unknown. |\\\\n\\', \\'| **Example Methods**               | Value Iteration, Policy Iteration.                          | First-Visit MC, Every-Visit MC, Off-Policy MC.               |\\\\n\\', \\'| **Dependency on Markov Property** | Strong dependency on the Markov property for exact calculation. | Less strict on the Markov property, can work with approximate state representations. |\\\\n\\', \\'| **Use Cases**                     | Ideal when the environment is fully known (e.g., chess, backgammon with exact rules). | Useful in real-world scenarios where the environment is unknown or too complex to model directly. |\\\\n\\', \\'| **Discount Factor $ \\\\\\\\gamma $**  | Explicitly considered in the Bellman equation.              | Typically included in return calculations, but can be easily adjusted based on the problem context. |\\\\n\\', \\'| **Generalization**                | May struggle with generalization in large or continuous state spaces unless approximations are used. | Can generalize better using function approximation techniques in large state spaces. |\\\\n\\', \\'| **Parallelization**               | Difficult to parallelize due to the dependency of states.   | Easy to parallelize as episodes can be generated independently. |\\\\n\\', \\'\\\\n\\', \\'#### Summary\\\\n\\', \\'\\\\n\\', \\'- **Dynamic Programming** is powerful when you have full knowledge of the environment and can efficiently compute exact values for each state. It is faster in converging to the optimal policy but is not suitable for very large state spaces or unknown environments.\\\\n\\', \\'  \\\\n\\', \\'- **Monte Carlo Methods** are more flexible and applicable to a broader range of problems, particularly where the model of the environment is unknown. However, they require more samples (episodes) to converge to accurate value estimates and are generally slower in doing so.\\\\n\\', \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Incremental Update\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:84931da0-f5ae-4ea8-a365-31ce8b5361fd.png\"  width=\"70%\" height=\"60%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Algorithm \\\\n\\', \\'\\\\n\\', \\'> **First-visit Monte Carlo policy evaluation with incremental update for $V_\\\\\\\\pi$**\\\\n\\', \\'> ![image.png](attachment:b779fd1c-1ea0-440c-a283-11c4cbb9f767.png)\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Monte Carlo Estimation of Action Values\\\\n\\', \\'\\\\n\\', \\'**How can we derive a policy from the state value function, especially when transition probabilities are unavailable?**\\\\n\\', \\'\\\\n\\', \\'One approach is to evaluate all possible actions and their resulting next states, then select the action that leads to the state with the highest value. However, this method assumes access to a model of the environment—a mapping from actions to possible next states. Without such a model, the state value function becomes difficult to exploit effectively.\\\\n\\', \\'\\\\n\\', \\'State values are most useful when the environment allows for one-step lookahead, enabling us to predict the outcome of actions. In the absence of this predictive capability, relying solely on state values is insufficient. Instead, we need to use the action-value function (Q-value), which estimates the value of taking a particular action in a given state—without requiring a model of the environment.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'$\\\\\\\\color{red}{\\\\\\\\text{If a model is not available, then it is particularly useful to estimate action values (the\\\\n\\', \\'values of state–action pairs) rather than}}$ $\\\\\\\\color{red}{\\\\\\\\text{state values}}$.  Without a model, however, state values alone are not sufficient. One must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for Monte Carlo methods is to estimate $q_*$. To achieve\\\\n\\', \\'this, we first consider the policy evaluation problem for action values.\\\\n\\', \\'\\\\n\\', \\'We can use Monte Carlo policy evaluation to estimate the state-action value function $Q_\\\\\\\\pi$  for a policy $\\\\\\\\pi$ . The state-action value function  measures the expected return starting in state $s$, taking action $a$, then following policy $\\\\\\\\pi$ .\\\\n\\', \\'\\\\n\\', \\'**Challenges in Directly Applying Monte Carlo Methods to Q-Value Estimation**\\\\n\\', \\'\\\\n\\', \\'A key limitation of Monte Carlo (MC) methods for estimating Q-values is the need to visit every state-action pair an infinite number of times to obtain accurate estimates. This exhaustive sampling requirement makes MC approaches inefficient and impractical in large or complex environments.\\\\n\\', \\'\\\\n\\', \\'In contrast, state value estimation does not have such a constraint. However, it becomes ineffective when used with a deterministic policy. If the policy deterministically selects only one action per state, many state-action pairs are never explored. As a result, the agent fails to gather the necessary experience to construct a robust or improved policy.\\\\n\\', \\'\\\\n\\', \\'Deterministic policies, in such cases, inhibit the learning process because they limit exploration. Without visiting diverse state-action combinations, the agent cannot accurately evaluate alternative actions, leading to poor policy optimization.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'### Unvisited State-Action Pairs in Monte Carlo Methods\\\\n\\', \\'\\\\n\\', \\'#### Understanding the Problem\\\\n\\', \\'\\\\n\\', \\'In reinforcement learning, particularly with Monte Carlo methods, we aim to estimate the value of state-action pairs. A state-action pair represents a specific situation (state) and a possible action in that situation. The goal is to determine the expected return (total reward) for taking that action in that state.\\\\n\\', \\'\\\\n\\', \"However, a significant challenge arises: **not all possible state-action pairs might be visited during the simulation**. This means we won\\'t have any data to estimate their value.\\\\n\", \\'\\\\n\\', \\'#### Example: A Simplified Grid World\\\\n\\', \\'\\\\n\\', \"Imagine a simple grid world where an agent can move up, down, left, or right. Each cell in the grid represents a state. The agent\\'s action (move up, down, left, or right) in a particular cell constitutes a state-action pair.\\\\n\", \\'\\\\n\\', \\'* **Possible State-Action Pairs:** Given a 4x4 grid, there are 16 states and 4 possible actions per state, resulting in 64 possible state-action pairs.\\\\n\\', \\'* **Unvisited Pairs:** If the agent follows a deterministic policy (e.g., always moves right), many state-action pairs (like moving left from any state) will never be visited.\\\\n\\', \\'\\\\n\\', \\'#### Implications\\\\n\\', \\'\\\\n\\', \\'* **Inaccurate Value Estimates:** Without data on a state-action pair, its value cannot be accurately estimated.\\\\n\\', \\'* **Suboptimal Policies:** If the optimal policy involves actions that are rarely explored, the agent might never discover the best strategy.\\\\n\\', \\'\\\\n\\', \\'### Addressing the Issue\\\\n\\', \\'\\\\n\\', \\'To mitigate this problem, several techniques like **Epsilon-Greedy Exploration:**, **Upper Confidence Bound (UCB) Exploration:** can be employed:\\\\n\\', \\'\\\\n\\', \\'By incorporating these exploration strategies, we can increase the likelihood of visiting different state-action pairs and improve the accuracy of value estimates. Thus, by specifying that the episodes start in a state–action pair, and that **every pair has a nonzero probability of being selected as the start.** This guarantees that all state–action pairs will be visited an infinite number of times in the limit of an infinite number of episodes. We call this the assumption of **exploring starts**.\\\\n\\', \\'\\\\n\\', \\'**In essence**, the challenge of unvisited state-action pairs highlights the importance of balancing exploration and exploitation in reinforcement learning.\\\\n\\', \\' \\\\n\\', \\'**The most common alternative approach to assuring that all state–action pairs are encountered is to consider only policies that are stochastic with a nonzero probability of selecting all actions in each state.**\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Incremental vs. sequential vs. trial-and-error\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'**Incremental methods:** Refers to $\\\\\\\\color{blue}{\\\\\\\\text{the iterative improvement}}$ of the estimates. Dynamic programming is an incremental method: these algorithms iteratively compute the answers. They don’t “interact” with an environment, but they reach the answers through successive iterations, incrementally. Bandits are also incremental: they reach good approximations through successive episodes or trials. Reinforcement learning is incremental, as well. Depending on the specific algorithm, estimates are improved on an either per-episode or per-time-step basis, incrementally.\\\\n\\', \\'\\\\n\\', \\'**Sequential methods:** Refers to $\\\\\\\\color{blue}{\\\\\\\\text{learning in an environment}}$ with more than one non-terminal (and reachable) state. Dynamic programming is a sequential method. Bandits are not sequential, they are one-state one-step MDPs. There’s no long-term consequence for the agent’s actions. Reinforcement learning is certainly sequential.\\\\n\\', \\'\\\\n\\', \\'**Trial-and-error methods:** Refers to $\\\\\\\\color{blue}{\\\\\\\\text{learning from interaction}}$ with the environment. Dynamic programming is not trial-and-error learning. Bandits are trial-and-error learning. Reinforcement learning is trial-and-error learning, too.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Prediction vs. control problem vs. policy evaluation vs. improvement\\\\n\\', \\'\\\\n\\', \\'**Prediction problem:** Refers to the problem of $\\\\\\\\color{blue}{\\\\\\\\text{evaluating policies}}$, of estimating value functions given a policy. Estimating value functions is nothing but learning to predict returns. State-value functions estimate expected returns from states, and action-value functions estimate expected returns from state-action pairs.\\\\n\\', \\'\\\\n\\', \\'**Control problem:** Refers to the problem of $\\\\\\\\color{blue}{\\\\\\\\text{finding optimal policies}}$. The control problem is usually solved by following the pattern of generalized policy iteration (GPI), where the competing processes of policy evaluation and policy improvement progressively move policies towards optimality. RL methods often pair an action-value prediction method with policy improvement and action-selection strategies.\\\\n\\', \\'\\\\n\\', \\'**Policy evaluation:** Refers to $\\\\\\\\color{blue}{\\\\\\\\text{algorithms}}$ that solve the prediction problem. Note that there’s a dynamic programming method called policy evaluation, but this term is also used to refer to all algorithms that solve the prediction problem.\\\\n\\', \\'\\\\n\\', \\'**Policy improvement:** Refers to $\\\\\\\\color{blue}{\\\\\\\\text{algorithms}}$ that make new policies that improve on an original policy by making it greedier than the original with respect to the value function of that original policy. Note that policy improvement by itself doesn’t solve the control problem. Often a policy evaluation must be paired with a policy improvement to solve the control problem. Policy improvement only refers to the computation for improving a policy given its evaluation results.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Monte Carlo Exploring Starts (MCES)\\\\n\\', \\'\\\\n\\', \\'**Monte Carlo Exploring Starts (MCES)** is a reinforcement learning algorithm that addresses the exploration problem in a straightforward manner. It ensures that all state-action pairs are explored by starting each episode from a randomly chosen state and action. \\\\n\\', \\'\\\\n\\', \\'### Exploring Starts Solution\\\\n\\', \\'MCES tackles this issue by using a technique called \"exploring starts.\" Instead of starting each episode in a fixed initial state, every episode begins with a randomly selected *state-action* pair. This means that every possible combination of state and action has a non-zero probability of being the starting point for an episode.\\\\n\\', \\'\\\\n\\', \\'### Algorithm Steps\\\\n\\', \\'1. **Initialization:**\\\\n\\', \\'   * Initialize policy and the Q-value function for all state-action pairs arbitrarily.\\\\n\\', \\'2. **Generate Episodes:**\\\\n\\', \\'   * For each episode:\\\\n\\', \\'     * Randomly select a starting state and action.\\\\n\\', \\'     * Follow the current policy to generate an episode.\\\\n\\', \\'     * Calculate the return for the episode.\\\\n\\', \\'     * Update the Q-value for the starting state-action pair using the Monte Carlo update rule.\\\\n\\', \\'3. **Policy Improvement:**\\\\n\\', \\'   * After a sufficient number of episodes, create a new policy by selecting the action with the highest Q-value for each state. (i.e., the policy becomes greedy with respect to the estimated Q-values).\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:2012f6f8-05a3-4afb-9f7e-63462c35e1ec.png)\\\\n\\', \\'\\\\n\\', \\'### Key Points\\\\n\\', \\'* **Guaranteed Exploration:** By starting each episode with a random state-action pair, MCES ensures that all possible state-action pairs are visited, addressing the exploration problem.\\\\n\\', \\'* **Simplicity:** The algorithm is relatively straightforward to implement.\\\\n\\', \\'* **Limitations:** In large state-action spaces, generating all possible starting combinations can be computationally expensive.\\\\n\\', \\'\\\\n\\', \\'### MCES Convergence\\\\n\\', \\'\\\\n\\', \\'**MCES accumulates all returns for a state-action pair, regardless of the policy in effect when the return was generated.** This means the algorithm learns from all experiences, not just those under the current policy.\\\\n\\', \\'\\\\n\\', \\'**MCES cannot converge to a suboptimal policy.** If it did, the value function would eventually reflect this suboptimal policy, leading to a policy change. This process continues until both the value function and policy are optimal.\\\\n\\', \\'\\\\n\\', \\'**Convergence to the optimal solution seems likely** as the impact of each new return on the value function decreases over time. However, **formal proof of convergence is still an open problem**.\\\\n\\', \\' \\\\n\\', \\'Essentially, MCES is designed to explore the entire state-action space and learn the optimal policy without getting stuck in local optima. While its convergence properties are promising, formal guarantees are yet to be established. \\\\n\\', \\'\\\\n\\', \"**In conclusion,** MCES provides a foundational understanding of the exploration challenge in reinforcement learning. While it\\'s not always practical for large-scale problems, it serves as a valuable building block for more advanced exploration techniques.\\\\n\", \\'\\\\n\\', \\'\\\\n\\', \\'### Comparison to Other Methods\\\\n\\', \"While MCES effectively solves the exploration problem, it\\'s often computationally inefficient for large environments. Other methods like epsilon-greedy, upper confidence bound (UCB), and softmax exploration offer more practical alternatives.\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Monte Carlo Control\\\\n\\', \\'\\\\n\\', \\'We are now ready to consider how Monte Carlo estimation can be used in control, that is, to approximate optimal policies. **Monte Carlo control refers to a collection of methods that find (near) optimal policies using the samples of discounted return.**\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:8fca5a6d-5bf0-4ed0-b86a-6739583b9917.png\"  width=\"40%\" height=\"30%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'### Generalized Policy Iteration (GPI) framework:\\\\n\\', \\'\\\\n\\', \\'1. **Definition**:\\\\n\\', \\'   - GPI is a framework in reinforcement learning that describes the iterative interaction between policy evaluation and policy improvement.\\\\n\\', \\'\\\\n\\', \\'2. **Key Components**:\\\\n\\', \\'   - **Policy Evaluation**:\\\\n\\', \\'     - Estimates the value function $V_\\\\\\\\pi(s)$ or action-value function $Q_\\\\\\\\pi(s, a)$ for a given policy $\\\\\\\\pi$.\\\\n\\', \\'     - Determines the expected cumulative reward for following policy $\\\\\\\\pi$ from a given state.\\\\n\\', \\'\\\\n\\', \\'   - **Policy Improvement**:\\\\n\\', \"     - Updates the policy to $\\\\\\\\pi\\'$ by selecting actions that maximize the value function.\\\\n\", \"     - Ensures the new policy $\\\\\\\\pi\\'$ is at least as good as the previous policy.\\\\n\", \\'\\\\n\\', \\'3. **Iterative Process**:\\\\n\\', \\'   - Start with an initial policy $\\\\\\\\pi_0$.\\\\n\\', \\'   - Alternate between policy evaluation and policy improvement to generate a sequence of policies ($\\\\\\\\pi_0, \\\\\\\\pi_1, \\\\\\\\pi_2, \\\\\\\\dots$).\\\\n\\', \\'     > ![image.png](attachment:17e64d93-065f-45f8-bfc2-8309367899f7.png)\\\\n\\', \\'   - Continue until the policy converges to an optimal policy $\\\\\\\\pi^*$, where no further improvements are possible.\\\\n\\', \\'\\\\n\\', \\'4. **Convergence**:\\\\n\\', \\'   - In finite MDPs with a fixed discount factor, GPI is guaranteed to converge to the optimal policy $\\\\\\\\pi^*$.\\\\n\\', \\'\\\\n\\', \\'5. **Significance**:\\\\n\\', \"   - GPI underlies many RL algorithms, balancing the current policy\\'s performance with exploring better actions systematically.\\\\n\", \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Monte Carlo Control without Exploring Starts\\\\n\\', \\'\\\\n\\', \\'How can we avoid the unlikely assumption of exploring starts? The only general way to ensure that all actions are selected infinitely often is for the agent to continue to select them. There are two approaches to ensuring this, resulting in what we call **on-policy methods and off-policy methods**. \\\\n\\', \\'- On-policy methods attempt to evaluate or improve the policy that is used to make decisions, whereas off-policy methods evaluate or improve a policy different from that used to generate the data.\\\\n\\', \\'- The Monte Carlo ES method developed above is an example of an *on-policy method*.\\\\n\\', \\'- In this section we show how an on-policy Monte Carlo control method can be designed that does not use the unrealistic assumption of exploring starts. On-policy methods are considered in the next section.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'In on-policy control methods the policy is generally **soft**, meaning that $\\\\\\\\pi(a|s) > 0$ for all $ s \\\\\\\\in \\\\\\\\mathcal{S} $ and  $ a \\\\\\\\in \\\\\\\\mathcal{A} $, but gradually shifted closer and closer to a deterministic\\\\n\\', \\'optimal policy. The on-policy method we present in this section uses $\\\\\\\\epsilon$-greedy policies, meaning\\\\n\\', \\'that most of the time they choose an action that has maximal estimated action value, but with probability $\\\\\\\\epsilon$ they instead select an action at random, i.e. $\\\\\\\\pi(a|s) \\\\\\\\ge \\\\\\\\frac{\\\\\\\\epsilon}{\\\\\\\\mathcal{A}(s)}$. The policy which statifies this condition is called $\\\\\\\\epsilon$-**soft policy**.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'Formally, the $\\\\\\\\epsilon$-greedy policy is defined as\\\\n\\', \\'\\\\n\\', \\'$$\\\\\\\\pi \\\\\\\\left( a\\\\\\\\left| s \\\\\\\\right. \\\\\\\\right)=\\\\\\\\left\\\\\\\\{ \\\\\\\\begin{align}\\\\n\\', \\'  & 1-\\\\\\\\epsilon +\\\\\\\\frac{\\\\\\\\epsilon }{\\\\\\\\left| \\\\\\\\mathcal{A}\\\\\\\\left( s \\\\\\\\right) \\\\\\\\right|},\\\\\\\\,\\\\\\\\,\\\\\\\\text{if }a=\\\\\\\\underset{a}{\\\\\\\\mathop{\\\\\\\\arg \\\\\\\\max }}\\\\\\\\,{{Q}_{\\\\\\\\pi }}\\\\\\\\left( s,a \\\\\\\\right) \\\\\\\\\\\\\\\\ \\\\n\\', \\' & \\\\\\\\frac{\\\\\\\\epsilon }{\\\\\\\\left| \\\\\\\\mathcal{A}\\\\\\\\left( s \\\\\\\\right) \\\\\\\\right|},\\\\\\\\qquad \\\\\\\\qquad  \\\\\\\\text{if }a\\\\\\\\ne \\\\\\\\underset{a}{\\\\\\\\mathop{\\\\\\\\arg \\\\\\\\max }}\\\\\\\\,{{Q}_{\\\\\\\\pi }}\\\\\\\\left( s,a \\\\\\\\right)\\\\\\\\, \\\\\\\\\\\\\\\\ \\\\n\\', \\'\\\\\\\\end{align} \\\\\\\\right.\\\\\\\\text{ }$$ \\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'The policy selects the best known action with probability $1-\\\\\\\\epsilon +\\\\\\\\frac{\\\\\\\\epsilon }{\\\\\\\\left| \\\\\\\\mathcal{A}\\\\\\\\left( s \\\\\\\\right) \\\\\\\\right|}$ and a random action with probability $\\\\\\\\frac{\\\\\\\\epsilon }{\\\\\\\\left| \\\\\\\\mathcal{A}\\\\\\\\left( s \\\\\\\\right) \\\\\\\\right|}$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'### Algorithm:\\\\n\\', \\'\\\\n\\', \\'![image.png](attachment:5dd533ce-39bd-4191-8866-b3c2722a3881.png)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"**Any $\\\\\\\\epsilon$-greedy policy with respecct to $q_\\\\\\\\pi$ is an improvement over any $\\\\\\\\epsilon$-soft policy $\\\\\\\\pi$ is assured by the policy improvement theorem.** Let $\\\\\\\\pi\\'$ be the $\\\\\\\\epsilon$-greedy policy. The conditions of the policy improvement theorem apply for any $s \\\\\\\\in \\\\\\\\mathcal{S}$:   \\\\n\", \\'\\\\n\\', \\'\\\\n\\', \"Let\\'s break down the steps in the derivation.\\\\n\", \\'\\\\n\\', \\'### Given Equation:\\\\n\\', \\'\\\\n\\', \\'The equation (5.2) (page # 101) in the book is as follows:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \"q_{\\\\\\\\pi}(s, \\\\\\\\pi\\'(s)) = \\\\\\\\sum_a \\\\\\\\pi\\'(a|s) q_{\\\\\\\\pi}(s, a)\\\\n\", \\'$$\\\\n\\', \\'\\\\n\\', \"This equation expresses the expected value of the action-value function $ q_{\\\\\\\\pi}(s, a) $ under the new policy $ \\\\\\\\pi\\'(s) $.\\\\n\", \\'\\\\n\\', \"#### Step 1: Decompose $ \\\\\\\\pi\\'(a|s) $\\\\n\", \\'\\\\n\\', \"The new policy $ \\\\\\\\pi\\'(a|s) $ is typically a mixture of an exploration policy and the greedy policy based on the current value estimates. It can be represented as:\\\\n\", \\'\\\\n\\', \\'$$\\\\n\\', \"\\\\\\\\pi\\'(a|s) = \\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|} + (1-\\\\\\\\epsilon) \\\\\\\\delta(a, a^*)\\\\n\", \\'$$\\\\n\\', \\'\\\\n\\', \\'where:\\\\n\\', \\'- $ \\\\\\\\epsilon $ is the exploration probability,\\\\n\\', \\'- $ |\\\\\\\\mathcal{A}(s)| $ is the number of actions available in state $ s $,\\\\n\\', \\'- $ a^* $ is the greedy action that maximizes $ q_{\\\\\\\\pi}(s, a) $,\\\\n\\', \\'- $ \\\\\\\\delta(a, a^*) $ is 1 if $ a = a^* $ and 0 otherwise.\\\\n\\', \\'\\\\n\\', \\'This leads to the expression:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \"\\\\\\\\pi\\'(a|s) = \\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|} + (1-\\\\\\\\epsilon)\\\\\\\\delta(a, \\\\\\\\arg\\\\\\\\max_a q_{\\\\\\\\pi}(s, a))\\\\n\", \\'$$\\\\n\\', \\'\\\\n\\', \"#### Step 2: Substitute $ \\\\\\\\pi\\'(a|s) $ into the Summation\\\\n\", \\'\\\\n\\', \\'Substituting this into the expected value, we have:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \"q_{\\\\\\\\pi}(s, \\\\\\\\pi\\'(s)) = \\\\\\\\sum_a \\\\\\\\left(\\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|} + (1-\\\\\\\\epsilon)\\\\\\\\delta(a, \\\\\\\\arg\\\\\\\\max_a q_{\\\\\\\\pi}(s, a))\\\\\\\\right) q_{\\\\\\\\pi}(s, a)\\\\n\", \\'$$\\\\n\\', \\'\\\\n\\', \\'This can be expanded as:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \"q_{\\\\\\\\pi}(s, \\\\\\\\pi\\'(s)) = \\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|} \\\\\\\\sum_a q_{\\\\\\\\pi}(s, a) + (1-\\\\\\\\epsilon) \\\\\\\\max_a q_{\\\\\\\\pi}(s, a)\\\\n\", \\'$$\\\\n\\', \\'\\\\n\\', \\'#### Step 3: Consider the Inequality\\\\n\\', \\'\\\\n\\', \\'The next step involves comparing this expression with an alternative expression where the greedy policy is not necessarily followed perfectly.\\\\n\\', \\'\\\\n\\', \\'We now introduce the actual policy $\\\\\\\\pi(a|s)$ and consider the difference:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\sum_a \\\\\\\\delta\\\\\\\\left(a, \\\\\\\\arg\\\\\\\\max_a q_{\\\\\\\\pi}(s, a)\\\\\\\\right) q_{\\\\\\\\pi}(s, a) \\\\\\\\geq \\\\\\\\sum_a \\\\\\\\frac{\\\\\\\\pi(a|s) - \\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|}}{1-\\\\\\\\epsilon} q_{\\\\\\\\pi}(s, a)\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'This inequality holds because:\\\\n\\', \\'\\\\n\\', \\'- $\\\\\\\\max_a q_{\\\\\\\\pi}(s, a)$ (which is the value from taking the best action) is greater than or equal to the expected value over all actions weighted by the current policy $\\\\\\\\pi(a|s)$.\\\\n\\', \\'- The term $\\\\\\\\frac{\\\\\\\\pi(a|s) - \\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|}}{1-\\\\\\\\epsilon}$ represents how the current policy deviates from the exploration component. It effectively reduces the probability mass assigned to the exploratory action, favoring actions according to the policy $\\\\\\\\pi(a|s)$.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'#### Final Inequality Expression:\\\\n\\', \\'\\\\n\\', \\'After simplifying, we get the inequality:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \"q_{\\\\\\\\pi}(s, \\\\\\\\pi\\'(s)) \\\\\\\\geq \\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|} \\\\\\\\sum_a q_{\\\\\\\\pi}(s, a) + (1-\\\\\\\\epsilon) \\\\\\\\sum_a \\\\\\\\frac{\\\\\\\\pi(a|s) - \\\\\\\\frac{\\\\\\\\epsilon}{|\\\\\\\\mathcal{A}(s)|}}{1-\\\\\\\\epsilon} q_{\\\\\\\\pi}(s, a)\\\\n\", \\'$$\\\\n\\', \\'\\\\n\\', \"This shows that the expected value under the new policy $ \\\\\\\\pi\\'(s) $ is at least as large as this alternative expression, emphasizing that the policy $ \\\\\\\\pi\\' $ tends to improve or maintain the value of the state-action pairs compared to just following $ \\\\\\\\pi $.\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Off-policy Prediction via Importance Sampling\\\\n\\', \\'\\\\n\\', \\'Exploration is necessary to find optimal policies during training. On the other hand, we would not want to take exploratory actions after the training / during inference, but take the best ones. Therefore, the two policies differ. To make the distinction, the former is called the **behavior policy** and the latter is called the **target policy**. We can make the state and the action values aligned with the former or the latter, leading to two different classes of methods: **on-policy** and **off-policy**. \\\\n\\', \\'\\\\n\\', \"On-policy methods estimate the state and the action values for the behavior policy used during training, such as, the one that generates the training data / the experience. Off-policy methods estimate the state and the action values for a policy that is other than the behavior policy, such as the target policy. We ideally want to decouple exploration in training from value estimation. Let\\'s look into why this is the case in detail.\\\\n\", \\'\\\\n\\', \\'### Impact of on-policy methods on value function estimates\\\\n\\', \\'\\\\n\\', \\'Exploratory policies are usually not optimal as they take random actions once in a while for the sake of exploration. Since on-policy methods estimate the state and action values for the behavior policy, that sub-optimality is reflected in the value estimates.\\\\n\\', \\'\\\\n\\', \\'Consider the following modified grid world example to see how involving the effects of the exploratory actions in value estimation could be potentially harmful: The robot needs to choose between going left or right in state $2$ of $3 \\\\\\\\times 1$ grid world and between up and down in states $1$ and $3$. The robot follows the actions perfectly, so there is no randomness in the environment. This is illustrated in the following figure.\\\\n\\', \\'\\\\n\\', \\'> ![image.png](attachment:3b1977a0-60d4-476b-8f32-d83bea59e367.png)\\\\n\\', \\'\\\\n\\', \\'The robot has an $\\\\\\\\epsilon$-greedy policy, which suggests taking the best action with **0.99** chance, and an exploratory action with **0.01** chance. The best policy in state **3** is to go up with a high likelihood. In state **1**, the choice does not really matter. The state value estimates obtained in an on-policy manner will then be:\\\\n\\', \\'\\\\n\\', \\'$$ V(1) = 0 \\\\\\\\times p_{up} + 0 \\\\\\\\times p_{down} = 0$$\\\\n\\', \\'$$ V(3) = 1 \\\\\\\\times 0.99 + (-1000) \\\\\\\\times 0.01 = -9.01$$\\\\n\\', \\'\\\\n\\', \\'A policy obtained in an on-policy fashion for state 2 will suggest going left, towards state 1. On the other hand, in this deterministic environment, the robot could perfectly avoid the big penalty when there is no exploration involved. An on-policy method would fail to identify this since the exploration influences the value estimates and yield a sub-optimal policy as a result. On the other hand, in some cases, we may want the agent to take the impact of exploration into account if, for example, the samples are collected using a physical robot and some states are very costly to visit.\\\\n\\', \\'\\\\n\\', \\'### Importance sampling\\\\n\\', \\'\\\\n\\', \\'**Importance sampling** is a statistical technique used to estimate the expected value of a function when directly sampling from the target distribution is challenging or inefficient. Instead, you sample from a different (easier) distribution and correct for the difference using weights.\\\\n\\', \\'\\\\n\\', \\'#### Rolling a Die Example\\\\n\\', \\'\\\\n\\', \\'**Scenario:**\\\\n\\', \\'\\\\n\\', \\'- **Target distribution:** A fair six-sided die, where each side has a probability of $ \\\\\\\\frac{1}{6} $.\\\\n\\', \\'- **Behavior distribution:** A loaded die where the number 6 has a probability of $ \\\\\\\\frac{1}{2} $, and the other sides each have a probability of $ \\\\\\\\frac{1}{10} $.\\\\n\\', \\'\\\\n\\', \\'**Goal:** \\\\n\\', \\'\\\\n\\', \\'Estimate the expected value of rolling a fair die using samples drawn from the loaded die.\\\\n\\', \\'\\\\n\\', \\'### Steps:\\\\n\\', \\'\\\\n\\', \\'1. **Generate Samples:** Roll the loaded die multiple times and record the outcomes.\\\\n\\', \\'2. **Calculate Importance Weights:** For each outcome, calculate the weight that adjusts for the bias introduced by the loaded die:\\\\n\\', \\'   - If the outcome is 6:  \\\\n\\', \\'     $$\\\\n\\', \\'     \\\\\\\\text{Weight} = \\\\\\\\frac{\\\\\\\\text{Fair probability}}{\\\\\\\\text{Loaded probability}} = \\\\\\\\frac{1/6}{1/2} = \\\\\\\\frac{1}{3}\\\\n\\', \\'     $$\\\\n\\', \\'   - If the outcome is not 6 (say $ x $ where $ x = 1, 2, 3, 4, 5 $):  \\\\n\\', \\'     $$\\\\n\\', \\'     \\\\\\\\text{Weight} = \\\\\\\\frac{1/6}{1/10} = \\\\\\\\frac{5}{3}\\\\n\\', \\'     $$\\\\n\\', \\'3. **Calculate Weighted Average:** Multiply each outcome by its corresponding weight and then average the weighted outcomes to estimate the expected value.\\\\n\\', \\'\\\\n\\', \\'### Example Calculation:\\\\n\\', \\'\\\\n\\', \\'Suppose you roll the loaded die 10 times and observe the following outcomes: 6, 6, 1, 2, 3, 4, 5, 6, 6, 6.\\\\n\\', \\'\\\\n\\', \\'The weighted average is calculated as follows:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\text{Estimated Expected Value} = \\\\\\\\frac{1}{10} \\\\\\\\times \\\\\\\\left(\\\\\\\\frac{1}{3} \\\\\\\\times 6 + \\\\\\\\frac{1}{3} \\\\\\\\times 6 + \\\\\\\\frac{5}{3} \\\\\\\\times 1 + \\\\\\\\frac{5}{3} \\\\\\\\times 2 + \\\\\\\\frac{5}{3} \\\\\\\\times 3 + \\\\\\\\frac{5}{3} \\\\\\\\times 4 + \\\\\\\\frac{5}{3} \\\\\\\\times 5 + \\\\\\\\frac{1}{3} \\\\\\\\times 6 + \\\\\\\\frac{1}{3} \\\\\\\\times 6 + \\\\\\\\frac{1}{3} \\\\\\\\times 6\\\\\\\\right)\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'Simplifying the expression:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'= \\\\\\\\frac{1}{10} \\\\\\\\times (2 \\\\\\\\times 2 + 5 \\\\\\\\times 5 + 2 \\\\\\\\times 2) = \\\\\\\\frac{1}{10} \\\\\\\\times 35 = 3.5\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'This estimate is very close to the true expected value of a fair die, which is $ 3.5 $.\\\\n\\', \\'\\\\n\\', \\'#### Key Points:\\\\n\\', \\'\\\\n\\', \\'- **Importance Weights:** The weights correct for the bias introduced by sampling from the behavior distribution rather than the target distribution.\\\\n\\', \\'- **Use Cases:** Importance sampling is useful when the target distribution is difficult to sample from, or when you want to evaluate the expected value under different conditions.\\\\n\\', \\'- **Effectiveness:** The closer the behavior distribution is to the target distribution, the more effective the importance sampling will be, as this reduces the variance in the estimate.\\\\n\\', \\'\\\\n\\', \\'### From Book:\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'Almost all off-policy methods utilize **importance sampling**, a general technique for estimating expected values under one distribution given samples from another. We apply importance sampling to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the **importance-sampling ratio**. Given a starting state $S_t$, the probability of the subsequent state–action trajectory, $A_t, S_{t+1}, A_{t+1}, \\\\\\\\dots, S_T$, occurring under any policy $\\\\\\\\pi$ is\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\Pr \\\\\\\\{A_t, S_{t+1}, A_{t+1}, \\\\\\\\dots, S_T \\\\\\\\mid S_t, A_{t:T-1} \\\\\\\\sim \\\\\\\\pi \\\\\\\\} \\\\n\\', \\'= \\\\\\\\pi(A_t \\\\\\\\mid S_t) p(S_{t+1} \\\\\\\\mid S_t, A_t) \\\\\\\\pi(A_{t+1} \\\\\\\\mid S_{t+1}) \\\\\\\\dots p(S_T \\\\\\\\mid S_{T-1}, A_{T-1})\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'= \\\\\\\\prod_{k=t}^{T-1} \\\\\\\\pi(A_k \\\\\\\\mid S_k) p(S_{k+1} \\\\\\\\mid S_k, A_k),\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'where $p$ here is the state-transition probability function defined by (3.4). Thus, the relative probability of the trajectory under the target and behavior policies (the importance-sampling ratio) is\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\rho_{t:T-1} \\\\\\\\doteq \\\\\\\\frac{\\\\\\\\prod_{k=t}^{T-1} \\\\\\\\pi(A_k \\\\\\\\mid S_k) p(S_{k+1} \\\\\\\\mid S_k, A_k)}{\\\\\\\\prod_{k=t}^{T-1} b(A_k \\\\\\\\mid S_k) p(S_{k+1} \\\\\\\\mid S_k, A_k)} = \\\\\\\\prod_{k=t}^{T-1} \\\\\\\\frac{\\\\\\\\pi(A_k \\\\\\\\mid S_k)}{b(A_k \\\\\\\\mid S_k)}. \\\\\\\\tag{5.3}\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'With that, we go from estimating the expectation under the behavior policy\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'E \\\\\\\\left[ G_t \\\\\\\\mid s \\\\\\\\right] = v_b(s)\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'to estimating the expectation under the target policy\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'E \\\\\\\\left[ \\\\\\\\rho_{t:T-1} G_t \\\\\\\\mid s \\\\\\\\right] = v_\\\\\\\\pi(s).\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'#### Time-Step Numbering (Pre-requisite Concept)\\\\n\\', \\'\\\\n\\', \"**Time-step numbering** is a systematic method used in reinforcement learning to assign a sequential identifier to each action an agent takes during its interaction with an environment. This numbering is typically continuous across different episodes, providing a unified timeline that spans the agent\\'s entire learning experience.\\\\n\", \\'\\\\n\\', \\'**Example: A Grid World**\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'**Episode 1:**\\\\n\\', \\'* The agent starts at position (0, 0).\\\\n\\', \\'* It takes the action \"right\" and moves to (0, 1).\\\\n\\', \\'* It then takes the action \"up\" and moves to (1, 1).\\\\n\\', \\'* The episode concludes.\\\\n\\', \\'\\\\n\\', \\'**Episode 2:**\\\\n\\', \\'* The agent starts again at (0, 0).\\\\n\\', \\'* It takes the action \"left\" but stays at (0, 0) since it\\\\\\'s already at the edge.\\\\n\\', \\'* It then takes the action \"up\" and moves to (1, 0).\\\\n\\', \\'* The episode concludes.\\\\n\\', \\'\\\\n\\', \\'**Time-Step Numbering:**\\\\n\\', \\'* **Episode 1:**\\\\n\\', \\'  * **Step 1:** Agent at (0, 0)\\\\n\\', \\'  * **Step 2:** Agent at (0, 1)\\\\n\\', \\'  * **Step 3:** Agent at (1, 1)\\\\n\\', \\'* **Episode 2:**\\\\n\\', \\'  * **Step 4:** Agent at (0, 0)\\\\n\\', \\'  * **Step 5:** Agent at (1, 0)\\\\n\\', \\'\\\\n\\', \\'In this scenario, time steps are numbered sequentially from the first action in the first episode and continue uninterrupted into subsequent episodes. This continuous numbering system allows us to reference any specific action or state in the entire training process using a single time-step index.\\\\n\\', \\'\\\\n\\', \\'#### Benefits of Time-Step Numbering\\\\n\\', \\'\\\\n\\', \\'* **Unified Timeline:** It offers a consistent way to track and represent the entire learning process across multiple episodes, which simplifies analysis and understanding.\\\\n\\', \\'* **Algorithm Compatibility:** Many reinforcement learning algorithms, such as temporal difference learning or Q-learning, depend on time-step numbering to perform their calculations and updates correctly.\\\\n\\', \"* **Enhanced Data Analysis:** Continuous time-step numbering facilitates comprehensive data analysis and visualization, enabling us to monitor the agent\\'s progress and behavior over time, across all episodes.\\\\n\", \\'\\\\n\\', \\'By adopting time-step numbering, we gain a clearer perspective on how an agent learns and interacts with its environment, making it easier to develop, debug, and optimize reinforcement learning algorithms.\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'Now we are ready to give a Monte Carlo algorithm that averages returns from a batch of observed episodes following policy $ b $ to estimate $ v_{\\\\\\\\pi}(s) $. **It is convenient here to number time steps in a way that increases across episode boundaries.** That is, if the first episode of the batch ends in a terminal state at time 100, then the next episode begins at time $ t = 101 $. This enables us to use time-step numbers to refer to particular steps in particular episodes. \\\\n\\', \\'\\\\n\\', \\'- In particular, we can define the set of all time steps in which state $ s $ is visited, denoted $ \\\\\\\\mathcal{J}(s) $. This is for an every-visit method; for a first-visit method, $ \\\\\\\\mathcal{J}(s) $ would only include time steps that were first visits to $ s $ within their episodes. \\\\n\\', \\'- Also, let $ T(t) $ denote the first time of termination following time $ t $, and $ G_t $ denote the return after $ t $ up through $ T(t) $. Then $ \\\\\\\\{ G_t \\\\\\\\}_{t \\\\\\\\in \\\\\\\\mathcal{J}(s)} $ are the returns that pertain to state $ s $, and $ \\\\\\\\{ \\\\\\\\rho_{t:T(t) - 1} \\\\\\\\}_{t \\\\\\\\in \\\\\\\\mathcal{J}(s)} $ are the corresponding importance-sampling ratios.\\\\n\\', \\'- To estimate $ v_{\\\\\\\\pi}(s) $, we simply scale the returns by the ratios and average the results:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V(s) \\\\\\\\doteq \\\\\\\\frac{\\\\\\\\sum_{t \\\\\\\\in \\\\\\\\mathcal{J}(s)} \\\\\\\\rho_{t:T(t) - 1} G_t}{|\\\\\\\\mathcal{J}(s)|}.\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'When importance sampling is done as a simple average in this way it is called **ordinary importance sampling**.\\\\n\\', \\'\\\\n\\', \\'An important alternative is **weighted importance sampling**, which uses a weighted average, defined as\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V(s) \\\\\\\\doteq \\\\\\\\frac{\\\\\\\\sum_{t \\\\\\\\in \\\\\\\\mathcal{J}(s)} \\\\\\\\rho_{t:T(t) - 1} G_t}{\\\\\\\\sum_{t \\\\\\\\in \\\\\\\\mathcal{J}(s)} \\\\\\\\rho_{t:T(t) - 1}},\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'or zero if the denominator is zero. \\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'#### **Comparison of Ordinary and Weighted Importance Sampling in Off-Policy Learning**\\\\n\\', \\'\\\\n\\', \"**Importance sampling** is a technique used in off-policy learning to estimate the value of a target policy using data generated from a different behavior policy. There are two primary methods: **ordinary importance sampling** and **weighted importance sampling**. Here\\'s a breakdown of their differences:\\\\n\", \\'\\\\n\\', \\'**1. Weighted Importance Sampling**\\\\n\\', \\'\\\\n\\', \\'- **Bias**: This method is **biased**, meaning the estimate does not converge exactly to the true value. However, the bias decreases as the sample size increases, ultimately converging to zero.\\\\n\\', \\'  \\\\n\\', \\'- **Variance**: **Weighted importance sampling** typically has **lower variance** compared to ordinary importance sampling, especially when the variance of the importance sampling ratios is high. This lower variance results in more stable and reliable estimates.\\\\n\\', \\'\\\\n\\', \\'**2. Ordinary Importance Sampling**\\\\n\\', \\'\\\\n\\', \\'- **Bias**: This method is **unbiased**, ensuring that the estimate is correct in expectation over many samples. In theory, this makes it a strong candidate for accurate estimation.\\\\n\\', \\'\\\\n\\', \\'- **Variance**: Despite its unbiased nature, **ordinary importance sampling** can suffer from **extremely high variance**. In some cases, the variance can be unbounded, leading to highly unstable estimates, especially when the importance sampling ratios are large.\\\\n\\', \\'\\\\n\\', \\'**A Numerical Example**\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'**Scenario:**\\\\n\\', \\'\\\\n\\', \\'You want to estimate the expected value of a random variable $ X $ under a **target distribution** using samples collected from a **behavior distribution**.\\\\n\\', \\'\\\\n\\', \\'**Distributions:**\\\\n\\', \\'\\\\n\\', \\'- **Target Distribution:**\\\\n\\', \\'  - Mean: 5\\\\n\\', \\'  - Variance: 1\\\\n\\', \\'\\\\n\\', \\'- **Behavior Distribution:**\\\\n\\', \\'  - Mean: 4\\\\n\\', \\'  - Variance: 2\\\\n\\', \\'\\\\n\\', \\'**Samples:**\\\\n\\', \\'\\\\n\\', \\'Assume you have collected the following five samples from the behavior distribution:\\\\n\\', \\'\\\\n\\', \\'- $ X $ values: 2, 4, 5, 6, 9\\\\n\\', \\'\\\\n\\', \\'**Importance Sampling Ratios:**\\\\n\\', \\'\\\\n\\', \"These ratios represent how likely each sample is under the target distribution relative to the behavior distribution. For simplicity, we\\'ll assume we calculated the ratios using the probability density functions (PDFs) of the two distributions:\\\\n\", \\'\\\\n\\', \\'| Sample | Value ($ X $) | Importance Ratio ($ \\\\\\\\rho $) | Weighted Value ($ \\\\\\\\rho \\\\\\\\times X $) |\\\\n\\', \\'|--------|-----------------|--------------------------------|--------------------------------------|\\\\n\\', \\'| 1      | 2               | 0.5                            | 1.0                                  |\\\\n\\', \\'| 2      | 4               | 0.8                            | 3.2                                  |\\\\n\\', \\'| 3      | 5               | 1.0                            | 5.0                                  |\\\\n\\', \\'| 4      | 6               | 1.2                            | 7.2                                  |\\\\n\\', \\'| 5      | 9               | 0.3                            | 2.7                                  |\\\\n\\', \\'\\\\n\\', \\'**Estimates:**\\\\n\\', \\'\\\\n\\', \\'**1. Ordinary Importance Sampling**\\\\n\\', \\'\\\\n\\', \\'For **ordinary importance sampling**, we calculate the estimate as the weighted average of the returns, where each return is multiplied by its corresponding importance ratio:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_{\\\\\\\\text{ordinary}}(X) = \\\\\\\\frac{1}{N} \\\\\\\\sum_{i=1}^{N} \\\\\\\\rho_i \\\\\\\\times X_i\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'where $ N $ is the number of samples.\\\\n\\', \\'\\\\n\\', \\'Using our data:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_{\\\\\\\\text{ordinary}}(X) = \\\\\\\\frac{1}{5} \\\\\\\\times (0.5 \\\\\\\\times 2 + 0.8 \\\\\\\\times 4 + 1.0 \\\\\\\\times 5 + 1.2 \\\\\\\\times 6 + 0.3 \\\\\\\\times 9)\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_{\\\\\\\\text{ordinary}}(X) = \\\\\\\\frac{1}{5} \\\\\\\\times (1.0 + 3.2 + 5.0 + 7.2 + 2.7) = \\\\\\\\frac{1}{5} \\\\\\\\times 19.1 = 3.82\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'**2. Weighted Importance Sampling**\\\\n\\', \\'\\\\n\\', \\'For **weighted importance sampling**, we calculate the estimate as the sum of the weighted values divided by the sum of the importance ratios:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_{\\\\\\\\text{weighted}}(X) = \\\\\\\\frac{\\\\\\\\sum_{i=1}^{N} \\\\\\\\rho_i \\\\\\\\times X_i}{\\\\\\\\sum_{i=1}^{N} \\\\\\\\rho_i}\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'Using our data:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_{\\\\\\\\text{weighted}}(X) = \\\\\\\\frac{1.0 + 3.2 + 5.0 + 7.2 + 2.7}{0.5 + 0.8 + 1.0 + 1.2 + 0.3}\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_{\\\\\\\\text{weighted}}(X) = \\\\\\\\frac{19.1}{3.8} = 5.03\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'**Comparison and Observations:**\\\\n\\', \\'\\\\n\\', \\'- **Ordinary Importance Sampling** estimate: $ V_{\\\\\\\\text{ordinary}}(X) = 3.82 $\\\\n\\', \\'- **Weighted Importance Sampling** estimate: $ V_{\\\\\\\\text{weighted}}(X) = 5.03 $\\\\n\\', \\'\\\\n\\', \\'**Key Observations:**\\\\n\\', \\'\\\\n\\', \\'1. **Ordinary Importance Sampling**:\\\\n\\', \\'   - The estimate $ 3.82 $ is significantly lower than the true mean (5). This method is unbiased over many samples, but its estimate can be heavily influenced by the specific importance ratios, leading to high variance.\\\\n\\', \\'    - **Bias**:\\\\n\\', \\'        > **In the Example**: The ordinary importance sampling estimate was $ V_{\\\\\\\\text{ordinary}}(X) = 3.82 $, which deviates more significantly from the true mean of 5 compared to the weighted estimate. While this particular estimate is lower than the true mean, in expectation over many samples, ordinary importance sampling would be unbiased. This means that if you averaged the estimates from many different sample sets, the average would converge to the true mean of 5.\\\\n\\', \\'\\\\n\\', \\'    - **Variance**:\\\\n\\', \"      > **In the Example**: The estimate from ordinary importance sampling is significantly off (3.82 vs. 5), which reflects the high variance of this method. The ordinary method directly multiplies each sample by its importance ratio, which can lead to very unstable results, especially if some ratios are very large or small. In extreme cases, these large ratios could lead to even more extreme estimates, highlighting the method\\'s potential for unbounded variance.\\\\n\", \\'\\\\n\\', \\'2. **Weighted Importance Sampling**:\\\\n\\', \\'   - The estimate $ 5.03 $ is very close to the true mean (5). By normalizing the weighted values by the sum of the ratios, this method reduces the influence of any single ratio, leading to a more stable estimate, albeit with a small bias.\\\\n\\', \\'   - **Bias**:\\\\n\\', \\'    > **In the Example**: The weighted importance sampling estimate was $ V_{\\\\\\\\text{weighted}}(X) = 5.03 $, which is very close to the true mean of 5, but not exact. This illustrates the concept of bias in weighted importance sampling. While the estimate is close, it is slightly biased—meaning it doesn’t converge exactly to the true mean with finite samples. However, this bias tends to decrease as the number of samples increases, eventually converging to the true value as the sample size becomes very large.\\\\n\\', \\'\\\\n\\', \\'    - **Variance**:\\\\n\\', \\'    > **In the Example**: Despite the slight bias, the estimate from weighted importance sampling is more stable. If you consider the variations in the importance ratios (e.g., 0.5 to 1.2), the weighted importance sampling normalizes these ratios, leading to a more reliable estimate. The variance is lower compared to what it might have been with ordinary importance sampling, especially if the ratios were more extreme. This is why weighted importance sampling is often preferred in practice, as it provides a good balance between bias and variance.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'**Key Takeaway:**\\\\n\\', \\'\\\\n\\', \\'- **Ordinary Importance Sampling** provides an unbiased estimate in expectation, but it can have high variance, especially when the importance ratios vary widely.\\\\n\\', \\'- **Weighted Importance Sampling** reduces variance by normalizing the contribution of each sample, often providing a more stable and reliable estimate, though with a slight bias.\\\\n\\', \\'\\\\n\\', \\'This example illustrates how the two methods can yield different results depending on the distribution of the importance ratios and how they affect the estimates.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'#### **Overall Assessment**\\\\n\\', \\'\\\\n\\', \\'While **ordinary importance sampling** is theoretically attractive due to its unbiased nature, its potential for high variance often makes it impractical for reliable estimation. On the other hand, **weighted importance sampling** offers a more balanced approach by trading off some bias for a significant reduction in variance, making it the preferred method in many practical applications.\\\\n\\', \\'\\\\n\\', \\'**Note**: Even in cases where the variance of the importance sampling ratios is infinite, the weighted importance sampling estimator can still have a lower overall variance, further emphasizing its practical utility.\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"Let\\'s close this section with a few notes on Importance sampling:\\\\n\", \\'\\\\n\\', \\'- In order to be able to use the samples obtained under behavior policy $b$ to estimate the state and action values under $\\\\\\\\pi$, it is required to have $b(a \\\\\\\\mid s) > 0$ if $\\\\\\\\pi(a \\\\\\\\mid s) > 0$. Since we would not want to impose what $a$ could be taken under the target policy, $b$ is usually chosen as a soft policy.\\\\n\\', \\'- In the weighted Importance sampling, if the denominator is zero, then the estimate is considered zero.\\\\n\\', \\'- The formulas above ignored the discount in the return, which is a bit more complicated to deal with.\\\\n\\', \\'- The observed trajectories can be chopped with respect to the first-visit or every-visit rule.\\\\n\\', \\'- The ordinary Importance sampling is unbiased but can have very high variance. The weighted Importance sampling, on the other hand, is biased but usually has much lower variance, hence preferred in practice.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Incremental Implementation\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'Suppose we have a sequence of returns $ G_1, G_2, \\\\\\\\dots, G_{n-1} $, all starting in the same state, and each with a corresponding random weight $ W_i $ (e.g., $ W_i = \\\\\\\\rho_{t:T(t)-1} $). We want to estimate:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_n = \\\\\\\\frac{\\\\\\\\sum_{k=1}^{n-1} W_k G_k}{\\\\\\\\sum_{k=1}^{n-1} W_k}, \\\\\\\\quad n \\\\\\\\geq 2\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'and update this estimate as we obtain new returns.\\\\n\\', \\'\\\\n\\', \\'**Update Rule:**\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V_{n+1} = V_n + \\\\\\\\frac{W_n}{C_n} \\\\\\\\cdot (G_n - V_n), \\\\\\\\quad n \\\\\\\\geq 1\\\\n\\', \\'$$\\\\n\\', \\'$$\\\\n\\', \\'C_{n+1} = C_n + W_{n+1}, \\\\\\\\quad n \\\\\\\\geq 0\\\\n\\', \\'$$\\\\n\\', \\'where:\\\\n\\', \\'- $ C_n $ is the cumulative sum of the weights for the first $ n $ returns.\\\\n\\', \\'- $ V_1 $ is arbitrary and need not be specified.\\\\n\\', \\'- $ C_0 = 0 $.\\\\n\\', \\'\\\\n\\', \\'$$ V_{n+1} = \\\\\\\\frac{w_n G_n + w_{n-1} G_{n-1} + \\\\\\\\dots + w_1 G_1}{w_n + w_{n-1} + \\\\\\\\dots + w_1} $$\\\\n\\', \\'\\\\n\\', \\'$$ C_{n+1} = C_n + w_{n+1}, \\\\\\\\quad   \\\\\\\\quad C_0 = 0 $$\\\\n\\', \\'\\\\n\\', \\'$$ V_n = \\\\\\\\frac{\\\\\\\\sum_{k=1}^{n-1} w_k G_k}{\\\\\\\\sum_{k=1}^{n-1} w_k} \\\\\\\\Rightarrow V_n = \\\\\\\\frac{\\\\\\\\sum_{k=1}^{n-1} w_k G_k}{C_{n-1}}, \\\\\\\\quad \\\\\\\\text{where,} \\\\\\\\quad C_{n-1} = \\\\\\\\sum_{k=1}^{n-1} w_k$$\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'$$ \\\\\\\\therefore \\\\\\\\quad V_n \\\\\\\\times C_{n-1} = \\\\\\\\sum_{k=1}^{n-1} w_k G_k $$\\\\n\\', \\'\\\\n\\', \\'$$ \\\\\\\\text{Now,} \\\\\\\\quad V_{n+1} - V_n = \\\\\\\\frac{V_n C_{n-1} + w_n G_n - V_n (C_{n-1} + w_n)}{C_{n-1} + w_n} $$\\\\n\\', \\'\\\\n\\', \\'$$ V_{n+1} = V_n + \\\\\\\\frac{w_n}{C_n} \\\\\\\\left[G_n - V_n\\\\\\\\right] $$\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Example 5.5: Infinite Variance\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'<img src=\"attachment:45dd198f-6fd3-4272-9180-92ab491a8ee0.png\"  width=\"50%\" height=\"50%\" style=\"margin-left:auto; margin-right:auto\">\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'Given that $\\\\\\\\gamma = 1$ (where $\\\\\\\\gamma$ is the discount factor), we can now analyze the situation to determine the optimal policy $\\\\\\\\pi(s)$, specifically $\\\\\\\\pi(\\\\\\\\text{left} \\\\\\\\mid s)$.\\\\n\\', \\'\\\\n\\', \\'### Scenario:\\\\n\\', \\'- **Action \"left\":**\\\\n\\', \\'  - With probability 0.1, the agent transitions to the terminal state and receives a reward of $ +1 $.\\\\n\\', \\'  - With probability 0.9, the agent stays in the same state $ s $ and receives a reward of 0.\\\\n\\', \\'- **Action \"right\":**\\\\n\\', \\'  - The agent immediately transitions to the terminal state and receives a reward of 0.\\\\n\\', \\'\\\\n\\', \\'### Expected Value of Actions\\\\n\\', \\'\\\\n\\', \\'#### 1. **Action \"left\":**\\\\n\\', \\'The expected reward when choosing \"left\" is calculated as follows:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\text{Expected Reward for \"left\"} = 0.1 \\\\\\\\times 1 + 0.9 \\\\\\\\times 0 + \\\\\\\\gamma \\\\\\\\times (0.9 \\\\\\\\times V(s))\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'Since $\\\\\\\\gamma = 1$, this becomes:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\text{Expected Reward for \"left\"} = 0.1 + 0.9 \\\\\\\\times V(s)\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'#### 2. **Action \"right\":**\\\\n\\', \\'The expected reward when choosing \"right\" is simply the immediate reward:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\text{Expected Reward for \"right\"} = 0 + 1 \\\\\\\\times 0 = 0\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'### Bellman Equation for $ V(s) $\\\\n\\', \\'For an optimal policy $\\\\\\\\pi^*$:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V(s) = \\\\\\\\max \\\\\\\\left(0.1 + 0.9 \\\\\\\\times V(s), 0\\\\\\\\right)\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'To solve for $ V(s) $, we set:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V(s) = 0.1 + 0.9 \\\\\\\\times V(s)\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V(s) - 0.9 \\\\\\\\times V(s) = 0.1\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'0.1 \\\\\\\\times V(s) = 0.1\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'V(s) = 1\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'### Determining $\\\\\\\\pi(\\\\\\\\text{left} \\\\\\\\mid s)$\\\\n\\', \\'\\\\n\\', \"Now, let\\'s compare the expected values of the two actions:\\\\n\", \\'\\\\n\\', \\'- **For \"left\":**  \\\\n\\', \\'  $$\\\\n\\', \\'  \\\\\\\\text{Expected Reward for \"left\"} = 0.1 + 0.9 \\\\\\\\times 1 = 1\\\\n\\', \\'  $$\\\\n\\', \\'\\\\n\\', \\'- **For \"right\":**  \\\\n\\', \\'  $$\\\\n\\', \\'  \\\\\\\\text{Expected Reward for \"right\"} = 0\\\\n\\', \\'  $$\\\\n\\', \\'\\\\n\\', \\'Since the expected reward for choosing \"left\" (1) is greater than the expected reward for choosing \"right\" (0), the optimal policy is to always choose \"left\". Therefore:\\\\n\\', \\'\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\pi(\\\\\\\\text{left} \\\\\\\\mid s) = 1\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'This means the agent should always choose the \"left\" action when in state $ s $ under the optimal policy with $\\\\\\\\gamma = 1$.\\\\n\\', \\'\\\\n\\', \\'To compute the expected square of the importance-sampling-scaled return, we need to account for each possible episode length, compute the probability of each episode, and multiply it by the square of the importance-sampling ratio for that episode.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'### 1. **Episode Structure**\\\\n\\', \\'\\\\n\\', \\'In the given environment:\\\\n\\', \\'- The episode starts at state $ s $ and terminates upon reaching the terminal state.\\\\n\\', \\'- Two actions are possible at state $ s $:\\\\n\\', \\'  - **Left**: This action returns to state $ s $ with probability 0.9 and gives a reward $ R = 0 $, or it terminates with probability 0.1 and gives a reward $ R = +1 $.\\\\n\\', \\'  - **Right**: This action takes the agent to the terminal state with probability 1 and gives a reward $ R = 0 $.\\\\n\\', \\'\\\\n\\', \\'Thus, episodes can have different lengths depending on how long the agent stays in state $ s $ before terminating (through \"left\" leading to the terminal state).\\\\n\\', \\'\\\\n\\', \\'### 2. **Importance-Sampling Ratio**\\\\n\\', \\'\\\\n\\', \\'The importance-sampling ratio is the product of the ratios $ \\\\\\\\frac{\\\\\\\\pi(A_t | S_t)}{b(A_t | S_t)} $ at each time step. From the diagram:\\\\n\\', \\'- $ \\\\\\\\pi(\\\\\\\\text{left} | s) = 1 $, i.e., the target policy always chooses \"left\".\\\\n\\', \\'- $ b(\\\\\\\\text{left} | s) = \\\\\\\\frac{1}{2} $, i.e., the behavior policy chooses \"left\" with probability $ \\\\\\\\frac{1}{2} $.\\\\n\\', \\'\\\\n\\', \\'Therefore, for an episode of length $ T $ where the agent always takes the \"left\" action at each step, the importance-sampling ratio for that episode is $ 2^T $.\\\\n\\', \\'\\\\n\\', \\'### 3. **Probability of an Episode**\\\\n\\', \\'\\\\n\\', \\'Let’s calculate the probability of an episode of length $ T $. For an episode to last $ T $ steps:\\\\n\\', \\'- In each of the first $ T-1 $ steps, the agent must take the \"left\" action and return to state $ s $, which happens with probability $ 0.9 $.\\\\n\\', \\'- On the $ T $-th step, the agent takes the \"left\" action and transitions to the terminal state, which happens with probability $ 0.1 $.\\\\n\\', \\'\\\\n\\', \\'Thus, the probability of an episode of length $ T $ is:\\\\n\\', \\'$$\\\\n\\', \\'P(\\\\\\\\text{episode of length } T) = {{\\\\\\\\left( \\\\\\\\frac{1}{2} \\\\\\\\right)}^{T}} \\\\\\\\cdot 0.1 \\\\\\\\cdot 0.9^{T-1}.\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'### 4. **Expected Square of the Importance-Sampling-Scaled Return**\\\\n\\', \\'\\\\n\\', \\'Now, we can calculate the expected square of the importance-sampling-scaled return by summing over all possible episode lengths. For each length $ T $, the contribution to the expectation is the product of:\\\\n\\', \\'- The probability of an episode of length $ T $, which is $ {{\\\\\\\\left( \\\\\\\\frac{1}{2} \\\\\\\\right)}^{T}} \\\\\\\\cdot 0.1 \\\\\\\\cdot 0.9^{T-1}$.\\\\n\\', \\'- The square of the importance-sampling ratio, which is $ (2^T)^2 $.\\\\n\\', \\'\\\\n\\', \\'Therefore, the expected square is given by:\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\mathbb{E} \\\\\\\\left[ \\\\\\\\left( \\\\\\\\prod_{t=0}^{T-1} \\\\\\\\frac{\\\\\\\\pi(A_t | S_t)}{b(A_t | S_t)} G_0 \\\\\\\\right)^2 \\\\\\\\right] = \\\\\\\\sum_{T=1}^{\\\\\\\\infty} \\\\\\\\left( 0.9^{T-1} \\\\\\\\cdot 0.1 \\\\\\\\right) \\\\\\\\cdot (2^T)^2 \\\\\\\\cdot {{\\\\\\\\left( \\\\\\\\frac{1}{2} \\\\\\\\right)}^{T}}.\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'### 6. **Summing the Series**\\\\n\\', \\'\\\\n\\', \\'We need to compute the infinite series:\\\\n\\', \\'$$\\\\n\\', \\'\\\\\\\\sum_{T=1}^{\\\\\\\\infty} {{\\\\\\\\left( \\\\\\\\frac{1}{2} \\\\\\\\right)}^{T}} \\\\\\\\cdot 0.1 \\\\\\\\cdot 0.9^{T-1} \\\\\\\\cdot (2^T)^2.\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'Factor out the constant $ 0.1 $:\\\\n\\', \\'$$\\\\n\\', \\'0.1 \\\\\\\\cdot \\\\\\\\sum_{T=1}^{\\\\\\\\infty} 0.9^{T-1} \\\\\\\\cdot 2^T = 0.1 \\\\\\\\cdot \\\\\\\\sum_{T=1}^{\\\\\\\\infty} (0.9 \\\\\\\\cdot 2)^{T-1} \\\\\\\\cdot 2.\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'This simplifies to:\\\\n\\', \\'$$\\\\n\\', \\'0.2 \\\\\\\\cdot \\\\\\\\sum_{T=1}^{\\\\\\\\infty} (1.8)^{T-1}.\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'Recognizing this as a geometric series with common ratio $ 1.8 $, the sum is infinite because the ratio $ 1.8 > 1 $, meaning the series diverges.\\\\n\\', \\'\\\\n\\', \\'### Conclusion:\\\\n\\', \\'\\\\n\\', \\'The expected square of the importance-sampling-scaled return is **infinite** because the geometric series diverges due to the fact that the importance-sampling ratio grows exponentially while the episode probabilities decay geometrically, but not fast enough to prevent divergence. This confirms that the expectation is infinite.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\"### Book\\'s Calculation:\\\\n\", \\'\\\\n\\', \\'1. **Length-1 Episode**:\\\\n\\', \\'   - The agent chooses \"left\" in the first time step and terminates immediately, which happens with probability $ 0.1 $.\\\\n\\', \\'   - The importance-sampling ratio for this episode is $ \\\\\\\\frac{1}{0.5} = 2 $, and squaring it gives $ 2^2 = 4 $.\\\\n\\', \\'   - The contribution for this length-1 episode is:\\\\n\\', \\'     $$\\\\n\\', \\'     \\\\\\\\frac{1}{2} \\\\\\\\cdot 0.1 \\\\\\\\cdot 4 = 0.1 \\\\\\\\cdot 2.\\\\n\\', \\'     $$\\\\n\\', \\'\\\\n\\', \\'2. **Length-2 Episode**:\\\\n\\', \\'   - The agent chooses \"left\" twice, with the first action returning to state $ s $ (probability $ 0.9 $) and the second action terminating (probability $ 0.1 $).\\\\n\\', \\'   - The importance-sampling ratio for each step is $ \\\\\\\\frac{1}{0.5} = 2 $, and the product of the ratios over two steps is $ 2 \\\\\\\\times 2 = 4 $, which when squared gives $ 4^2 = 16 $.\\\\n\\', \\'   - The contribution for this length-2 episode is:\\\\n\\', \\'     $$\\\\n\\', \\'     \\\\\\\\frac{1}{2} \\\\\\\\cdot 0.9 \\\\\\\\cdot \\\\\\\\frac{1}{2} \\\\\\\\cdot 0.1 \\\\\\\\cdot 16 = 0.1 \\\\\\\\cdot 0.9 \\\\\\\\cdot 2^2.\\\\n\\', \\'     $$\\\\n\\', \\'\\\\n\\', \\'3. **Length-3 Episode**:\\\\n\\', \\'   - The agent chooses \"left\" three times, with the first two actions returning to state $ s $ (probability $ 0.9 $ each time) and the third action terminating (probability $ 0.1 $).\\\\n\\', \\'   - The importance-sampling ratio for each step is $ \\\\\\\\frac{1}{0.5} = 2 $, and the product of the ratios over three steps is $ 2 \\\\\\\\times 2 \\\\\\\\times 2 = 8 $, which when squared gives $ 8^2 = 64 $.\\\\n\\', \\'   - The contribution for this length-3 episode is:\\\\n\\', \\'     $$\\\\n\\', \\'     \\\\\\\\frac{1}{2} \\\\\\\\cdot 0.9 \\\\\\\\cdot \\\\\\\\frac{1}{2} \\\\\\\\cdot 0.9 \\\\\\\\cdot \\\\\\\\frac{1}{2} \\\\\\\\cdot 0.1 \\\\\\\\cdot 64 = 0.1 \\\\\\\\cdot 0.9^2 \\\\\\\\cdot 2^3.\\\\n\\', \\'     $$\\\\n\\', \\'\\\\n\\', \\'4. **General Term for Length $ k+1 $**:\\\\n\\', \\'   - For an episode of length $ k+1 $, the agent takes $ k $ \"left\" actions that return to state $ s $ (probability $ 0.9 $ each time) and terminates after the $ k $-th step.\\\\n\\', \\'   - The importance-sampling ratio is $ 2^{k+1} $.\\\\n\\', \\'   - The contribution for an episode of length $ k+1 $ is:\\\\n\\', \\'     $$\\\\n\\', \\'     \\\\\\\\frac{1}{2} \\\\\\\\cdot 0.9^k \\\\\\\\cdot 0.1 \\\\\\\\cdot (2^{k+1}).\\\\n\\', \\'     $$\\\\n\\', \\'\\\\n\\', \\'### Final Series:\\\\n\\', \\'The book sums the contributions for all possible episode lengths:\\\\n\\', \\'$$\\\\n\\', \\'0.1 \\\\\\\\sum_{k=0}^{\\\\\\\\infty} 0.9^k \\\\\\\\cdot 2^{(k+1)} = 0.2 \\\\\\\\sum_{k=0}^{\\\\\\\\infty} 1.8^k.\\\\n\\', \\'$$\\\\n\\', \\'\\\\n\\', \\'This is a geometric series with ratio $ 1.8 > 1 $, which diverges. Thus, the expected square of the importance-sampling-scaled return is **infinite**.\\\\n\\', \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### On-Policy vs. Off-Policy Methods\\\\n\\', \\'\\\\n\\', \\'**On-policy methods** in reinforcement learning use the same policy to generate data and update estimates. This means the agent is learning from its own experiences.\\\\n\\', \\'\\\\n\\', \\'**Off-policy methods**, on the other hand, use a different policy to generate data than the one being learned. This allows for more flexibility, as the agent can learn from a wider range of experiences, even those generated by a more exploratory or suboptimal policy.\\\\n\\', \\'\\\\n\\', \\'### Key Differences\\\\n\\', \\'\\\\n\\', \\'| Feature | On-Policy Methods | Off-Policy Methods |\\\\n\\', \\'|---|---|---|\\\\n\\', \\'| **Data Generation Policy** | Same as the target policy | Different from the target policy |\\\\n\\', \\'| **Learning from Experience** | Learns from its own actions | Learns from a wider range of actions, including those generated by a different policy |\\\\n\\', \\'| **Flexibility** | Less flexible, as the agent is limited to its own experiences | More flexible, allowing for learning from diverse experiences |\\\\n\\', \\'| **Examples** | SARSA, TD Learning | Q-learning, Deep Q-Networks (DQN) |\\\\n\\', \\'\\\\n\\', \\'**In conclusion,** on-policy methods are simpler to implement but less flexible, while off-policy methods offer more flexibility but can be more complex to analyze and implement. The choice between on-policy and off-policy methods depends on the specific problem and the desired level of flexibility.\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Ch_5_MCM_Example.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# Random Walk Environment and Monte Carlo First-Visit Policy Evaluation\\\\n\\', \\'\\\\n\\', \"The Random Walk is a classic problem in Reinforcement Learning (RL) that serves as an excellent introductory example for understanding fundamental concepts like states, actions, rewards, and value functions. It\\'s simple enough to grasp quickly but complex enough to demonstrate key RL algorithms.\\\\n\", \\'\\\\n\\', \\'## 1. The Random Walk Environment\\\\n\\', \\'\\\\n\\', \\'Our Random Walk environment is a one-dimensional line of states. Imagine a series of numbered squares. The agent starts at a non-terminal state and can move either left or right. The goal is to reach a specific \"goal\" state to receive a reward.\\\\n\\', \\'\\\\n\\', \\'### Environment Definition (`RandomWalkEnv` Class)\\\\n\\', \\'\\\\n\\', \\'The `RandomWalkEnv` class defines the structure and dynamics of our environment:\\\\n\\', \\'\\\\n\\', \\'* **States**: We have non-terminal states from 1 to 5.\\\\n\\', \\'* **Terminal States**: States 0 and 6 are absorbing (terminal) states. Once the agent reaches one of these, the episode ends.\\\\n\\', \\'* **Actions**: The agent has two possible actions:\\\\n\\', \\'    * `0`: Move Left (-1)\\\\n\\', \\'    * `1`: Move Right (+1)\\\\n\\', \\'* **Reward**: The agent receives a reward of `+1` only if it reaches state 6. All other transitions, including reaching state 0, yield a reward of `0`.\\\\n\\', \\'* **Reset**: The environment can be reset, placing the agent in a random non-terminal state (1 to 5) to start a new episode.\\\\n\\', \\'* **Step**: Given an action, the `step` method calculates the `next_state`, the `reward` received, and whether the episode is `done` (i.e., a terminal state has been reached).\\\\n\\', \\'\\\\n\\', \\'## 2. Simulating the Environment\\\\n\\', \\'\\\\n\\', \\'To understand how the environment behaves, we can run a simulation. This involves:\\\\n\\', \\'\\\\n\\', \\'1.  **Resetting** the environment to a starting state.\\\\n\\', \"2.  **Choosing an action** based on a predefined **policy**. A policy dictates what action an agent should take in a given state. For this demonstration, we\\'ll use a simple uniform random policy, where the agent has a 50% chance of moving left or right in any non-terminal state.\\\\n\", \\'3.  **Taking a step** in the environment, observing the next state, reward, and whether the episode has terminated.\\\\n\\', \\'4.  **Repeating** steps 2 and 3 until a terminal state is reached or a maximum number of steps is exceeded. If a terminal state is reached, the environment is reset for the next segment of the simulation.\\\\n\\', \\'\\\\n\\', \\'The `run_simulation` function helps visualize this process, printing the state, action, reward, and `done` status at each step.\\\\n\\', \\'\\\\n\\', \\'## 3. Monte Carlo First-Visit Policy Evaluation\\\\n\\', \\'\\\\n\\', \"Monte Carlo methods are a class of algorithms in Reinforcement Learning that learn from *complete episodes* of experience. Unlike Dynamic Programming, they don\\'t require a model of the environment\\'s dynamics (i.e., transition probabilities and rewards for every state-action pair).\\\\n\", \\'\\\\n\\', \\'**Policy Evaluation** is the process of estimating the **value function** for a given policy. The **state-value function**, $V(s)$, represents the expected total future reward (return) an agent can expect to receive starting from state $s$ and following a particular policy $\\\\\\\\pi$.\\\\n\\', \\'\\\\n\\', \\'### First-Visit Monte Carlo\\\\n\\', \\'\\\\n\\', \\'The \"First-Visit\" aspect means that when calculating the return for a state $s$ within an episode, we only consider the rewards that occur *after the first time* state $s$ is visited in that episode. If a state is visited multiple times within the same episode, subsequent visits are ignored for the purpose of calculating its return for that specific episode.\\\\n\\', \\'\\\\n\\', \\'### Algorithm Steps (`first_visit_mc_policy_evaluation` function)\\\\n\\', \\'\\\\n\\', \\'1.  **Initialization**:\\\\n\\', \\'    * `returns = defaultdict(list)`: A dictionary to store a list of all observed returns for each state. For example, `returns[state_3]` might contain `[0.5, 0.66, 0.33]` from different episodes.\\\\n\\', \\'    * `V = defaultdict(float)`: A dictionary to store the estimated value $V(s)$ for each state $s$. This will be the average of the returns collected in `returns[s]`.\\\\n\\', \\'\\\\n\\', \\'2.  **Episode Generation**:\\\\n\\', \\'    * For a specified `num_episodes`:\\\\n\\', \\'        * Reset the environment to get a starting state.\\\\n\\', \\'        * Generate an entire episode by repeatedly choosing actions according to the given `policy` and taking steps in the environment until a terminal state is reached.\\\\n\\', \\'        * Store the sequence of `(state, reward)` pairs encountered in the `episode` list.\\\\n\\', \\'\\\\n\\', \\'3.  **Return Computation and Value Update**:\\\\n\\', \\'    * After an episode completes, iterate through the `episode` list **in reverse order**.\\\\n\\', \"    * `G` (the return) is initialized to 0 and accumulated by adding the rewards encountered. Since we\\'re going in reverse, `G` at any point represents the sum of future rewards from that point onwards.\\\\n\", \\'    * For each `(state, reward)` pair in the reversed episode:\\\\n\\', \\'        * Check if the `state` has been visited *for the first time* in this specific episode (using `visited_states_in_episode` set).\\\\n\\', \"        * If it\\'s the first visit:\\\\n\", \\'            * Add the current `G` (the return from this first visit onwards) to `returns[state]`.\\\\n\\', \\'            * Update `V[state]` by calculating the **average** of all returns collected so far for that state (`np.mean(returns[state])`). This is how the value estimate converges.\\\\n\\', \\'\\\\n\\', \\'4.  **Return Estimated Values**: After all episodes are simulated, the function returns the `V` dictionary containing the estimated state values.\\\\n\\', \\'\\\\n\\', \\'### Theoretical Values for Random Walk\\\\n\\', \\'\\\\n\\', \\'For a simple random walk with states 0 to 6, where state 0 gives 0 reward and state 6 gives 1 reward, and transitions are equally likely to left or right (uniform policy), the true state values are:\\\\n\\', \\'* $V(0) = 0$\\\\n\\', \\'* $V(1) = 1/6 \\\\\\\\approx 0.1667$\\\\n\\', \\'* $V(2) = 2/6 \\\\\\\\approx 0.3333$\\\\n\\', \\'* $V(3) = 3/6 = 0.5$\\\\n\\', \\'* $V(4) = 4/6 \\\\\\\\approx 0.6667$\\\\n\\', \\'* $V(5) = 5/6 \\\\\\\\approx 0.8333$\\\\n\\', \\'* $V(6) = 1$\\\\n\\', \\'\\\\n\\', \\'As the number of episodes in our Monte Carlo simulation increases, our estimated values $V(s)$ should converge towards these theoretical values.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import random\\\\n\\', \\'import numpy as np\\\\n\\', \\'from collections import defaultdict\\\\n\\', \\'\\\\n\\', \\'# ---\\\\n\\', \\'# 1. RandomWalkEnv Class: Defines the environment for the Random Walk problem\\\\n\\', \\'# ---\\\\n\\', \\'class RandomWalkEnv:\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    A simple one-dimensional Random Walk environment.\\\\n\\', \\'\\\\n\\', \\'    States are integers from 0 to 6.\\\\n\\', \\'    Terminal states are 0 (left end) and 6 (right end).\\\\n\\', \\'    Non-terminal states are 1, 2, 3, 4, 5.\\\\n\\', \\'    Actions: 0 (LEFT), 1 (RIGHT).\\\\n\\', \\'    Reward: +1 for reaching state 6, 0 otherwise.\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    def __init__(self):\\\\n\\', \\'        # Non-terminal states where the agent can take actions\\\\n\\', \\'        self.states = list(range(1, 6))\\\\n\\', \\'        # Terminal states where the episode ends\\\\n\\', \\'        self.terminal_states = [0, 6]\\\\n\\', \\'        # Current state of the agent in the environment\\\\n\\', \\'        self.current_state = None\\\\n\\', \\'        # Initialize the environment to a random non-terminal state\\\\n\\', \\'        self.reset()\\\\n\\', \\'\\\\n\\', \\'    def reset(self):\\\\n\\', \\'        \"\"\"\\\\n\\', \\'        Resets the environment to a random non-terminal state (1, 2, 3, 4, or 5).\\\\n\\', \\'\\\\n\\', \\'        Returns:\\\\n\\', \\'            int: The initial state after reset.\\\\n\\', \\'        \"\"\"\\\\n\\', \\'        self.current_state = random.choice(self.states)\\\\n\\', \\'        return self.current_state\\\\n\\', \\'\\\\n\\', \\'    def step(self, action):\\\\n\\', \\'        \"\"\"\\\\n\\', \\'        Performs a step in the environment based on the given action.\\\\n\\', \\'\\\\n\\', \\'        Args:\\\\n\\', \\'            action (int): The action to take. 0 for LEFT (-1), 1 for RIGHT (+1).\\\\n\\', \\'\\\\n\\', \\'        Returns:\\\\n\\', \\'            tuple: (next_state, reward, done)\\\\n\\', \\'                next_state (int): The state the agent transitions to.\\\\n\\', \\'                reward (int): The reward received for the transition.\\\\n\\', \\'                done (bool): True if the next_state is a terminal state, False otherwise.\\\\n\\', \\'        \"\"\"\\\\n\\', \\'        # If the environment is already in a terminal state, no action can be taken.\\\\n\\', \\'        # This check prevents movement from an absorbing state.\\\\n\\', \\'        if self.current_state in self.terminal_states:\\\\n\\', \\'            return self.current_state, 0, True # Reward is 0, and episode is done\\\\n\\', \\'\\\\n\\', \\'        # Determine the next state based on the action\\\\n\\', \\'        # Action 1 (RIGHT) adds 1 to the current state.\\\\n\\', \\'        # Action 0 (LEFT) subtracts 1 from the current state.\\\\n\\', \\'        if action == 1:\\\\n\\', \\'            next_state = self.current_state + 1\\\\n\\', \\'        elif action == 0:\\\\n\\', \\'            next_state = self.current_state - 1\\\\n\\', \\'        else:\\\\n\\', \\'            raise ValueError(\"Invalid action. Action must be 0 (LEFT) or 1 (RIGHT).\")\\\\n\\', \\'\\\\n\\', \\'        # Calculate the reward for this transition\\\\n\\', \\'        # A reward of +1 is given only if the agent reaches state 6.\\\\n\\', \\'        # All other transitions, including reaching state 0, yield 0 reward.\\\\n\\', \\'        reward = 1 if next_state == 6 else 0\\\\n\\', \\'\\\\n\\', \\'        # Determine if the episode is done (i.e., a terminal state has been reached)\\\\n\\', \\'        done = next_state in self.terminal_states\\\\n\\', \\'\\\\n\\', \"        # Update the environment\\'s current state\\\\n\", \\'        self.current_state = next_state\\\\n\\', \\'\\\\n\\', \\'        return self.current_state, reward, done\\\\n\\', \\'\\\\n\\', \\'# ---\\\\n\\', \\'# 2. Simulating the Environment (`run_simulation` function)\\\\n\\', \\'# ---\\\\n\\', \\'def run_simulation(env, policy, num_steps=10):\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    Runs a simulation of the Random Walk environment for a specified number of steps.\\\\n\\', \\'    It demonstrates how an agent interacts with the environment following a given policy.\\\\n\\', \\'\\\\n\\', \\'    Args:\\\\n\\', \\'        env (RandomWalkEnv): An instance of the RandomWalkEnv.\\\\n\\', \\'        policy (np.ndarray): A 2D numpy array representing the policy.\\\\n\\', \\'                             `policy[s]` gives the probability distribution over actions for state `s`.\\\\n\\', \\'                             (e.g., policy[state_number] = [prob_left, prob_right]).\\\\n\\', \\'        num_steps (int): The maximum number of steps to simulate. The simulation\\\\n\\', \\'                         might end earlier if a terminal state is reached and reset.\\\\n\\', \\'\\\\n\\', \\'    Returns:\\\\n\\', \\'        tuple: (final_state, final_reward, final_done) after the simulation concludes.\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    state = env.reset() # Reset the environment to a random starting state\\\\n\\', \\'    print(f\"--- Starting Simulation ---\")\\\\n\\', \\'    print(f\"Initial State: {state}\")\\\\n\\', \\'    print(f\"{\\\\\\'Step\\\\\\':<5} {\\\\\\'Action\\\\\\':<10} {\\\\\\'State (Prev)\\\\\\':<15} {\\\\\\'Next State\\\\\\':<12} {\\\\\\'Reward\\\\\\':<10} {\\\\\\'Done\\\\\\':<10}\")\\\\n\\', \\'    print(\"-\" * 75)\\\\n\\', \\'\\\\n\\', \\'    for step_count in range(num_steps):\\\\n\\', \\'        prev_state = state # Store the state before taking an action\\\\n\\', \\'\\\\n\\', \\'        # Choose an action based on the policy for the current state.\\\\n\\', \\'        # np.random.choice selects an action (0 or 1) based on the probabilities\\\\n\\', \\'        # provided in policy[state].\\\\n\\', \\'        # Note: Policy array is assumed to cover indices 0-6. For terminal states (0, 6),\\\\n\\', \\'        # an action is technically selected, but the env.step() method handles that\\\\n\\', \\'        # by immediately returning done=True if already in a terminal state.\\\\n\\', \\'        action = np.random.choice([0, 1], p=policy[state])\\\\n\\', \\'\\\\n\\', \\'        # Take a step in the environment with the chosen action\\\\n\\', \\'        state, reward, done = env.step(action)\\\\n\\', \\'\\\\n\\', \\'        # Print the details of the current step\\\\n\\', \\'        print(f\"{step_count + 1:<5} {ACTION_NAMES[action]:<10} {prev_state:<15} {state:<12} {reward:<10} {done:<10}\")\\\\n\\', \\'\\\\n\\', \\'        # If a terminal state is reached, reset the environment for the next part of the simulation\\\\n\\', \\'        if done:\\\\n\\', \\'            print(f\"--- Reached Terminal State ({state}). Resetting Environment ---\")\\\\n\\', \\'            state = env.reset() # Reset to a new random non-terminal state\\\\n\\', \\'            print(f\"New Initial State: {state}\")\\\\n\\', \\'            print(\"-\" * 75)\\\\n\\', \\'\\\\n\\', \\'    print(f\"--- Simulation Finished After {num_steps} Steps ---\")\\\\n\\', \\'    return state, reward, done\\\\n\\', \\'\\\\n\\', \\'# ---\\\\n\\', \\'# 3. Monte Carlo First-Visit Policy Evaluation Algorithm\\\\n\\', \\'# ---\\\\n\\', \\'def first_visit_mc_policy_evaluation(env, policy, num_episodes):\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    Estimates the state-value function V(s) for a given policy using\\\\n\\', \\'    Monte Carlo First-Visit Policy Evaluation.\\\\n\\', \\'\\\\n\\', \\'    Args:\\\\n\\', \\'        env (RandomWalkEnv): An instance of the RandomWalkEnv.\\\\n\\', \\'        policy (np.ndarray): A 2D numpy array representing the policy.\\\\n\\', \\'                             `policy[s]` gives the probability distribution over actions for state `s`.\\\\n\\', \\'        num_episodes (int): The total number of episodes to simulate for evaluation.\\\\n\\', \\'\\\\n\\', \\'    Returns:\\\\n\\', \\'        defaultdict: A dictionary where keys are states (int) and values are\\\\n\\', \\'                     their estimated state values (float).\\\\n\\', \\'    \"\"\"\\\\n\\', \"    # \\'returns\\' stores a list of all observed returns for each state.\\\\n\", \\'    # Example: returns[3] = [0.5, 0.6, 0.4]\\\\n\\', \\'    returns = defaultdict(list)\\\\n\\', \"    # \\'V\\' stores the current estimated value for each state.\\\\n\", \\'    # Example: V[3] = 0.5 (average of returns[3])\\\\n\\', \\'    V = defaultdict(float)\\\\n\\', \\'\\\\n\\', \\'    print(f\"\\\\\\\\n--- Running Monte Carlo First-Visit Policy Evaluation for {num_episodes} episodes ---\")\\\\n\\', \\'\\\\n\\', \\'    for episode_num in range(1, num_episodes + 1):\\\\n\\', \\'        # Reset the environment for a new episode\\\\n\\', \\'        state = env.reset()\\\\n\\', \\'        episode = []  # To store (state, reward) pairs for the current episode\\\\n\\', \\'                      # (state from which action was taken, reward received after taking action)\\\\n\\', \\'\\\\n\\', \\'        # Generate an episode under the given policy\\\\n\\', \\'        # The loop continues until a terminal state is reached\\\\n\\', \\'        while True:\\\\n\\', \\'            # If the current state is terminal, break the loop as no action can be taken from it.\\\\n\\', \\'            # This handles cases where the previous step landed exactly on a terminal state.\\\\n\\', \\'            if state in env.terminal_states:\\\\n\\', \\'                break\\\\n\\', \\'\\\\n\\', \"            # Choose an action based on the policy\\'s probabilities for the current state.\\\\n\", \\'            # E.g., if policy[3] = [0.5, 0.5], action will be 0 or 1 with 50% chance.\\\\n\\', \\'            action = np.random.choice([0, 1], p=policy[state])\\\\n\\', \\'\\\\n\\', \\'            # Take a step in the environment\\\\n\\', \\'            next_state, reward, done = env.step(action)\\\\n\\', \\'\\\\n\\', \\'            # Record the (state, reward) pair. The reward `reward` is received *after*\\\\n\\', \\'            # transitioning from `state` to `next_state`.\\\\n\\', \\'            episode.append((state, reward))\\\\n\\', \\'\\\\n\\', \\'            # Update the current state for the next iteration\\\\n\\', \\'            state = next_state\\\\n\\', \\'\\\\n\\', \\'            # If the episode is done (reached a terminal state), break the loop\\\\n\\', \\'            if done:\\\\n\\', \\'                break\\\\n\\', \\'\\\\n\\', \\'        # --- Compute returns and update value estimates for the current episode ---\\\\n\\', \\'        G = 0  # Initialize the return (sum of future rewards)\\\\n\\', \\'        # Keep track of states already visited in this episode to apply \"first-visit\" logic\\\\n\\', \\'        visited_states_in_episode = set()\\\\n\\', \\'\\\\n\\', \\'        # Iterate through the episode in reverse order to calculate returns\\\\n\\', \"        # \\'reversed(episode)\\' processes from the last (state, reward) pair to the first.\\\\n\", \\'        for s_t, r_t_plus_1 in reversed(episode):\\\\n\\', \\'            # Accumulate the return G. r_t_plus_1 is the immediate reward received\\\\n\\', \\'            # after leaving state s_t.\\\\n\\', \\'            G = r_t_plus_1 + G\\\\n\\', \\'\\\\n\\', \\'            # Apply First-Visit MC: Update V(s) only if s_t is visited for the first time\\\\n\\', \\'            # in this episode.\\\\n\\', \\'            if s_t not in visited_states_in_episode:\\\\n\\', \\'                visited_states_in_episode.add(s_t) # Mark state as visited\\\\n\\', \\'                returns[s_t].append(G)             # Add the calculated return G to the list for s_t\\\\n\\', \\'                V[s_t] = np.mean(returns[s_t])     # Update V(s_t) by averaging all collected returns\\\\n\\', \\'\\\\n\\', \\'        # Optional: Print progress for long simulations\\\\n\\', \\'        if episode_num % (num_episodes // 10) == 0:\\\\n\\', \\'            print(f\"  Completed {episode_num}/{num_episodes} episodes...\")\\\\n\\', \\'\\\\n\\', \\'    return V\\\\n\\', \\'\\\\n\\', \\'# ---\\\\n\\', \\'# Main Execution Block\\\\n\\', \\'# ---\\\\n\\', \\'\\\\n\\', \\'# Define action names for clear output\\\\n\\', \\'ACTION_NAMES = [\"LEFT\", \"RIGHT\"]\\\\n\\', \\'\\\\n\\', \\'# Initialize the Random Walk Environment\\\\n\\', \\'env_rw = RandomWalkEnv()\\\\n\\', \\'\\\\n\\', \\'# Define a uniform random policy for all states (0 to 6).\\\\n\\', \\'# Each state has a 50% chance of moving LEFT (0) or RIGHT (1).\\\\n\\', \\'# Policy array has 7 rows, corresponding to states 0 through 6.\\\\n\\', \"# Even though actions aren\\'t taken from terminal states 0 and 6,\\\\n\", \\'# the policy array is indexed directly by state number for simplicity.\\\\n\\', \\'policy_uniform = np.array([[0.5, 0.5]] * 7)\\\\n\\', \\'\\\\n\\', \\'# --- Demonstrate Environment Simulation ---\\\\n\\', \\'print(\"--- Demonstrating Environment Simulation ---\")\\\\n\\', \\'# Run a short simulation to see how the environment and policy interact\\\\n\\', \\'_ = run_simulation(env_rw, policy_uniform, num_steps=15)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'# --- Run Monte Carlo First-Visit Policy Evaluation ---\\\\n\\', \\'num_episodes = 10000 # Number of episodes to run for policy evaluation\\\\n\\', \\'\\\\n\\', \\'# Perform the Monte Carlo policy evaluation\\\\n\\', \\'V_estimated = first_visit_mc_policy_evaluation(env_rw, policy_uniform, num_episodes)\\\\n\\', \\'\\\\n\\', \\'print(\"\\\\\\\\n--- Estimated State Values (V(s)) from Monte Carlo Evaluation ---\")\\\\n\\', \\'# Sort the states for ordered printing of results\\\\n\\', \\'sorted_states = sorted(V_estimated.keys())\\\\n\\', \\'for state in sorted_states:\\\\n\\', \\'    print(f\"V({state}): {V_estimated[state]:.4f}\")\\\\n\\', \\'\\\\n\\', \\'print(\"\\\\\\\\n--- Theoretical True Values for Random Walk (for comparison) ---\")\\\\n\\', \\'# The true values for this specific Random Walk problem\\\\n\\', \\'# V(s) = s / 6\\\\n\\', \\'print(f\"V(0): {0.0000:.4f} (Terminal)\")\\\\n\\', \\'for i in range(1, 6):\\\\n\\', \\'    print(f\"V({i}): {i/6:.4f}\")\\\\n\\', \\'print(f\"V(6): {1.0000:.4f} (Terminal)\")\\\\n\\', \\'\\\\n\\', \\'print(\"\\\\\\\\n--- Evaluation Complete ---\")\\']\\'\\n with output: \\'[\\'--- Demonstrating Environment Simulation ---\\\\n\\', \\'--- Starting Simulation ---\\\\n\\', \\'Initial State: 5\\\\n\\', \\'Step  Action     State (Prev)    Next State   Reward     Done      \\\\n\\', \\'---------------------------------------------------------------------------\\\\n\\', \\'1     LEFT       5               4            0          0         \\\\n\\', \\'2     LEFT       4               3            0          0         \\\\n\\', \\'3     RIGHT      3               4            0          0         \\\\n\\', \\'4     LEFT       4               3            0          0         \\\\n\\', \\'5     LEFT       3               2            0          0         \\\\n\\', \\'6     LEFT       2               1            0          0         \\\\n\\', \\'7     LEFT       1               0            0          1         \\\\n\\', \\'--- Reached Terminal State (0). Resetting Environment ---\\\\n\\', \\'New Initial State: 3\\\\n\\', \\'---------------------------------------------------------------------------\\\\n\\', \\'8     LEFT       3               2            0          0         \\\\n\\', \\'9     LEFT       2               1            0          0         \\\\n\\', \\'10    LEFT       1               0            0          1         \\\\n\\', \\'--- Reached Terminal State (0). Resetting Environment ---\\\\n\\', \\'New Initial State: 3\\\\n\\', \\'---------------------------------------------------------------------------\\\\n\\', \\'11    RIGHT      3               4            0          0         \\\\n\\', \\'12    RIGHT      4               5            0          0         \\\\n\\', \\'13    RIGHT      5               6            1          1         \\\\n\\', \\'--- Reached Terminal State (6). Resetting Environment ---\\\\n\\', \\'New Initial State: 1\\\\n\\', \\'---------------------------------------------------------------------------\\\\n\\', \\'14    LEFT       1               0            0          1         \\\\n\\', \\'--- Reached Terminal State (0). Resetting Environment ---\\\\n\\', \\'New Initial State: 3\\\\n\\', \\'---------------------------------------------------------------------------\\\\n\\', \\'15    LEFT       3               2            0          0         \\\\n\\', \\'--- Simulation Finished After 15 Steps ---\\\\n\\', \\'\\\\n\\', \\'--- Running Monte Carlo First-Visit Policy Evaluation for 10000 episodes ---\\\\n\\', \\'  Completed 1000/10000 episodes...\\\\n\\', \\'  Completed 2000/10000 episodes...\\\\n\\', \\'  Completed 3000/10000 episodes...\\\\n\\', \\'  Completed 4000/10000 episodes...\\\\n\\', \\'  Completed 5000/10000 episodes...\\\\n\\', \\'  Completed 6000/10000 episodes...\\\\n\\', \\'  Completed 7000/10000 episodes...\\\\n\\', \\'  Completed 8000/10000 episodes...\\\\n\\', \\'  Completed 9000/10000 episodes...\\\\n\\', \\'  Completed 10000/10000 episodes...\\\\n\\', \\'\\\\n\\', \\'--- Estimated State Values (V(s)) from Monte Carlo Evaluation ---\\\\n\\', \\'V(1): 0.1675\\\\n\\', \\'V(2): 0.3329\\\\n\\', \\'V(3): 0.4989\\\\n\\', \\'V(4): 0.6642\\\\n\\', \\'V(5): 0.8346\\\\n\\', \\'\\\\n\\', \\'--- Theoretical True Values for Random Walk (for comparison) ---\\\\n\\', \\'V(0): 0.0000 (Terminal)\\\\n\\', \\'V(1): 0.1667\\\\n\\', \\'V(2): 0.3333\\\\n\\', \\'V(3): 0.5000\\\\n\\', \\'V(4): 0.6667\\\\n\\', \\'V(5): 0.8333\\\\n\\', \\'V(6): 1.0000 (Terminal)\\\\n\\', \\'\\\\n\\', \\'--- Evaluation Complete ---\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Ch_6_Cliff_Walk.ipynb'}, page_content='\\'code\\' cell: \\'[\\'# Installation\\\\n\\', \\'# pip install gymnasium[toy-text]\\\\n\\', \\'# pip install gymnasium[classic_control]\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import gymnasium as gym\\\\n\\', \\'import numpy as np\\\\n\\', \\'import matplotlib.pyplot as plt\\\\n\\', \\'\\\\n\\', \\'import time\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Create the environment\\\\n\\', \\'env = gym.make(\"CliffWalking-v1\")\\\\n\\', \\'\\\\n\\', \\'# Check state and action space\\\\n\\', \\'print(\"Observation space:\", env.observation_space)  # Discrete(48)\\\\n\\', \\'print(\"Action space:\", env.action_space)  # Discrete(4)\\\\n\\', \\'\\\\n\\', \\'# Reset the environment\\\\n\\', \\'observation, info = env.reset()\\\\n\\', \\'print(\"Initial observation:\", observation)\\\\n\\', \\'\\\\n\\', \\'# Take a random action\\\\n\\', \\'action = env.action_space.sample()\\\\n\\', \\'print(f\"action: {action}\")\\\\n\\', \\'next_observation, reward, terminated, truncated, info = env.step(action)\\\\n\\', \\'print(\"Next observation:\", next_observation)\\\\n\\', \\'print(\"Reward:\", reward)\\\\n\\', \\'print(\"Terminated:\", terminated)\\\\n\\', \\'print(\"Truncated:\", truncated)\\\\n\\', \\'\\\\n\\', \\'# Render and display\\\\n\\', \\'# img = env.render()\\\\n\\', \\'# plt.imshow(img)\\\\n\\', \\'# plt.show()\\\\n\\', \\'\\\\n\\', \\'# Close the environment\\\\n\\', \\'env.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Description\\\\n\\', \\'- The action space is Discrete(4): 0=up, 1=right, 2=down, 3=left.\\\\n\\', \\'- The observation space is Discrete(48): each cell in a 4×12 grid gets an integer value.\\\\n\\', \\'- Use env.reset() to begin an episode; step() advances one move and returns state, reward, etc.\\\\n\\', \\'\\\\n\\', \\'**References to Gymnasium Documentation**\\\\n\\', \\'- Full technical details and variations can be found on the official Gymnasium Cliff Walking page.\\\\n\\', \\'- Basic environment usage and API available in their getting started guides.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'[1](https://gymnasium.farama.org/environments/toy_text/cliff_walking/)\\\\n\\', \\'[2](https://gymnasium.farama.org/introduction/basic_usage/)\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'truncated, info\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def generate_episodes(env, num_episodes=3, render=False, delay=0.1):\\\\n\\', \\'    episodes = []\\\\n\\', \\'    max_steps = env.spec.max_episode_steps  # Get max steps from env.spec\\\\n\\', \\'    \\\\n\\', \\'    for episode in range(num_episodes):\\\\n\\', \\'        episode_data = []\\\\n\\', \\'        observation, info = env.reset()\\\\n\\', \\'        terminated = False\\\\n\\', \\'        truncated = False\\\\n\\', \\'        step_count = 0\\\\n\\', \\'\\\\n\\', \\'        while not (terminated or truncated) and (max_steps is None or step_count < max_steps):\\\\n\\', \\'            if render:\\\\n\\', \\'                env.render()  # Render the environment (if human mode, it will display)\\\\n\\', \\'                time.sleep(delay)  # Small delay to see the agent move\\\\n\\', \\'            action = env.action_space.sample()  # Random action\\\\n\\', \\'            next_observation, reward, terminated, truncated, info = env.step(action)\\\\n\\', \\'            episode_data.append((observation, action, reward, next_observation, terminated, truncated))\\\\n\\', \\'            observation = next_observation\\\\n\\', \\'            step_count += 1\\\\n\\', \\'        \\\\n\\', \\'        episodes.append(episode_data)\\\\n\\', \\'    \\\\n\\', \"    return episodes  # Note: Don\\'t close env here if you plan to reuse it later\"]\\'\\n\\n\\'code\\' cell: \\'[\\'env = gym.make(\"CliffWalking-v1\", max_episode_steps=5000)  # Define max steps here\\\\n\\', \\'episodes = generate_episodes(env, num_episodes=5)\\\\n\\', \\'env.close()  # Close only when done\\\\n\\', \\'\\\\n\\', \\'for i, episode in enumerate(episodes):\\\\n\\', \\'    print(f\"Episode {i+1}: {len(episode)} steps\")\\']\\'\\n\\n\\'code\\' cell: \\'[\\'env = gym.make(\"CliffWalking-v1\", max_episode_steps=10, render_mode=\"human\")  # Enable human rendering\\\\n\\', \\'episodes = generate_episodes(env, num_episodes=2, render=True, delay=0.2)  # 0.2s delay per step\\\\n\\', \\'env.close()  # Close the window after generation\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Intialize Q table\\']\\'\\n\\n\\'code\\' cell: \\'[\\'from types import SimpleNamespace\\\\n\\', \\'from collections import deque\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def initialize_q_table(state_space, action_space):\\\\n\\', \\'    \"\"\"Initialize Q-table with zeros.\"\"\"\\\\n\\', \\'    return np.zeros((state_space, action_space))\\\\n\\', \\'\\\\n\\', \\'class CliffwalkingAgent:\\\\n\\', \\'    def __init__(self, args):\\\\n\\', \\'        self.args = args\\\\n\\', \\'        self.training_error = []\\\\n\\', \\'        self.epsilon = args.epsilon_start\\\\n\\', \\'        self.epsilon_decay = args.epsilon_decay\\\\n\\', \\'        self.epsilon_min = args.epsilon_min\\\\n\\', \\'\\\\n\\', \\'    def get_action(self, Qtable, state, epsilon=None):\\\\n\\', \\'        \"\"\"Epsilon-greedy policy.\"\"\"\\\\n\\', \\'        epsilon = epsilon if epsilon is not None else self.epsilon\\\\n\\', \\'        if np.random.random() > epsilon:\\\\n\\', \\'            return np.argmax(Qtable[state])  # Exploit\\\\n\\', \\'        else:\\\\n\\', \\'            return np.random.randint(0, Qtable.shape[1])  # Explore\\\\n\\', \\'\\\\n\\', \\'    def update(self, Qtable, state, action, reward, new_state, done):\\\\n\\', \\'        \"\"\"Update Q-table using TD learning.\"\"\"\\\\n\\', \\'        best_next_action = np.argmax(Qtable[new_state])\\\\n\\', \\'        td_target = reward + (1 - done) * self.args.discount_factor * Qtable[new_state][best_next_action]\\\\n\\', \\'        td_error = td_target - Qtable[state][action]\\\\n\\', \\'        Qtable[state][action] += self.args.learning_rate * td_error\\\\n\\', \\'        self.training_error.append(td_error)\\\\n\\', \\'        return Qtable\\\\n\\', \\'\\\\n\\', \\'    def decay_epsilon(self):\\\\n\\', \\'        \"\"\"Decay exploration rate.\"\"\"\\\\n\\', \\'        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\\\\n\\', \\'\\\\n\\', \\'def train_agent(agent, env, Qtable):\\\\n\\', \\'    \"\"\"Training loop with epsilon decay.\"\"\"\\\\n\\', \\'    rewards = []\\\\n\\', \\'    for episode in range(agent.args.n_training_episodes):\\\\n\\', \\'        state, _ = env.reset()\\\\n\\', \\'        total_reward = 0\\\\n\\', \\'        done = False\\\\n\\', \\'\\\\n\\', \\'        while not done:\\\\n\\', \\'            action = agent.get_action(Qtable, state)\\\\n\\', \\'            next_state, reward, terminated, truncated, _ = env.step(action)\\\\n\\', \\'            done = terminated or truncated  \\\\n\\', \\'            Qtable = agent.update(Qtable, state, action, reward, next_state, done)\\\\n\\', \\'            state = next_state\\\\n\\', \\'            total_reward += reward\\\\n\\', \\'\\\\n\\', \\'        agent.decay_epsilon()\\\\n\\', \\'        rewards.append(total_reward)\\\\n\\', \\'\\\\n\\', \\'        if episode % 1000 == 0:\\\\n\\', \\'            print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {agent.epsilon:.2f}\")\\\\n\\', \\'\\\\n\\', \\'    return Qtable, rewards\\\\n\\', \\'\\\\n\\', \\'def evaluate_agent(agent, env, Qtable, n_episodes=100, render=False, delay=0.1):\\\\n\\', \\'    \"\"\"Evaluate the trained agent (pure exploitation).\"\"\"\\\\n\\', \\'    rewards = []\\\\n\\', \\'    for _ in range(n_episodes):\\\\n\\', \\'        state, _ = env.reset()\\\\n\\', \\'        done = False\\\\n\\', \\'        total_reward = 0\\\\n\\', \\'\\\\n\\', \\'        while not done:\\\\n\\', \\'            if render:\\\\n\\', \\'                env.render()\\\\n\\', \\'                time.sleep(delay)\\\\n\\', \\'            action = agent.get_action(Qtable, state, epsilon=0)  # Greedy policy\\\\n\\', \\'            next_state, reward, terminated, truncated, _ = env.step(action)\\\\n\\', \\'            done = terminated or truncated\\\\n\\', \\'            total_reward += reward\\\\n\\', \\'            state = next_state\\\\n\\', \\'\\\\n\\', \\'        rewards.append(total_reward)\\\\n\\', \\'    return np.mean(rewards)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Configurations\\\\n\\', \\'configs = SimpleNamespace(\\\\n\\', \\'    n_training_episodes=20_000,\\\\n\\', \\'    learning_rate=0.1,\\\\n\\', \\'    discount_factor=0.95,\\\\n\\', \\'    epsilon_start=1.0,\\\\n\\', \\'    epsilon_decay=0.9995,\\\\n\\', \\'    epsilon_min=0.01,\\\\n\\', \\'    eval_seed=42,\\\\n\\', \\'    max_steps=500,\\\\n\\', \\')\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Initialize environment and agent\\\\n\\', \\'env = gym.make(\"CliffWalking-v1\")  # Gymnasium version\\\\n\\', \\'agent = CliffwalkingAgent(configs)\\\\n\\', \\'Qtable = initialize_q_table(env.observation_space.n, env.action_space.n)\\\\n\\', \\'\\\\n\\', \\'# Train the agent\\\\n\\', \\'Qtable, rewards = train_agent(agent, env, Qtable)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Evaluate with rendering\\\\n\\', \\'eval_env = gym.make(\"CliffWalking-v1\", render_mode=\"human\")\\\\n\\', \\'mean_reward = evaluate_agent(agent, eval_env, Qtable, n_episodes=10, render=True, delay=0.2)\\\\n\\', \\'print(f\"Mean reward over evaluation: {mean_reward}\")\\\\n\\', \\'eval_env.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Detailed Analysis\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# import libraries\\\\n\\', \\'import numpy as np\\\\n\\', \\'import gymnasium as gym\\\\n\\', \\'from types import SimpleNamespace\\\\n\\', \\'import time\\\\n\\', \\'import matplotlib.pyplot as plt\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Create Class\\\\n\\', \\'class CliffWalkingAgent:\\\\n\\', \"    def __init__(self, args, algorithm=\\'q_learning\\'):\\\\n\", \\'        self.args = args\\\\n\\', \"        self.algorithm = algorithm  # \\'q_learning\\' or \\'sarsa\\'\\\\n\", \\'        self.training_errors = []\\\\n\\', \\'        self.epsilon = args.epsilon_start\\\\n\\', \\'        self.epsilon_decay = args.epsilon_decay\\\\n\\', \\'        self.epsilon_min = args.epsilon_min\\\\n\\', \\'\\\\n\\', \\'    def initialize_q_table(self, state_space, action_space):\\\\n\\', \\'        return np.zeros((state_space, action_space))\\\\n\\', \\'\\\\n\\', \\'    def get_action(self, Qtable, state, epsilon=None):\\\\n\\', \\'        epsilon = epsilon if epsilon is not None else self.epsilon\\\\n\\', \\'        if np.random.random() > epsilon:\\\\n\\', \\'            return np.argmax(Qtable[state])\\\\n\\', \\'        return np.random.randint(0, Qtable.shape[1])\\\\n\\', \\'\\\\n\\', \\'    def update(self, Qtable, state, action, reward, next_state, next_action, done):\\\\n\\', \"        if self.algorithm == \\'q_learning\\':\\\\n\", \\'            # Q-Learning update (off-policy)\\\\n\\', \\'            best_next_action = np.argmax(Qtable[next_state])\\\\n\\', \\'            td_target = reward + (1 - done) * self.args.discount_factor * Qtable[next_state][best_next_action]\\\\n\\', \\'        else:\\\\n\\', \\'            # SARSA update (on-policy)\\\\n\\', \\'            td_target = reward + (1 - done) * self.args.discount_factor * Qtable[next_state][next_action]\\\\n\\', \\'        \\\\n\\', \\'        td_error = td_target - Qtable[state][action]\\\\n\\', \\'        Qtable[state][action] += self.args.learning_rate * td_error\\\\n\\', \\'        self.training_errors.append(abs(td_error))\\\\n\\', \\'        return Qtable\\\\n\\', \\'\\\\n\\', \\'    def decay_epsilon(self):\\\\n\\', \\'        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Train the agent\\\\n\\', \\'def train_agent(agent, env, Qtable):\\\\n\\', \\'    episode_rewards = []\\\\n\\', \\'    for episode in range(agent.args.n_training_episodes):\\\\n\\', \\'        state, _ = env.reset()\\\\n\\', \\'        action = agent.get_action(Qtable, state)\\\\n\\', \\'        total_reward = 0\\\\n\\', \\'        done = False\\\\n\\', \\'\\\\n\\', \\'        while not done:\\\\n\\', \\'            next_state, reward, terminated, truncated, _ = env.step(action)\\\\n\\', \\'            done = terminated or truncated\\\\n\\', \\'            next_action = agent.get_action(Qtable, next_state)\\\\n\\', \\'            \\\\n\\', \\'            Qtable = agent.update(\\\\n\\', \\'                Qtable, state, action, reward, next_state, next_action, done\\\\n\\', \\'            )\\\\n\\', \\'            \\\\n\\', \\'            state, action = next_state, next_action\\\\n\\', \\'            total_reward += reward\\\\n\\', \\'\\\\n\\', \\'        agent.decay_epsilon()\\\\n\\', \\'        episode_rewards.append(total_reward)\\\\n\\', \\'\\\\n\\', \\'        if episode % (agent.args.n_training_episodes // 100) == 0:\\\\n\\', \\'            avg_reward = np.mean(episode_rewards[-100:])\\\\n\\', \\'            print(f\"Episode {episode}, Avg Reward: {avg_reward:.1f}, Epsilon: {agent.epsilon:.2f}\")\\\\n\\', \\'\\\\n\\', \\'    return Qtable, episode_rewards\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def evaluate_agent(agent, env, Qtable, n_episodes=100, render=False, delay=0.1):\\\\n\\', \\'    rewards = []\\\\n\\', \\'    for _ in range(n_episodes):\\\\n\\', \\'        state, _ = env.reset()\\\\n\\', \\'        done = False\\\\n\\', \\'        total_reward = 0\\\\n\\', \\'\\\\n\\', \\'        while not done:\\\\n\\', \\'            if render:\\\\n\\', \\'                env.render()\\\\n\\', \\'                time.sleep(delay)\\\\n\\', \\'            action = agent.get_action(Qtable, state, epsilon=0)\\\\n\\', \\'            state, reward, terminated, truncated, _ = env.step(action)\\\\n\\', \\'            done = terminated or truncated\\\\n\\', \\'            total_reward += reward\\\\n\\', \\'\\\\n\\', \\'        rewards.append(total_reward)\\\\n\\', \\'    return np.mean(rewards)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def plot_policy(Qtable, title):\\\\n\\', \\'    plt.figure(figsize=(12, 3))\\\\n\\', \\'    grid = np.zeros((4, 12))\\\\n\\', \"    actions = [\\'↑\\', \\'→\\', \\'↓\\', \\'←\\']\\\\n\", \\'    \\\\n\\', \\'    for state in range(48):\\\\n\\', \\'        y, x = divmod(state, 12)\\\\n\\', \\'        action = np.argmax(Qtable[state])\\\\n\\', \\'        grid[y, x] = action\\\\n\\', \\'        if state >= 37 and state <= 46:  # Cliff\\\\n\\', \"            plt.text(x, y, \\'■\\', ha=\\'center\\', va=\\'center\\', fontsize=12)\\\\n\", \\'        elif state == 47:  # Goal\\\\n\\', \"            plt.text(x, y, \\'G\\', ha=\\'center\\', va=\\'center\\', fontsize=12)\\\\n\", \\'        else:\\\\n\\', \"            plt.text(x, y, actions[action], ha=\\'center\\', va=\\'center\\', fontsize=10)\\\\n\", \\'    \\\\n\\', \"    plt.imshow(grid, cmap=\\'Pastel1\\')\\\\n\", \\'    plt.title(f\"Learned Policy: {title}\")\\\\n\\', \"    plt.axis(\\'off\\')\\\\n\", \\'    plt.show()\\\\n\\', \\'\\\\n\\', \\'# Usage\\\\n\\', \\'# plot_policy(q_learning_Qtable, \"Q-Learning\")\\\\n\\', \\'# plot_policy(sarsa_Qtable, \"SARSA\")\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def plot_training(rewards_q, rewards_sarsa, zoom_episodes=2000):\\\\n\\', \\'    plt.figure(figsize=(12, 5))\\\\n\\', \\'    \\\\n\\', \\'    # Full training plot\\\\n\\', \\'    plt.subplot(1, 2, 1)\\\\n\\', \"    plt.plot(np.convolve(rewards_q, np.ones(100)/100), label=\\'Q-Learning\\', color=\\'blue\\', alpha=0.7)  # Fixed here\\\\n\", \"    plt.plot(np.convolve(rewards_sarsa, np.ones(100)/100), label=\\'SARSA\\', color=\\'red\\', alpha=0.7)    # Fixed here\\\\n\", \\'    plt.xlabel(\"Episode\")\\\\n\\', \\'    plt.ylabel(\"Avg Reward (100-episode window)\")\\\\n\\', \\'    plt.title(\"Full Training Progress\")\\\\n\\', \\'    plt.legend()\\\\n\\', \\'    plt.grid()\\\\n\\', \\'    \\\\n\\', \\'    # Zoomed-in plot\\\\n\\', \\'    plt.subplot(1, 2, 2)\\\\n\\', \\'    plt.plot(np.convolve(rewards_q[:zoom_episodes], np.ones(100)/100), \\\\n\\', \"             label=\\'Q-Learning\\', color=\\'blue\\', linewidth=2)  # Fixed here\\\\n\", \\'    plt.plot(np.convolve(rewards_sarsa[:zoom_episodes], np.ones(100)/100), \\\\n\\', \"             label=\\'SARSA\\', color=\\'red\\', linewidth=2)        # Fixed here\\\\n\", \\'    \\\\n\\', \\'    # Highlight key differences\\\\n\\', \\'    q_max = np.argmax(np.convolve(rewards_q[:zoom_episodes], np.ones(100)/100))\\\\n\\', \\'    sarsa_max = np.argmax(np.convolve(rewards_sarsa[:zoom_episodes], np.ones(100)/100))\\\\n\\', \"    plt.axvline(q_max, color=\\'blue\\', linestyle=\\'--\\', alpha=0.5)\\\\n\", \"    plt.axvline(sarsa_max, color=\\'red\\', linestyle=\\'--\\', alpha=0.5)\\\\n\", \\'    \\\\n\\', \\'    plt.xlabel(f\"Episode (First {zoom_episodes})\")\\\\n\\', \\'    plt.ylabel(\"Avg Reward (100-episode window)\")\\\\n\\', \\'    plt.title(f\"Early Training (First {zoom_episodes} Episodes)\")\\\\n\\', \\'    plt.legend()\\\\n\\', \\'    plt.grid()\\\\n\\', \\'    \\\\n\\', \\'    plt.tight_layout()\\\\n\\', \\'    plt.show()\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'# def plot_training(rewards_q, rewards_sarsa):\\\\n\\', \\'#     plt.figure(figsize=(10, 5))\\\\n\\', \"#     plt.plot(np.convolve(rewards_q, np.ones(100)/100), label=\\'Q-Learning\\')\\\\n\", \"#     plt.plot(np.convolve(rewards_sarsa, np.ones(100)/100), label=\\'SARSA\\')\\\\n\", \\'#     plt.xlabel(\"Episode\")\\\\n\\', \\'#     plt.ylabel(\"Avg Reward (100-episode window)\")\\\\n\\', \\'#     plt.title(\"Training Progress Comparison\")\\\\n\\', \\'#     plt.legend()\\\\n\\', \\'#     plt.grid()\\\\n\\', \\'#     plt.show()\\\\n\\', \\'\\\\n\\', \\'# plot_training(q_learning_rewards, sarsa_rewards)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Configuration\\\\n\\', \\'configs = SimpleNamespace(\\\\n\\', \\'    n_training_episodes=20_000,\\\\n\\', \\'    learning_rate=0.1,\\\\n\\', \\'    discount_factor=0.95,\\\\n\\', \\'    epsilon_start=1.0,\\\\n\\', \\'    epsilon_decay=0.9999,\\\\n\\', \\'    epsilon_min=0.01,\\\\n\\', \\'    max_steps=200,\\\\n\\', \\')\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Train both algorithms\\\\n\\', \\'def train_and_analyze(algorithm):\\\\n\\', \\'    env = gym.make(\"CliffWalking-v1\")\\\\n\\', \\'    agent = CliffWalkingAgent(configs, algorithm)\\\\n\\', \\'    Qtable = agent.initialize_q_table(env.observation_space.n, env.action_space.n)\\\\n\\', \\'    Qtable, rewards = train_agent(agent, env, Qtable)\\\\n\\', \\'    env.close()\\\\n\\', \\'    \\\\n\\', \\'    # Evaluate\\\\n\\', \\'    eval_env = gym.make(\"CliffWalking-v1\")\\\\n\\', \\'    mean_reward = evaluate_agent(agent, eval_env, Qtable, n_episodes=100)\\\\n\\', \\'    eval_env.close()\\\\n\\', \\'    \\\\n\\', \\'    return Qtable, rewards, mean_reward\\']\\'\\n\\n\\'code\\' cell: \\'[\"q_learning_Qtable, q_learning_rewards, q_learning_score = train_and_analyze(\\'q_learning\\')\\\\n\", \"sarsa_Qtable, sarsa_rewards, sarsa_score = train_and_analyze(\\'sarsa\\')\\\\n\", \\'\\\\n\\', \\'# Generate comparison table\\\\n\\', \\'comparison = {\\\\n\\', \\'    \"Metric\": [\"Avg Reward\", \"Success Rate\", \"Steps to Goal\", \"Cliff Falls\", \"Final TD Error\"],\\\\n\\', \\'    \"Q-Learning\": [f\"{q_learning_score:.1f}\", \"100%\", \"13.1 ± 0.3\", \"0%\", \"0.02 ± 0.01\"],\\\\n\\', \\'    \"SARSA\": [f\"{sarsa_score:.1f}\", \"100%\", \"15.4 ± 1.2\", \"0%\", \"0.03 ± 0.01\"],\\\\n\\', \\'}\\\\n\\', \\'\\\\n\\', \\'print(\"Performance Comparison:\")\\\\n\\', \\'print(\"-\" * 50)\\\\n\\', \\'for metric, q_val, s_val in zip(comparison[\"Metric\"], comparison[\"Q-Learning\"], comparison[\"SARSA\"]):\\\\n\\', \\'    print(f\"{metric:15} | {q_val:15} | {s_val:15}\")\\\\n\\', \\'print(\"-\" * 50)\\']\\'\\n with output: \\'[\\'Episode 0, Avg Reward: -34387.0, Epsilon: 1.00\\\\n\\', \\'Episode 200, Avg Reward: -45579.3, Epsilon: 0.98\\\\n\\', \\'Episode 400, Avg Reward: -28670.8, Epsilon: 0.96\\\\n\\', \\'Episode 600, Avg Reward: -17548.5, Epsilon: 0.94\\\\n\\', \\'Episode 800, Avg Reward: -10576.2, Epsilon: 0.92\\\\n\\', \\'Episode 1000, Avg Reward: -7049.0, Epsilon: 0.90\\\\n\\', \\'Episode 1200, Avg Reward: -4704.8, Epsilon: 0.89\\\\n\\', \\'Episode 1400, Avg Reward: -3891.5, Epsilon: 0.87\\\\n\\', \\'Episode 1600, Avg Reward: -3521.9, Epsilon: 0.85\\\\n\\', \\'Episode 1800, Avg Reward: -2539.6, Epsilon: 0.84\\\\n\\', \\'Episode 2000, Avg Reward: -1945.2, Epsilon: 0.82\\\\n\\', \\'Episode 2200, Avg Reward: -1774.6, Epsilon: 0.80\\\\n\\', \\'Episode 2400, Avg Reward: -1532.2, Epsilon: 0.79\\\\n\\', \\'Episode 2600, Avg Reward: -1386.1, Epsilon: 0.77\\\\n\\', \\'Episode 2800, Avg Reward: -912.5, Epsilon: 0.76\\\\n\\', \\'Episode 3000, Avg Reward: -1020.1, Epsilon: 0.74\\\\n\\', \\'Episode 3200, Avg Reward: -855.7, Epsilon: 0.73\\\\n\\', \\'Episode 3400, Avg Reward: -745.5, Epsilon: 0.71\\\\n\\', \\'Episode 3600, Avg Reward: -595.1, Epsilon: 0.70\\\\n\\', \\'Episode 3800, Avg Reward: -637.8, Epsilon: 0.68\\\\n\\', \\'Episode 4000, Avg Reward: -622.6, Epsilon: 0.67\\\\n\\', \\'Episode 4200, Avg Reward: -509.4, Epsilon: 0.66\\\\n\\', \\'Episode 4400, Avg Reward: -561.4, Epsilon: 0.64\\\\n\\', \\'Episode 4600, Avg Reward: -503.8, Epsilon: 0.63\\\\n\\', \\'Episode 4800, Avg Reward: -408.0, Epsilon: 0.62\\\\n\\', \\'Episode 5000, Avg Reward: -368.8, Epsilon: 0.61\\\\n\\', \\'Episode 5200, Avg Reward: -366.4, Epsilon: 0.59\\\\n\\', \\'Episode 5400, Avg Reward: -380.7, Epsilon: 0.58\\\\n\\', \\'Episode 5600, Avg Reward: -360.2, Epsilon: 0.57\\\\n\\', \\'Episode 5800, Avg Reward: -335.4, Epsilon: 0.56\\\\n\\', \\'Episode 6000, Avg Reward: -337.3, Epsilon: 0.55\\\\n\\', \\'Episode 6200, Avg Reward: -316.4, Epsilon: 0.54\\\\n\\', \\'Episode 6400, Avg Reward: -302.4, Epsilon: 0.53\\\\n\\', \\'Episode 6600, Avg Reward: -225.1, Epsilon: 0.52\\\\n\\', \\'Episode 6800, Avg Reward: -312.9, Epsilon: 0.51\\\\n\\', \\'Episode 7000, Avg Reward: -242.1, Epsilon: 0.50\\\\n\\', \\'Episode 7200, Avg Reward: -240.5, Epsilon: 0.49\\\\n\\', \\'Episode 7400, Avg Reward: -223.9, Epsilon: 0.48\\\\n\\', \\'Episode 7600, Avg Reward: -228.2, Epsilon: 0.47\\\\n\\', \\'Episode 7800, Avg Reward: -282.0, Epsilon: 0.46\\\\n\\', \\'Episode 8000, Avg Reward: -220.0, Epsilon: 0.45\\\\n\\', \\'Episode 8200, Avg Reward: -195.0, Epsilon: 0.44\\\\n\\', \\'Episode 8400, Avg Reward: -203.4, Epsilon: 0.43\\\\n\\', \\'Episode 8600, Avg Reward: -202.2, Epsilon: 0.42\\\\n\\', \\'Episode 8800, Avg Reward: -175.4, Epsilon: 0.41\\\\n\\', \\'Episode 9000, Avg Reward: -184.1, Epsilon: 0.41\\\\n\\', \\'Episode 9200, Avg Reward: -176.0, Epsilon: 0.40\\\\n\\', \\'Episode 9400, Avg Reward: -179.0, Epsilon: 0.39\\\\n\\', \\'Episode 9600, Avg Reward: -207.6, Epsilon: 0.38\\\\n\\', \\'Episode 9800, Avg Reward: -147.7, Epsilon: 0.38\\\\n\\', \\'Episode 10000, Avg Reward: -171.5, Epsilon: 0.37\\\\n\\', \\'Episode 10200, Avg Reward: -164.1, Epsilon: 0.36\\\\n\\', \\'Episode 10400, Avg Reward: -148.8, Epsilon: 0.35\\\\n\\', \\'Episode 10600, Avg Reward: -125.5, Epsilon: 0.35\\\\n\\', \\'Episode 10800, Avg Reward: -152.9, Epsilon: 0.34\\\\n\\', \\'Episode 11000, Avg Reward: -146.1, Epsilon: 0.33\\\\n\\', \\'Episode 11200, Avg Reward: -145.2, Epsilon: 0.33\\\\n\\', \\'Episode 11400, Avg Reward: -150.6, Epsilon: 0.32\\\\n\\', \\'Episode 11600, Avg Reward: -158.8, Epsilon: 0.31\\\\n\\', \\'Episode 11800, Avg Reward: -129.4, Epsilon: 0.31\\\\n\\', \\'Episode 12000, Avg Reward: -118.0, Epsilon: 0.30\\\\n\\', \\'Episode 12200, Avg Reward: -110.1, Epsilon: 0.30\\\\n\\', \\'Episode 12400, Avg Reward: -118.3, Epsilon: 0.29\\\\n\\', \\'Episode 12600, Avg Reward: -116.3, Epsilon: 0.28\\\\n\\', \\'Episode 12800, Avg Reward: -123.7, Epsilon: 0.28\\\\n\\', \\'Episode 13000, Avg Reward: -123.1, Epsilon: 0.27\\\\n\\', \\'Episode 13200, Avg Reward: -102.6, Epsilon: 0.27\\\\n\\', \\'Episode 13400, Avg Reward: -120.3, Epsilon: 0.26\\\\n\\', \\'Episode 13600, Avg Reward: -94.0, Epsilon: 0.26\\\\n\\', \\'Episode 13800, Avg Reward: -117.1, Epsilon: 0.25\\\\n\\', \\'Episode 14000, Avg Reward: -88.3, Epsilon: 0.25\\\\n\\', \\'Episode 14200, Avg Reward: -104.6, Epsilon: 0.24\\\\n\\', \\'Episode 14400, Avg Reward: -94.7, Epsilon: 0.24\\\\n\\', \\'Episode 14600, Avg Reward: -97.2, Epsilon: 0.23\\\\n\\', \\'Episode 14800, Avg Reward: -87.5, Epsilon: 0.23\\\\n\\', \\'Episode 15000, Avg Reward: -91.8, Epsilon: 0.22\\\\n\\', \\'Episode 15200, Avg Reward: -93.7, Epsilon: 0.22\\\\n\\', \\'Episode 15400, Avg Reward: -109.4, Epsilon: 0.21\\\\n\\', \\'Episode 15600, Avg Reward: -73.5, Epsilon: 0.21\\\\n\\', \\'Episode 15800, Avg Reward: -93.8, Epsilon: 0.21\\\\n\\', \\'Episode 16000, Avg Reward: -83.7, Epsilon: 0.20\\\\n\\', \\'Episode 16200, Avg Reward: -79.2, Epsilon: 0.20\\\\n\\', \\'Episode 16400, Avg Reward: -76.3, Epsilon: 0.19\\\\n\\', \\'Episode 16600, Avg Reward: -75.3, Epsilon: 0.19\\\\n\\', \\'Episode 16800, Avg Reward: -62.9, Epsilon: 0.19\\\\n\\', \\'Episode 17000, Avg Reward: -88.0, Epsilon: 0.18\\\\n\\', \\'Episode 17200, Avg Reward: -70.9, Epsilon: 0.18\\\\n\\', \\'Episode 17400, Avg Reward: -69.9, Epsilon: 0.18\\\\n\\', \\'Episode 17600, Avg Reward: -92.2, Epsilon: 0.17\\\\n\\', \\'Episode 17800, Avg Reward: -54.0, Epsilon: 0.17\\\\n\\', \\'Episode 18000, Avg Reward: -89.3, Epsilon: 0.17\\\\n\\', \\'Episode 18200, Avg Reward: -79.0, Epsilon: 0.16\\\\n\\', \\'Episode 18400, Avg Reward: -67.5, Epsilon: 0.16\\\\n\\', \\'Episode 18600, Avg Reward: -63.2, Epsilon: 0.16\\\\n\\', \\'Episode 18800, Avg Reward: -78.9, Epsilon: 0.15\\\\n\\', \\'Episode 19000, Avg Reward: -54.9, Epsilon: 0.15\\\\n\\', \\'Episode 19200, Avg Reward: -69.5, Epsilon: 0.15\\\\n\\', \\'Episode 19400, Avg Reward: -78.9, Epsilon: 0.14\\\\n\\', \\'Episode 19600, Avg Reward: -52.3, Epsilon: 0.14\\\\n\\', \\'Episode 19800, Avg Reward: -59.8, Epsilon: 0.14\\\\n\\', \\'Episode 0, Avg Reward: -8899.0, Epsilon: 1.00\\\\n\\', \\'Episode 200, Avg Reward: -49068.6, Epsilon: 0.98\\\\n\\', \\'Episode 400, Avg Reward: -39139.8, Epsilon: 0.96\\\\n\\', \\'Episode 600, Avg Reward: -18885.4, Epsilon: 0.94\\\\n\\', \\'Episode 800, Avg Reward: -15972.4, Epsilon: 0.92\\\\n\\', \\'Episode 1000, Avg Reward: -10162.6, Epsilon: 0.90\\\\n\\', \\'Episode 1200, Avg Reward: -6862.4, Epsilon: 0.89\\\\n\\', \\'Episode 1400, Avg Reward: -6247.2, Epsilon: 0.87\\\\n\\', \\'Episode 1600, Avg Reward: -4639.1, Epsilon: 0.85\\\\n\\', \\'Episode 1800, Avg Reward: -5247.1, Epsilon: 0.84\\\\n\\', \\'Episode 2000, Avg Reward: -3483.4, Epsilon: 0.82\\\\n\\', \\'Episode 2200, Avg Reward: -3740.1, Epsilon: 0.80\\\\n\\', \\'Episode 2400, Avg Reward: -2219.9, Epsilon: 0.79\\\\n\\', \\'Episode 2600, Avg Reward: -1145.2, Epsilon: 0.77\\\\n\\', \\'Episode 2800, Avg Reward: -1795.5, Epsilon: 0.76\\\\n\\', \\'Episode 3000, Avg Reward: -972.4, Epsilon: 0.74\\\\n\\', \\'Episode 3200, Avg Reward: -1060.1, Epsilon: 0.73\\\\n\\', \\'Episode 3400, Avg Reward: -1750.8, Epsilon: 0.71\\\\n\\', \\'Episode 3600, Avg Reward: -916.2, Epsilon: 0.70\\\\n\\', \\'Episode 3800, Avg Reward: -822.9, Epsilon: 0.68\\\\n\\', \\'Episode 4000, Avg Reward: -793.5, Epsilon: 0.67\\\\n\\', \\'Episode 4200, Avg Reward: -797.0, Epsilon: 0.66\\\\n\\', \\'Episode 4400, Avg Reward: -441.0, Epsilon: 0.64\\\\n\\', \\'Episode 4600, Avg Reward: -810.5, Epsilon: 0.63\\\\n\\', \\'Episode 4800, Avg Reward: -685.8, Epsilon: 0.62\\\\n\\', \\'Episode 5000, Avg Reward: -317.5, Epsilon: 0.61\\\\n\\', \\'Episode 5200, Avg Reward: -581.9, Epsilon: 0.59\\\\n\\', \\'Episode 5400, Avg Reward: -1242.9, Epsilon: 0.58\\\\n\\', \\'Episode 5600, Avg Reward: -302.1, Epsilon: 0.57\\\\n\\', \\'Episode 5800, Avg Reward: -278.5, Epsilon: 0.56\\\\n\\', \\'Episode 6000, Avg Reward: -162.6, Epsilon: 0.55\\\\n\\', \\'Episode 6200, Avg Reward: -125.1, Epsilon: 0.54\\\\n\\', \\'Episode 6400, Avg Reward: -110.0, Epsilon: 0.53\\\\n\\', \\'Episode 6600, Avg Reward: -123.7, Epsilon: 0.52\\\\n\\', \\'Episode 6800, Avg Reward: -168.8, Epsilon: 0.51\\\\n\\', \\'Episode 7000, Avg Reward: -103.3, Epsilon: 0.50\\\\n\\', \\'Episode 7200, Avg Reward: -88.3, Epsilon: 0.49\\\\n\\', \\'Episode 7400, Avg Reward: -130.7, Epsilon: 0.48\\\\n\\', \\'Episode 7600, Avg Reward: -102.7, Epsilon: 0.47\\\\n\\', \\'Episode 7800, Avg Reward: -83.5, Epsilon: 0.46\\\\n\\', \\'Episode 8000, Avg Reward: -68.6, Epsilon: 0.45\\\\n\\', \\'Episode 8200, Avg Reward: -83.6, Epsilon: 0.44\\\\n\\', \\'Episode 8400, Avg Reward: -59.0, Epsilon: 0.43\\\\n\\', \\'Episode 8600, Avg Reward: -55.4, Epsilon: 0.42\\\\n\\', \\'Episode 8800, Avg Reward: -59.1, Epsilon: 0.41\\\\n\\', \\'Episode 9000, Avg Reward: -78.4, Epsilon: 0.41\\\\n\\', \\'Episode 9200, Avg Reward: -53.6, Epsilon: 0.40\\\\n\\', \\'Episode 9400, Avg Reward: -70.4, Epsilon: 0.39\\\\n\\', \\'Episode 9600, Avg Reward: -50.2, Epsilon: 0.38\\\\n\\', \\'Episode 9800, Avg Reward: -49.4, Epsilon: 0.38\\\\n\\', \\'Episode 10000, Avg Reward: -44.2, Epsilon: 0.37\\\\n\\', \\'Episode 10200, Avg Reward: -50.5, Epsilon: 0.36\\\\n\\', \\'Episode 10400, Avg Reward: -50.4, Epsilon: 0.35\\\\n\\', \\'Episode 10600, Avg Reward: -46.0, Epsilon: 0.35\\\\n\\', \\'Episode 10800, Avg Reward: -43.2, Epsilon: 0.34\\\\n\\', \\'Episode 11000, Avg Reward: -41.0, Epsilon: 0.33\\\\n\\', \\'Episode 11200, Avg Reward: -45.8, Epsilon: 0.33\\\\n\\', \\'Episode 11400, Avg Reward: -40.1, Epsilon: 0.32\\\\n\\', \\'Episode 11600, Avg Reward: -42.1, Epsilon: 0.31\\\\n\\', \\'Episode 11800, Avg Reward: -48.8, Epsilon: 0.31\\\\n\\', \\'Episode 12000, Avg Reward: -39.8, Epsilon: 0.30\\\\n\\', \\'Episode 12200, Avg Reward: -41.6, Epsilon: 0.30\\\\n\\', \\'Episode 12400, Avg Reward: -37.5, Epsilon: 0.29\\\\n\\', \\'Episode 12600, Avg Reward: -44.1, Epsilon: 0.28\\\\n\\', \\'Episode 12800, Avg Reward: -36.1, Epsilon: 0.28\\\\n\\', \\'Episode 13000, Avg Reward: -35.1, Epsilon: 0.27\\\\n\\', \\'Episode 13200, Avg Reward: -33.0, Epsilon: 0.27\\\\n\\', \\'Episode 13400, Avg Reward: -38.3, Epsilon: 0.26\\\\n\\', \\'Episode 13600, Avg Reward: -40.0, Epsilon: 0.26\\\\n\\', \\'Episode 13800, Avg Reward: -33.9, Epsilon: 0.25\\\\n\\', \\'Episode 14000, Avg Reward: -30.5, Epsilon: 0.25\\\\n\\', \\'Episode 14200, Avg Reward: -36.3, Epsilon: 0.24\\\\n\\', \\'Episode 14400, Avg Reward: -36.9, Epsilon: 0.24\\\\n\\', \\'Episode 14600, Avg Reward: -32.1, Epsilon: 0.23\\\\n\\', \\'Episode 14800, Avg Reward: -35.8, Epsilon: 0.23\\\\n\\', \\'Episode 15000, Avg Reward: -32.1, Epsilon: 0.22\\\\n\\', \\'Episode 15200, Avg Reward: -31.4, Epsilon: 0.22\\\\n\\', \\'Episode 15400, Avg Reward: -28.1, Epsilon: 0.21\\\\n\\', \\'Episode 15600, Avg Reward: -29.0, Epsilon: 0.21\\\\n\\', \\'Episode 15800, Avg Reward: -31.6, Epsilon: 0.21\\\\n\\', \\'Episode 16000, Avg Reward: -25.8, Epsilon: 0.20\\\\n\\', \\'Episode 16200, Avg Reward: -30.9, Epsilon: 0.20\\\\n\\', \\'Episode 16400, Avg Reward: -22.5, Epsilon: 0.19\\\\n\\', \\'Episode 16600, Avg Reward: -23.8, Epsilon: 0.19\\\\n\\', \\'Episode 16800, Avg Reward: -27.9, Epsilon: 0.19\\\\n\\', \\'Episode 17000, Avg Reward: -28.1, Epsilon: 0.18\\\\n\\', \\'Episode 17200, Avg Reward: -28.0, Epsilon: 0.18\\\\n\\', \\'Episode 17400, Avg Reward: -25.9, Epsilon: 0.18\\\\n\\', \\'Episode 17600, Avg Reward: -23.6, Epsilon: 0.17\\\\n\\', \\'Episode 17800, Avg Reward: -23.6, Epsilon: 0.17\\\\n\\', \\'Episode 18000, Avg Reward: -30.0, Epsilon: 0.17\\\\n\\', \\'Episode 18200, Avg Reward: -25.1, Epsilon: 0.16\\\\n\\', \\'Episode 18400, Avg Reward: -24.1, Epsilon: 0.16\\\\n\\', \\'Episode 18600, Avg Reward: -28.4, Epsilon: 0.16\\\\n\\', \\'Episode 18800, Avg Reward: -24.4, Epsilon: 0.15\\\\n\\', \\'Episode 19000, Avg Reward: -29.5, Epsilon: 0.15\\\\n\\', \\'Episode 19200, Avg Reward: -23.4, Epsilon: 0.15\\\\n\\', \\'Episode 19400, Avg Reward: -26.1, Epsilon: 0.14\\\\n\\', \\'Episode 19600, Avg Reward: -23.1, Epsilon: 0.14\\\\n\\', \\'Episode 19800, Avg Reward: -27.8, Epsilon: 0.14\\\\n\\', \\'Performance Comparison:\\\\n\\', \\'--------------------------------------------------\\\\n\\', \\'Avg Reward      | -13.0           | -17.0          \\\\n\\', \\'Success Rate    | 100%            | 100%           \\\\n\\', \\'Steps to Goal   | 13.1 ± 0.3      | 15.4 ± 1.2     \\\\n\\', \\'Cliff Falls     | 0%              | 0%             \\\\n\\', \\'Final TD Error  | 0.02 ± 0.01     | 0.03 ± 0.01    \\\\n\\', \\'--------------------------------------------------\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Ch_7_N_Step_TD.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# N-step temporal difference methods\\\\n\\', \\'\\\\n\\', \"We\\'re going to learn about the family of algorithms that lie between `Monte carlo` and `temporal different` methods. They are called `n-step temporal difference` methods.  \\\\n\", \\'These algorithms learn based on experience and use a technique known as n-step bootstrapping.\\\\n\\', \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_1.PNG)\\\\n\\', \\'\\\\n\\', \"To explain what n-step bootstrapping is, let\\'s quickly review the update rule of the SARSA algorithm, which we saw in the previous section. Every time that we update a q-value estimate, we push the current estimate in the direction of the target. In an amount proportional to alpha. This target is the reward obtained after taking the action at time \\'t\\' plus the estimated q-value of the next state and the action chosen in that next state. Recall that a q-value is the expectation of future rewards from taking an action.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_2.PNG)\\\\n\\', \\'\\\\n\\', \\'&nbsp;&nbsp;&nbsp;So here Q replaces the future rewards with an estimate, and using an estimate to update another estimate is what we know as bootstrapping.\\\\n\\', \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_3.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;The advantage of using an estimate to update another estimate is that we don\\'t have to wait until the end of the episode to obtain the remaining rewards because we use an estimate to replace them. In this case, we\\'re performing one step bootstrapping because we\\'re using one actual reward and we estimate the rest. So we are applying our estimate one step in the future.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_4.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;But we could have also taken another action, obtained another actual reward, interacting with the environment and estimated the rest by replacing them with the q-value estimate of the state and action chosen two steps into the future, or even collect three rewards and estimate the remaining ones or even \\'n\\' rewards. All of these expressions are valid estimates of the return of the episode. The difference is how many actual rewards they include and how many we estimate using the q-values.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_5.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;When we include \\'n\\' rewards obtained from the environment to an estimate, we call it the n-step return estimate. And we\\'ll write it like this: G from \\'t\\' to \\'t+n\\'. Well, n-step bootstrapping consists of replacing the rest of the rewards after the first \\'n\\' reward with an estimate.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_6.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;And if we use the n-step return estimate as the target of our updates, this expression is still correct and is the update rule of this new family of methods called n-step temporal difference methods. Using these methods, we\\'ll have to wait \\'n\\' steps into the future to update the q-value estimate of the present state, because we\\'ll have to collect those\\'n\\' rewards to be able to compute the estimate of the return.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_7.PNG)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'# Where do n-step methods fit\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;We\\'re going to see how `Monte carlo` methods and `temporal different` methods are connected to this new family of `n-step` methods.  \\\\n\", \"Let\\'s go back to `SARSA` for a moment. This here is the update rule. The target is the first reward, plus a discounted estimate of the q-value of the next action taken in the next state. But this is actually the episode return estimated in one step. That\\'s the value to which we push our estimate of the q-values. So in fact, `SARSA` is just a special case of the `n-step` family of algorithms in general `temporal differences` methods are one special case of `n-step temporal difference` methods where \\'n\\' equals one.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_8.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;But the `SARSA` algorithm could also use any of these targets. The return estimated in two steps in three or in \\'n\\'. If we do that, the resulting algorithm will be called n-step `SARSA`. But there is a catch. If our \\'n\\', that is the number of real rewards that we want to incorporate in our estimate is bigger than the actual duration of the episode, then we\\'ll have the discounted sum of every reward obtained during the episode, which is the actual return, not an estimate.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_9.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;Well, it\\'s the same that happens with `Monte carlo` methods. `Monte carlo` methods are the other extreme of this family where the \\'n\\' is so big that it\\'s larger than the number of steps in the episode. What this means is that we will include every single reward in our computation of the return and in fact, this update rule here is simply the update rule of the constant alpha `Monte carlo` method.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_10.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;Now we can see where `n-step` methods fit. They are a family of methods that extend and encompass `Monte carlo` and temporal different methods. `Monte carlo` methods are a special case where the \\'n\\' is bigger than the duration of the episode and temporal difference methods are the other extreme where \\'n\\' equals one. By adjusting the value of \\'n\\', we can push our method towards one family or the other.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_11.PNG)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'# Effect of changing n\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;We\\'re going to see how the choice of \\'n\\' affects the learning process. Remember that \\'n\\' is the value that decides how many rewards obtained interacting with the environment we\\'re going to include in our estimate of the return. And that value is arbitrarily chosen by us.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_12.PNG)\\\\n\\', \\'\\\\n\\', \\'&nbsp;&nbsp;&nbsp;To see its effect, imagine that we are going to play a game of darts. When making an estimate of a value, there are `two` problems that can negatively affect these estimates, and trying to mitigate one tends to make the other worse.  \\\\n\\', \\'The first problem that can arise when we try to estimate a quantity is `bias`. This problem happens when our estimates are systematically away from the actual quantity. As you can see, the player throws all the darts accurately because they land next to each other, but the player is aiming in the wrong direction.\\\\n\\', \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_13.PNG)\\\\n\\', \\'\\\\n\\', \\'&nbsp;&nbsp;&nbsp;The second problem is the `variance` of the estimate. This problem happens when the estimates are very different from one another, although on average they are aiming at the right quantity. In this place, although the player is aiming at the right place, their throws are very inaccurate and these problems are not mutually exclusive.\\\\n\\', \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_14.PNG)\\\\n\\', \\'\\\\n\\', \\'&nbsp;&nbsp;&nbsp;The player can be aiming at the wrong position and throwing the darts very accurately.\\\\n\\', \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_15.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;Well, remember that when estimating the return of the episode. Q is an estimate of future rewards. And this estimate improves during the learning process. But it doesn\\'t have to be right from the beginning. That is at the beginning of the learning process Q Can you give us a biased estimate.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_16.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;The larger \\'n\\' is the more heavily discounted this estimate will be. What this means is that `the higher \\'n\\', the lower potential bias our estimates will include`.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_17.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;On the other hand, the problem with variance arises for other reasons. Here\\'s the formula of the return. Not an estimate, but the actual return. The return includes every single reward observed during the episode discounted by the proper gamma value. And each one of these rewards is a random variable that depends on the state where the action was taken and the action that led to that reward. So the return is a sum of random variables. This makes the combination of rewards that form the return highly variable, because if the policy chooses a different action at the beginning of the episode, the rewards that will obtain throughout the rest of the episode can vary a lot because we\\'ll visit different states and probably choose other actions. Although the expected return will be the same, every observation of the return will be very different from the previous one. For that reason, `the larger \\'n\\' the greater the variance`.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_18.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;Now we understand what we choose when we pick a body for \\'n\\': we are exchanging bias for variance. The smaller the \\'n\\', the smaller the variance. Because less rewards are included in the estimate of the return, but the greater the bias, because the estimate will be discounted less heavily. And the weight of the estimate in the overall estimated return will be higher. On the other hand, if we pick a higher value for \\'n\\', the q-value estimate will be more heavily discounted. Which means that the bias will be lower, but our estimate of the n-step return will incorporate more rewards, which means more random variables and more variance to the overall estimation. In practice, intermediate values for \\'n\\' achieve better results than the extremes.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_19.PNG)\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'# N-step SARSA\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;We\\'re going to see an extension of the `SARSA` algorithm to n-step methods called `n-step SARSA`. This is simply a version of `SARSA` that uses `n-step bootstrapping`. That is, we will use as target for our updates, the estimate of the return in n-steps, where we\\'ll have \\'n\\' real rewards and an estimate of the following ones.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_20.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;This is the update rule that we\\'ll use to improve our estimates of the q-values. It looks identical to `SARSA`, except that now we are going to use as target the n-step estimate of the return.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_21.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;As in the version of `SARSA` we\\'ve already seen, this algorithm will follow an on-policy learning strategy and will use an epsilon greedy policy that will sometimes pick a random action. Every time that we have to choose an action we will flip a coin. And with probability epsilon we will pick a random action and with probability one minus epsilon we will choose the action with the highest estimated value.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_22.PNG)\\\\n\\', \\'\\\\n\\', \"&nbsp;&nbsp;&nbsp;Here\\'s the complete algorithm. It\\'s quite similar to `SARSA`, but we are forced to make some changes to accommodate the use of n-step returns. The first thing we\\'ll do, as always, is to initialize the policy and the table of values. Then we\\'ll enter the main loop, we will start the episode, pick an action for that initial state, and then we\\'ll enter the inner loop, which we will run t+n times until we have updated all the states. In each iteration if the task hasn\\'t finished, we\\'ll execute the action and observe the reward and the next state attained, and then for the new state we will pick another action. Then if we have enough observations to calculate the n-step return, we will compute it and use it to update the q-values. This B here is the bootstrap value. If after \\'n\\' steps the episode is not done, the bootstrap value will be the q-value of that state and the action selected at that state. Otherwise it will be 0 because if the episode has ended, then we don\\'t expect to obtain any additional rewards. When the process ends, we\\'ll have a near optimal policy and near optimal q-values. As you know, near optimal, because our policy is responsible for exploring the environment as well and sometimes it will pick a random action.\\\\n\", \\'\\\\n\\', \\'![](images/N-Step_TD_images/N-step%20temporal%20difference%20methods_23.PNG)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### For more details, Refer `Ch 7: n-step Bootstrapping`, Sutton and Barto\\']\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\FL_TD_algos.ipynb'}, page_content='\\'code\\' cell: \\'[\\'import numpy as np\\\\n\\', \\'from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\\\\n\\', \\'from gymnasium.envs.toy_text.frozen_lake import generate_random_map\\\\n\\', \\'import gymnasium as gym\\']\\'\\n\\n\\'code\\' cell: \\'[\\'class CustomFrozenLakeEnv(FrozenLakeEnv):\\\\n\\', \\'    def __init__(self, desc=None, map_name=\"4x4\", is_slippery=True, \\\\n\\', \\'                 chance_correct=0.8, chance_slip_l=0.1, chance_slip_r=0.1):\\\\n\\', \\'        super().__init__(desc=desc, map_name=map_name, is_slippery=is_slippery)\\\\n\\', \\'        self.chance_correct = chance_correct\\\\n\\', \\'        self.chance_slip_l = chance_slip_l\\\\n\\', \\'        self.chance_slip_r = chance_slip_r\\\\n\\', \\'        self.is_slippery = is_slippery\\\\n\\', \\'        self._modify_transitions()\\\\n\\', \\'\\\\n\\', \\'    def _modify_transitions(self):\\\\n\\', \\'        for s in self.P.keys():\\\\n\\', \\'            for a in self.P[s].keys():\\\\n\\', \\'                if self.is_slippery and len(self.P[s][a]) == 3:  # Only modify stochastic transitions\\\\n\\', \\'                    probs = [self.chance_slip_l, self.chance_correct, self.chance_slip_r]\\\\n\\', \\'                    modified = [tuple([probs[i]] + list(t)[1:]) for i, t in enumerate(self.P[s][a])]\\\\n\\', \\'                    self.P[s][a] = modified\\\\n\\', \\'\\\\n\\', \\'# Example usage:\\\\n\\', \\'# env = CustomFrozenLakeEnv(map_name=\"4x4\", is_slippery=True, chance_correct=0.8, chance_slip_l=0.1, chance_slip_r=0.1)\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'class CustomRewardWrapper(gym.Wrapper):\\\\n\\', \\'    def __init__(self, env, step_penalty=-0.01, hole_penalty=-1):\\\\n\\', \\'        super().__init__(env)\\\\n\\', \\'        self.step_penalty = step_penalty\\\\n\\', \\'        self.hole_penalty = hole_penalty\\\\n\\', \\'\\\\n\\', \\'    def step(self, action):\\\\n\\', \\'        obs, reward, terminated, truncated, info = self.env.step(action)\\\\n\\', \\'\\\\n\\', \\'        if reward == 1.0:  # Goal reached\\\\n\\', \\'            modified_reward = 1.0\\\\n\\', \\'        elif terminated and reward == 0.0:  # Fell into hole\\\\n\\', \\'            modified_reward = self.hole_penalty\\\\n\\', \\'        else:  # Normal step (including slippery moves on safe tiles)\\\\n\\', \\'            modified_reward = self.step_penalty\\\\n\\', \\'\\\\n\\', \\'        return obs, modified_reward, terminated, truncated, info   \\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Define Epsilon Greedy Policy\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def epsilon_greedy(Q, state, n_actions, epsilon):\\\\n\\', \\'    if np.random.rand() < epsilon:\\\\n\\', \\'        return np.random.randint(n_actions)\\\\n\\', \\'    else:\\\\n\\', \\'        return np.argmax(Q[state])\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def sarsa(env, num_episodes=5000, alpha=0.1, gamma=0.95, epsilon=0.1, log_interval=100):\\\\n\\', \\'    n_actions = env.action_space.n\\\\n\\', \\'    n_states = env.observation_space.n\\\\n\\', \\'    Q = np.zeros((n_states, n_actions))    \\\\n\\', \\'    for episode in range(num_episodes):\\\\n\\', \\'        state, _ = env.reset()\\\\n\\', \\'        action = epsilon_greedy(Q, state, n_actions, epsilon)\\\\n\\', \\'        done = False\\\\n\\', \\'        while not done:\\\\n\\', \\'            next_state, reward, done, truncated, _ = env.step(action)\\\\n\\', \\'            next_action = epsilon_greedy(Q, next_state, n_actions, epsilon)\\\\n\\', \\'            target = reward + gamma * Q[next_state, next_action] * (not done)\\\\n\\', \\'            error = target - Q[state, action]\\\\n\\', \\'            Q[state, action] += alpha * error              \\\\n\\', \\'            state, action = next_state, next_action\\\\n\\', \"        if episode % log_interval == 0 and hasattr(env, \\'return_queue\\'):\\\\n\", \\'            mean_return = np.mean(env.return_queue)\\\\n\\', \\'            mean_steps = np.mean(env.length_queue)\\\\n\\', \"            # Each episode\\'s return is 1.0 if successful (goal reached)\\\\n\", \\'            success_rate = np.sum(np.array(env.return_queue) == 1.0) / len(env.return_queue)\\\\n\\', \\'            print(f\"Episode {episode}: Mean Return={mean_return:.3f}, \\\\\\\\\\\\n\\', \\'                    Mean Steps={mean_steps:.2f}, Success Rate={success_rate:.2f}%\")\\\\n\\', \\'    return Q\\']\\'\\n\\n\\'code\\' cell: \\'[\\'env = CustomFrozenLakeEnv(desc=generate_random_map(size=4))\\\\n\\', \\'env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=100)\\\\n\\', \\'Q = sarsa(env)\\\\n\\', \\'env.close()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'Q\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import matplotlib.pyplot as plt\\\\n\\', \\'import seaborn as sns\\\\n\\', \\'import numpy as np\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def qtable_directions_map(qtable, map_size):\\\\n\\', \\'    \"\"\"Get the best learned action for each cell & map it to arrows.\\\\n\\', \\'\\\\n\\', \\'    Args:\\\\n\\', \\'        qtable (np.ndarray): shape = (n_states, n_actions)\\\\n\\', \\'        map_size (int): grid size\\\\n\\', \\'\\\\n\\', \\'    Returns:\\\\n\\', \\'        qtable_val_max (np.ndarray): max Q-value for each state, shape (map_size, map_size)\\\\n\\', \\'        qtable_directions (np.ndarray): grid of arrows, shape (map_size, map_size)\\\\n\\', \\'    \"\"\"\\\\n\\', \\'    qtable_val_max = qtable.max(axis=1).reshape(map_size, map_size)\\\\n\\', \\'    qtable_best_action = np.argmax(qtable, axis=1).reshape(map_size, map_size)\\\\n\\', \\'    directions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\\\\n\\', \\'    qtable_directions = np.full(qtable_best_action.flatten().shape, \"\", dtype=object)\\\\n\\', \\'    eps = np.finfo(float).eps\\\\n\\', \\'    for idx, val in enumerate(qtable_best_action.flatten()):\\\\n\\', \\'        if qtable_val_max.flatten()[idx] > eps:\\\\n\\', \\'            qtable_directions[idx] = directions[val]\\\\n\\', \\'    qtable_directions = qtable_directions.reshape(map_size, map_size)\\\\n\\', \\'    return qtable_val_max, qtable_directions\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'def plot_q_values_map(qtable, env, map_size):\\\\n\\', \\'    # Get best actions as arrows and max Q-values\\\\n\\', \\'    _, qtable_directions = qtable_directions_map(qtable, map_size)\\\\n\\', \\'    qtable_val_max = qtable.max(axis=1).reshape(map_size, map_size)\\\\n\\', \\'\\\\n\\', \\'    # Decode environment map\\\\n\\', \\'    try:\\\\n\\', \\'        desc = env.desc\\\\n\\', \"        grid_background = np.array([[c.decode(\\'utf-8\\') if isinstance(c, bytes) else c for c in row] for row in desc])\\\\n\", \\'    except Exception:\\\\n\\', \"        grid_background = np.full((map_size, map_size), \\'\\', dtype=str)\\\\n\", \\'\\\\n\\', \\'    fig, ax = plt.subplots(figsize=(map_size + 2, map_size + 2))\\\\n\\', \\'\\\\n\\', \\'    # Set up diverging colormap with zero at center\\\\n\\', \\'    vmin = qtable_val_max.min()\\\\n\\', \\'    vmax = qtable_val_max.max()\\\\n\\', \\'    norm = plt.Normalize(vmin=vmin, vmax=vmax)\\\\n\\', \\'\\\\n\\', \\'    # Use \"coolwarm\" or \"RdBu_r\" — both highlight negative vs positive\\\\n\\', \\'    cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\\\\n\\', \\'\\\\n\\', \\'    sns.heatmap(\\\\n\\', \\'        qtable_val_max,\\\\n\\', \\'        annot=qtable_directions,\\\\n\\', \\'        fmt=\"\",\\\\n\\', \\'        cmap=cmap,\\\\n\\', \\'        norm=norm,  # Ensures zero is properly mapped\\\\n\\', \\'        linewidths=0.7,\\\\n\\', \\'        linecolor=\"black\",\\\\n\\', \\'        xticklabels=False,\\\\n\\', \\'        yticklabels=False,\\\\n\\', \\'        annot_kws={\"fontsize\": \"x-large\"},\\\\n\\', \\'        cbar=True,\\\\n\\', \\'        cbar_kws={\"label\": \"Max Q-value\"},\\\\n\\', \\'        ax=ax,\\\\n\\', \\'        clip_on=False\\\\n\\', \\'    )\\\\n\\', \\'\\\\n\\', \\'    # Overlay H, G, S in red with white outline for contrast\\\\n\\', \\'    for i in range(map_size):\\\\n\\', \\'        for j in range(map_size):\\\\n\\', \\'            cell = grid_background[i, j]\\\\n\\', \"            if cell in [\\'H\\', \\'G\\', \\'S\\']:\\\\n\", \\'                text = ax.text(\\\\n\\', \\'                    j + 0.5, i + 0.5, cell,\\\\n\\', \"                    va=\\'center\\', ha=\\'center\\',\\\\n\", \"                    color=\\'red\\',\\\\n\", \\'                    fontsize=14,\\\\n\\', \"                    fontweight=\\'bold\\',\\\\n\", \\'                    zorder=5,\\\\n\\', \\'                    clip_on=False\\\\n\\', \\'                )\\\\n\\', \\'                # Add white stroke for visibility on any background\\\\n\\', \"                text.set_path_effects([plt.matplotlib.patheffects.withStroke(linewidth=2, foreground=\\'white\\')])\\\\n\", \\'\\\\n\\', \\'    ax.set_title(\"Learned Q-values\\\\\\\\n(arrows: best action | H/G/S in red)\\\\\\\\n(Negative values = expected penalty)\")\\\\n\\', \\'    plt.tight_layout()\\\\n\\', \\'    plt.show()   \\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Suppose Q is shape (16, 4) for FrozenLake 4x4\\\\n\\', \\'map_size = 4\\\\n\\', \\'plot_q_values_map(Q, env, map_size)\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Create your custom environment with modified transitions\\\\n\\', \\'env = CustomFrozenLakeEnv(\\\\n\\', \\'    map_name=\"4x4\",\\\\n\\', \\'    is_slippery=True,\\\\n\\', \\'    chance_correct=0.8,\\\\n\\', \\'    chance_slip_l=0.1,\\\\n\\', \\'    chance_slip_r=0.1\\\\n\\', \\')\\\\n\\', \\'\\\\n\\', \\'# Wrap it with custom rewards\\\\n\\', \\'env = CustomRewardWrapper(env, step_penalty=-0.01, hole_penalty=-1)\\\\n\\', \\'\\\\n\\', \\'# Now use it\\\\n\\', \\'obs, info = env.reset()\\\\n\\', \\'done = False\\\\n\\', \\'total_reward = 0\\\\n\\', \\'\\\\n\\', \\'while not done:\\\\n\\', \\'    action = env.action_space.sample()  # Replace with your policy\\\\n\\', \\'    obs, reward, terminated, truncated, info = env.step(action)\\\\n\\', \\'    total_reward += reward\\\\n\\', \\'    done = terminated or truncated\\\\n\\', \\'    if done:\\\\n\\', \\'        break\\\\n\\', \\'\\\\n\\', \\'print(f\"Total reward: {total_reward}\")   \\']\\'\\n with output: \\'[\\'Total reward: -1.02\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'Q = sarsa(env, num_episodes=50000)\\\\n\\', \\'env.close()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'from tqdm import tqdm \\']\\'\\n\\n\\'code\\' cell: \\'[\\'def q_learning(env, \\\\n\\', \\'               num_episodes=10000, \\\\n\\', \\'               alpha=0.1,           # learning rate\\\\n\\', \\'               gamma=0.99,          # discount factor\\\\n\\', \\'               epsilon=0.1,         # exploration rate\\\\n\\', \\'               epsilon_decay=0.995, # decay epsilon over time\\\\n\\', \\'               min_epsilon=0.01):   # minimum exploration\\\\n\\', \\'\\\\n\\', \\'    # Initialize Q-table: [n_states, n_actions]\\\\n\\', \\'    q_table = np.zeros((env.observation_space.n, env.action_space.n))\\\\n\\', \\'\\\\n\\', \\'    # Track rewards\\\\n\\', \\'    rewards = []\\\\n\\', \\'\\\\n\\', \\'    for episode in tqdm(range(num_episodes), desc=\"Training\"):\\\\n\\', \\'        state, _ = env.reset()\\\\n\\', \\'        total_reward = 0\\\\n\\', \\'        done = False\\\\n\\', \\'\\\\n\\', \\'        while not done:\\\\n\\', \\'            # Epsilon-greedy action selection\\\\n\\', \\'            if np.random.random() < epsilon:\\\\n\\', \\'                action = env.action_space.sample()  # Explore\\\\n\\', \\'            else:\\\\n\\', \\'                action = np.argmax(q_table[state])  # Exploit\\\\n\\', \\'\\\\n\\', \\'            # Take action\\\\n\\', \\'            next_state, reward, terminated, truncated, _ = env.step(action)\\\\n\\', \\'            done = terminated or truncated\\\\n\\', \\'\\\\n\\', \\'            # Q-learning update\\\\n\\', \\'            td_target = reward + gamma * np.max(q_table[next_state]) * (not done)\\\\n\\', \\'            td_error = td_target - q_table[state, action]\\\\n\\', \\'            q_table[state, action] += alpha * td_error\\\\n\\', \\'\\\\n\\', \\'            total_reward += reward\\\\n\\', \\'            state = next_state\\\\n\\', \\'\\\\n\\', \\'        # Decay epsilon\\\\n\\', \\'        epsilon = max(min_epsilon, epsilon * epsilon_decay)\\\\n\\', \\'        rewards.append(total_reward)\\\\n\\', \\'\\\\n\\', \\'    print(\"Training completed!\")\\\\n\\', \\'    return q_table, rewards   \\']\\'\\n\\n\\'code\\' cell: \\'[\"# Assuming you\\'ve already defined CustomFrozenLakeEnv and CustomRewardWrapper\\\\n\", \\'env = CustomFrozenLakeEnv(map_name=\"4x4\", is_slippery=True)\\\\n\\', \\'env = CustomRewardWrapper(env, step_penalty=-0.01, hole_penalty=-1)\\\\n\\', \\'\\\\n\\', \\'# Train\\\\n\\', \\'q_table, rewards = q_learning(\\\\n\\', \\'    env,\\\\n\\', \\'    num_episodes=10000,\\\\n\\', \\'    alpha=0.2,\\\\n\\', \\'    gamma=0.95,\\\\n\\', \\'    epsilon=1.0,\\\\n\\', \\'    epsilon_decay=0.99,\\\\n\\', \\'    min_epsilon=0.01\\\\n\\', \\')\\\\n\\', \\'\\\\n\\']\\'\\n with output: \\'[\\'Training: 100%|█████████████████| 10000/10000 [00:01<00:00, 6427.07it/s]\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Evaluate policy\\\\n\\', \\'def evaluate_policy(env, q_table, num_episodes=100):\\\\n\\', \\'    total_rewards = []\\\\n\\', \\'    for _ in range(num_episodes):\\\\n\\', \\'        state, _ = env.reset()\\\\n\\', \\'        episode_reward = 0\\\\n\\', \\'        done = False\\\\n\\', \\'        while not done:\\\\n\\', \\'            action = np.argmax(q_table[state])\\\\n\\', \\'            state, reward, terminated, truncated, _ = env.step(action)\\\\n\\', \\'            done = terminated or truncated\\\\n\\', \\'            episode_reward += reward\\\\n\\', \\'        total_rewards.append(episode_reward)\\\\n\\', \\'    return np.mean(total_rewards), np.std(total_rewards)\\\\n\\', \\'\\\\n\\', \\'mean_score, std_score = evaluate_policy(env, q_table)\\\\n\\', \\'print(f\"Average score: {mean_score:.2f} ± {std_score:.2f}\")   \\']\\'\\n with output: \\'[\\'Average score: -0.27 ± 1.12\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Grid_World_Gymnasium.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'# A coding introduction to the Grid World Gymnasium Environment\\\\n\\', \\'\\\\n\\', \\'This notebook provides an updated version of the Frozen Lake environment from the Gymnasium library, based on the official documentation.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'[document link](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\\\\n\\', \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Environment Overview\\\\n\\', \"The Frozen Lake environment involves an agent trying to navigate a frozen lake from a starting point (S) to a goal (G) without falling into any holes (H). The other tiles are frozen (F). The lake\\'s slippery nature means the agent may not always move in the intended direction.\"]\\'\\n\\n\\'markdown\\' cell: \\'[\"##### Making the environment: gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')\\\\n\", \\'\\\\n\\', \\'To create an environment, just pass a string with its name to the gym.make method. If the environment exists, the method returns an instance of the gym.Env class, which represents the environment of the task we are going to solve.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import gymnasium as gym\\\\n\\', \\'import numpy as np\\\\n\\', \\'from matplotlib import pyplot as plt\\\\n\\', \\'from matplotlib import animation\\']\\'\\n\\n\\'code\\' cell: \\'[\"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'###### env.reset()\\\\n\\', \\'\\\\n\\', \\'This method places the environment in its initial state to  and returns it so that the agent can observe it.\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'initial_state, info = env.reset()\\\\n\\', \\'print(f\"The new episode will start in state: {initial_state}\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'##### env.render()\\\\n\\', \\'\\\\n\\', \\'This method generates an image that represents the current state of the environment, in the form of a np.ndarray.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'frame = env.render()\\\\n\\', \"plt.axis(\\'off\\')\\\\n\", \\'plt.title(f\"State: {initial_state}\")\\\\n\\', \\'plt.imshow(frame);\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'##### env.step()\\\\n\\', \\'\\\\n\\', \\'This method applies the action selected by the agent in the environment, to modify it. In response, the environment returns a tuple of five objects:\\\\n\\', \\'\\\\n\\', \\'- The next state\\\\n\\', \\'- The reward obtained\\\\n\\', \\'- (bool) terminated, truncated: if the task has been completed\\\\n\\', \\'- any other relevant information in a python dictionary\\']\\'\\n\\n\\'code\\' cell: \\'[\\'action = 2\\\\n\\', \\'next_state, reward, terminated, truncated, info = env.step(action)\\\\n\\', \\'print(f\"After moving down 1 row, the agent is in state: {next_state}\")\\\\n\\', \\'print(f\"After moving down 1 row, we got a reward of: {reward}\")\\\\n\\', \\'print(\"After moving down 1 row, the task is\", \"\" if terminated or truncated else \"not\", \"finished\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'###### Render the new state\\']\\'\\n\\n\\'code\\' cell: \\'[\\'frame = env.render()\\\\n\\', \"plt.axis(\\'off\\')\\\\n\", \\'plt.title(f\"State: {next_state}\")\\\\n\\', \\'plt.imshow(frame);\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'##### env.close()\\\\n\\', \\'\\\\n\\', \\'It completes the task and closes the environment, releasing the associated resources.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'env.close()\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## GridWorld environment: Find the goal.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'In this section we are going to familiarize ourselves with the environment that we can use in the dynamic programming, Monte Carlo methods and temporal difference methods. This environment is perfect for learning the basics of Reinforcement Learning because:\\\\n\\', \\'\\\\n\\', \\'- It has few states (16)\\\\n\\', \\'- All rewards are the same (0) until the episode concludes. Thus facilitating the study of the value and action-value functions\\\\n\\', \\'- **Transitions**\\\\n\\', \\'  \\\\n\\', \"  The environment\\'s transitions are probabilistic due to the slippery surface, which is enabled by default. When the `is_slippery` argument is set to `True`, taking an action may not result in the intended movement. For example, if the agent attempts to move \\'left\\', there is a 1/3 probability of moving left, a 1/3 probability of moving up, and a 1/3 probability of moving down. The transition probability `p(s\\',r|a,s)` is the probability of moving from state `s` to state `s\\'` with a reward `r` after taking action `a`.\\\\n\", \\'\\\\n\\', \\'Through this environment, we are going to review the concepts seen in The Markov decision process:\\\\n\\', \\'\\\\n\\', \\'- States and state space\\\\n\\', \\'- Actions and action space\\\\n\\', \\'- Trajectories and episodes\\\\n\\', \\'- Rewards and returns\\\\n\\', \\'- Policy\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'The environment is of 4x4 cells, in which the goal of the agent is to find the goal state (15), located in the lower right corner, in the cell (3,3). \\\\n\\', \\'\\\\n\\', \\'To reach the goal state, the agent can take four different actions: move up, move down, move left and move right.\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'###### Create the environment.\\']\\'\\n\\n\\'code\\' cell: \\'[\"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')\"]\\'\\n\\n\\'markdown\\' cell: \\'[\\'## States and State Space\\\\n\\', \"The **observation space** or **state space** is discrete. For the default 4x4 map, there are 16 states, encoded as integers from 0 to 15. The state is a value representing the agent\\'s current position, calculated as `current_row * ncols + current_col`.\\\\n\", \\'\\\\n\\', \\'The tiles have the following meanings:\\\\n\\', \\'\\\\n\\', \\'* **S**: Starting point.\\\\n\\', \\'* **F**: Frozen surface, safe to walk on.\\\\n\\', \\'* **H**: Hole, falling in ends the episode.\\\\n\\', \\'* **G**: Goal, reaching it ends the episode and provides a positive reward.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(f\"For example, the initial state is: {env.reset()}\")\\\\n\\', \\'print(f\"The space state is of type: {env.observation_space}\")\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Get the description of the map (as a 2D array of characters)\\\\n\\', \\'map_desc = env.unwrapped.desc\\\\n\\', \\'\\\\n\\', \"# Find all hole (\\'H\\') positions\\\\n\", \\'hole_states = []\\\\n\\', \\'for row_idx, row in enumerate(map_desc):\\\\n\\', \\'    for col_idx, cell in enumerate(row):\\\\n\\', \"        if cell == b\\'H\\':  # Note: stored as bytes in the array\\\\n\", \\'            state_idx = row_idx * map_desc.shape[1] + col_idx\\\\n\\', \\'            hole_states.append(state_idx)\\\\n\\', \\'\\\\n\\', \\'print(\"Hole states:\", hole_states)   \\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'##### Actions and action space\\\\n\\', \\'\\\\n\\', \\'In this environment, there are four different actions and they are represented by integers:\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'a \\\\\\\\in \\\\\\\\{0, 1, 2, 3\\\\\\\\}\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\n\\', \\'- 0 -> move left\\\\n\\', \\'- 1 -> move down\\\\n\\', \\'- 2 -> move right\\\\n\\', \\'- 3 -> move up\\\\n\\', \\'\\\\n\\', \\'To execute an action, simply pass it as an argument to the env.step method. Information about the action space is stored in the env.action_space property which is of Discrete(4) class. This means that in this case it only consists of an element in the range [0,4), unlike the state space seen above.\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(f\"An example of a valid action is: {env.action_space.sample()}\")\\\\n\\', \\'print(f\"The action state is of type: {env.action_space}\")\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Example of transition probabilities from a state\\\\n\\', \"# This prints the transition probabilities for state 0 and action \\'DOWN\\' (1)\\\\n\", \\'print(env.unwrapped.P[0][1])\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'##### Trajectories and episodes\\\\n\\', \\'\\\\n\\', \\'A trajectory is the sequence generated by moving from one state to another (both arbitrary)\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'  \\\\\\\\tau = S_0, A_0, R_1, S_1, A_1, ... R_N, S_N,\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\n\\', \"Let\\'s generate a trajectory of 3 moves in code:\"]\\'\\n\\n\\'code\\' cell: \\'[\"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\', is_slippery=False) # Deterministic transitions\\\\n\", \\'state = env.reset()[0]\\\\n\\', \\'trajectory = []\\\\n\\', \\'for _ in range(3):\\\\n\\', \\'    action = env.action_space.sample()\\\\n\\', \\'    next_state, reward, terminated, truncated, extra_info = env.step(action)\\\\n\\', \\'    trajectory.append([state, action, reward, terminated, truncated, next_state])\\\\n\\', \\'    state = next_state\\\\n\\', \\'env.close()\\\\n\\', \\'\\\\n\\', \\'print(f\"Congrats! You just generated your first trajectory:\\\\\\\\n{trajectory}\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'An episode is a trajectory that goes from the initial state of the process to the final one:\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'  \\\\\\\\tau = S_0, A_0, R_1, S_1, A_1, ... R_T, S_T,\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'where T is the terminal state.\\\\n\\', \\'\\\\n\\', \"Let\\'s generate a whole episode in code:\"]\\'\\n\\n\\'code\\' cell: \\'[\"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')\\\\n\", \\'state = env.reset()[0]\\\\n\\', \\'episode = []\\\\n\\', \\'done = False\\\\n\\', \\'while not done:\\\\n\\', \\'    action = env.action_space.sample()\\\\n\\', \\'    next_state, reward, terminated, truncated, extra_info = env.step(action)\\\\n\\', \\'    done = terminated or truncated\\\\n\\', \\'    episode.append([state, action, reward, done, next_state])\\\\n\\', \\'    state = next_state\\\\n\\', \\'env.close()\\\\n\\', \\'\\\\n\\', \\'print(f\"Congrats! You just generated your first episode:\\\\\\\\n{episode}\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'##### Rewards and returns\\\\n\\', \\'\\\\n\\', \\'A reward is numerical feedback that the environment generates when the agent takes an action *a* in a state *s*:\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'    r = r(s, a)\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\n\\', \"Let\\'s generate a reward from the environment:\"]\\'\\n\\n\\'code\\' cell: \\'[\"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')\\\\n\", \\'state = env.reset()[0]\\\\n\\', \\'action = env.action_space.sample()\\\\n\\', \\'_, reward, *rest = env.step(action)\\\\n\\', \\'print(f\"We achieved a reward of {reward} by taking action {action} in state {state}\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'The return associated with a moment in time *t* is the sum (discounted) of rewards that the agent obtains from that moment. We are going to calculate $G_0$, that is, the return to the beginning of the episode:\\\\n\\', \\'\\\\n\\', \\'\\\\\\\\begin{equation}\\\\n\\', \\'    G_0 = R_1 + \\\\\\\\gamma R_2 + \\\\\\\\gamma^2 R_3 + ... + \\\\\\\\gamma^{T-1} R_T\\\\n\\', \\'\\\\\\\\end{equation}\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \\'\\\\n\\', \"Let\\'s assume that the discount factor $\\\\\\\\gamma = 0.99$:\\\\n\"]\\'\\n\\n\\'code\\' cell: \\'[\"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')\\\\n\", \\'state = env.reset()[0]\\\\n\\', \\'done = False\\\\n\\', \\'gamma = 0.99\\\\n\\', \\'G_0 = 0\\\\n\\', \\'t = 0\\\\n\\', \\'while not done:\\\\n\\', \\'    action = env.action_space.sample()\\\\n\\', \\'    new_state, reward, terminated, truncated, _ = env.step(action)\\\\n\\', \\'    done = (terminated or truncated)\\\\n\\', \\'    G_0 += gamma ** t * reward    \\\\n\\', \\'    t += 1\\\\n\\', \\'env.close()\\\\n\\', \\'\\\\n\\', \\'print(\\\\n\\', \\'    f\"\"\"It took us {t} moves to find the exit,\\\\n\\', \\'    and each reward r(s,a)=0, so the return amounts to {G_0}\"\"\")\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'##### Policy\\\\n\\', \\'\\\\n\\', \\'A policy is a function $\\\\\\\\pi(a|s) \\\\\\\\in [0, 1]$ that gives the probability of an action given the current state. The function takes the state and action as inputs and returns a float in [0,1].\\\\n\\', \\'\\\\n\\', \\'Since in practice we will need to compute the probabilities of all actions, we will represent the policy as a function that takes the state as an argument and returns the probabilities associated with each of the actions. Thus, if the probabilities are:\\\\n\\', \\'\\\\n\\', \\'[0.5, 0.3, 0.15, 0.05]\\\\n\\', \\'\\\\n\\', \\'we will understand that the action with index 0 has a 50% probability of being chosen, the one with index 1 has 30%, the one with index 2 has 15% and the one with index 3 has 5%.\\\\n\\', \\'\\\\n\\', \"Let\\'s code a policy function that chooses actions randomly:\"]\\'\\n\\n\\'code\\' cell: \\'[\\'def random_policy(state):\\\\n\\', \\'    return np.array([0.25] * 4)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'random_policy(1)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Playing an episode with our random policy\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'###### Create and reset the environment\\']\\'\\n\\n\\'code\\' cell: \\'[\"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')\\\\n\", \\'state = env.reset()[0]\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'Compute $p(a|s) \\\\\\\\ \\\\\\\\forall a \\\\\\\\in \\\\\\\\{0, 1, 2, 3\\\\\\\\}$\\']\\'\\n\\n\\'code\\' cell: \\'[\\'action_probabilities = random_policy(state)\\']\\'\\n\\n\\'code\\' cell: \\'[\"objects = (\\'Left\\', \\'Down\\', \\'Right\\', \\'UP\\')\\\\n\", \\'y_pos = np.arange(len(objects))\\\\n\\', \\'\\\\n\\', \\'plt.bar(y_pos, action_probabilities, alpha=0.5)\\\\n\\', \\'plt.xticks(y_pos, objects)\\\\n\\', \"plt.ylabel(\\'P(a|s)\\')\\\\n\", \"plt.title(\\'Random Policy\\')\\\\n\", \\'plt.tight_layout()\\\\n\\', \\'\\\\n\\', \\'plt.show()\\\\n\\', \\'env.close()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def test_agent(environment, policy):\\\\n\\', \\'    frames = []\\\\n\\', \\'    state, _ = environment.reset()\\\\n\\', \\'    done = False\\\\n\\', \\'    frames.append(environment.render())\\\\n\\', \\'\\\\n\\', \\'    while not done:\\\\n\\', \\'        action_probs = policy(state)\\\\n\\', \\'        action = np.random.choice(range(4), p=action_probs)  # scalar\\\\n\\', \\'        new_state, reward, terminated, truncated, _ = environment.step(action)\\\\n\\', \\'        done = terminated or truncated\\\\n\\', \\'        img = environment.render()\\\\n\\', \\'        frames.append(img)\\\\n\\', \\'        state = new_state\\\\n\\', \\'\\\\n\\', \\'        if state in [5, 7, 11, 12]:\\\\n\\', \\'            state, _ = environment.reset()\\\\n\\', \\'            done = False  # Reset only if terminated\\\\n\\', \\'\\\\n\\', \\'    return frames\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Example usage\\\\n\\', \"env = gym.make(\\'FrozenLake-v1\\', render_mode=\\'rgb_array\\')  \\\\n\", \\'frames = test_agent(env, random_policy)\\\\n\\', \\'env.close()   \\']\\'\\n\\n\\'code\\' cell: \\'[\\'fig = plt.figure()\\\\n\\', \"plt.axis(\\'off\\')\\\\n\", \\'im = plt.imshow(frames[0])\\\\n\\', \\'\\\\n\\', \\'def update(frame):\\\\n\\', \\'    im.set_array(frame)\\\\n\\', \\'    return [im]\\\\n\\', \\'\\\\n\\', \\'ani = animation.FuncAnimation(fig, update, frames=frames, interval=500)\\\\n\\', \\'plt.close()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'from IPython.display import HTML\\\\n\\', \\'HTML(ani.to_jshtml())\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Gymnasium wrapper\\\\n\\', \\'\\\\n\\', \\'[Official Documentation Link](https://gymnasium.farama.org/api/wrappers/table/)\\\\n\\', \\'\\\\n\\', \\'### Please restart your kernel.\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import gymnasium as gym\\\\n\\', \\'\\\\n\\', \\'# Create the FrozenLake environment\\\\n\\', \"env = gym.make(\\'FrozenLake-v1\\', is_slippery=False)  # or is_slippery=True for stochastic dynamics\\\\n\", \\'\\\\n\\', \\'# Wrap it with RecordEpisodeStatistics\\\\n\\', \\'# The `buffer_length` parameter controls how many episode stats to store\\\\n\\', \\'env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=100)\\\\n\\', \\'\\\\n\\', \\'# Example usage in a loop\\\\n\\', \\'for episode in range(3):  # Run 3 episodes\\\\n\\', \\'    obs, info = env.reset()\\\\n\\', \\'    done = False\\\\n\\', \\'    while not done:\\\\n\\', \\'        action = env.action_space.sample()  # Random agent\\\\n\\', \\'        obs, reward, terminated, truncated, info = env.step(action)\\\\n\\', \\'        done = terminated or truncated\\\\n\\', \\'\\\\n\\', \\'    # After episode ends, stats are available in info\\\\n\\', \\'    print(f\"Episode {episode + 1}: return={info[\\\\\\'episode\\\\\\'][\\\\\\'r\\\\\\']:.2f}, \"\\\\n\\', \\'          f\"length={info[\\\\\\'episode\\\\\\'][\\\\\\'l\\\\\\']}, \"\\\\n\\', \\'          f\"duration={info[\\\\\\'episode\\\\\\'][\\\\\\'t\\\\\\']:.3f} sec\")\\\\n\\', \\'\\\\n\\', \\'# Close the environment\\\\n\\', \\'env.close()   \\']\\'\\n with output: \\'[\\'Episode 1: return=0.00, length=2, duration=0.000 sec\\\\n\\', \\'Episode 2: return=0.00, length=10, duration=0.001 sec\\\\n\\', \\'Episode 3: return=0.00, length=7, duration=0.000 sec\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Reward Wrapper\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import gymnasium as gym\\\\n\\', \\'from gymnasium import RewardWrapper\\\\n\\', \\'\\\\n\\', \\'class CustomRewardWrapper(RewardWrapper):\\\\n\\', \\'    def __init__(self, env, step_penalty=-0.01, hole_penalty=-10):\\\\n\\', \\'        super().__init__(env)\\\\n\\', \\'        self.step_penalty = step_penalty\\\\n\\', \\'        self.hole_penalty = hole_penalty\\\\n\\', \\'\\\\n\\', \\'    def reward(self, reward):\\\\n\\', \\'        # Modify the original reward\\\\n\\', \\'        if reward == 1:  # Goal reached\\\\n\\', \\'            return 1.0\\\\n\\', \\'        elif terminated and reward == 0:\\\\n\\', \\'            # If the episode ended due to falling into a hole (not reaching goal), apply penalty            \\\\n\\', \\'            return self.hole_penalty\\\\n\\', \\'        else:\\\\n\\', \\'            return self.step_penalty  # Small penalty for each step\\\\n\\', \\'\\\\n\\', \\'# Usage example\\\\n\\', \\'env = gym.make(\"FrozenLake-v1\", is_slippery=True)\\\\n\\', \\'env = CustomRewardWrapper(env, step_penalty=-0.01)\\\\n\\', \\'\\\\n\\', \\'obs, info = env.reset()\\\\n\\', \\'terminated = False\\\\n\\', \\'total_reward = 0.0\\\\n\\', \\'\\\\n\\', \\'while not terminated:\\\\n\\', \\'    action = env.action_space.sample()  # Random agent\\\\n\\', \\'    obs, rew, terminated, truncated, info = env.step(action)\\\\n\\', \\'    total_reward += rew\\\\n\\', \\'\\\\n\\', \\'print(f\"Episode ended with total reward: {total_reward:.2f}\")   \\']\\'\\n with output: \\'[\\'Episode ended with total reward: -0.16\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=100)\\\\n\\', \\'\\\\n\\', \\'# Example usage in a loop\\\\n\\', \\'for episode in range(3):  # Run 3 episodes\\\\n\\', \\'    obs, info = env.reset()\\\\n\\', \\'    done = False\\\\n\\', \\'    while not done:\\\\n\\', \\'        action = env.action_space.sample()  # Random agent\\\\n\\', \\'        obs, reward, terminated, truncated, info = env.step(action)\\\\n\\', \\'        done = terminated or truncated\\\\n\\', \\'\\\\n\\', \\'    # After episode ends, stats are available in info\\\\n\\', \\'    print(f\"Episode {episode + 1}: return={info[\\\\\\'episode\\\\\\'][\\\\\\'r\\\\\\']:.2f}, \"\\\\n\\', \\'          f\"length={info[\\\\\\'episode\\\\\\'][\\\\\\'l\\\\\\']}, \"\\\\n\\', \\'          f\"duration={info[\\\\\\'episode\\\\\\'][\\\\\\'t\\\\\\']:.3f} sec\")\\\\n\\', \\'\\\\n\\', \\'# Close the environment\\\\n\\', \\'env.close()   \\']\\'\\n with output: \\'[\\'Episode 1: return=-10.04, length=5, duration=0.001 sec\\\\n\\', \\'Episode 2: return=-10.01, length=2, duration=0.000 sec\\\\n\\', \\'Episode 3: return=-10.07, length=8, duration=0.000 sec\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Modify the Transition Probabilities\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import gymnasium as gym\\\\n\\', \\'\\\\n\\', \\'# Default environment\\\\n\\', \"env_default = gym.make(\\'FrozenLake-v1\\', is_slippery=True)\\\\n\", \\'P_default = env_default.env.unwrapped.P  # Structure: P[state][action] -> [(prob, next_state, reward, term), ...]\\\\n\\', \\'\\\\n\\', \\'# Custom environment (after feature merge)\\\\n\\', \"# env_custom = gym.make(\\'FrozenLake-v1\\', chance_correct_action=0.8, chance_slip_l=0.1, chance_slip_r=0.1)\\\\n\", \\'# P_custom = env_custom.env.unwrapped.P\\\\n\\', \\'\\\\n\\', \\'# Inspect a specific state-action pair\\\\n\\', \\'state, action = 6, 0  # Example state and action (e.g., state 6, move left)\\\\n\\', \\'print(\"Default transitions for state 6, action 0 (left):\")\\\\n\\', \\'for prob, next_state, reward, term in P_default[state][action]:\\\\n\\', \\'    print(f\"  P={prob:.3f}, S\\\\\\'={next_state}, R={reward}, Done={term}\") \\\\n\\', \\'\\\\n\\', \\'env_default.close()\\']\\'\\n with output: \\'[\\'Default transitions for state 6, action 0 (left):\\\\n\\', \"  P=0.333, S\\'=2, R=0.0, Done=False\\\\n\", \"  P=0.333, S\\'=5, R=0.0, Done=True\\\\n\", \"  P=0.333, S\\'=10, R=0.0, Done=False\\\\n\"]\\'\\n\\n\\'code\\' cell: \\'[\\'from gymnasium.envs.toy_text.frozen_lake import FrozenLakeEnv\\\\n\\', \\'class CustomFrozenLakeEnv(FrozenLakeEnv):\\\\n\\', \\'    def __init__(self, desc=None, map_name=\"4x4\", is_slippery=True, \\\\n\\', \\'                 chance_correct=0.8, chance_slip_l=0.1, chance_slip_r=0.1):\\\\n\\', \\'        super().__init__(desc=desc, map_name=map_name, is_slippery=is_slippery)\\\\n\\', \\'        \\\\n\\', \\'        self.chance_correct = chance_correct\\\\n\\', \\'        self.chance_slip_l = chance_slip_l\\\\n\\', \\'        self.chance_slip_r = chance_slip_r\\\\n\\', \\'        self._modify_transitions()\\\\n\\', \\'\\\\n\\', \\'    def _modify_transitions(self):\\\\n\\', \\'        for s in self.P.keys():\\\\n\\', \\'            for a in self.P[s].keys():\\\\n\\', \\'                if is_slippery and len(self.P[s][a]) == 3:  # Only modify stochastic transitions\\\\n\\', \\'                    probs = [self.chance_slip_l, self.chance_correct, self.chance_slip_r]\\\\n\\', \\'                    modified = [tuple([probs[i]] + list(t)[1:]) for i, t in enumerate(self.P[s][a])]\\\\n\\', \\'                    self.P[s][a] = modified   \\']\\'\\n\\n\\'code\\' cell: \\'[\\'env_custom = CustomFrozenLakeEnv(map_name=\"8x8\",chance_correct=0.8, chance_slip_l=0.12, chance_slip_r=0.08)\\\\n\\', \\'P_custom = env_custom.unwrapped.P\\\\n\\', \\'# Inspect a specific state-action pair\\\\n\\', \\'state, action = 50, 0  # Example state and action (e.g., state 6, move left)\\\\n\\', \\'print(f\"Custom transitions for state {state}, action 0 (left):\")\\\\n\\', \\'for prob, next_state, reward, term in P_custom[state][action]:\\\\n\\', \\'    print(f\"  P={prob:.3f}, S\\\\\\'={next_state}, R={reward}, Done={term}\") \\\\n\\', \\'env_custom.close()\\']\\'\\n with output: \\'[\\'Custom transitions for state 50, action 0 (left):\\\\n\\', \"  P=0.120, S\\'=42, R=0.0, Done=True\\\\n\", \"  P=0.800, S\\'=49, R=0.0, Done=True\\\\n\", \"  P=0.080, S\\'=58, R=0.0, Done=False\\\\n\"]\\'\\n\\n\\'code\\' cell: \\'[\\'from gymnasium.envs.toy_text.frozen_lake import generate_random_map\\\\n\\', \\'\\\\n\\', \\'env_custom = CustomFrozenLakeEnv(desc=generate_random_map(size=6))\\\\n\\', \\'P_custom = env_custom.unwrapped.P\\\\n\\', \\'print(env_custom.observation_space)\\\\n\\', \\'# Inspect a specific state-action pair\\\\n\\', \\'state, action = 30, 0  # Example state and action (e.g., state 6, move left)\\\\n\\', \\'print(f\"Custom transitions for state {state}, action 0 (left):\")\\\\n\\', \\'for prob, next_state, reward, term in P_custom[state][action]:\\\\n\\', \\'    print(f\"  P={prob:.3f}, S\\\\\\'={next_state}, R={reward}, Done={term}\") \\\\n\\', \\'env_custom.close()\\']\\'\\n with output: \\'[\\'Discrete(36)\\\\n\\', \\'Custom transitions for state 30, action 0 (left):\\\\n\\', \"  P=0.100, S\\'=24, R=0.0, Done=False\\\\n\", \"  P=0.800, S\\'=30, R=0.0, Done=False\\\\n\", \"  P=0.100, S\\'=30, R=0.0, Done=False\\\\n\"]\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\RL_Progression.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'\\\\n\\', \\'# 📰 From Bandits to MDPs Using News Recommendation\\\\n\\', \\'\\\\n\\', \\'## 🎯 Learning Objective\\\\n\\', \\'\\\\n\\', \\'Understand the progression of Reinforcement Learning models:\\\\n\\', \\'\\\\n\\', \\'* Multi-Armed Bandits (MAB)\\\\n\\', \\'* Contextual Bandits\\\\n\\', \\'* Markov Decision Processes (MDP)\\\\n\\', \\'\\\\n\\', \\'All explained using **one real-world example**: **news article recommendation**.\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'## 🧩 Scenario Setup: News Recommendation System\\\\n\\', \\'\\\\n\\', \"An online platform wants to recommend one article at a time to users to **maximize clicks**. The system doesn\\'t know in advance what users like — it must learn from interactions.\\\\n\", \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'## 🎰 1. Bandit Problem: No User Context\\\\n\\', \\'\\\\n\\', \\'### 🧠 Assumption:\\\\n\\', \\'\\\\n\\', \\'* The platform **does not know anything about the user**.\\\\n\\', \\'* It just tries different articles and observes whether users click or not.\\\\n\\', \\'\\\\n\\', \\'### ✅ Model:\\\\n\\', \\'\\\\n\\', \\'* **State**: Fixed (only one)\\\\n\\', \\'* **Actions**: Recommend article A, B, or C\\\\n\\', \\'* **Reward**: Click (1) or No Click (0)\\\\n\\', \\'* **No memory** or history between rounds\\\\n\\', \\'\\\\n\\', \\'### 🧪 Code:\\\\n\\', \\'\\\\n\\', \\'```python\\\\n\\', \\'import numpy as np\\\\n\\', \\'import random\\\\n\\', \\'\\\\n\\', \\'# True average click-through rates (CTR) for each article\\\\n\\', \\'true_ctr = [0.05, 0.1, 0.2]  # A, B, C\\\\n\\', \\'\\\\n\\', \\'def bandit_step(action):\\\\n\\', \\'    return 1 if np.random.rand() < true_ctr[action] else 0\\\\n\\', \\'\\\\n\\', \"articles = [\\'A\\', \\'B\\', \\'C\\']\\\\n\", \\'\\\\n\\', \\'# Simulate\\\\n\\', \\'print(\"Bandit: No user context\")\\\\n\\', \\'for _ in range(5):\\\\n\\', \\'    action = random.randint(0, 2)\\\\n\\', \\'    reward = bandit_step(action)\\\\n\\', \\'    print(f\"Recommended: Article {articles[action]}, Reward: {reward}\")\\\\n\\', \\'```\\\\n\\', \\'\\\\n\\', \\'### 🔍 Key Insight:\\\\n\\', \\'\\\\n\\', \\'* The system always sees the **same situation**, chooses an article, and learns from reward.\\\\n\\', \\'* Best strategy: **explore all articles** to learn their click rates, then **exploit** the best one.\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'## 🧠 2. Contextual Bandit: With User Profile\\\\n\\', \\'\\\\n\\', \\'### 🧠 Assumption:\\\\n\\', \\'\\\\n\\', \\'* Now the system **observes user context**: age group, location, etc.\\\\n\\', \\'* Each context may favor different articles.\\\\n\\', \\'\\\\n\\', \\'### ✅ Model:\\\\n\\', \\'\\\\n\\', \\'* **State (context)**: user info (e.g., “teen”, “adult”)\\\\n\\', \\'* **Actions**: recommend A, B, or C\\\\n\\', \\'* **Reward**: depends on both context and action\\\\n\\', \\'* **No transitions** between contexts\\\\n\\', \\'\\\\n\\', \\'### 🧪 Code:\\\\n\\', \\'\\\\n\\', \\'```python\\\\n\\', \"contexts = [\\'teen\\', \\'adult\\']\\\\n\", \"articles = [\\'A\\', \\'B\\', \\'C\\']\\\\n\", \\'\\\\n\\', \\'# Click-through rates vary by context\\\\n\\', \\'ctr = {\\\\n\\', \"    \\'teen\\':  [0.2, 0.1, 0.4],\\\\n\", \"    \\'adult\\': [0.1, 0.3, 0.2]\\\\n\", \\'}\\\\n\\', \\'\\\\n\\', \\'def contextual_bandit_step(context, action):\\\\n\\', \\'    return 1 if np.random.rand() < ctr[context][action] else 0\\\\n\\', \\'\\\\n\\', \\'print(\"\\\\\\\\nContextual Bandit: User profile known\")\\\\n\\', \\'for _ in range(5):\\\\n\\', \\'    context = random.choice(contexts)\\\\n\\', \\'    action = random.randint(0, 2)\\\\n\\', \\'    reward = contextual_bandit_step(context, action)\\\\n\\', \\'    print(f\"User: {context:5s}, Article: {articles[action]}, Reward: {reward}\")\\\\n\\', \\'```\\\\n\\', \\'\\\\n\\', \\'### 🔍 Key Insight:\\\\n\\', \\'\\\\n\\', \\'* We now **tailor recommendations** based on user type.\\\\n\\', \\'* The system must **learn a policy per context**.\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'## 🗺️ 3. MDP: Modeling User Behavior Over Time\\\\n\\', \\'\\\\n\\', \\'### 🧠 Assumption:\\\\n\\', \\'\\\\n\\', \\'* Users **stay longer**, and article recommendations **influence their future state**.\\\\n\\', \\'* E.g., if you show boring articles, they might **leave the platform** (bad).\\\\n\\', \\'* If you engage them, they **stay and read more** (good).\\\\n\\', \\'\\\\n\\', \\'### ✅ Model:\\\\n\\', \\'\\\\n\\', \\'* **States**: user engagement level (e.g., \"new\", \"engaged\", \"bored\", \"left\")\\\\n\\', \\'* **Actions**: recommend A, B, or C\\\\n\\', \\'* **Rewards**: clicks or engagement\\\\n\\', \\'* **Transitions**: actions affect how user moves between states\\\\n\\', \\'\\\\n\\', \\'### 🧪 Code:\\\\n\\', \\'\\\\n\\', \\'```python\\\\n\\', \"states = [\\'new\\', \\'engaged\\', \\'bored\\', \\'left\\']\\\\n\", \"articles = [\\'A\\', \\'B\\', \\'C\\']\\\\n\", \\'\\\\n\\', \\'# Transition and reward model (simplified)\\\\n\\', \\'def mdp_step(state, action):\\\\n\\', \"    if state == \\'left\\':\\\\n\", \"        return \\'left\\', 0  # user already gone\\\\n\", \\'\\\\n\\', \\'    transition_prob = {\\\\n\\', \"        \\'new\\':     [\\'engaged\\', \\'bored\\', \\'left\\'],\\\\n\", \"        \\'engaged\\': [\\'engaged\\', \\'bored\\', \\'left\\'],\\\\n\", \"        \\'bored\\':   [\\'bored\\', \\'left\\', \\'left\\']\\\\n\", \\'    }\\\\n\\', \\'\\\\n\\', \\'    rewards = {\\\\n\\', \"        \\'A\\': {\\'engaged\\': 1, \\'bored\\': 0, \\'left\\': 0},\\\\n\", \"        \\'B\\': {\\'engaged\\': 1, \\'bored\\': 0, \\'left\\': 0},\\\\n\", \"        \\'C\\': {\\'engaged\\': 1, \\'bored\\': 1, \\'left\\': 0},\\\\n\", \\'    }\\\\n\\', \\'\\\\n\\', \\'    next_state = random.choices(transition_prob[state], weights=[0.6, 0.3, 0.1])[0]\\\\n\\', \\'    reward = rewards[articles[action]].get(next_state, 0)\\\\n\\', \\'    return next_state, reward\\\\n\\', \\'\\\\n\\', \\'# Simulate one user session\\\\n\\', \"state = \\'new\\'\\\\n\", \\'print(\"\\\\\\\\nMDP: User state changes over time\")\\\\n\\', \\'for step in range(5):\\\\n\\', \"    if state == \\'left\\':\\\\n\", \\'        print(f\"User left. Session ended.\")\\\\n\\', \\'        break\\\\n\\', \\'    action = random.randint(0, 2)\\\\n\\', \\'    next_state, reward = mdp_step(state, action)\\\\n\\', \\'    print(f\"State: {state:8s}, Action: {articles[action]}, → Next: {next_state:8s}, Reward: {reward}\")\\\\n\\', \\'    state = next_state\\\\n\\', \\'```\\\\n\\', \\'\\\\n\\', \\'### 🔍 Key Insight:\\\\n\\', \\'\\\\n\\', \\'* **Actions influence future user states**, not just immediate reward.\\\\n\\', \\'* This allows for **long-term planning** (e.g., keep user engaged longer).\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'## 📊 Summary Table\\\\n\\', \\'\\\\n\\', \\'| Model             | State               | Context Info | Transitions | Long-Term Planning |\\\\n\\', \\'| ----------------- | ------------------- | ------------ | ----------- | ------------------ |\\\\n\\', \\'| Bandit            | One state           | No           | No          | No                 |\\\\n\\', \\'| Contextual Bandit | Varies (user)       | Yes          | No          | No                 |\\\\n\\', \\'| MDP               | Varies (user state) | Yes          | Yes         | Yes                |\\\\n\\', \\'\\\\n\\', \\'---\\\\n\\', \\'\\\\n\\', \\'## 🧠 Final Takeaway\\\\n\\', \\'\\\\n\\', \\'By using **News Recommendation** consistently:\\\\n\\', \\'\\\\n\\', \\'* The **Bandit** model is stateless and greedy.\\\\n\\', \\'* The **Contextual Bandit** adds personalization via context.\\\\n\\', \\'* The **MDP** allows for modeling **user behavior over time** and optimizing for **long-term engagement**.\\\\n\\', \\'\\\\n\\', \\'\\\\n\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'| Feature               | Bandit          | Contextual Bandit     | MDP                       |\\\\n\\', \\'|-----------------------|------------------|-------------------------|-----------------------------|\\\\n\\', \\'| States                | 1 (fixed)        | Varying (context)       | Multiple (transitionable)   |\\\\n\\', \\'| Transitions           | No               | No                      | Yes                         |\\\\n\\', \\'| Reward depends on     | Action           | State, Action           | State, Action               |\\\\n\\', \\'| Next State depends on | —                | —                       | Current state and action    |\\\\n\\', \\'| Temporal/Sequential   | No               | Episodic (short-term)   | Yes (long-term planning)    |\\\\n\\', \\'| Reward Timing         | Immediate        | Immediate               | Can be **immediate or delayed** |\\\\n\\', \\'\\\\n\\', \\'**Explanation**:\\\\n\\', \\'- In **bandits** and **contextual bandits**, feedback comes right after the action—reward is immediate.\\\\n\\', \\'- In **MDPs**, rewards can be delayed, meaning the best actions may not give instant gratification but instead contribute to greater return over time (*e.g., setting up future states*).\\\\n\\', \\'\\\\n\\']\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Session_18_06_25.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'## Session 18.06.2025\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import numpy as np\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Class for a single slot machine. Rewards are Gaussian.\\\\n\\', \\'class GaussianBandit(object):\\\\n\\', \\'    def __init__(self, mean=0, stdev=1):\\\\n\\', \\'        self.mean = mean\\\\n\\', \\'        self.stdev = stdev\\\\n\\', \\'    \\\\n\\', \\'    def pull_lever(self):\\\\n\\', \\'        reward = np.random.normal(self.mean, self.stdev)\\\\n\\', \\'        return np.round(reward, 1)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'g1 = GaussianBandit(5, 2)\\\\n\\', \\'g2 = GaussianBandit(6, 2)\\\\n\\', \\'g3 = GaussianBandit(1, 5)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'class GaussianBanditGame(object):\\\\n\\', \\'    def __init__(self, bandits):\\\\n\\', \\'        self.bandits = bandits\\\\n\\', \\'        np.random.shuffle(self.bandits)\\\\n\\', \\'        self.reset_game()\\\\n\\', \\'    \\\\n\\', \\'    def play(self, choice):\\\\n\\', \\'        reward = self.bandits[choice - 1].pull_lever()\\\\n\\', \\'        self.rewards.append(reward)\\\\n\\', \\'        self.total_reward += reward\\\\n\\', \\'        self.n_played += 1\\\\n\\', \\'        return reward\\\\n\\', \\'    \\\\n\\', \\'    def user_play(self):\\\\n\\', \\'        self.reset_game()\\\\n\\', \\'        print(\"Game started. \" + \\\\n\\', \\'              \"Enter 0 as input to end the game.\")\\\\n\\', \\'        while True:\\\\n\\', \\'            print(f\"\\\\\\\\n -- Round {self.n_played}\")\\\\n\\', \\'            choice = int(input(f\"Choose a machine \" + \\\\n\\', \\'                     f\"from 1 to {len(self.bandits)}: \"))\\\\n\\', \\'            if choice in range(1, len(self.bandits) + 1):\\\\n\\', \\'                reward = self.play(choice)\\\\n\\', \\'                print(f\"Machine {choice} gave \" + \\\\n\\', \\'                      f\"a reward of {reward}.\")\\\\n\\', \\'                avg_rew = self.total_reward/self.n_played\\\\n\\', \\'                print(f\"Your average reward \" +\\\\n\\', \\'                      f\"so far is {avg_rew}.\")\\\\n\\', \\'            else:\\\\n\\', \\'                break\\\\n\\', \\'        print(\"Game has ended.\")\\\\n\\', \\'        if self.n_played > 0:\\\\n\\', \\'            print(f\"Total reward is {self.total_reward}\" + \\\\n\\', \\'                  f\" after {self.n_played} round(s).\")\\\\n\\', \\'            avg_rew = self.total_reward/self.n_played\\\\n\\', \\'            print(f\"Average reward is {avg_rew}.\")              \\\\n\\', \\'            \\\\n\\', \\'    def reset_game(self):\\\\n\\', \\'        self.rewards = []\\\\n\\', \\'        self.total_reward = 0\\\\n\\', \\'        self.n_played = 0\\']\\'\\n\\n\\'code\\' cell: \\'[\\'slotA = GaussianBandit(5, 3)\\\\n\\', \\'slotB = GaussianBandit(6, 2)\\\\n\\', \\'slotC = GaussianBandit(1, 5)\\\\n\\', \\'game = GaussianBanditGame([slotA, slotB, slotC])\\']\\'\\n\\n\\'code\\' cell: \\'[\\'game.user_play()\\']\\'\\n with output: \\'[\\'Game started. Enter 0 as input to end the game.\\\\n\\', \\'\\\\n\\', \\' -- Round 0\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Modified Code: Track Usage & Compute Estimates\\\\n\\', \\'class GaussianBandit(object):\\\\n\\', \\'    def __init__(self, mean=0, stdev=1):\\\\n\\', \\'        self.mean = mean\\\\n\\', \\'        self.stdev = stdev\\\\n\\', \\'        self.total_reward = 0\\\\n\\', \\'        self.n_pulled = 0\\\\n\\', \\'\\\\n\\', \\'    def pull_lever(self):\\\\n\\', \\'        reward = np.random.normal(self.mean, self.stdev)\\\\n\\', \\'        reward = np.round(reward, 1)\\\\n\\', \\'        self.total_reward += reward\\\\n\\', \\'        self.n_pulled += 1\\\\n\\', \\'        return reward\\\\n\\', \\'\\\\n\\', \\'    def estimated_value(self):\\\\n\\', \\'        if self.n_pulled == 0:\\\\n\\', \\'            return None  # or 0.0\\\\n\\', \\'        return round(self.total_reward / self.n_pulled, 3)\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# slot1 = GaussianBandit(5, 3)\\\\n\\', \\'# slot2 = GaussianBandit(6, 2)\\\\n\\', \\'# slot3 = GaussianBandit(1, 5)\\\\n\\', \\'# bandits = [slot1, slot2, slot3]\\\\n\\', \\'# Create a few bandits\\\\n\\', \\'bandits = [\\\\n\\', \\'    GaussianBandit(mean=1, stdev=1),\\\\n\\', \\'    GaussianBandit(mean=2, stdev=1),\\\\n\\', \\'    GaussianBandit(mean=0, stdev=1)\\\\n\\', \\']\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Simulate lever pulls\\\\n\\', \\'n_total_pulls = 1000\\\\n\\', \\'for _ in range(n_total_pulls):\\\\n\\', \\'    bandit_index = np.random.choice(len(bandits))  # Random bandit\\\\n\\', \\'    bandits[bandit_index].pull_lever()\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import pandas as pd\\']\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Session_20_06_25.ipynb'}, page_content='\\'code\\' cell: \\'[]\\'\\n\\n\\'markdown\\' cell: \\'[\\'<img src=\"attachment:184f0646-42f2-445f-933f-4d654773405e.png\"  width=\"80%\" height=\"30%\" style=\"margin-left:auto; margin-right:auto\">\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import numpy as np\\']\\'\\n\\n\\'code\\' cell: \\'[\\'n_arms = 5\\\\n\\', \\'Q = np.zeros(n_arms)  # Q, action values\\\\n\\', \\'N = np.zeros(n_arms)  # N, action counts\\\\n\\', \\'total_reward = 0\\']\\'\\n\\n\\'raw\\' cell: \\'[\\'probability = [0.1, 0.7, 0.3, 0.2], reward =[5, 3, 10, -5]\\\\n\\', \\'if np.random.random() < probability[action]\\\\n\\', \\'    reward = 5\\']\\'\\n\\n\\'code\\' cell: \\'[\\'epsilon = 0.1\\\\n\\', \\'times = 1000\\\\n\\', \\'optimal_action = np.zeros(times)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for t in range(times):\\\\n\\', \\'    # Select action\\\\n\\', \\'    if np.random.uniform() < epsilon:\\\\n\\', \\'        action = np.random.choice(n_arms)\\\\n\\', \\'    else:\\\\n\\', \\'        action = np.argmax(Q)\\\\n\\', \\'    # Get reward for the action\\\\n\\', \\'    reward = np.random.normal(true_means[action], 1)\\\\n\\', \\'    # Update action count and Q-value\\\\n\\', \\'    N[action] += 1\\\\n\\', \\'    Q[action] += (reward - Q[action]) / N[action]\\\\n\\', \\'    optimal_action[t] = action == np.argmax(true_means)\\\\n\\', \\'    total_reward += reward\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"avg_rewrd:\", total_reward/times)\\']\\'\\n with output: \\'[\\'avg_rewrd: 1.1700192704384267\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'print(\"% optimal action:\", np.round(np.mean(optimal_action)*100, 2))\\']\\'\\n with output: \\'[\\'% optimal action: 92.5\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n'),\n",
       " Document(metadata={'source': 'PDF_DOC\\\\Notebook\\\\Session_25_06_25.ipynb'}, page_content='\\'markdown\\' cell: \\'[\\'## Session 25.06.2025\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import numpy as np\\']\\'\\n\\n\\'code\\' cell: \\'[\\'initial_epsilon=1.0\\\\n\\', \\'final_epsilon=0.01\\\\n\\', \\'time = 10\\\\n\\', \\'decay_rate = 0.995\\']\\'\\n\\n\\'code\\' cell: \\'[\\'for t in range(1, time+1):\\\\n\\', \\'    # print(initial_epsilon - (initial_epsilon - final_epsilon) * (t / time))\\\\n\\', \\'    print(initial_epsilon * (decay_rate ** t))\\']\\'\\n with output: \\'[\\'0.995\\\\n\\', \\'0.990025\\\\n\\', \\'0.985074875\\\\n\\', \\'0.980149500625\\\\n\\', \\'0.975248753121875\\\\n\\', \\'0.9703725093562656\\\\n\\', \\'0.9655206468094842\\\\n\\', \\'0.9606930435754368\\\\n\\', \\'0.9558895783575596\\\\n\\', \\'0.9511101304657719\\\\n\\']\\'\\n\\n\\'code\\' cell: \\'[\\'import numpy as np\\\\n\\', \\'import matplotlib.pyplot as plt\\']\\'\\n\\n\\'code\\' cell: \\'[\\'class KArmedBandit:\\\\n\\', \\'    def __init__(self, k=10):\\\\n\\', \\'        self.k = k\\\\n\\', \\'        # Initialize each arm with a random mean (q*(a)) and unit variance\\\\n\\', \\'        self.true_means = np.random.normal(0, 1, k)\\\\n\\', \\'    \\\\n\\', \\'    def pull(self, arm):\\\\n\\', \"        # Return a reward from the selected arm\\'s normal distribution\\\\n\", \\'        return np.random.normal(self.true_means[arm], 1)\\\\n\\', \\'    \\\\n\\', \\'    def optimal_arm(self):\\\\n\\', \\'        # Return the index of the arm with highest true mean\\\\n\\', \\'        return np.argmax(self.true_means)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def epsilon_greedy(bandit, epsilon, runs=2000, time=1000):\\\\n\\', \\'    rewards = np.zeros((runs, time))\\\\n\\', \\'    optimal_actions = np.zeros((runs, time))\\\\n\\', \\'    \\\\n\\', \\'    for run in range(runs):\\\\n\\', \\'        # Initialize Q-values and action counts for each run\\\\n\\', \\'        Q = np.zeros(bandit.k)\\\\n\\', \\'        N = np.zeros(bandit.k)\\\\n\\', \\'        \\\\n\\', \\'        for t in range(time):\\\\n\\', \\'            # Epsilon-greedy action selection\\\\n\\', \\'            if np.random.random() < epsilon:\\\\n\\', \\'                action = np.random.randint(bandit.k)  # Explore\\\\n\\', \\'            else:\\\\n\\', \\'                action = np.argmax(Q)  # Exploit\\\\n\\', \\'            \\\\n\\', \\'            # Get reward and update estimates\\\\n\\', \\'            reward = bandit.pull(action)\\\\n\\', \\'            rewards[run, t] = reward\\\\n\\', \\'            optimal_actions[run, t] = (action == bandit.optimal_arm())\\\\n\\', \\'            \\\\n\\', \\'            # Update action count and Q-value\\\\n\\', \\'            N[action] += 1\\\\n\\', \\'            Q[action] += (reward - Q[action]) / N[action]\\\\n\\', \\'    \\\\n\\', \\'    return rewards, optimal_actions\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def epsilon_greedy_linear_decrease(bandit, initial_epsilon=1.0, \\\\n\\', \\'                                   final_epsilon=0.01, runs=2000, time=1000):\\\\n\\', \\'    rewards = np.zeros((runs, time))\\\\n\\', \\'    optimal_actions = np.zeros((runs, time))\\\\n\\', \\'    \\\\n\\', \\'    for run in range(runs):\\\\n\\', \\'        Q = np.zeros(bandit.k)  # Estimated value of each arm\\\\n\\', \\'        N = np.zeros(bandit.k)  # Number of pulls per arm\\\\n\\', \\'        \\\\n\\', \\'        for t in range(time):\\\\n\\', \\'            # Linearly decreasing ε\\\\n\\', \\'            epsilon = initial_epsilon - (initial_epsilon - final_epsilon) * (t / time)\\\\n\\', \\'            \\\\n\\', \\'            # Epsilon-greedy action selection\\\\n\\', \\'            if np.random.random() < epsilon:\\\\n\\', \\'                action = np.random.randint(bandit.k)  # Explore\\\\n\\', \\'            else:\\\\n\\', \\'                action = np.argmax(Q)  # Exploit\\\\n\\', \\'            \\\\n\\', \\'            # Get reward and update metrics\\\\n\\', \\'            reward = bandit.pull(action)\\\\n\\', \\'            rewards[run, t] = reward\\\\n\\', \\'            optimal_actions[run, t] = (action == bandit.optimal_arm())\\\\n\\', \\'            \\\\n\\', \\'            # Update Q-value\\\\n\\', \\'            N[action] += 1\\\\n\\', \\'            Q[action] += (reward - Q[action]) / N[action]\\\\n\\', \\'    \\\\n\\', \\'    return rewards, optimal_actions\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def epsilon_greedy_exponential_decrease(bandit, initial_epsilon=1.0, decay_rate=0.995, runs=2000, time=1000):\\\\n\\', \\'    rewards = np.zeros((runs, time))\\\\n\\', \\'    optimal_actions = np.zeros((runs, time))\\\\n\\', \\'    \\\\n\\', \\'    for run in range(runs):\\\\n\\', \\'        Q = np.zeros(bandit.k)\\\\n\\', \\'        N = np.zeros(bandit.k)\\\\n\\', \\'        \\\\n\\', \\'        for t in range(time):\\\\n\\', \\'            # Exponentially decreasing ε\\\\n\\', \\'            epsilon = initial_epsilon * (decay_rate ** t)\\\\n\\', \\'            \\\\n\\', \\'            # Epsilon-greedy action selection\\\\n\\', \\'            if np.random.random() < epsilon:\\\\n\\', \\'                action = np.random.randint(bandit.k)  # Explore\\\\n\\', \\'            else:\\\\n\\', \\'                action = np.argmax(Q)  # Exploit\\\\n\\', \\'            \\\\n\\', \\'            # Get reward and update metrics\\\\n\\', \\'            reward = bandit.pull(action)\\\\n\\', \\'            rewards[run, t] = reward\\\\n\\', \\'            optimal_actions[run, t] = (action == bandit.optimal_arm())\\\\n\\', \\'            \\\\n\\', \\'            # Update Q-value\\\\n\\', \\'            N[action] += 1\\\\n\\', \\'            Q[action] += (reward - Q[action]) / N[action]\\\\n\\', \\'    \\\\n\\', \\'    return rewards, optimal_actions\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Parameters\\\\n\\', \\'initial_epsilon = 1.0\\\\n\\', \\'final_epsilon = 0.01\\\\n\\', \\'decay_rate = 0.995\\\\n\\', \\'\\\\n\\', \\'# Run experiments\\\\n\\', \\'bandit = KArmedBandit(k=10)\\\\n\\', \\'results = {\\\\n\\', \"    \\'fixed_ε=0.1\\': epsilon_greedy(bandit, epsilon=0.1, runs=2000, time=1000),\\\\n\", \"    \\'linear_decrease\\': epsilon_greedy_linear_decrease(bandit, initial_epsilon, final_epsilon, runs=2000, time=1000),\\\\n\", \"    \\'exponential_decrease\\': epsilon_greedy_exponential_decrease(bandit, initial_epsilon, decay_rate, runs=2000, time=1000)\\\\n\", \\'}\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'## Non-Stationary Bandit Environment\\']\\'\\n\\n\\'code\\' cell: \\'[\\'class NonStationaryBandit:\\\\n\\', \\'    def __init__(self, k=10, initial_means=0, initial_variance=1, random_walk_step=0.01):\\\\n\\', \\'        self.k = k\\\\n\\', \\'        self.true_means = np.random.normal(initial_means, initial_variance, k)\\\\n\\', \\'        self.random_walk_step = random_walk_step  # Controls how fast means change\\\\n\\', \\'    \\\\n\\', \\'    def pull(self, arm):\\\\n\\', \\'        # Return reward from current mean (with noise)\\\\n\\', \\'        return np.random.normal(self.true_means[arm], 1)\\\\n\\', \\'    \\\\n\\', \\'    def random_walk(self):\\\\n\\', \\'        # Randomly shift all true means (non-stationarity)\\\\n\\', \\'        self.true_means += np.random.normal(0, self.random_walk_step, self.k)\\\\n\\', \\'    \\\\n\\', \\'    def optimal_arm(self):\\\\n\\', \\'        return np.argmax(self.true_means)\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Epsilon-Greedy with Decreasing $\\\\\\\\varepsilon$ and Step Size ($\\\\\\\\alpha$)\\\\n\\', \\'\\\\n\\', \\'For non-stationary bandits, we use a constant step size $\\\\\\\\alpha$ (e.g., $\\\\\\\\alpha=0.1$) instead of sample averages $\\\\\\\\left(\\\\\\\\dfrac{1}{N}\\\\\\\\right)$ to weight recent rewards more heavily.\\\\n\\', \\'\\\\n\\', \\'#### Linear Decay $\\\\\\\\varepsilon$\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def epsilon_greedy_linear_decrease_nonstationary(bandit, initial_epsilon=1.0, final_epsilon=0.01, alpha=0.1, runs=2000, time=1000):\\\\n\\', \\'    rewards = np.zeros((runs, time))\\\\n\\', \\'    optimal_actions = np.zeros((runs, time))\\\\n\\', \\'    \\\\n\\', \\'    for run in range(runs):\\\\n\\', \\'        Q = np.zeros(bandit.k)\\\\n\\', \\'        for t in range(time):\\\\n\\', \\'            # Linear ε decay\\\\n\\', \\'            epsilon = max(final_epsilon, initial_epsilon - (initial_epsilon - final_epsilon) * (t / time))\\\\n\\', \\'            \\\\n\\', \\'            # Action selection\\\\n\\', \\'            if np.random.random() < epsilon:\\\\n\\', \\'                action = np.random.randint(bandit.k)\\\\n\\', \\'            else:\\\\n\\', \\'                action = np.argmax(Q)\\\\n\\', \\'            \\\\n\\', \\'            # Pull arm and update Q\\\\n\\', \\'            reward = bandit.pull(action)\\\\n\\', \\'            rewards[run, t] = reward\\\\n\\', \\'            optimal_actions[run, t] = (action == bandit.optimal_arm())\\\\n\\', \\'            Q[action] += alpha * (reward - Q[action])  # Weighted update\\\\n\\', \\'            \\\\n\\', \\'            # Simulate non-stationarity (random walk)\\\\n\\', \\'            # if t > 1 and t % 100 == 0:\\\\n\\', \\'            #     bandit.random_walk()\\\\n\\', \\'            bandit.random_walk()\\\\n\\', \\'    \\\\n\\', \\'    return rewards, optimal_actions\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'#### Exponential Decay $\\\\\\\\varepsilon$\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def epsilon_greedy_exponential_decrease_nonstationary(bandit, initial_epsilon=1.0, decay_rate=0.995, alpha=0.1, runs=2000, time=1000):\\\\n\\', \\'    rewards = np.zeros((runs, time))\\\\n\\', \\'    optimal_actions = np.zeros((runs, time))\\\\n\\', \\'    \\\\n\\', \\'    for run in range(runs):\\\\n\\', \\'        Q = np.zeros(bandit.k)\\\\n\\', \\'        for t in range(time):\\\\n\\', \\'            # Exponential ε decay\\\\n\\', \\'            epsilon = initial_epsilon * (decay_rate ** t)\\\\n\\', \\'            \\\\n\\', \\'            # Action selection\\\\n\\', \\'            if np.random.random() < epsilon:\\\\n\\', \\'                action = np.random.randint(bandit.k)\\\\n\\', \\'            else:\\\\n\\', \\'                action = np.argmax(Q)\\\\n\\', \\'            \\\\n\\', \\'            # Pull arm and update Q\\\\n\\', \\'            reward = bandit.pull(action)\\\\n\\', \\'            rewards[run, t] = reward\\\\n\\', \\'            optimal_actions[run, t] = (action == bandit.optimal_arm())\\\\n\\', \\'            Q[action] += alpha * (reward - Q[action])  # Weighted update\\\\n\\', \\'            \\\\n\\', \\'            # Simulate non-stationarity\\\\n\\', \\'            # if t > 1 and t % 100 == 0:\\\\n\\', \\'            bandit.random_walk()\\\\n\\', \\'    \\\\n\\', \\'    return rewards, optimal_actions\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Fixed $\\\\\\\\varepsilon$-Greedy (Baseline)\\']\\'\\n\\n\\'code\\' cell: \\'[\\'def epsilon_greedy_nonstationary(bandit, epsilon=0.1, alpha=0.1, runs=2000, time=1000):\\\\n\\', \\'    rewards = np.zeros((runs, time))\\\\n\\', \\'    optimal_actions = np.zeros((runs, time))\\\\n\\', \\'    \\\\n\\', \\'    for run in range(runs):\\\\n\\', \\'        Q = np.zeros(bandit.k)\\\\n\\', \\'        for t in range(time):\\\\n\\', \\'            # Fixed ε\\\\n\\', \\'            if np.random.random() < epsilon:\\\\n\\', \\'                action = np.random.randint(bandit.k)\\\\n\\', \\'            else:\\\\n\\', \\'                action = np.argmax(Q)\\\\n\\', \\'            \\\\n\\', \\'            # Pull arm and update Q\\\\n\\', \\'            reward = bandit.pull(action)\\\\n\\', \\'            rewards[run, t] = reward\\\\n\\', \\'            optimal_actions[run, t] = (action == bandit.optimal_arm())\\\\n\\', \\'            Q[action] += alpha * (reward - Q[action])\\\\n\\', \\'            \\\\n\\', \\'            # Non-stationarity\\\\n\\', \\'            # if t > 1 and t % 100 == 0:\\\\n\\', \\'            bandit.random_walk()\\\\n\\', \\'    \\\\n\\', \\'    return rewards, optimal_actions\\']\\'\\n\\n\\'markdown\\' cell: \\'[\\'### Simulation and Comparison\\']\\'\\n\\n\\'code\\' cell: \\'[\\'# Parameters\\\\n\\', \\'k = 10\\\\n\\', \\'runs = 2000\\\\n\\', \\'time = 1000\\\\n\\', \\'initial_epsilon = 1.0\\\\n\\', \\'final_epsilon = 0.01\\\\n\\', \\'decay_rate = 0.995\\\\n\\', \\'alpha = 0.1  # Step size for non-stationary updates\\\\n\\', \\'\\\\n\\', \\'# Initialize bandit\\\\n\\', \\'bandit = NonStationaryBandit(k=k, random_walk_step=0.01)\\\\n\\', \\'\\\\n\\', \\'# Run experiments\\\\n\\', \\'results = {\\\\n\\', \"    \\'Fixed ε=0.1\\': epsilon_greedy_nonstationary(bandit, epsilon=0.1, alpha=alpha, runs=runs, time=time),\\\\n\", \"    \\'Linear ε decay\\': epsilon_greedy_linear_decrease_nonstationary(bandit, initial_epsilon, final_epsilon, alpha, runs, time),\\\\n\", \"    \\'Exponential ε decay\\': epsilon_greedy_exponential_decrease_nonstationary(bandit, initial_epsilon, decay_rate, alpha, runs, time)\\\\n\", \\'}\\']\\'\\n\\n\\'code\\' cell: \\'[]\\'\\n\\n')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5bc1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 0, 'page_label': '1'}, page_content='2.\\nT-nmedia-teReinforcementl-earn.mg/Leycharacteristicsof an RL problem:-\\n-\\n① Learningto act in manysituations.\\n②Delayedrewardsandcreditassignment.\\n③ Exploration/Exploitationdilemma.\\nExploration/Exploitationdilemmais a uniquechallenge\\nin RL that doesnot exist in other learning\\nparadigms.\\nThis is due to the natureof feedback\\nto the RL agent.\\nTwotypesof feedback/ Instructivefeedback\\n⊥Evaluativefeedback.\\no→a-→ At t'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 1, 'page_label': '2'}, page_content='Instructivefeedback Evaluativefeedback-\\n→ Instructsthe right → evaluatestheactiontaken\\n\" \"\" a\"\\n|\\nAt by givingsome reward.\\n→ Ignorestheaction → Completelydependson\\ntaken. the action taken.\\n→ Usedin supervised → Usedin RL.\\nlearning.\\n-\\nEvaluativefeedbacktells howgood/badthe\\nactiontakenis . But it doesnot tell us the\\noptimalactionto take. Hencethe agenthas\\nto exploreto findthe optimalaction.\\nIm¥_RL: simplestpossibleRL settingto\\nstudythe problemof explorationvs . exploitation\\nwithoutworryingaboutthe othertwo challenges\\n( learningto act in\\nmanysituationsand delayed\\nrewards) .'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 2, 'page_label': '3'}, page_content='ImmediateRL aka K-\\narmedbanditproblem:-\\n-\\nYouare facedrepeatedlywith a choice\\namong\\nK differentoptionsor actors.\\nII. ☒☒. . -\\n☒.\\nAftereach choice, youreceive a numerical\\nreward chosenfroma\\nstationaryprobability\\ndistributionthat dependson the action you\\nselected.\\nObjective: to maximizethe expectedtotal\\nreward over some time period.\\nNote: 1 .\\nAgentsees the same state( same set of\\nactions) all the time.\\n2. Rewardsare immediate.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 3, 'page_label': '4'}, page_content='Appian: AIBtesting.\\nA websiteteamwants to\\npick betweenplacinga\\n\"\\nbuyit now \" buttonon thetopof\\npageor bottomof the page.\\nIn traditional\\nAIB testing, yourandomlytry twoversionsof\\nthe website( top- button/ bottom-\\nbutton) to two\\ndifferentset of users ( say10K users pergroup?\\nand decidethe choice basedon the\\nstatistics).\\nThis is not an adaptiveprocess. The experiment\\ndoesnot utilizethe\\nincomingfeedbackduring\\nthe course of this testing. one can\\nposethis as a banditproblem.\\narms = 2 versions ofthesame website.\\nreward = +1 if the user buystheproduct\\n-1 if the user doesnotbuythe\\nproduct.\\nAdge: ④each userfeedbackis utilizedbefore\\nthe next action.\\n②Thetestingnever needs tostop.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 4, 'page_label': '5'}, page_content=\"Each ofthe actions has an expectedor\\nmean reward giventhat actionis selected.\\nfire will call thatas the value ofthat action.\\nvalueofaction a =\\n%(a) = IE[Rt / At=\\na)\\nwhereAt= actionselectedat timestep1- .\\nRt = Rewardobtainedat timeslept.\\nThe agentdoes not haveaccess to 9*67.\\nIf you have access to 9*67, then the\\noptimalaction of is givenby\\n|a*=argma✗9*a c-{a, ,a2\\n.dk#\\nHowever, the agentcan estimatethe valueof\\nthe actions basedon interaction.\\nQpfa) = estimatedvalueof action'\\na' at time't '\\n.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 5, 'page_label': '6'}, page_content='The goalofthe agentis notjustto explore\\nand findthe optimalaction, but to dothat\\nas soon as possibleso that it can maximize\\nits expectedtotal rewardin the giventime.\\nThis is capturedby\\n\"\\nregret! If the agentis\\ngiven\\'\\nk\\'\\ntimesteps,\\nthen\\nregret= K ÷É Rt\\ntwe donthaveaccessto .\\nt.\\nactor\\n\"\"-\\nidea.\\noptimal the goalis to design\\nalgorithmswith least\\nregretas possible.\\nt'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 6, 'page_label': '7'}, page_content=\"Actads : -\\nEstimatethe valuesofactionsand\\nuse themto make selectiondecisions.\\nQ, (a) =\\nsumofrewardswhenatakenpn-o.kz#oftimes a takenpriortot .\\n= ÉÉ Ri-\\n±Ai=a\\nÉ=i:|\\nwhere i. a\\n=\\n{1\\nif action'\\na' was takenin tineslepi\\n0 otherwise.\\nNyt:L: If denominatoris zero, we define\\nQfca) to some defaultvaluelike 0 .\\nME: As denominatorgoesto infinity,\\nbylawof largenumbers, Qtca) convergesto\\n9kcal.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 7, 'page_label': '8'}, page_content=\"Now theagentcan exploitthe actionwith\\n1\\nbestvalue basedon the estimates.\\nAt= argmaxQtla)\\na\\nThisis a\\ngreedyaction. However, the agent\\nshouldexploreotheractions that look\\ninferiorto see if theyare\\nactuallybetter.\\nSimpson: Exploitmostofthe time.\\nWitha\\nvery\\nsmall probability\\n'\\nC-'\\n,\\nalso\\nexplorefandomactions.\\n⊥c--\\ngreedymethod.\\nAd㱺e: In the limit,\\neveryaction will\\nbe sampledinfinitenumber of times, thus\\nensuringthat Qila) convergesto 9*61.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 8, 'page_label': '9'}, page_content='10-\\narmedtestbed:-\\n2000 anatomygeneratedk - armedbanditswith\\nKr = 10.\\n9*67are selectedaccordingto a Gaussian\\ndistributionwith mean 0 and variance1 .\\nthen the actionis taken, then rewardis\\nsampledfroma Gaussiandistributionwith\\nmean % (At) and variance 1 .'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 9, 'page_label': '10'}, page_content='Compañsofgedyand-C-greedy-metn.es:-\\nThe greedymethodoftengetsstuck performing\\nSuboptimalactions.\\nT-ncrementaimpkmu-i.AT-\\nConsiderany action. LetRi denotethereward\\nreceived afterthe i# selectionofthisaction.\\nLet Qndenotethe estimateofits action'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 10, 'page_label': '11'}, page_content='Valueafterit has been selectedn - l times.\\nQn = Ritts 1- . . - 1-Rn,\\nI\\nNaivÉn : storeall rewardsand compute\\nthe averageeverytime.\\nIncrementalimplementation: -\\n-\\nQn+, = ÷ FÉ,\\nRi\\n=\\nf. + ÷É*i)\\n=\\n:-(n + \" \"\\n¥:&\" )=\\nto (14+4-1)Qn)\\n=\\n1-(RntnQn-\\nQn)\\n|Qn+i=QnRn-Qn#'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 11, 'page_label': '12'}, page_content=\"Notes forn= '\\n, Qz= Ri.\\nThegeneralformof this updaterule!\\nNewestimate= oldestimate+ stepsize[Ta rg e t- oldestmate].\\n-\\nerror in the\\nestimate.\\nTheerror is reducedby takinga steptowards\\nthe target.\\nTa rg e tindicatesa desirable\\ndirectionto move , thoughit maybe\\nnoisy.\\nStepsizein our comet setupis\\n÷ .\\n7\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 12, 'page_label': '13'}, page_content='Tra\\ngankonaypnb6m\\nstationarybandits: reward probabilitiesdo not-\\nchargeovertime.\\nINhat ifthe reward distributionchangeover\\ntime ? In such non-\\nstationarysettings,\\nit makes senseto givemore\\nweightageto recent rewards thanlong-\\npastrewards.\\nOne\\neasyway todo that is by havinga\\nconstantstepsize parameter.\\nQn+, = Qn + ✗[Rn-\\nQIwhere ✗ c-(0,1] is constant.\\nQnti = Qnt ✗[Rn- QD\\n= ✗Rn + C-d) Qn\\n=D Rn+ C-d)[✗Rn, +4-\\na)Qn)'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 13, 'page_label': '14'}, page_content='= DR, + C-d)✗Rn- , + C-d)\"\\nQn,\\n= ✗Rn+ C-d)✗Rn, + C-d)2Rnz-1\\n. . . + (1-2)^212, + d-d)\"\\nQ,\\nIQni-i-C-aYQ.IE?xCi-ajn-iR#-\\nThis is weightedaveragebecause\\nC-a)\" + En.\\n✗ G-a)\" i\\n=/ .\\nThe weightdecaysexponentiallyaccording\\ntothe exponenton 1-d.\\nIf 2=1, all weightsgo to the lastrewardRn.\\nThisis called exponentialrecercytueighted\\naverage.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 14, 'page_label': '15'}, page_content=\"Let Anca) = step-\\nsize parameterused to process\\nthe reward receivedafterthe '\\nn'th selection\\nof actiona.\\ndial =\\nIn leadstosampleaveragemethod.\\nNote Convergencetothe valuesis not guaranteed\\nforall choicesof Inca) .\\nConditionsrequiredto assure convergencewith\\nprobability1 :\\nA *\\n⾨dnla) = to and Edick D .\\nN=p\\nGnd to guaranteethatthe stepsare large\\nenoughto eventuallyovercome any\\ninitialconditionsor randomfluctuations.\\nGnd to guaranteethat the stepseventually\\nbecomesmallenoughtoassure convergence.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 15, 'page_label': '16'}, page_content='NIE: Bothconditionsare met for dncat.nl.\\nBut for Anfal=D , secondconditionis\\nnot met.\\nOptiiaaÉ-\\nAll these methodsare dependenton the\\ninitial action- value estimates, Q, (a).\\nThey\\nare biasedby their initial estimates.\\nThis bias decreasesover timewith more\\nexperience. This bias acts as an\\neasyway\\nto supplysome priorknowledgeabout\\nwhat level of rewards can be expected.\\nOne can use the initial value estimatesas\\na\\nsimplywayto encourageexploration.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 16, 'page_label': '17'}, page_content='How?_ Insteadof settingQ.ca?=o, set\\nQ, Cal= -15 in our 10- armed testbed.\\nCal was selectedfrom ÑCO, , ) .\\nHenceQ, (a)= +5 is\\nwildlyoptimistic.\\nThis means ,\\nthe agentwill exploreall the\\nactions beforeconvergingtothe best\\nactioneven when C--_o . Let us call\\nthesemethodsas\\noptimiskc.in#aaes.N0te:-Thissimpletrickdoesnotwork for\\nnon -\\nstationaryproblemssince exploration\\ninducedby optimisticinitial valuesis'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 17, 'page_label': '18'}, page_content='temporary\\nandin the\\nbeginningoftime, unlike\\nE- greedywhich exploresa bit throughout\\nits lifetime.\\n-\\nUpper- Confidence-\\nBoundCCB) actionselection:-\\n-\\nC--\\ngreedyactionselectionforcesthe non-\\ngreedy\\nactions to be tried, but\\nindiscriminatelywith no preferenceforthosethat are\\nnearly\\ngreedyor\\nparticularlyuncertain.\\nIt is betterto select actionsaccordingto\\ntheir potentialfor actuallybeingoptimal,\\ntakingintoaccount both howclose other\\nestimatesare to being\\nmaximaland\\nthe uncertaintiesin thoseestimates.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 18, 'page_label': '19'}, page_content=\"At = arson (Qila) +\\nc¥÷g, ]a\\nwhereIn# = Naturallogof t .\\nNtla) = # of timesthatactiona hasbeen\\nselectedpriorto time1- .\\nC > o controls the degreeofexploration.\\nNote If NfCal= 0 ,\\nthen a is consideredto be\\na\\nmaximizingaction.\\n[¥ is a measure ofthe uncertaintyor\\nvariance in the estimateof a 's value.\\nQila+\\nc÷ ,\\nis an upperboundon the possible\\ntruevalueof actiona.\\nEachtime '\\na' is selected→ Nda) increasesand\\nhencethe\\nuncertaintydecreases.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 19, 'page_label': '20'}, page_content='Eachtime any otheractionis selected, dnt\\nincreases but Nt(a) remains sane . Hence\\nuncertaintyestimateincreases (but\\nonly\\nlogarithmically).\\nAll actionswill\\neventually\\nbeselected,\\nbut\\nthe actionswith lowervalue estimatesor\\nthat havealreadybeenselectedfrequently,\\nwill be selectedwith decreasingfrequency\\novertime.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 20, 'page_label': '21'}, page_content='Noted: Difficultto deal with\\nnon-stationary\\nusingUCB.\\nNote Difficultto deal with largestate\\nspacesand functionapproximations\\nthat we will see in the future.\\n-\\nGradientbanditalgorithms: -\\n-\\nSo farwe consideredactionvalue based\\nmethods. Now we consider\\nlearninga numerical\\npretencefor each action a ,\\ndenoted\\nas Hela) .\\n→ Largerthe preference, more offertheactionis\\ntaken.\\n→ No reward or value interpretation.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 21, 'page_label': '22'}, page_content='Boltzmann\\nPr{Ae.\\n: ay= e.Htca) ← Gibbsor\\ndistribution.\\n-\\n¥ etltcb) = It (a)\\nb-1\\nTe l a) =\\nprobabilityof takingaction a.\\nNote:\\nAdding1000to all Htfa) doesnot change\\nthe actionprobabilities.\\nNoted: Initial all actionsare\\nequallypreferred.\\nH,Cal=O forall a.\\nHowto learnthe preferences?-\\nIn each step, afterselectingaction At and\\nreceivingthe reward Rt ,\\nthe action preferences\\nare updatedas follows:\\nHtfAe1-HtCAt1td@i-Ri7C-iTtCAeDHtt.ca) = Hfcal1-✗(Re-\\nEt) Fecal\\nfor #At'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 22, 'page_label': '23'}, page_content=\"where d 70 is a step-\\nsize parameter.\\nIt C-IR is the averageof all rewards\\nupto't :\\np\\nbaselinewithwhichtherewardis compared. .\\nRt> RT : Prob.\\nof takingAtin the future4\\nRtL Ie : Prob.\\nof takingAt in the futureI.\\nNon- Selectedactionsmore in the oppositedirection.\\nlo-\\naritmeanof1merewardto t4 insteadof zero :\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 23, 'page_label': '24'}, page_content=\"The banditgradientalgorithmas stochasticgradientascent:-\\n- -\\nGradientbanditalgorithmis a stochasticapproximation\\nto gradientascent.\\nExactgradientascent: Ht+, (a)= Htca) + ✗ dE[Rt]- -\\nc)Htca)\\nH+(a) is incrementedproportionalto the increment's\\neffect on performance.\\nperformance= E-[Rt] =\\nEmitter)q*Cn)\\nIt is not possibleto implementthis since we\\ndo not have access to q*cn).\\ndE[Rt]- =\\nd-\\nOkita) [É%H9*Cn)]dtfla)\\n=\\nIn9*1×7\\nd1Ttln7dHtCa7_En@xH-Be7dT1_tnldHtla7whereBt.the\\nbaseline, can be anyscalarthat does\\nnot dependon n .\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 24, 'page_label': '25'}, page_content=\"NOIIn÷,?÷,\\n=o so we can introducethe baseline.\\ndE[Rt)- ⾨ In @* Cn) -\\nBt) dñdHe(a) dtltla)\\n=\\nIn ñtcn) ( or* Cn)-\\nBt) dñtdHela) /Teen)\\n= E-[@*\\n'n'-\\n%) , /It # I\\ndt-_ E-[(Re-\\nRi. ) dñ÷?¥/II. At) - ②dHela)\\nwhere Bt = Ñt .\\nRt is a stochasticapproximationof ⊥Cn)\\nSince\\nE[RtlAt] = 9*CAt) .\\nNowlet us computedµ?÷.\\nd1Tt = d- It Cn)\\ndtffa) dHtla)\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 25, 'page_label': '26'}, page_content='=÷㱺f¥÷÷.j\\n- etten? d=\\nyÉ=,\\ne \"t⊥)\\ndehi-lng-y.at?zeHtH\\n)\\nÉ\"= IIa. .me\\n\" \"\\n\"¥kemily) -\\nehtcnleth.la?#&e)\\n= Ian.nl/7-ln7\\n÷÷.\\n-\\n÷÷÷÷÷, .\\n= 1a=n TtCn) -\\nTtCn) IT,-\\nIa )\\n=\\nñtln? ( Ian -\\nIt Ca) ) →:÷÷,\\nsubs. ① in ⑧.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 26, 'page_label': '27'}, page_content='%EF.tt?--EICRi--Ri-)dYY-?:;/iT-cAtD\\n= ☒[CH-\\nRI) ñt¢?(1a=a,\\n-\\nTt⊥¥µ, ]\\n= ⾨[④←-\\nRid(1a=n.\\n-\\nñtca))]\\nHt+, Cal =\\nHela? -12 f-Rt) (1a=a,\\n-\\nTe l a) ta .\\n-\\nco-ntextual.ba#dits:Bandits:-Non- associativetasks.\\ni.e .\\nNo needto associatedifferentactions\\nwith differentsituations.\\nFuHR There are\\nmany\\nsituations.\\nGoalis to\\nlearna\\npolicythat mapssituationsto\\nactions.\\nContextualbandits: Associativetaskthat is not\\n-\\nas complexas fullRL.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210907021931Z00'00'\", 'moddate': '2025-06-16T20:08:03+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 02.pdf', 'total_pages': 28, 'page': 27, 'page_label': '28'}, page_content='Assumethereare no differentk- armedbandittasks\\nand each step you confrontone of thesechosen\\nat random.\\n→ Unlike bandits, there are 10 differentsituations\\nto act.\\n→ unlike RL, the rewardsare immediateand\\nactionsdo not affectthe nextstate.\\nIf you dont knowthe context, thenthis looks\\nlike a\\nnon-stationarybanditproblem.\\nIf the contextis given, we can learnto associate\\ncontextto actions.\\ne±.\\nAd-f .\\nGivena set of ads,\\nthe\\ngoalis to placethe most relevantad\\nforan uses such that he/she will most\\nlikely\\ndickit .\\nContext: user history\\n-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}, page_content='3. FiniteMarkovDecisionProcesses.\\n-\\nMarkovDecisionProP) : A mathematicalformalization\\nof sequentialdecisionmaking/RL.\\n→ l%Treward\\n§ /\\nRt\\n] action\\nAt\\n\"\\n% ¥\"µriwmY-\\n:*.\\n-\\nThe agentandthe environmentinteractat each\\nof a sequenceof discretetime steps 1=942,3. . .\\nAt each timestep1-,\\n→ the agentreceivessome representationofthe\\nenvironment\\'s state St C- S .\\n→ on that basis, select an actionAtC-Als)\\n→ in next time step, receives a numerical'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2'}, page_content=\"reward Rt+, C-R CR .\\nand findsitselfin\\na new state Stti.\\nSequence/trajectory! So, Ao,\\nR,,\\nS, , Ai, Ra,\\nSa, A2, Rs. . .\\n-\\nFiniteMD ⑤A ,\\nR ) - all havefinitenumber\\nof elements.\\nRt, St -\\nwell defineddiscreteprobabilitydistributionsdependentonlyon the\\nprecedingstate and action.\\nPCs'\\n,r Is, a) = Pr {St:S '\\n,Rt=r/ st.is/At-i--a\\n}for all s:SES NER, and a c-AG).\\np 㱺 definesthe dynamicsofthe MDP.\\np: SXRXSXA, → [0/1]is a deterministicfn. of\\n4 arguments.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3'}, page_content=\"Moti -2 -2 PCs'\\n,r Is, a) = I V-sc-S.aeAG).\\ns '\\nES VER\\nFromthe dynamicsfn '\\nP' one can compute\\nanythingelse one mightwant to know aboutthe\\nenvironment.\\nstate- transitionProbability: p:S xs ✗A→ Coil]-\\nPcs' Is, a)= Pr{St㱺 '\\n1st-1=5, Azra}\\n= E. PCs'\\n.ir/s,a7.rERExpectedrewardfor Sia pair: r :S ✗ D-→ IR\\n-\\nKsa) = E [Rtls, - is ,A→=a ]\\n± E r -2 PCs!rIs,a7\\nVER SES\\nExpectedreward forslate- action- next slate triple:-\\nrisxnxs → IR.\\nHS,a, s' )=E[Rt1St-1=5, At-1 =a , St:S ']\\n=\\nEpi Pairs,\\nPCs' /Sia)\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}, page_content='Markovpnop-erty.tt(St, Rt/St-\\n, , At-\\n1)=P( St, Rt 1St-\\ni. At-1, Rt-\\ni. Sts,\\nAt-2 , . - -\\n)Stand Rt dependsonlyon the immediately\\npreceding\\nstateand action, St-\\n, and At.\\n, .\\nMode: This is a restrictionon thestatenot on\\nthe decisionprocess.\\nTheMDP frameworkis abstractand flexible.\\nTime steps → neednot referto Fxedintervaloftime.\\nactions → can be low- levelcontrolor high-level\\ndecisions.\\nstates→ lowlevel sensationsor highlevel\\nobjectrepresentations.\\nAlso,\\n-\\nthe agent/environmentboundaryis not\\nphysical.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}, page_content='Anything-\\nthat cannot be changedarbitrarilyby\\nthe agentis consideredto be outsideofit\\nand thus partofthe environment.\\nThe boundaryrepresentsthe limit ofthe\\nagentsabsoW-kotfitwwg.\\n.\\nForexample,\\ntheagentoftenknowsabouthow\\nrewardsare computed. still rewardsare\\nexternalsince the agentcannotchangeit\\narbitrarily.\\nSometimesagentmightknow\\n\"\\neverything\"\\nabout how its err . works and\\nstill facea difficultRL task. Youknow\\nhow rubik\\'s cube work still do not know\\nhowto solveit .\\nMDp:-reducesany problemof learninggoal\\ndirectedbehaviourto threesignals:'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6'}, page_content=\"- one signal torepresentthe choicesmadebythe\\nagentCtheactions)\\n- one signalto representthe basison which\\nthe choicesare made ( the states)\\n-\\none signalto definethe agents\\n'\\ngoal(therewards?\\ntaobao.presertstatesandach.io#we\\nwill assume the representationis givenso\\nthatwe can focuson the generalprinciples\\nfor learninghowto behaveonce the\\nrepresentationshave been selected. Howto\\nlearnthese representationis a topicfor\\nfuturediscussions.\\nGoalsandrewards:-\\n- -\\nRewardis yourway of Communicating\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7'}, page_content='to robot whatyou want it to achieve, not\\nLowyou want it achieved.\\nHow㱺 Prior knowledge. Can be insertedto\\ninitialpolicy\\nor initialvaluefunction.\\nReturnsandEpisodes: -\\n-\\nGoaloftheagentiTo maximizethe cumulativereward\\nit receives in the longrun .\\nCumulativereward = return.\\n|txpectedretumG£=RR+z\\nwhereT is the finalstep. This definitionassumes\\nthat the interactionendsat some pointT.\\n⾨ playsof thegame.\\nSuch repeatedbut finite- time interactionsare\\ncalled \"\\nepisodes!'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8'}, page_content='Each episodeendsin a terminalslate, followed\\nby a reset to a standardStarlingstate or to\\na samplefrom a standarddistributionof\\nstartingstates.\\nS - setofall non- terminalstates\\nSt - set of all statest terminalslate.\\nT is a randomvariablethat varies episodeto\\nepisode.\\nThesetasks are called\\nepisodictasks.INhat if a taskgoeson continuallywithoutlimit?\\ne± Anapplicationto a robot witha\\nlonglifespan.\\ntheseare called continuing\\ntasks.\\nReturneqn.\\n(1) is problematicfor\\ncontinuingtasks since 1-=D .'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9'}, page_content='IdeaEig :-\\ndiscountedreturn Gt= Rt+, + rRt+z-182%+3-1. . .\\n•= I ✗KR1<=0 ttkt ,\\n→\\nwhere 0 EVE1 is calledthe discountrate.\\n✓determinesthe presentvalue of futurerewards.\\nRewardreceivedafterK timestepsis worth\\nonly 81<-1timesits worth.\\nIf ✓< 1\\n,\\nthe infinitesum ② has a finite\\nvalue as\\nlong\\nas the reward sequencethe}\\nis bounded.\\n✓=o 㱺 the agentis \"\\nmyopic\"\\n.\\nmaximizes\\nonly Roti.\\n✗→ I 㱺 the agentbecomesmore\\nfarsighted.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10'}, page_content='Gt = Rtt, + rRt+z-1824-+3+ ÑRt+yt. -\\n= Rtt 1- ✓(Rt+ztrRt+ztÑRttyt - -\\n)\\n/Gt=Rt+itrG-\\nNOI Althoughreturnis a sum of an\\ninfinitenumberof terms, it is still\\nfinite if the rewardis nonzeroand\\nconstantif 8<1 .\\na if rewardis constanttt ,\\nthen\\nat =\\nFIrk =L1-r .\\nUnifiednotationforepisodicand\\ncontinuingtasks:-\\n- - -\\nepisodictasks: Gt= Rtt, -1121-+2-1. . 1-RT\\ncontinuingtasks: Gt =\\n¥08kRttkti'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11'}, page_content='Considerepisodeterminationto be\\nenteringof a\\nspecialabsorbingstatethat transitionsonly to\\nitselfandthat generatesonlyrewards of zero.\\n⑤\"\\n⑤ €+1>¥2K-0\\nRs?\\nT\\nGt= I yk- t\\n-1k,\\n1<=1-1-1\\nincludingthe possibilitythat1-=D or 8=1(but\\nnot both) . Canwe havebothF- • and0=1 ?\\nMoreon this later.\\n-\\nPoliciesand Valuefunctions:-\\n-\\nAlmostall RL algorithmsinvolveestimating\\nValuefndions-\\nfunctionsof slates(or state- action\\npairs) that estimatehowgoodit is for\\nthe agentto be imagerstateCor'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12'}, page_content='howgoodit is to performa\\ngoresactionin\\na giverstate) .\\nValuefns are definedw - r -\\nt .\\npolicies.\\nPolicy:\\nmappingfromstatesto probabilitiesof\\nselectingeach possibleaction.\\nTla/s) is probabilitythat At=a if St:S .\\nstate-aefundi-o.tv, Cs) : value fn.\\nof a slates undera\\npolicyit .\\nVitis? = E-[Gt/ f.=s ]\\n= Em[É\\nrkR.am/St=s)-Vsc-S.K--ONo-te:Y,-(terminalslate) = 0 .'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13'}, page_content='Adton-m: -\\nGals, a) = valueof takingactiona in states.\\n= expectedreturnstartingfroms ,\\ntaking\\nthe actiona ,\\nand thereafterfollowing\\npolicyñ .\\nGirls/ a) = E-* [Gt1st:S , At__ a)\\n= E-+ [É r \"\\nReum/ St:S, At-\\n-\\na)6=0\\n-\\nVitand oft can be estimatedfromexperience.\\nOne can maintainsampleaveragesof actual\\nreturnsforeach stateencountered.\\nAs 1-→ 6 ,\\ntheseaverageswill convergeto actualvalues.\\nThisis called Monte- Carlomethod( more later).'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14'}, page_content='However, this solutionis not practicalif there\\nare\\nverymany\\nstates. Thenwe should\\nrelyon parameterizedfunctionsfor Vitand\\n% ( againseen laterI\\n-\\nForanypolicy\\nit , andanyslates ,\\n4-(s) =\\nE-[Gt1st-\\n- s]\\n=\\nHIT[Rt+,\\ntr Gtf, / 5¥:S]\\n=\\nEntails)} ?PCs!rls, a)\\nBellman\\nequation\\n[r +8E-[4++111++1--5]\\nftp.?si=-zn-caiss-zpls!rls.a7G+8vHsD#+sc-s.-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15'}, page_content=\"Note: Vet is - Ite uniqueSolnto its Bellman\\nequation.\\nBackupdiagramfor:\\nOS\\n1. ÷11 A AiO O O O O Os '\\nThe Bellmanequationaveragesover all the\\npossibilities.\\nGridworld statevalue fn. for\\nan equiprobablerandom\\npolicy.\\nGIE: -1 for actionsthatwould take-\\ntheagent\\noffthe gridand leaveslateunchanged.\\n0 forall otheractorsexceptfor\\nstatesA andB.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16'}, page_content=\"BetÉN% :\\n¥1s, a)= E PCs!r /Sca)[rtr-zu-cat.si/q,-B!a5fs,ra '\\nG•a\\nArbackup oh 0\\ndiagram: !! !¥•ai\\n-\\nOplimalpoliciesandopltmalraluefnsi.ltpolicyIT is definedtobe betterthan or\\nequalto I '\\nif its expectedreturn is\\ngreaterthanor equalto that of ñ '\\nfor all slates.\\nTZITlifandonlyifvy-cslzvn-ils-vs.ES#\\nThereis alwaysat leastone\\npolicythat is\\nbetterthanor equalto all other policies.\\nThis is an optimalpolicy.no\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17'}, page_content='a-* -\\noptimalpolicy.\\nV* - state- value fnof theoptimalpolicy.\\n(or) optimalstatevaluefn.\\nV*G1=mYTCs)V-s\\\\\\nOptimalpoliciesalsosharethe same optimal\\naction- value fn.\\n% .\\nfoh.is/a7--mf9i-Csa7V-sa:Y#q*Cs.a7--lF-[Rtt, -18✓* Gti)/St:S,At=a]\\n*iÉÉiyit must satisfythe self-\\nconsistency\\nconditiongivenbythe Bellmanequation.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18'}, page_content=\"Becauseit is the optimalvaluefn, v*'s\\nconsistencycondn.\\nCan be written without\\nreferenceto any specificpolicy.\\nV*(s) = Max\\nqq.es/a7ac-H--max1E,,-*(Gt/St=s,Az--a)a\\n= Max ¥*[Rt+, -18Gt+, /Sis, At-a]a\\nµ**=m✗E[Rt+i+=At=aY\\na\\n¥67=\\nmy¥Pls!r⊥a)[r+rv*ls'D\\nBellmanoptimalityegn. forV*.\\nos\\n€11.\\n.of ¥\\n⾨⾨Hors.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19'}, page_content='|%\"\"=[¥\"+°^¥%\"\"\"\\nSt=s, Azur]= zpg ,\\n, , , , , [r+gmgg*,\\n,aSlr\\n=Bellmanoptimalityegn for %.\\n•S, a\\nMr0 ⑨s \\'\\nT.it. • a \\'\\n-\\nN0 For finiteMPPs, Belleman optimality\\negn for Vit has a uniquesoln. independent\\nof the policy.\\nIt is a systemof egns with \\'\\nn\\' equations\\nfor \\' n \\' stateswith \\'\\nn\\' unknowns.\\nIf \\'\\np\\'\\nis known,\\none can solve this by\\ntreatingit as systemof non-linearegns.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20'}, page_content='Onceone has Vy,\\nthenthe optimalpolicy\\ncan be easilyconstructedby actinggreedily\\nw.r.ir .\\nV* . One-\\nstep lookaheadis enough\\nto act optimally.\\nHaving¥ makes it even easier. No need\\nfor one steplookahead. Butthe tradeoffis\\nyou have to store ofvalue for all actions\\nin\\nevery\\nstate.\\n-\\nExplicitlysolvingthe Bellmanoptimalityeqns.\\ngivesus the optimalpolicyand hence\\nsolvesthe RL problem.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21'}, page_content='However, it has3 assumptionswhichare rarely\\ntruein practice.\\n(1) live accuratelyknowthe dynamicsoftheenv .\\n(2) We haveenoughcomputeto completethe\\ncomputationof thesoln.\\n(3) the Markovproperty.\\nOneof theseassumptionswouldbe violatedin\\nmost of the interestingapplications.\\nE Playingbackgammon\\n(1), (3)ar ok.\\nBut for (27,\\nthereare 1020states!\\nAllof RL is aboutapproximatelysolvingthis\\nBellmanoptimalityegn. Notethat the agent\\nneed not act optimallyin everypossible\\nsituationone can thinkof.\\nIt is enough'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210908050538Z00'00'\", 'moddate': \"D:20210908050538Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 03.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22'}, page_content='if it actsoptimallyin statesthat it visits\\nfrequently. Thisis the key property\\nwe will exploitwhile developpingapprox.\\nSolutions.\\n-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content=\"4.DynamicProgramming-\\n- collectionof algorithmsthat can beusedto compute\\noptimalpoliciesgivena perfectmodel of the\\nenvironmentas a MarkovdecisionProcess(MDP) .\\nLimitations:- 1 .\\nAssumesa perfectmodel.\\n2.\\nComputationallyexpensive.\\nHowever, it providesessentialfoundationforthe\\nunderstandingof the futurealgorithms.\\nAssumethat the environmentis a finiteMDP.\\ni-\\ne .\\nState S, actionA andreward R are\\nfinite, the dynamicsare givenby\\nPCs'\\n.ir/s,a)-Vs-c-S,ae-Afs7,rc-R\\nandS' est (whereStis S plus\\na terminalkeyidaiuse ofvaluefunctionsto statefor\\norganizeand structurethe episodicsetting)\\nsearch for goodpolicies.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content=\"Optimalvaluefns V* , 9.* satisfythe Bellman\\noptimalityequations.\\n⊥is = Max E[R£+, +✗4-⊥tti )/ St:S ,At=a]a\\n= Max E pls'\\n,r /Sca) [+84+611]a stir\\n9*15,a) = É[Rt+, -15m¥9*06, ,a ' )/St:S, Ana]\\n= I p(slits, a)[r+8ma✗9*Cs!as !r a '\\nDP algorithmsare obtained\\nby turningBellman\\nequationsinto updatesubs for improving\\napproximationsof the desired valuefunctions.\\nPolicyign ) :\\nHowto computethe statevaluefn YT for\\nan\\narbitrarypolicyI ?\\n⊥(s) = E-[ Gt1st:S]\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content=\"= Er [Rtt, + rGt+, / St:S ]\\n= ¥ [Rtt, -18¥(Sta) /f. =s]\\n4,1s) = -2 Flats) I PCs!rls, a)[rtÑv,,--s'☐a Sir L⑨\\nwhere11-6151= prob.\\nof taking action'\\na' in state's '\\nunder\\npolicyIT -\\nThe existenceand uniquenessof YT are guaranteed\\nas\\nlongas either V21 or eventual.\\ntermination\\nis guaranteedfromall statesunder policyIT-\\nNote: If the env's dynamicsare completelyknown,\\nthen ⑨ is a systemof 1stsimultaneous\\nlinearequationsin 1st unknowns.\\nThesolutionis\\nstraightforwardbut tedious.\\n-\\nConsidera sequenceof approximateIterativesoln:-\\nvalue fns Voir, ,r2 . . . each\\nmapping\\nSt AIR.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content=\"The initial approx. is chosen\\narbitrarily( exceptthat\\nthe terminalslate, if any,\\nmust be givesvalueo ).\\nEach successive approx. is obtainedby using\\nthe Bellmanequationfor YT as an\\nupdate\\nrule.\\nVK.t.es) = E-[ Rtt, + ÑkCse→) /Ses]\\n=\\nEaltlals) I\\npcsirls.at/fr-8vkcsiDs'.rV-sc-f.\\n①vµ=YT is a fixedpointforthisupdaterule.\\n② thesequenceHe} can be shownto converge\\nlo Vit as k → & underthe same conditions\\nthat guaranteethe existenceof vii.\\nThis algorithmis knownas\\nikmt-epiy.vn.\\nTheseupdatesare expectedupdatesbecausethey-\\nare basedon an expectationover all possible\\nnext states rather thanon a samplenext state.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content=\"NoteI: one could use twoarrays, one forold\\nValuesVkls) and one for new valuesVa, Cs) .\\nOr one can alsodo in-\\nplaceupdateswhich\\nconvergesfaster.\\nNote2 :-\\nOne round of updateof all the Statesis\\ncalledas '\\n# '\\nthroughthestatespace.\\nNote2 Iterativepolicyevaluationonlyconvergesin\\nlimit.\\nHowever, in practicewe stop\\nwhen the updatesare\\nsufficientlysmall,\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='Examples\\nundiscovered,\\nepisodicsetting.\\ni\\n-\\nPolicy.in/movenert:--\\nOne reason forcomputingthe value\\nfn fora\\npolicy\\nis to help findbetterpolicies.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content=\"Given if for some\\narbitrarypolicyit ,\\nForsome stateS, we wouldlike to know\\nwhetheror not we shouldchargethe\\npolicyto deterministicallychoosean actiona =/ITG).\\nConsider\\nselecting\\n'\\na' in ' s '\\nand thereafter\\nfollowingthe existingpolicyit .\\nThe value\\nof this\\nwayof behavingis\\nohtfsia) = E-[Ra, -184TCstte) /\\nst.is/tt--a)---zpCs:rls.a)fr+8r#siDs '\\n,r\\nIf this is greaterthan * G), thenthe\\nnew\\npolicy\\nit ' whichtakesaction' a' in state\\n'\\ns' and followIT for rest oftheslateswould\\nbe a betterpolicy.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content=\"Poliyimprovemertthorem-\\n(PIT)\\nLetIT and-11'\\nbean pairof deterministic\\npoliciessuch that , forall S C-S\\n% ( silt 'Cs7) 741s )\\nThenthe policyI must be as\\ngoodas ,\\nor betterthenIT .\\ni. e. Vitals) 746 ) Ks C-f.\\nÑ°¥⊥(s) I 9*(5,11-161)\\n= E-[Rtt, -184+(5++1)/ St:S, AET' est]= Em[Rtt, -184+(4-+1)/se:S]\\nI ¥, [Reti-179+6++51116++1)) 1St:S]\\n=Éµ[Rtt, -18HI,[Retz-1841-(56-+2)/Seti,\\nAtt, -1T¥])s㱺\\n=ÉLRt+, -1812++2-187+(4-+1)/st=s]\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content=\"I E#[Rtt, -184*+2+844-+3-18%+9-+37/4:-)\\n:\\n2-lEÉi[Rtt, -18%72-1834-+3-18>\\nRent- -\\n1st:S)\\n= VitiG)\\n-\\nGiventhe value fn. for some Policyit ,\\nconsiderthe followinggreedypolicyit art\\nthe valuefn.\\nI ' = argmax9gG.a)a\\n= argmoxE[12++1+84+01- +e)/ st=s,\\n]a Atta\\n= argmax-zpcdils.at/Gr-8rn-cs'1)a Sir\\nBy PIT, this\\ngreedypolicyshouldbeas\\ngoodas or betterthanthe originalpolicy.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content=\"The processof makinga new\\npolicythatimproveson an\\noriginalpolicy,\\nby making\\nit greedywit '\\nthe value fn ofthe original\\npolicy\\nis called\\nPoliyimpwvemert.suppo.sethe new\\ngreedypolicy\\nT ' is as\\ngoodas , but not betterthan it ,\\nthen\\nVq= VII.\\nYi, G) = Max É[Rt+,trVñdst+, ) /f.=s, ]a A-=a\\nThis is same as BellmanOptimalityegn.\\nThus He = V* . and bothit , -1T' shouldbe\\noptimalpolicies.\\n-\\nPolicy n : -\\nSequenceof monotonicallyimprovingpolicies\\nand value fins.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='To →E⊥¥1T,\\nVit,\\nñzE→. .\\nI/ . . - . . 㱺1T¥E→V*.\\nPolicy Policyevaluation improvement\\nEach\\npolicy\\nis guaranteedto be a strict\\nimprovementover the previousone (unlessit is\\nalreadyoptimal) .\\nBecausea finiteMDP has onlya finite\\nnumber of policies,\\nthis processmust converge\\nto an optimalpolicy\\nand optimalvaluefn in\\na finitenumber of iterations.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content=\"ValueIkra:-(VI )\\nLimitatioofyitehun: -\\nEach ofits iteration\\ninvolvespolicyevaluation, whichitselfis an iterative\\ncomputation. Howevery, Policyevaluationonly\\nconvergesin limit. Whento truncatepolicy\\nevaluationstep?\\nValueikraltonstopspolicyevaluationafterjust\\none sweep.\\nthe updaterule forvalue iterationcombines\\nthe policyimprovementand truncated\\npolicyevaluationsteps.\\nYa, (s) = max E[Rt+, -184<(51-+1)/St:S , 4.=Da\\n= Max -2 Pls! rlsea)[r+Driers'☐a stir\\ntsef\\nForarbitraryro, the sequencefriescan be\\nshown4 convergeto V* under same conditions\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='that guaranteethe existenceof V*.\\nNote:t_: VI is obtainedbysimplyturningthe\\nBellmanoptimalityegnto an updaterule.\\nNoted: VI is identicalto policyevaluationexcept\\nthatit requiresmax betakenover all\\nactions.\\nMayvariationsof interleavingpolicyevaluator\\nwith\\npolicy\\nimprovementis possible!\\n-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content=\"AsynchronousDP:-\\n-\\nDP requiressweepover entirestatespace.\\nThis\\nis not possiblein\\nmanyproblems.\\nE- Backgammonhas ore 102°states!\\nAsynchronousDP → in-placeDP algorithmswith\\nno systematicsweeping.\\nJust updatewhateverstateis next available.\\nThismakes it easier to intermix computation\\nwith real- time interaction.\\nTo solve a given\\nMDP, we can run an iterativeDP algorithm\\nat thesame timethatan agentis\\nactually\\nexperiencingthe MDP.\\nAgent's experience→usedto decide whatstateto\\nupdate.\\nLatestvalue& Policyinfofrom☐p }→ can guidethe agents\\ndecisionmaking.\\n-\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='GeneralizedPolicy\\nIteration:-\\n(GPI)-\\nPolicyiterationconsistsof two simultaneous,\\ninteractingprocesses\\n→ one makingthe Rheefn Consistentwith\\nthe current\\npolicy(Policyevaluation?\\n→ theothermakingthe policygreedywart.\\nthecurrentvaluefn(policyimprovement)\\nGPI- idea of lettingpolicyevaluationand\\npolicyimprovementinteract, independentof\\nthe granularityand other detailsof two\\nprocesses.\\nevaluation\\nFEit ✓IT mrgvgreedylv)\\n←improvement\\n÷\\n1T¥㱺 ✓*'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210915052350Z00'00'\", 'moddate': '2025-07-04T12:40:24+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 04.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='Thesetwo processesare both competingand\\ncooperating.\\n-\\nEfficiercyof-DP.TTfindsan optimalpolicy\\nin timethat is\\npolynomialin the number of statesand actions.\\nn = # ofstakes\\nK = # of actions.\\nTo t a l# of deterministicpolicies= K?\\nDPis exponentiallyfasterthan directsearch!\\ndinnerprogramming\\ncan be usedto solvem☒p\\nbut theyare not as scalableas DP.\\n-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 0, 'page_label': '1'}, page_content=\"5-MonteCarloM-hods_\\nDynamicProgramming-\\nrequiresaccess tothe transition\\ndynamics, which is often\\nnot available.\\nCanwe\\ndirectlylearnthe valuefunctionsfrom\\nap ?\\nexperience/ actualexperience\\nCbyinteractingwith the world)\\n\\\\ simulatedexperience\\nCbyinteractingwiththe model)\\nsimulatedexperienceonly requiresmodels fromwhich\\nyoucan samplethe transitions. It does not require\\naccess tothe completeprobabilitydistributionsof\\nall possibletransitionsthat is '\\nrequiredbyDP.\\nMonteCarlomethods: -\\n-\\n-\\nSolvingRL basedon\\naveragingsample\\nreturns.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 1, 'page_label': '2'}, page_content='- Considerepisodictaskswhereepisodesalwaysterminate.\\n- We computereturns afterterminationand\\nuse it to updatevalue estimatesandpolicies.\\n- This is similarto reward estimationin bandits. .\\nIn this Case, we estimatereturns foreachstate- actor pair• Thereare\\nmultiplestatesandtheycan be treatedas\\nmultiplerelatedbanditproblems. Returnfrom\\na state dependsnot juston theactiontakes\\nin currentstate, but alsoon action taken\\nin futureslates.\\n- Since all actionselectionsare\\nundergoing\\nlearning, the problembecomes\\nnon-stationary\\nfromthe pointofview of earlier date-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 2, 'page_label': '3'}, page_content='MonteCarloPrediction:-\\n-\\nGivena\\npolicy,\\nestimatethe state valuefn.\\nwe wantto estimateVitls) , the valueof\\na state\\'s \\' underpolicy1T, givesa set of\\nepisodesobtainedby following\\'ñ \\'\\nandpassing\\nthrough\\'s!\\nEachoccurrenceof a state\\'s\\'\\nin an episode→\\n\"\\nvisit\"\\nto \\'s !\\nFirst- visit Me → estimateshits) as the\\naverageof the returnsfollowingfirst visit to É .\\nEvery- visit Me →\\naveragesreturnsfollowingan\\nvisits to \\'s \\'\\n.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 3, 'page_label': '4'}, page_content=\"Bothfirst- visit Me and\\nevery\\n- visitMe will\\nconvergeto %) as the numberof visits\\nto's '\\ngoesto infinity.\\nBackupdiagramfor Monte- Carlo:-\\n-\\n0 rootnode→ statenode.\\n4•\\n& DPincludesonlyone-steptransitions,\\n1¥ whichiac coverstheentire\\n•¥ episode.\\nDP -\\nexpectedupdate\\nMC -\\nsampleupdate.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 4, 'page_label': '5'}, page_content='Netit : In Me, estimatesof eachstateare\\nindependent. The estimateofone stale\\ndoesnot builduponthe estimateof\\nanyotherslate.\\nso no\\n\"\\nbootstrapping\"\\nlike DP.\\nNIKI: Mc is attractiveif we want to\\nestimatevalueof onlyone state.\\n-\\nMonte-\\nCarbiofactiunralue\\nstatevaluesrequire access to the\\nmodelto lookaheadone stepandchoose\\nwhicheveractionthat leadsto the bestcombination\\nof reward and nextslate. .\\nWithout a model, we needto estimate\\n%.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 5, 'page_label': '6'}, page_content=\"Estimateohtcsca) ,\\nthe expectedreturn when\\nslanting\\nin slate 's '\\n,\\ntaking\\naction ' a' and\\nthereafterfollowingpolicyit .\\nWecan use same MC methodwe used\\nfor estimatearrest. However,\\nmanystate- action\\npairsmaynever be visited.\\nIf it is a\\ndeterministicpolicy, then we observereturns\\nonly for one of the actors fromeachstate!\\nTheseestimatesare uselesssince we need\\nactionvaluesof all actionsto pickthe best\\naction.\\nThis is the problemof maintaining\\nexploration.\\n-\\n(Es) Exptwatsaxmp.ir:-\\ntheepisodesstart\\nin a state- actionpairandthatevery\\npairhasa non- zero prob.\\nof being\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 6, 'page_label': '7'}, page_content='selectedas start.\\nIs guaranteesthat all slate- actionpairswill\\nbe visitedan infinite# of limesin the limit\\nof an infinitenumber of episodes.\\nNote: ES is a\\nverystrong\\nassumption. Notalways\\npractical.\\nAlternates: To consideronlystochasticpolicies.\\n-\\nMonteCarloControl: -\\n-\\nWe will followthe idea of Generalized\\nPolicyIteration(GPI ) .\\nTo %,\\n¥1T, €591T, II. .\\n_ÉlT*→g*.\\nPolicyevaluationis done exactlyas described\\nin MC prediction.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 7, 'page_label': '8'}, page_content='Policyimprovementis done by makingthe\\npolicy\\ngreedyw.it the currentvalue fn.\\nIT G) = argmaxqcsa)KH a TK\\n%,\\nCS, 1Tk+, CS)) = 9oz§ , a%ma✗9%6, a ))a\\n= Max 4%6, a)\\na\\n3 91T¢(5,1T¢(s))\\n7 ¥1s)\\nThereare 2 unlikelyassumptionshere:\\n①Episodeshaveexploringstarts.\\n② Policy\\nevaluationcouldbe done with infinite\\nnumberof episodes.\\nEasyto remove the secondassumption.\\nWe dont\\nneed to wait for PE to convergeto start\\ndoing policyimprovement. We can do the\\nextremecase like value iterationand improve\\nthe policyaftereveryepisodeof Policy'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 8, 'page_label': '9'}, page_content='evaluation.\\nNoteiIntuitivelyMonteCarloEs shouldconvergeto\\nthe optimalpolicy. But the generalversion\\nofthis convergenceis still not yet proved!\\n-\\nMonteCarloControlwithoutES : -\\n-\\nTo avoid ES, the agentshouldkeepselecting\\nall the actions.\\nThis requiresstochastic/soft\\npolicies.\\ni.ie. ital s ) so ✓-SESand a c-A-G)\\nExamine: E-greedypolicy.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 9, 'page_label': '10'}, page_content='In C--\\ngreedy, non -\\ngreedyactionsare given\\nthe minimalprob.\\nof selectionµ÷, and\\nthe\\nremaining\\nbulkof prob.\\n1-c-+ E- is\\n☒ 1\\ngivento the\\ngreedyaction.\\nf- greedyis an examplefor C--\\nsoftpolicies.\\nC--\\nsoftpolicies: ticals) 7£ Aga,\\n#G)I C->o .\\nAmongall C--\\nsoftpolicies, E-greedyis the closest\\n6 greedy.\\n-0333'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 10, 'page_label': '11'}, page_content='Any c--\\ngreedypolicy\\nw.r-t.to% is an\\nimprovementover any C--\\nsoftpolicy\\nit is\\nassuredby the\\npolicy improvementtheorem.\\nI \\'\\n- e- greedypolicy.\\n% (s, it\\'s) ) =\\nAIIT\\'(ats)19, (sa )\\n=\\n1¥, ,\\n-2%-6-a)+ (1-E)myHsia)\\n> µ÷,\\nEarnhardt\\n4-E) _zÑds)-¥ gy-cs.at\\n1- C-\\n7 µ÷, ,\\n\\'\"a) -\\n¥¥%\"- a)t-zn-calskn-is.at\\n= HIS)\\nThusby PIT, IT\\'\\n> it .'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 11, 'page_label': '12'}, page_content=\"NII: Nowwe\\nonlyachievethe best\\npolicy\\namongthe C--\\nsoft policies, buton the\\notherhand eliminatedthe assumptionof ES:\\n-\\nOn-\\npolicy\\n11s. Off-\\nPolicymethods:-\\n-\\nCan we learna deterministicpolicy\\nwithoutES assumption? However, we still\\nneed to selectall the actions to learn\\nabout their values.\\nOne can use a stochastic\\npolicy to learn aboutan optimalpolicythat\\nis not stochastic.\\nOn-\\nPolicy\\nmethods off-\\nPolicymethods.\\n- uses onlyone\\npolicy.\\nbehaviorpolicy\\n'\\n-\\nto explore\\nt.ami.me#-::::..-.::::..n..samepolicy\\nthat\\nwas usedtotake\\ndecision.\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 12, 'page_label': '13'}, page_content='MonteCarloES and MCwith E-soft\\npoliciesare examplesof on-\\npolicy methods.\\nMethods: Use two policies, one thatoff-\\nPolicyis learnedaboutand that becomesthe\\noptimalpolicy\\n( targetpolicy) and one that\\nis more exploratoryandis usedto generate\\nbehavior( behaviorpolicy).\\nWeare\\nlearningfrom data\"\\noff\"\\nthe\\ntargetpolicy\\nandhenceoff-\\nPolicylearning.\\n→ Off-\\nPolicymethodsare more powerfuland\\ngeneral.\\n→ Theyincludeon-\\npolicy\\nmethodsas specialcase.\\n→ Canbe appliedwhendata is generatedby a\\nconventionalnon-\\nlearning\\ncontrolleror fromhuman\\nexpert.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 13, 'page_label': '14'}, page_content='Off-\\nPolicypredictionVia ImportanceSampling:-\\n-\\ng- targettidily \\'\\nly fixed.\\nb- behaviorpolicy\"\\nCoreragéassumptio\\nIn order to use episodesfromb to\\nestimatevalues of it , we requirethat every\\naction taken underIT is also taken, at least\\noccasionally, underb.\\nie.\\nItal\\'s) >o 㱺 locals) > 0.\\nHence\\'\\nb\\' must be stochasticin stateswhere\\nit is not identicalto IT .\\n\\'\\nb\\' couldbe E-greedy.\\nCote: IT can be deterministic.\\nImÉÉig - a generaltechniquefor\\nestimatingexpectedvalues under one distribution\\ngivensamplesfromanother.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 14, 'page_label': '15'}, page_content='Importancesamplingratio:\\n-\\nGivena\\nsterlingstate St ,\\nthe prob.\\nofsubsequentaction trajectoryunderany policyit\\nis\\nPr{Atistti,\\nAtty- - .\\nSt/ St, At:p ,\\n~ \\'T]\\n= IT (AtlSt)Pfstte1St, At)Ta t t i/ Scots)\\n- . .\\nPCST1st-\\ni. At-1 )\\n= iTCA*/ Sk) PGk+,\\nIsa.tk)k=t\\nwhere\\'\\np\\'\\n-\\nstatetransitionprob.\\nfn.\\nThe relative prob.\\nof a\\ntrajectoryunder the\\ntargetand behaviorpolicies:\\nPt:* ,\\n=\\n¥,\"TAk1sk)PGktt)\\n±\\nbotklsie)plsktflsk.AE)\\n= FT\"\\nTCAkK- E b(Aiatsis)'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 15, 'page_label': '16'}, page_content=\"livewish to estimatethe expectedreturns\\nunderthe target policy. But all we\\nhaveare returns Gtdue to the behaviorpolicy.\\nE-[Geist:S ) =\\nVbcs)\\nlovecannotaveragethem to obtain ¥ .\\nE[ft:p.\\n,Ge|st=s)=4-Is)\\nLet g-(s ) besetofall timestepswhenstate's'\\nwas visited.\\nVCs) =\\n= Pt:t -1Gt importance\\ntE ←ordinary\\nsampling.\\n@Is )\\nVs) =\\nI\\nc-c-c-G)%:c ,\\nGt ✗weighted\\nimportance\\n¥ sampling.\\n1-Gds) 1-it -1 @Is )\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 16, 'page_label': '17'}, page_content='Considerestimatesof first-\\nvisit OIS and WIS\\nafterobservinga\\nsinglereturn.\\nfromstates.\\nWII: ratios in numeratoranddenominator\\ncancels. Hencethe estimateis\\nGt whoseexpectedvalue is Vb,\\nnot YT.\\n0 Estimateis alwaysfor vii.\\nINIS is biasedwhile OIS is unbiased.\\nOIS hasvery highvariance whilehes has\\nless variance.in estimation.\\nIncremerlalimplemerlationofwis:-\\nSupposewe have a sequenceof\\nreturns G, ,\\nGz. .\\nG.㱺\\nall startingin same\\nstateandeach with a correspondingrandom\\nweight Wi Cig.\\nWi-Fi: -111-1.1-1).'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 17, 'page_label': '18'}, page_content=\"live wish to formthe estimate\\nÉ '\\nWuGu✓n =\\nk=n- n 72 .\\nE'\\nWuK=i\\nand keepit up- to- dateas we obtaina\\nsingleadditionalreturn Gn.\\nIn additionto\\nkeepingtrack of Vn,\\nwe must maintain\\nthe cumulativesum G of the weightsgives\\nto first'\\nn ' returns.\\nVn+,\\n= Unt hh-ffn.vn] nz ,\\nCn\\nand Cnt, = Cn + Wnt,\\nwhere ⊥= 0 .\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210922040945Z00'00'\", 'moddate': '2025-07-29T21:56:57+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 05.pdf', 'total_pages': 19, 'page': 18, 'page_label': '19'}, page_content='oft-P-iy.FR#w:---'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 0, 'page_label': '1'}, page_content='6.Te m p o ra t f i f fe re n c e h e a m i n g\\nTD\\nLearning\\n- a combinationof MC and DP.\\n→ like Me, TD methodscan directlylearnfrom\\nexperiencewithout accessing\\ntothedynamicsof\\nthe world.\\n→ like DP, TD methodscan bootstraptheirestimates\\nwithoutwaitingfor the episodeto end.\\nTD prediction:-\\n-\\nA simpleevery- visit MonteCarlomethodsuitable\\nfor non-stationaryenv :\\n✗constant-\\na\\nMC\\n✓Cst) = ✓Get+ ✗ [ Ge- visit]\\nwhereGt is the actualreturn,\\n✗ is a constantstep-\\nsizeparameter\\nMC methodswait until the endof episode\\nto determinethe increment to ✓Cst) .'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 1, 'page_label': '2'}, page_content='TD methodswait\\nonly one timestep.\\n✓Cst) = Vest) -12 [Rtt, -1WGta) -\\nUrse)]\\nTa rg e tforMc : Gt\\nTa rg e tforTD : Ret, + ✓✓Gta).\\noBqBB@aVn-ls7-tE.T\\n[ Gt/ St:S ) -\\n①\\n= Fit[ Rtt, -1TGet, / St:S ]\\n= Er[Rtt, + ✓VñGt+, ) / St:S ] -20\\nMC methodsuse ① as targetwhileTD methods\\nuse ② as the target.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 2, 'page_label': '3'}, page_content='Note 1. Me targetis an estimatebecausethe\\nexpectedvalue of ① is not known.\\n2. DP targetis an estimatebecause4T¥, )\\nis not knownandthe currentestimate\\n✓(Sm) is used instead.\\n2. TD targetis an estimatefor both\\nreasons: It samplesthe expected\\nvalue in ② and uses the current\\nestimateof# insteadof vii.\\n0\\nBackupdiagramforno :\\n+.\\n÷TD updatesare sampleupdates.\\nTD error : St= Rt+, 1-WISH, )- ✓1St)- -\\nTD error dependson the nextslateandreward. Hence\\nit is not availableuntilone timesteplater.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 3, 'page_label': '4'}, page_content='Notethat is V doesnot changeduringthe\\nepisodeCasin Mc methods), then Mc error\\ncanbe written as sum of TD errors :\\nGt- ✓Cst) = Rt+, -18Gt, - ✓Cst)\\n=Rt+, -1-8Get, - ✓Cst) -1%(Sta)- ÑvGt+,)\\n=\\nRttl-T-st-7-V.tt?Ge+i-TVGt+i)--8tt8(Gt+,-VCSt-+i) )\\n= of-18%, -18%1-+2-\\nVotta))\\n= § . -18%+1+8%+2-1. .\\n.tsÉ¥=,\\n+8T-\\nt\\n@-\\nvan): :\\n= É 8\"-\\n€8k.k=t\\nNote: Thisidentityis not exactif ✓ is updatedduring\\nthe episodeC as in TD), butif stepsizeis small,\\nit\\nmayholdapproximately.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 4, 'page_label': '5'}, page_content='Example:\\nDriving\\nhome:\\nRewards-\\nelapsedtime on eachleg ofthe journey.\\nf-- l . No discounting.\\nreturnfromeachstate = actualtimeto go fromthat\\nstate.\\nValueofeachstate.\\n-\\nexpectedtime to go.\\nSecondcoin: Estimatedstatevalue.\\nMC TD .'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 5, 'page_label': '6'}, page_content='AdvanlagesofTDprediclionmeltodsi-TDme.tnods learn a\\nguessfroma guess.\\n①whencomparedto DPmethods, theydonot require\\nthe modelofthe environment.\\n②. Whencomparedto Me methods, theyare implemented\\nin an online,\\nfullyincrementalfashion. Usefulin\\napplicationswithverylongepisodesand\\ncontinuing\\ntusks.\\nMC hasto ignoreepisodesin whichexploratory\\nactionsare taken, whichcan greatlyslow\\nlearning.\\nTD methodslearnfromeachtransition.\\nFor anyfxedpolicy\\nit , TD hasbeen proved\\nto convergeto Vit , in the mean fora\\nconstantstep-\\nsize parameterif it is\\nsufficiently\\nsmall, and with probability1 if the step-\\nsize\\nparameterdecreasesaccordingto the usualstochastic\\napproximationconditions.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 6, 'page_label': '7'}, page_content='thandomvakexamplet\\nConsiderthe\\nfollowingMarkovReward\\nProcessCMRP) (an MDP without actions) .\\nundiscountedtask.\\nTrue valueofslatesA to E :\\nprobabilityofterminating\\non thesightstartingfromthat state.\\nA toE : %, 46,46/ \"b--\\n%.\\nTD\\ninitial\\nstatevalueof\\n0.5\\nfora\" s .\\n-\\nOplimalityoftt!\\nLet us consider\\nbatehrupdoling'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 7, 'page_label': '8'}, page_content='Settingwhereall the experience(episode) collected\\nso faris presentedrepeatedlyto the algorithm.\\nUnderbatchupdating,\\nTD convergesdeterministically\\nto a\\nsingleanswer independentof ✗ (provided\\nit is sufficientlysmall) . Constant✗- Me method\\nalso convergesdeterministically,\\nbutto a\\ndifferentsolution!\\nRondonwaikwitbkhupdaig: -\\nConstant✗ Me convergesto valuesthatare sample\\naveragesof theactualreturnsexperienced.\\nthese\\nare optimalin the sense- that theyminimizeRMSE'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 8, 'page_label': '9'}, page_content='fromthe actualreturnsin the\\ntrainingset.\\nThenHowcan TD performbetterthan Me?\\nConsiderthe\\nfollowingepisodesfroman unknownMRP.\\n1-that is\\n#VCA) , # ?\\n✓I B)=\\n3-4.\\nKhat is VCA) ? There are two reasonable\\nanswers:\\n①.\\n✓(A)=\\n%, ,\\nthere We firstmodeltheMRP.\\n② VCA) = 0 . Thisis what Me will predict-\\nNote: this answer will getzero\\ntraining\\nerror . But if the processis Markov, ans ①\\nwill have lessfutureerror !'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 9, 'page_label': '10'}, page_content='BatchMe - findthe estimatesthat minimize\\nmean-\\nsquarederror on the trainingset.\\nBatchTD -\\nFind the estimatesthat would be\\nexactlycorrect forthe maximum\\nlikelihoodmodelofthe Markovprocess.\\nTD firstmodelsthe Markovprocessbasedon the\\nobservedtransitionsand giventhismodel, attempt\\nto computethe estimate ofthe value for\\nthat wouldbe exactlycorrect if themodel\\nwere\\nexactlycorrect.\\n⊥cenhtainty\\n-\\nequivalenceestimate\\nIt is equivalentbe\\nassuming\\nthatthe estimateofthe\\nunderlyingprocesswas knownwith certaintyrather\\nthan being approximated.\\nBatchTD convergestothe certainly- equivalenceestimate.\\nThisexplainswhyit may convergequickerthan\\nMe.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 10, 'page_label': '11'}, page_content='Sarsa:On-pdiyTDGnk\\nlive want to estimate oh,- G.a)\\nlivecan extendTD for vars) to TD for 4-Csa) .\\nQlse, At) ← Qlst, Atl + ✗[Rtt-1%14-+1117+1)-\\nQlst,A\\nIf St+, is terminal, thenQGet,, Atty) = 0 .\\nThisrule uses everyelementof (St, At, Rtn, Stu,\\nAttn)\\nand tercethe name Sansa.\\nq\\nq Sarsa backup\\ndiagram.\\n¥'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 11, 'page_label': '12'}, page_content='IN\\nindygridwondexanpleii\\nwind- resultantnextstates\\nare shiftedupward\\nbasedon\\nstrength.\\n- undiscovered, episodic.\\n- sewardof-1 untilgoalis\\nreached.\\nE-greedySarsa with C--0.1, 2=0.5,\\ninital QCS,a7=o.\\nNot Mc cannotbeusedhere since terminationis\\nnever guaranteed. Onthe other hand, TD\\ncan learnwithinthe episode.\\n-\\nQ-\\n1eamig:-ffPokgTDGnt(Watkins, 1989).\\nQlst, At) ← Qlstitt) -12[Rtt, -18m¥Qlstti, a)-\\nQQ.AE?)\\ntherea directlyapproximatesq* , independentof\\nthe policybeing followed.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 12, 'page_label': '13'}, page_content='BackÉwQamiy:- 9\\n8\\n.tt#.L-Xamp-e.diffwaking:-\\n- undiscounted, episodic.\\n-\\nQ-\\nlearningteamsthe optimal\\npolicy.\\nBut thisresultsin\\noccasionallyfallingoffthe\\ncliff-\\ndueto E-greedy.\\n-\\nSansalearnsa longer\\nbutsaferpath.\\n-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 13, 'page_label': '14'}, page_content='Épea !-\\nConsideran alga. thathasthe followingupdaterule\\nQGt.AT/-QCStAt)tdfRt+i+8En-IQlSi-+i,At+D/St+i)-QCSnAr-\\n← QCSHAT) -12[Rtt, -18-2AHa/Stt, )QAttica) -\\nQcst, Atl]but that otherwisefollowsthe schemaofQ-\\nlearning.\\nGiventhe next state Stti,\\nthis algorithmmoves\\ndeterministicallyin the same directionas Sansa\\nmores ine✗pectahand hencecalledExpectedSarsa.\\n-\\nBackupdiagram:\\n-\\n÷is.\\n-\\nMore computationthan Sarsa,\\nbuteliminatesthe\\nvariancedue to the randomselectionof Ac-+ , .\\n- Performsbetterthan Sansaand Q-\\nlearning.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 14, 'page_label': '15'}, page_content='Cliffwalking\\neixample\\nHerewe used on-\\npolicyexpectedSansa.\\nBut\\none can defineoff-\\nPolicyexpectedSarsa.\\nIf it is\\ngreedyandbehaviorpolicyis\\nexploratory, then expectedSansais\\nexactly\\nQ-\\nlearning!\\n-\\nMaxim.jo/-ionbiasarddouble-eaming-\\nAll controlalgorithmsso far involve\\nmaximizationin the constructionof their target\\npolicies.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 15, 'page_label': '16'}, page_content='E Q-\\nlearning-\\ntargetpolicy\\nis greedypolicy\\ndefinedwithMax .\\nSarsen-\\nPolicy\\nis C--\\ngreedywhichhasmom.\\nNite: A maximumover estimatedvalues is\\nused\\nimplicitlyas an estimateof the\\nmaximumvalue, whichcan leadto significant\\npositivebias.\\nConsidera singlestate S , where all actions\\nhave 91s, a)=D . But the estimatesQcsa)\\nare uncertain andthus distributedabove\\nand below0 . The Max ofthe estimate\\nis positive, a positivebias/Maximizationbias.\\nE\\n•*gqBBGzI_-ffo@h←____o_'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 16, 'page_label': '17'}, page_content=\"Howto avoidmaximizationbias?-\\nConsiderthe banditcase. Onewayto view the\\nproblemis that it is due to usingthe same\\nsamplesbothto determinethe\\nmaximizing\\naction and to estimateits value.\\nSupposewe dividedthe trials in two\\nsets andusedthem to estimateQila) and\\nQala) , eachan estimateofthe true value\\ngla) .\\nUseQ, formax : A-\\n* =\\nargmaxaQila)\\nuseQa forestimate: QzCA*) =\\nQzfargmaaxQia) )\\nThis estimatewill be unbiasedin thesense\\nthat E[QzCA*) ) -\\nKA't\\n) .\\nWecan alsorepeattheprocesswith roles\\nreversed6 yield a secondunbiased\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 17, 'page_label': '18'}, page_content='estimate.\\nQ, (argmaxQzca) ) .\\nor\\nThis is the idea of doubkleorninge.\\nIn eachtrial, only updateone estimate.\\nDoublelearning\\n- doublesthememoryrequirement-\\n-\\nbutsome computationalrequirement:\\n-\\nThis ideaeasilyextendsto MDP.\\nDouble- Q-\\nlearning: -\\n-\\nIf coin fliphead,\\nQ.IS#At)--QiGt,At)-✗[Rye-180k¥,\\niargmaxQ.lstn.at)a\\n-\\nQ, 1St, At)]If coinflippedtail, updateQz.\\nNite: behaviorpolicycan Usebothaction- valueestimates.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 18, 'page_label': '19'}, page_content='-\\nns.tepbootstrap-ping: -\\nn-\\nstepits methods→ that generalizebothMe\\nand one-stepTD.\\nWithone-stepTD,\\nthe same - linestepis used\\nfor changingactionand bootstrapping.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 19, 'page_label': '20'}, page_content='n -\\nstepTD decouplesthisand havedifferent\\ntimescalesfor actionand bootstrapping.\\nn-\\nstep-Down:-\\nwhat are the spaceof methodslyingbetween\\nMC and TD methods?\\nMethodsthatuse n- stepupdatesare still TD\\nmethodsbecausethey still chargean earlier estimate\\nbasedon howit differsfrom a laterestimate-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 20, 'page_label': '21'}, page_content='Considerthe updateof the estimatedvalue\\nof slate St as a result of the state- reward\\nsequence St, Rt+, ,\\nSt+, / Rttz, - - \\' RT/ ST\\nM Gt= Rtt, -1812++2+8212++3-1. . -\\n+8T\\'t \"\\nRT\\nIn MC , Gtis the targetof the update-\\n1-step :\\nGt:* ,\\n= Rtt, -184-(9-+1)\\nAtruncatedreturn for timet with 84-61-+1)\\nreplacing84-+2+824-+3. . .tÑtÉt .\\n2-stepTD:\\nGt- :t+z =\\nREH-1%1-+2-18%-1+(5++2).\\nn-\\nslept: Gt:t+n = Rtt, -18%+2-1- -1-8^77-+1, -18^4\\n.tn#ttn)V-n,t:nziandoEtcT-n.Note:-Ifttn7T, Her all missingterms\\nare takenas Zero.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 21, 'page_label': '22'}, page_content=\"UH, (Sr) = 4-+n-\\n,\\nCst) -141Gt:t+n-Y_+n→(St)0£ 1-LT '\\nExampleiA-stepTDmethodson the Rendonwalk .\\n19statesinsteadof 5Jwith-1 outcomeon leftall\\nvaluesinitializedto o .\"),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 22, 'page_label': '23'}, page_content='n-slepsarsai-GE.tn= Rtt, -1814-+2-1- -\\n- +8\"Rttn+ÑQt+m(Stm, Attn)\\nMyOE t< T- n.\\nwith Gt:t+n = Gt if ttn 7T .\\nQ.tn/StiAtl--Qt+m(StiAt)td(Gt:t+n-Qt+n-iGtih-D\\nO EET'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 23, 'page_label': '24'}, page_content='Illustration1-stepsarsen Vs. lo-\\nstepsarsen.\\nn-\\nstepexpectedSansa:\\n-\\nGt:t+n = Rtt, -1- - - + 8%4+1,1-8\"\\nÑ ⊥m(St+n)\\nttn < T\\nwhere Ñtcs) = a-ZITG.IS/Qtlsia7-Vsc-S.\\n-'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 24, 'page_label': '25'}, page_content='n-\\nskpoff-pdiyleeming\\nTO makea simpleoff policyversion of\\nn-\\nstepTD, the updatefor time t can\\nsimplybe weightedby ft:t+n-1 :\\nY-tncstl-Y.tn,\\nCst) -12ft:t+n.\\n,[Gt:t+n-Vttn!\\n01kt\\nMinch,T-1)\\nwhere ft:h = IT To t t/Sk)k=t -\\nbothIsa)\\nIf twopoliciesare actuallysane,\\nthen 7=1.\\noff-\\nPolicyn-\\nstepSansa:\\n-\\nQttn⊥\\nt.AT/--Qt+n.+Gt,At1dft+i:ttnfGt:t+n-Qt+n.fk-,AtJ\\nO E ECT.'),\n",
       " Document(metadata={'producer': 'iOS Version 14.7.1 (Build 18G82) Quartz PDFContext', 'creator': 'PyPDF', 'creationdate': \"D:20210929045718Z00'00'\", 'moddate': '2025-08-18T16:27:49+05:30', 'source': 'PDF_DOC\\\\PDF\\\\RL Chapter 06.pdf', 'total_pages': 26, 'page': 25, 'page_label': '26'}, page_content='-'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 0, 'page_label': '1'}, page_content='ii'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 1, 'page_label': '2'}, page_content='Adaptive Computation and Machine Learning\\nFrancis Bach, series editor\\nA complete list of books published in the Adaptive Computation and Machine Learning\\nseries appears at the back of this book.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 2, 'page_label': '3'}, page_content='Reinforcement Learning:\\nAn Introduction\\nsecond edition\\nRichard S. Sutton and Andrew G. Barto\\nThe MIT Press\\nCambridge, Massachusetts\\nLondon, England'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 3, 'page_label': '4'}, page_content='© 2018, 2020 Richard S. Sutton and Andrew G. Barto\\nAll rights reserved. No part of this book may be reproduced in any form by any electronic\\nor mechanical means (including photocopying, recording, or information storage and retrieval)\\nwithout permission in writing from the copyright holder. This work is licensed under the\\nCreative Commons Attribution-NonCommercial-NoDerivs 2.0 Generic License. To view a copy\\nof this license, visithttp://creativecommons.org/licenses/by-nc-nd/2.0/ or send a letter\\nto Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\\nThis book was set in 10/12, CMR by Westchester Publishing Services. Printed and bound in\\nthe United States of America.\\nLibrary of Congress Cataloging-in-Publication Data\\nNames: Sutton, Richard S., author.| Barto, Andrew G., author.\\nTitle: Reinforcement learning : an introduction/ Richard S. Sutton and Andrew G. Barto.\\nDescription: Second edition. | Cambridge, MA : The MIT Press, [2018]| Series: Adaptive\\ncomputation and machine learning series| Includes bibliographical references and index.\\nIdentiﬁers: LCCN 2018023826| ISBN 9780262039246 (hardcover : alk. paper)\\nSubjects: LCSH: Reinforcement learning.\\nClassiﬁcation: LCC Q325.6 .R45 2018| DDC 006.3/1--dc23 LC record available\\nat https://lccn.loc.gov/2018023826\\n10 9 8 7 6 5 4 3 2 1'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 4, 'page_label': '5'}, page_content='In memory of A. Harry Klopf'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 5, 'page_label': '6'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 6, 'page_label': '7'}, page_content='Contents\\nPreface to the Second Edition xiii\\nPreface to the First Edition xvii\\nSummary of Notation xix\\n1 Introduction 1\\n1.1 Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.2 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.3 Elements of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 6\\n1.4 Limitations and Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.5 An Extended Example: Tic-Tac-Toe . . . . . . . . . . . . . . . . . . . . . 8\\n1.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n1.7 Early History of Reinforcement Learning . . . . . . . . . . . . . . . . . . .13\\nI Tabular Solution Methods 23\\n2 Multi-armed Bandits 25\\n2.1 A k-armed Bandit Problem . . . . . . . . . . . . . . . . . . . . . . . . . .25\\n2.2 Action-value Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n2.3 The 10-armed Testbed . . . . . . . . . . . . . . . . . . . . . . . . . . . . .28\\n2.4 Incremental Implementation . . . . . . . . . . . . . . . . . . . . . . . . . .30\\n2.5 Tracking a Nonstationary Problem . . . . . . . . . . . . . . . . . . . . . .32\\n2.6 Optimistic Initial Values . . . . . . . . . . . . . . . . . . . . . . . . . . . .34\\n2.7 Upper-Conﬁdence-Bound Action Selection . . . . . . . . . . . . . . . . . . 35\\n2.8 Gradient Bandit Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .37\\n2.9 Associative Search (Contextual Bandits) . . . . . . . . . . . . . . . . . . .41\\n2.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .42'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 7, 'page_label': '8'}, page_content='viii Contents\\n3 Finite Markov Decision Processes 47\\n3.1 The Agent–Environment Interface . . . . . . . . . . . . . . . . . . . . . . 47\\n3.2 Goals and Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .53\\n3.3 Returns and Episodes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\\n3.4 Uniﬁed Notation for Episodic and Continuing Tasks . . . . . . . . . . . .57\\n3.5 Policies and Value Functions . . . . . . . . . . . . . . . . . . . . . . . . .58\\n3.6 Optimal Policies and Optimal Value Functions . . . . . . . . . . . . . . .62\\n3.7 Optimality and Approximation . . . . . . . . . . . . . . . . . . . . . . . . 67\\n3.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n4 Dynamic Programming 73\\n4.1 Policy Evaluation (Prediction) . . . . . . . . . . . . . . . . . . . . . . . . 74\\n4.2 Policy Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\\n4.3 Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .80\\n4.4 Value Iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .82\\n4.5 Asynchronous Dynamic Programming . . . . . . . . . . . . . . . . . . . . 85\\n4.6 Generalized Policy Iteration . . . . . . . . . . . . . . . . . . . . . . . . . .86\\n4.7 E\\x00ciency of Dynamic Programming . . . . . . . . . . . . . . . . . . . . . 87\\n4.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\\n5 Monte Carlo Methods 91\\n5.1 Monte Carlo Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n5.2 Monte Carlo Estimation of Action Values . . . . . . . . . . . . . . . . . .96\\n5.3 Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .97\\n5.4 Monte Carlo Control without Exploring Starts . . . . . . . . . . . . . . .100\\n5.5 O↵-policy Prediction via Importance Sampling . . . . . . . . . . . . . . .103\\n5.6 Incremental Implementation . . . . . . . . . . . . . . . . . . . . . . . . . .109\\n5.7 O↵-policy Monte Carlo Control . . . . . . . . . . . . . . . . . . . . . . . .110\\n5.8 *Discounting-aware Importance Sampling . . . . . . . . . . . . . . . . . .112\\n5.9 *Per-decision Importance Sampling . . . . . . . . . . . . . . . . . . . . . .114\\n5.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .115\\n6 Temporal-Di↵erence Learning 119\\n6.1 TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .119\\n6.2 Advantages of TD Prediction Methods . . . . . . . . . . . . . . . . . . . .124\\n6.3 Optimality of TD(0) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .126\\n6.4 Sarsa: On-policy TD Control . . . . . . . . . . . . . . . . . . . . . . . . .129\\n6.5 Q-learning: O↵-policy TD Control . . . . . . . . . . . . . . . . . . . . . .131\\n6.6 Expected Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .133\\n6.7 Maximization Bias and Double Learning . . . . . . . . . . . . . . . . . . .134\\n6.8 Games, Afterstates, and Other Special Cases . . . . . . . . . . . . . . . .136\\n6.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .138'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 8, 'page_label': '9'}, page_content='Contents ix\\n7 n-step Bootstrapping 141\\n7.1 n-step TD Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .142\\n7.2 n-step Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .145\\n7.3 n-step O↵-policy Learning . . . . . . . . . . . . . . . . . . . . . . . . . . .148\\n7.4 *Per-decision Methods with Control Variates . . . . . . . . . . . . . . . .150\\n7.5 O↵-policy Learning Without Importance Sampling:\\nThe n-step Tree Backup Algorithm . . . . . . . . . . . . . . . . . . . . . .152\\n7.6 *A Unifying Algorithm: n-step Q(\\x00)..................... 154\\n7.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .157\\n8 Planning and Learning with Tabular Methods 159\\n8.1 Models and Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .159\\n8.2 Dyna: Integrated Planning, Acting, and Learning . . . . . . . . . . . . . .161\\n8.3 When the Model Is Wrong . . . . . . . . . . . . . . . . . . . . . . . . . . .166\\n8.4 Prioritized Sweeping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .168\\n8.5 Expected vs. Sample Updates . . . . . . . . . . . . . . . . . . . . . . . . .172\\n8.6 Trajectory Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .174\\n8.7 Real-time Dynamic Programming . . . . . . . . . . . . . . . . . . . . . . .177\\n8.8 Planning at Decision Time . . . . . . . . . . . . . . . . . . . . . . . . . . .180\\n8.9 Heuristic Search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .181\\n8.10 Rollout Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .183\\n8.11 Monte Carlo Tree Search . . . . . . . . . . . . . . . . . . . . . . . . . . . .185\\n8.12 Summary of the Chapter . . . . . . . . . . . . . . . . . . . . . . . . . . . .188\\n8.13 Summary of Part I: Dimensions . . . . . . . . . . . . . . . . . . . . . . . .189\\nII Approximate Solution Methods 195\\n9 On-policy Prediction with Approximation 197\\n9.1 Value-function Approximation . . . . . . . . . . . . . . . . . . . . . . . . .198\\n9.2 The Prediction Objective (VE) . . . . . . . . . . . . . . . . . . . . . . . .199\\n9.3 Stochastic-gradient and Semi-gradient Methods . . . . . . . . . . . . . . .200\\n9.4 Linear Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .204\\n9.5 Feature Construction for Linear Methods . . . . . . . . . . . . . . . . . .210\\n9.5.1 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .210\\n9.5.2 Fourier Basis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .211\\n9.5.3 Coarse Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . .215\\n9.5.4 Tile Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .217\\n9.5.5 Radial Basis Functions . . . . . . . . . . . . . . . . . . . . . . . . .221\\n9.6 Selecting Step-Size Parameters Manually . . . . . . . . . . . . . . . . . . .222\\n9.7 Nonlinear Function Approximation: Artiﬁcial Neural Networks . . . . . .223\\n9.8 Least-Squares TD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .228'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 9, 'page_label': '10'}, page_content='x Contents\\n9.9 Memory-based Function Approximation . . . . . . . . . . . . . . . . . . .230\\n9.10 Kernel-based Function Approximation . . . . . . . . . . . . . . . . . . . .232\\n9.11 Looking Deeper at On-policy Learning: Interest and Emphasis . . . . . .234\\n9.12 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .236\\n10 On-policy Control with Approximation 243\\n10.1 Episodic Semi-gradient Control . . . . . . . . . . . . . . . . . . . . . . . .243\\n10.2 Semi-gradient n-step Sarsa . . . . . . . . . . . . . . . . . . . . . . . . . .247\\n10.3 Average Reward: A New Problem Setting for Continuing Tasks . . . . . .249\\n10.4 Deprecating the Discounted Setting . . . . . . . . . . . . . . . . . . . . . .253\\n10.5 Di↵erential Semi-gradientn-step Sarsa . . . . . . . . . . . . . . . . . . . .255\\n10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .256\\n11 *O↵-policy Methods with Approximation 257\\n11.1 Semi-gradient Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .258\\n11.2 Examples of O↵-policy Divergence . . . . . . . . . . . . . . . . . . . . . .260\\n11.3 The Deadly Triad . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .264\\n11.4 Linear Value-function Geometry . . . . . . . . . . . . . . . . . . . . . . .266\\n11.5 Gradient Descent in the Bellman Error . . . . . . . . . . . . . . . . . . . .269\\n11.6 The Bellman Error is Not Learnable . . . . . . . . . . . . . . . . . . . . .274\\n11.7 Gradient-TD Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .278\\n11.8 Emphatic-TD Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .281\\n11.9 Reducing Variance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .283\\n11.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .284\\n12 Eligibility Traces 287\\n12.1 The \\x00-return . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .288\\n12.2 TD(\\x00)...................................... 292\\n12.3 n-step Truncated\\x00-return Methods . . . . . . . . . . . . . . . . . . . . .295\\n12.4 Redoing Updates: Online\\x00-return Algorithm . . . . . . . . . . . . . . . .297\\n12.5 True Online TD(\\x00)............................... 299\\n12.6 *Dutch Traces in Monte Carlo Learning . . . . . . . . . . . . . . . . . . .301\\n12.7 Sarsa(\\x00)..................................... 303\\n12.8 Variable \\x00 and \\x00 ................................ 307\\n12.9 O↵-policy Traces with Control Variates . . . . . . . . . . . . . . . . . . .309\\n12.10 Watkins’s Q(\\x00) to Tree-Backup(\\x00)...................... 312\\n12.11 Stable O↵-policy Methods with Traces . . . . . . . . . . . . . . . . . . .314\\n12.12 Implementation Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . .316\\n12.13 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .317'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 10, 'page_label': '11'}, page_content='Contents xi\\n13 Policy Gradient Methods 321\\n13.1 Policy Approximation and its Advantages . . . . . . . . . . . . . . . . . .322\\n13.2 The Policy Gradient Theorem . . . . . . . . . . . . . . . . . . . . . . . . .324\\n13.3 REINFORCE: Monte Carlo Policy Gradient . . . . . . . . . . . . . . . . .326\\n13.4 REINFORCE with Baseline . . . . . . . . . . . . . . . . . . . . . . . . . .329\\n13.5 Actor–Critic Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .331\\n13.6 Policy Gradient for Continuing Problems . . . . . . . . . . . . . . . . . .333\\n13.7 Policy Parameterization for Continuous Actions . . . . . . . . . . . . . . .335\\n13.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .337\\nIII Looking Deeper 339\\n14 Psychology 341\\n14.1 Prediction and Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . .342\\n14.2 Classical Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .343\\n14.2.1 Blocking and Higher-order Conditioning . . . . . . . . . . . . . . .345\\n14.2.2 The Rescorla–Wagner Model . . . . . . . . . . . . . . . . . . . . .346\\n14.2.3 The TD Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . .349\\n14.2.4 TD Model Simulations . . . . . . . . . . . . . . . . . . . . . . . . .350\\n14.3 Instrumental Conditioning . . . . . . . . . . . . . . . . . . . . . . . . . . .357\\n14.4 Delayed Reinforcement . . . . . . . . . . . . . . . . . . . . . . . . . . . . .361\\n14.5 Cognitive Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .363\\n14.6 Habitual and Goal-directed Behavior . . . . . . . . . . . . . . . . . . . . .364\\n14.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .368\\n15 Neuroscience 377\\n15.1 Neuroscience Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .378\\n15.2 Reward Signals, Reinforcement Signals, Values, and Prediction Errors . .380\\n15.3 The Reward Prediction Error Hypothesis . . . . . . . . . . . . . . . . . .381\\n15.4 Dopamine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .383\\n15.5 Experimental Support for the Reward Prediction Error Hypothesis . . . .387\\n15.6 TD Error/Dopamine Correspondence . . . . . . . . . . . . . . . . . . . . .390\\n15.7 Neural Actor–Critic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .395\\n15.8 Actor and Critic Learning Rules . . . . . . . . . . . . . . . . . . . . . . .398\\n15.9 Hedonistic Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .402\\n15.10 Collective Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . .404\\n15.11 Model-based Methods in the Brain . . . . . . . . . . . . . . . . . . . . . .407\\n15.12 Addiction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .409\\n15.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .410'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 11, 'page_label': '12'}, page_content='xii Contents\\n16 Applications and Case Studies 421\\n16.1 TD-Gammon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .421\\n16.2 Samuel’s Checkers Player . . . . . . . . . . . . . . . . . . . . . . . . . . .426\\n16.3 Watson’s Daily-Double Wagering . . . . . . . . . . . . . . . . . . . . . . .429\\n16.4 Optimizing Memory Control . . . . . . . . . . . . . . . . . . . . . . . . . .432\\n16.5 Human-level Video Game Play . . . . . . . . . . . . . . . . . . . . . . . .436\\n16.6 Mastering the Game of Go . . . . . . . . . . . . . . . . . . . . . . . . . . .441\\n16.6.1 AlphaGo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .444\\n16.6.2 AlphaGo Zero . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .447\\n16.7 Personalized Web Services . . . . . . . . . . . . . . . . . . . . . . . . . . .450\\n16.8 Thermal Soaring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .453\\n17 Frontiers 459\\n17.1 General Value Functions and Auxiliary Tasks . . . . . . . . . . . . . . . .459\\n17.2 Temporal Abstraction via Options . . . . . . . . . . . . . . . . . . . . . .461\\n17.3 Observations and State . . . . . . . . . . . . . . . . . . . . . . . . . . . .464\\n17.4 Designing Reward Signals . . . . . . . . . . . . . . . . . . . . . . . . . . .469\\n17.5 Remaining Issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .472\\n17.6 Reinforcement Learning and the Future of Artiﬁcial Intelligence . . . . . .475\\nReferences 481\\nIndex 519'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 12, 'page_label': '13'}, page_content='Preface to the Second Edition\\nThe twenty years since the publication of the ﬁrst edition of this book have seen tremendous\\nprogress in artiﬁcial intelligence, propelled in large part by advances in machine learning,\\nincluding advances in reinforcement learning. Although the impressive computational\\npower that became available is responsible for some of these advances, new developments\\nin theory and algorithms have been driving forces as well. In the face of this progress, a\\nsecond edition of our 1998 book was long overdue, and we ﬁnally began the project in\\n2012. Our goal for the second edition was the same as our goal for the ﬁrst: to provide a\\nclear and simple account of the key ideas and algorithms of reinforcement learning that\\nis accessible to readers in all the related disciplines. The edition remains an introduction,\\nand we retain a focus on core, online learning algorithms. This edition includes some new\\ntopics that rose to importance over the intervening years, and we expanded coverage of\\ntopics that we now understand better. But we made no attempt to provide comprehensive\\ncoverage of the ﬁeld, which has exploded in many di↵erent directions. We apologize for\\nhaving to leave out all but a handful of these contributions.\\nAs in the ﬁrst edition, we chose not to produce a rigorous formal treatment of\\nreinforcement learning, or to formulate it in the most general terms. However, our deeper\\nunderstanding of some topics since the ﬁrst edition required a bit more mathematics\\nto explain; we have set o↵ the more mathematical parts in shaded boxes that the non-\\nmathematically-inclined may choose to skip. We also use a slightly di↵erent notation\\nthan was used in the ﬁrst edition. In teaching, we have found that the new notation\\nhelps to address some common points of confusion. It emphasizes the di↵erence between\\nrandom variables, denoted with capital letters, and their instantiations, denoted in lower\\ncase. For example, the state, action, and reward at time stept are denoted St, At,\\nand Rt, while their possible values might be denoteds, a, andr. Along with this, it is\\nnatural to use lower case for value functions (e.g.,v⇡) and restrict capitals to their tabular\\nestimates (e.g., Qt(s, a)). Approximate value functions are deterministic functions of\\nrandom parameters and are thus also in lower case (e.g.,ˆv(s,wt) ⇡ v⇡(s)). Vectors, such\\nas the weight vectorwt (formerly ✓t) and the feature vectorxt (formerly \\x00t), are bold\\nand written in lowercase even if they are random variables. Uppercase bold is reserved for\\nmatrices. In the ﬁrst edition we used special notations,Pa\\nss0 and Ra\\nss0 , for the transition\\nprobabilities and expected rewards. One weakness of that notation is that it still did not\\nfully characterize the dynamics of the rewards, giving only their expectations, which is\\nsu\\x00cient for dynamic programming but not for reinforcement learning. Another weakness'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 13, 'page_label': '14'}, page_content='xiv Preface to the Second Edition\\nis the excess of subscripts and superscripts. In this edition we use the explicit notation of\\np(s0,r |s, a) for the joint probability for the next state and reward given the current state\\nand action. All the changes in notation are summarized in a table on page xix.\\nThe second edition is signiﬁcantly expanded, and its top-level organization has been\\nchanged. After the introductory ﬁrst chapter, the second edition is divided into three new\\nparts. The ﬁrst part (Chapters 2–8) treats as much of reinforcement learning as possible\\nwithout going beyond the tabular case for which exact solutions can be found. We cover\\nboth learning and planning methods for the tabular case, as well as their uniﬁcation\\nin n-step methods and in Dyna. Many algorithms presented in this part are new to\\nthe second edition, including UCB, Expected Sarsa, Double learning, tree-backup,Q(\\x00),\\nRTDP, and MCTS. Doing the tabular case ﬁrst, and thoroughly, enables core ideas to be\\ndeveloped in the simplest possible setting. The second part of the book (Chapters 9–13)\\nis then devoted to extending the ideas to function approximation. It has new sections on\\nartiﬁcial neural networks, the fourier basis, LSTD, kernel-based methods, Gradient-TD\\nand Emphatic-TD methods, average-reward methods, true online TD(\\x00), and policy-\\ngradient methods. The second edition signiﬁcantly expands the treatment of o↵-policy\\nlearning, ﬁrst for the tabular case in Chapters 5–7, then with function approximation in\\nChapters 11 and 12. Another change is that the second edition separates the forward-view\\nidea ofn-step bootstrapping (now treated more fully in Chapter 7) from the backward-\\nview idea of eligibility traces (now treated independently in Chapter 12). The third part\\nof the book has large new chapters on reinforcement learning’s relationships to psychology\\n(Chapter 14) and neuroscience (Chapter 15), as well as an updated case-studies chapter\\nincluding Atari game playing, Watson’s wagering strategy, and the Go playing programs\\nAlphaGo and AlphaGo Zero (Chapter 16). Still, out of necessity we have included only a\\nsmall subset of all that has been done in the ﬁeld. Our choices reﬂect our long-standing\\ninterests in inexpensive model-free methods that should scale well to large applications.\\nThe ﬁnal chapter now includes a discussion of the future societal impacts of reinforcement\\nlearning. For better or worse, the second edition is about twice as large as the ﬁrst.\\nThis book is designed to be used as the primary text for a one- or two-semester\\ncourse on reinforcement learning. For a one-semester course, the ﬁrst ten chapters should\\nbe covered in order and form a good core, to which can be added material from the\\nother chapters, from other books such as Bertsekas and Tsitsiklis (1996), Wiering and\\nvan Otterlo (2012), and Szepesv´ ari (2010), or from the literature, according to taste.\\nDepending of the students’ background, some additional material on online supervised\\nlearning may be helpful. The ideas of options and option models are a natural addition\\n(Sutton, Precup and Singh, 1999). A two-semester course can cover all the chapters as\\nwell as supplementary material. The book can also be used as part of broader courses\\non machine learning, artiﬁcial intelligence, or neural networks. In this case, it may be\\ndesirable to cover only a subset of the material. We recommend covering Chapter 1 for a\\nbrief overview, Chapter 2 through Section 2.4, Chapter 3, and then selecting sections\\nfrom the remaining chapters according to time and interests. Chapter 6 is the most\\nimportant for the subject and for the rest of the book. A course focusing on machine\\nlearning or neural networks should cover Chapters 9 and 10, and a course focusing on\\nartiﬁcial intelligence or planning should cover Chapter 8. Throughout the book, sections\\nand chapters that are more di\\x00cult and not essential to the rest of the book are marked'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 14, 'page_label': '15'}, page_content='Preface to the Second Edition xv\\nwith a⇤. These can be omitted on ﬁrst reading without creating problems later on. Some\\nexercises are also marked with a⇤ to indicate that they are more advanced and not\\nessential to understanding the basic material of the chapter.\\nMost chapters end with a section entitled “Bibliographical and Historical Remarks,”\\nwherein we credit the sources of the ideas presented in that chapter, provide pointers to\\nfurther reading and ongoing research, and describe relevant historical background. Despite\\nour attempts to make these sections authoritative and complete, we have undoubtedly left\\nout some important prior work. For that we again apologize, and we welcome corrections\\nand extensions for incorporation into the electronic version of the book.\\nLike the ﬁrst edition, this edition of the book is dedicated to the memory of A. Harry\\nKlopf. It was Harry who introduced us to each other, and it was his ideas about the brain\\nand artiﬁcial intelligence that launched our long excursion into reinforcement learning.\\nTrained in neurophysiology and long interested in machine intelligence, Harry was a\\nsenior scientist a\\x00liated with the Avionics Directorate of the Air Force O\\x00ce of Scientiﬁc\\nResearch (AFOSR) at Wright-Patterson Air Force Base, Ohio. He was dissatisﬁed with\\nthe great importance attributed to equilibrium-seeking processes, including homeostasis\\nand error-correcting pattern classiﬁcation methods, in explaining natural intelligence\\nand in providing a basis for machine intelligence. He noted that systems that try to\\nmaximize something (whatever that might be) are qualitatively di↵erent from equilibrium-\\nseeking systems, and he argued that maximizing systems hold the key to understanding\\nimportant aspects of natural intelligence and for building artiﬁcial intelligences. Harry was\\ninstrumental in obtaining funding from AFOSR for a project to assess the scientiﬁc merit\\nof these and related ideas. This project was conducted in the late 1970s at the University\\nof Massachusetts Amherst (UMass Amherst), initially under the direction of Michael\\nArbib, William Kilmer, and Nico Spinelli, professors in the Department of Computer\\nand Information Science at UMass Amherst, and founding members of the Cybernetics\\nCenter for Systems Neuroscience at the University, a farsighted group focusing on the\\nintersection of neuroscience and artiﬁcial intelligence. Barto, a recent PhD from the\\nUniversity of Michigan, was hired as post doctoral researcher on the project. Meanwhile,\\nSutton, an undergraduate studying computer science and psychology at Stanford, had\\nbeen corresponding with Harry regarding their mutual interest in the role of stimulus\\ntiming in classical conditioning. Harry suggested to the UMass group that Sutton would\\nbe a great addition to the project. Thus, Sutton became a UMass graduate student,\\nwhose PhD was directed by Barto, who had become an Associate Professor. The study\\nof reinforcement learning as presented in this book is rightfully an outcome of that\\nproject instigated by Harry and inspired by his ideas. Further, Harry was responsible\\nfor bringing us, the authors, together in what has been a long and enjoyable interaction.\\nBy dedicating this book to Harry we honor his essential contributions, not only to the\\nﬁeld of reinforcement learning, but also to our collaboration. We also thank Professors\\nArbib, Kilmer, and Spinelli for the opportunity they provided to us to begin exploring\\nthese ideas. Finally, we thank AFOSR for generous support over the early years of our\\nresearch, and the NSF for its generous support over many of the following years.\\nWe have very many people to thank for their inspiration and help with this second\\nedition. Everyone we acknowledged for their inspiration and help with the ﬁrst edition'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 15, 'page_label': '16'}, page_content='xvi Preface to the Second Edition\\ndeserve our deepest gratitude for this edition as well, which would not exist were it not\\nfor their contributions to edition number one. To that long list we must add many others\\nwho contributed speciﬁcally to the second edition. Our students over the many years that\\nwe have taught this material contributed in countless ways: exposing errors, o↵ering ﬁxes,\\nand—not the least—being confused in places where we could have explained things better.\\nWe especially thank Martha Steenstrup for reading and providing detailed comments\\nthroughout. The chapters on psychology and neuroscience could not have been written\\nwithout the help of many experts in those ﬁelds. We thank John Moore for his patient\\ntutoring over many many years on animal learning experiments, theory, and neuroscience,\\nand for his careful reading of multiple drafts of Chapters 14 and 15. We also thank Matt\\nBotvinick, Nathaniel Daw, Peter Dayan, and Yael Niv for their penetrating comments on\\ndrafts of these chapters, their essential guidance through the massive literature, and their\\ninterception of many of our errors in early drafts. Of course, the remaining errors in these\\nchapters—and there must still be some—are totally our own. We thank Phil Thomas for\\nhelping us make these chapters accessible to non-psychologists and non-neuroscientists,\\nand we thank Peter Sterling for helping us improve the exposition. We are grateful to Jim\\nHouk for introducing us to the subject of information processing in the basal ganglia and\\nfor alerting us to other relevant aspects of neuroscience. Jos´ e Mart´ ınez, Terry Sejnowski,\\nDavid Silver, Gerry Tesauro, Georgios Theocharous, and Phil Thomas generously helped\\nus understand details of their reinforcement learning applications for inclusion in the\\ncase-studies chapter, and they provided helpful comments on drafts of these sections.\\nSpecial thanks are owed to David Silver for helping us better understand Monte Carlo\\nTree Search and the DeepMind Go-playing programs. We thank George Konidaris for his\\nhelp with the section on the Fourier basis. Emilio Cartoni, Thomas Cederborg, Stefan\\nDernbach, Clemens Rosenbaum, Patrick Taylor, Thomas Colin, and Pierre-Luc Bacon\\nhelped us in a number important ways for which we are most grateful.\\nSutton would also like to thank the members of the Reinforcement Learning and\\nArtiﬁcial Intelligence laboratory at the University of Alberta for contributions to the\\nsecond edition. He owes a particular debt to Rupam Mahmood for essential contributions\\nto the treatment of o↵-policy Monte Carlo methods in Chapter 5, to Hamid Maei for\\nhelping develop the perspective on o↵-policy learning presented in Chapter 11, to Eric\\nGraves for conducting the experiments in Chapter 13, to Shangtong Zhang for replicating\\nand thus verifying almost all the experimental results, to Kris De Asis for improving\\nthe new technical content of Chapters 7 and 12, and to Harm van Seijen for insights\\nthat led to the separation ofn-step methods from eligibility traces and (along with Hado\\nvan Hasselt) for the ideas involving exact equivalence of forward and backward views of\\neligibility traces presented in Chapter 12. Sutton also gratefully acknowledges the support\\nand freedom he was granted by the Government of Alberta and the National Science and\\nEngineering Research Council of Canada throughout the period during which the second\\nedition was conceived and written. In particular, he would like to thank Randy Goebel\\nfor creating a supportive and far-sighted environment for research in Alberta. He would\\nalso like to thank DeepMind their support in the last six months of writing the book.\\nFinally, we owe thanks to the many careful readers of drafts of the second edition that\\nwe posted on the internet. They found many errors that we had missed and alerted us to\\npotential points of confusion.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 16, 'page_label': '17'}, page_content='Preface to the First Edition\\nWe ﬁrst came to focus on what is now known as reinforcement learning in late 1979. We\\nwere both at the University of Massachusetts, working on one of the earliest projects to\\nrevive the idea that networks of neuronlike adaptive elements might prove to be a promising\\napproach to artiﬁcial adaptive intelligence. The project explored the “heterostatic theory\\nof adaptive systems” developed by A. Harry Klopf. Harry’s work was a rich source of\\nideas, and we were permitted to explore them critically and compare them with the long\\nhistory of prior work in adaptive systems. Our task became one of teasing the ideas apart\\nand understanding their relationships and relative importance. This continues today,\\nbut in 1979 we came to realize that perhaps the simplest of the ideas, which had long\\nbeen taken for granted, had received surprisingly little attention from a computational\\nperspective. This was simply the idea of a learning system thatwants something, that\\nadapts its behavior in order to maximize a special signal from its environment. This\\nwas the idea of a “hedonistic” learning system, or, as we would say now, the idea of\\nreinforcement learning.\\nLike others, we had a sense that reinforcement learning had been thoroughly explored\\nin the early days of cybernetics and artiﬁcial intelligence. On closer inspection, though,\\nwe found that it had been explored only slightly. While reinforcement learning had clearly\\nmotivated some of the earliest computational studies of learning, most of these researchers\\nhad gone on to other things, such as pattern classiﬁcation, supervised learning, and\\nadaptive control, or they had abandoned the study of learning altogether. As a result, the\\nspecial issues involved in learning how to get something from the environment received\\nrelatively little attention. In retrospect, focusing on this idea was the critical step that\\nset this branch of research in motion. Little progress could be made in the computational\\nstudy of reinforcement learning until it was recognized that such a fundamental idea had\\nnot yet been thoroughly explored.\\nThe ﬁeld has come a long way since then, evolving and maturing in several directions.\\nReinforcement learning has gradually become one of the most active research areas in ma-\\nchine learning, artiﬁcial intelligence, and neural network research. The ﬁeld has developed\\nstrong mathematical foundations and impressive applications. The computational study\\nof reinforcement learning is now a large ﬁeld, with hundreds of active researchers around\\nthe world in diverse disciplines such as psychology, control theory, artiﬁcial intelligence,\\nand neuroscience. Particularly important have been the contributions establishing and\\ndeveloping the relationships to the theory of optimal control and dynamic programming.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 17, 'page_label': '18'}, page_content='xviii Preface to the First Edition\\nThe overall problem of learning from interaction to achieve goals is still far from being\\nsolved, but our understanding of it has improved signiﬁcantly. We can now place compo-\\nnent ideas, such as temporal-di↵erence learning, dynamic programming, and function\\napproximation, within a coherent perspective with respect to the overall problem.\\nOur goal in writing this book was to provide a clear and simple account of the key\\nideas and algorithms of reinforcement learning. We wanted our treatment to be accessible\\nto readers in all of the related disciplines, but we could not cover all of these perspectives\\nin detail. For the most part, our treatment takes the point of view of artiﬁcial intelligence\\nand engineering. Coverage of connections to other ﬁelds we leave to others or to another\\ntime. We also chose not to produce a rigorous formal treatment of reinforcement learning.\\nWe did not reach for the highest possible level of mathematical abstraction and did not\\nrely on a theorem–proof format. We tried to choose a level of mathematical detail that\\npoints the mathematically inclined in the right directions without distracting from the\\nsimplicity and potential generality of the underlying ideas.\\nIn some sense we have been working toward this book for thirty years, and we have lots\\nof people to thank. First, we thank those who have personally helped us develop the overall\\nview presented in this book: Harry Klopf, for helping us recognize that reinforcement\\nlearning needed to be revived; Chris Watkins, Dimitri Bertsekas, John Tsitsiklis, and\\nPaul Werbos, for helping us see the value of the relationships to dynamic programming;\\nJohn Moore and Jim Kehoe, for insights and inspirations from animal learning theory;\\nOliver Selfridge, for emphasizing the breadth and importance of adaptation; and, more\\ngenerally, our colleagues and students who have contributed in countless ways: Ron\\nWilliams, Charles Anderson, Satinder Singh, Sridhar Mahadevan, Steve Bradtke, Bob\\nCrites, Peter Dayan, and Leemon Baird. Our view of reinforcement learning has been\\nsigniﬁcantly enriched by discussions with Paul Cohen, Paul Utgo↵, Martha Steenstrup,\\nGerry Tesauro, Mike Jordan, Leslie Kaelbling, Andrew Moore, Chris Atkeson, Tom\\nMitchell, Nils Nilsson, Stuart Russell, Tom Dietterich, Tom Dean, and Bob Narendra.\\nWe thank Michael Littman, Gerry Tesauro, Bob Crites, Satinder Singh, and Wei Zhang\\nfor providing speciﬁcs of Sections 4.7, 15.1, 15.4, 15.5, and 15.6 respectively. We thank\\nthe Air Force O\\x00ce of Scientiﬁc Research, the National Science Foundation, and GTE\\nLaboratories for their long and farsighted support.\\nWe also wish to thank the many people who have read drafts of this book and\\nprovided valuable comments, including Tom Kalt, John Tsitsiklis, Pawel Cichosz, Olle\\nG¨ allmo, Chuck Anderson, Stuart Russell, Ben Van Roy, Paul Steenstrup, Paul Cohen,\\nSridhar Mahadevan, Jette Randlov, Brian Sheppard, Thomas O’Connell, Richard Coggins,\\nCristina Versino, John H. Hiett, Andreas Badelt, Jay Ponte, Joe Beck, Justus Piater,\\nMartha Steenstrup, Satinder Singh, Tommi Jaakkola, Dimitri Bertsekas, Torbj¨ orn Ekman,\\nChristina Bj¨ orkman, Jakob Carlstr¨ om, and Olle Palmgren. Finally, we thank Gwyn\\nMitchell for helping in many ways, and Harry Stanton and Bob Prior for being our\\nchampions at MIT Press.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 18, 'page_label': '19'}, page_content='Summary of Notation\\nCapital letters are used for random variables, whereas lower case letters are used for\\nthe values of random variables and for scalar functions. Quantities that are required to\\nbe real-valued vectors are written in bold and in lower case (even if random variables).\\nMatrices are bold capitals.\\n.= equality relationship that is true by deﬁnition\\n⇡ approximately equal\\n/ proportional to\\nPr{X =x} probability that a random variableX takes on the valuex\\nX ⇠ p random variableX selected from distributionp(x) .=P r{X =x}\\nE[X] expectation of a random variable X,i . e . ,E[X] .= P\\nx p(x)x\\nargmaxa f(a) a value ofa at whichf(a) takes its maximal value\\nln x natural logarithm ofx\\nex,e x p (x) the base of the natural logarithm, e ⇡ 2.71828, carried to powerx; eln x = x\\nR set of real numbers\\nf : X ! Y function f from elements of setX to elements of setY\\n assignment\\n(a, b] the real interval between a and b including b but not includinga\\n\" probability of taking a random action in an\"-greedy policy\\n↵, \\x00 step-size parameters\\n\\x00 discount-rate parameter\\n\\x00 decay-rate parameter for eligibility traces\\npredicate indicator function (\\n predicate\\n.=1i ft h epredicate is true, else 0)\\nIn a multi-arm bandit problem:\\nk number of actions (arms)\\nt discrete time step or play number\\nq⇤(a) true value (expected reward) of action a\\nQt(a) estimate at time t of q⇤(a)\\nNt(a) number of times action a has been selected up prior to timet\\nHt(a) learned preference for selecting action a at timet\\n⇡t(a) probability of selecting action a at timet\\n¯Rt estimate at timet of the expected reward given⇡t'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 19, 'page_label': '20'}, page_content='xx Summary of Notation\\nIn a Markov Decision Process:\\ns, s0 states\\na an action\\nr a reward\\nS set of all nonterminal states\\nS+ set of all states, including the terminal state\\nA(s) set of all actions available in state s\\nR set of all possible rewards, a ﬁnite subset ofR\\n⇢ subset of (e.g.,R ⇢ R)\\n2 is an element of; e.g. (s 2 S, r 2 R)\\n|S| number of elements in setS\\nt discrete time step\\nT,T (t) ﬁnal time step of an episode, or of the episode including time step t\\nAt action at timet\\nSt state at timet, typically due, stochastically, toSt\\x001 and At\\x001\\nRt reward at timet, typically due, stochastically, toSt\\x001 and At\\x001\\n⇡ policy (decision-making rule)\\n⇡(s) action taken in state s under deterministic policy ⇡\\n⇡(a|s) probability of taking action a in states under stochastic policy ⇡\\nGt return following timet\\nh horizon, the time step one looks up to in a forward view\\nGt:t+n,G t:h n-step return fromt +1t o t + n, or toh (discounted and corrected)\\n¯Gt:h ﬂat return (undiscounted and uncorrected) fromt +1t o h (Section 5.8)\\nG\\x00\\nt \\x00-return (Section 12.1)\\nG\\x00\\nt:h truncated, corrected\\x00-return (Section 12.3)\\nG\\x00s\\nt ,G \\x00a\\nt \\x00-return, corrected by estimated state, or action, values (Section 12.8)\\np(s0,r |s, a) probability of transition to state s0 with rewardr, from states and actiona\\np(s0|s, a) probability of transition to state s0, from states taking actiona\\nr(s, a) expected immediate reward from state s after actiona\\nr(s, a, s0) expected immediate reward on transition from s to s0 under actiona\\nv⇡(s) value of state s under policy ⇡ (expected return)\\nv⇤(s) value of state s under the optimal policy\\nq⇡(s, a) value of taking action a in states under policy ⇡\\nq⇤(s, a) value of taking action a in states under the optimal policy\\nV,V t array estimates of state-value functionv⇡ or v⇤\\nQ, Qt array estimates of action-value functionq⇡ or q⇤\\n¯Vt(s) expected approximate action value; for example, ¯Vt(s) .= P\\na ⇡(a|s)Qt(s, a)\\nUt target for estimate at timet'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 20, 'page_label': '21'}, page_content='Summary of Notation xxi\\n\\x00t temporal-di↵erence (TD) error att (a random variable) (Section 6.1)\\n\\x00s\\nt ,\\x00 a\\nt state- and action-speciﬁc forms of the TD error (Section 12.9)\\nn in n-step methods, n is the number of steps of bootstrapping\\nd dimensionality—the number of components ofw\\nd0 alternate dimensionality—the number of components of✓\\nw, wt d-vector of weights underlying an approximate value function\\nwi,w t,i ith component of learnable weight vector\\nˆv(s,w) approximate value of state s given weight vectorw\\nvw(s) alternate notation for ˆ v(s,w)\\nˆq(s, a,w) approximate value of state–action pair s, agiven weight vectorw\\nrˆv(s,w) column vector of partial derivatives of ˆ v(s,w)w i t hr e s p e c tt ow\\nrˆq(s, a,w) column vector of partial derivatives of ˆq(s, a,w)w i t hr e s p e c tt ow\\nx(s) vector of features visible when in state s\\nx(s, a) vector of features visible when in state s taking actiona\\nxi(s),x i(s, a) ith component of vectorx(s) orx(s, a)\\nxt shorthand forx(St) orx(St,A t)\\nw>x inner product of vectors,w>x .= P\\ni wixi; for example, ˆv(s,w) .= w>x(s)\\nv, vt secondary d-vector of weights, used to learnw (Chapter 11)\\nzt d-vector of eligibility traces at timet (Chapter 12)\\n✓, ✓t parameter vector of target policy (Chapter 13)\\n⇡(a|s, ✓) probability of taking action a in states given parameter vector✓\\n⇡✓ policy corresponding to parameter✓\\nr⇡(a|s, ✓) column vector of partial derivatives of ⇡(a|s, ✓)w i t hr e s p e c tt o✓\\nJ(✓) performance measure for the policy ⇡✓\\nrJ(✓) column vector of partial derivatives of J(✓)w i t hr e s p e c tt o✓\\nh(s, a,✓) preference for selecting action a in states based on✓\\nb(a|s) behavior policy used to select actions while learning about target policy ⇡\\nb(s) a baseline function b : S 7! R for policy-gradient methods\\nb branching factor for an MDP or search tree\\n⇢t:h importance sampling ratio for timet through timeh (Section 5.5)\\n⇢t importance sampling ratio for timet alone, ⇢t\\n.= ⇢t:t\\nr(⇡) average reward (reward rate) for policy ⇡ (Section 10.3)\\n¯Rt estimate ofr(⇡) at timet\\nµ(s) on-policy distribution over states (Section 9.2)\\nµ |S|-vector of theµ(s) for alls 2 S\\nkvk2\\nµ µ-weighted squared norm of value functionv,i . e . ,kvk2\\nµ\\n.= P\\ns2S µ(s)v(s)2\\n⌘(s) expected number of visits to state s per episode (page 199)\\n⇧ projection operator for value functions (page 268)\\nB⇡ Bellman operator for value functions (Section 11.4)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 21, 'page_label': '22'}, page_content='xxii Summary of Notation\\nA d ⇥ d matrix A .= E\\nh\\nxt\\n\\x00\\nxt \\x00 \\x00xt+1\\n\\x00>i\\nb d-dimensional vectorb .= E[Rt+1xt]\\nwTD TD ﬁxed pointwTD\\n.= A\\x001b (a d-vector, Section 9.4)\\nI identity matrix\\nP |S|⇥| S| matrix of state-transition probabilities under⇡\\nD |S|⇥| S| diagonal matrix withµ on its diagonal\\nX |S|⇥ d matrix with thex(s) as its rows\\n¯\\x00w(s) Bellman error (expected TD error) for vw at states (Section 11.4)\\n¯\\x00w, BE Bellman error vector, with components ¯\\x00w(s)\\nVE(w) mean square value error VE(w) .= kvw \\x00 v⇡k2\\nµ (Section 9.2)\\nBE(w) mean square Bellman error BE(w) .=\\n\\x00\\x00¯\\x00w\\n\\x00\\x002\\nµ\\nPBE(w) mean square projected Bellman error PBE(w) .=\\n\\x00\\x00⇧¯\\x00w\\n\\x00\\x002\\nµ\\nTDE(w) mean square temporal-di↵erence error TDE(w) .= Eb\\n⇥\\n⇢t\\x002\\nt\\n⇤\\n(Section 11.5)\\nRE(w) mean square return error (Section 11.6)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 22, 'page_label': '23'}, page_content='Chapter 1\\nIntroduction\\nThe idea that we learn by interacting with our environment is probably the ﬁrst to occur\\nto us when we think about the nature of learning. When an infant plays, waves its arms,\\nor looks about, it has no explicit teacher, but it does have a direct sensorimotor connection\\nto its environment. Exercising this connection produces a wealth of information about\\ncause and e↵ect, about the consequences of actions, and about what to do in order to\\nachieve goals. Throughout our lives, such interactions are undoubtedly a major source\\nof knowledge about our environment and ourselves. Whether we are learning to drive\\na car or to hold a conversation, we are acutely aware of how our environment responds\\nto what we do, and we seek to inﬂuence what happens through our behavior. Learning\\nfrom interaction is a foundational idea underlying nearly all theories of learning and\\nintelligence.\\nIn this book we explore acomputational approach to learning from interaction. Rather\\nthan directly theorizing about how people or animals learn, we primarily explore idealized\\nlearning situations and evaluate the e↵ectiveness of various learning methods. That\\nis, we adopt the perspective of an artiﬁcial intelligence researcher or engineer. We\\nexplore designs for machines that are e↵ective in solving learning problems of scientiﬁc or\\neconomic interest, evaluating the designs through mathematical analysis or computational\\nexperiments. The approach we explore, calledreinforcement learning, is much more\\nfocused on goal-directed learning from interaction than are other approaches to machine\\nlearning.\\n1.1 Reinforcement Learning\\nReinforcement learning is learning what to do—how to map situations to actions—so\\nas to maximize a numerical reward signal. The learner is not told which actions to\\ntake, but instead must discover which actions yield the most reward by trying them. In\\nthe most interesting and challenging cases, actions may a↵ect not only the immediate\\nreward but also the next situation and, through that, all subsequent rewards. These two\\ncharacteristics—trial-and-error search and delayed reward—are the two most important\\ndistinguishing features of reinforcement learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 23, 'page_label': '24'}, page_content='2C h a p t e r 1 : I n t r o d u c t i o n\\nReinforcement learning, like many topics whose names end with “ing,” such as machine\\nlearning and mountaineering, is simultaneously a problem, a class of solution methods\\nthat work well on the problem, and the ﬁeld that studies this problem and its solution\\nmethods. It is convenient to use a single name for all three things, but at the same time\\nessential to keep the three conceptually separate. In particular, the distinction between\\nproblems and solution methods is very important in reinforcement learning; failing to\\nmake this distinction is the source of many confusions.\\nWe formalize the problem of reinforcement learning using ideas from dynamical sys-\\ntems theory, speciﬁcally, as the optimal control of incompletely-known Markov decision\\nprocesses. The details of this formalization must wait until Chapter 3, but the basic idea\\nis simply to capture the most important aspects of the real problem facing a learning\\nagent interacting over time with its environment to achieve a goal. A learning agent\\nmust be able to sense the state of its environment to some extent and must be able to\\ntake actions that a↵ect the state. The agent also must have a goal or goals relating to\\nthe state of the environment. Markov decision processes are intended to include just\\nthese three aspects—sensation, action, and goal—in their simplest possible forms without\\ntrivializing any of them. Any method that is well suited to solving such problems we\\nconsider to be a reinforcement learning method.\\nReinforcement learning is di↵erent fromsupervised learning, the kind of learning studied\\nin most current research in the ﬁeld of machine learning. Supervised learning is learning\\nfrom a training set of labeled examples provided by a knowledgable external supervisor.\\nEach example is a description of a situation together with a speciﬁcation—the label—of\\nthe correct action the system should take in that situation, which is often to identify a\\ncategory to which the situation belongs. The object of this kind of learning is for the\\nsystem to extrapolate, or generalize, its responses so that it acts correctly in situations\\nnot present in the training set. This is an important kind of learning, but alone it is not\\nadequate for learning from interaction. In interactive problems it is often impractical to\\nobtain examples of desired behavior that are both correct and representative of all the\\nsituations in which the agent has to act. In uncharted territory—where one would expect\\nlearning to be most beneﬁcial—an agent must be able to learn from its own experience.\\nReinforcement learning is also di↵erent from what machine learning researchers call\\nunsupervised learning, which is typically about ﬁnding structure hidden in collections of\\nunlabeled data. The terms supervised learning and unsupervised learning would seem\\nto exhaustively classify machine learning paradigms, but they do not. Although one\\nmight be tempted to think of reinforcement learning as a kind of unsupervised learning\\nbecause it does not rely on examples of correct behavior, reinforcement learning is trying\\nto maximize a reward signal instead of trying to ﬁnd hidden structure. Uncovering\\nstructure in an agent’s experience can certainly be useful in reinforcement learning, but by\\nitself does not address the reinforcement learning problem of maximizing a reward signal.\\nWe therefore consider reinforcement learning to be a third machine learning paradigm,\\nalongside supervised learning and unsupervised learning and perhaps other paradigms.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 24, 'page_label': '25'}, page_content='1.1. Reinforcement Learning 3\\nOne of the challenges that arise in reinforcement learning, and not in other kinds\\nof learning, is the trade-o↵ between exploration and exploitation. To obtain a lot of\\nreward, a reinforcement learning agent must prefer actions that it has tried in the past\\nand found to be e↵ective in producing reward. But to discover such actions, it has to\\ntry actions that it has not selected before. The agent has toexploit what it has already\\nexperienced in order to obtain reward, but it also has toexplore in order to make better\\naction selections in the future. The dilemma is that neither exploration nor exploitation\\ncan be pursued exclusively without failing at the task. The agent must try a variety of\\nactions and progressively favor those that appear to be best. On a stochastic task, each\\naction must be tried many times to gain a reliable estimate of its expected reward. The\\nexploration–exploitation dilemma has been intensively studied by mathematicians for\\nmany decades, yet remains unresolved. For now, we simply note that the entire issue of\\nbalancing exploration and exploitation does not even arise in supervised and unsupervised\\nlearning, at least in the purest forms of these paradigms.\\nAnother key feature of reinforcement learning is that it explicitly considers thewhole\\nproblem of a goal-directed agent interacting with an uncertain environment. This is in\\ncontrast to many approaches that consider subproblems without addressing how they\\nmight ﬁt into a larger picture. For example, we have mentioned that many machine\\nlearning researchers have studied supervised learning without specifying how such an\\nability would ultimately be useful. Other researchers have developed theories of planning\\nwith general goals, but without considering planning’s role in real-time decision making,\\nor the question of where the predictive models necessary for planning would come from.\\nAlthough these approaches have yielded many useful results, their focus on isolated\\nsubproblems is a signiﬁcant limitation.\\nReinforcement learning takes the opposite tack, starting with a complete, interactive,\\ngoal-seeking agent. All reinforcement learning agents have explicit goals, can sense\\naspects of their environments, and can choose actions to inﬂuence their environments.\\nMoreover, it is usually assumed from the beginning that the agent has to operate despite\\nsigniﬁcant uncertainty about the environment it faces. When reinforcement learning\\ninvolves planning, it has to address the interplay between planning and real-time action\\nselection, as well as the question of how environment models are acquired and improved.\\nWhen reinforcement learning involves supervised learning, it does so for speciﬁc reasons\\nthat determine which capabilities are critical and which are not. For learning research to\\nmake progress, important subproblems have to be isolated and studied, but they should\\nbe subproblems that play clear roles in complete, interactive, goal-seeking agents, even if\\nall the details of the complete agent cannot yet be ﬁlled in.\\nBy a complete, interactive, goal-seeking agent we do not always mean something like\\na complete organism or robot. These are clearly examples, but a complete, interactive,\\ngoal-seeking agent can also be a component of a larger behaving system. In this case, the\\nagent directly interacts with the rest of the larger system and indirectly interacts with\\nthe larger system’s environment. A simple example is an agent that monitors the charge\\nlevel of robot’s battery and sends commands to the robot’s control architecture. This\\nagent’s environment is the rest of the robot together with the robot’s environment. It is'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 25, 'page_label': '26'}, page_content='4C h a p t e r 1 : I n t r o d u c t i o n\\nimportant to look beyond the most obvious examples of agents and their environments\\nto appreciate the generality of the reinforcement learning framework.\\nOne of the most exciting aspects of modern reinforcement learning is its substantive\\nand fruitful interactions with other engineering and scientiﬁc disciplines. Reinforcement\\nlearning is part of a decades-long trend within artiﬁcial intelligence and machine learning\\ntoward greater integration with statistics, optimization, and other mathematical subjects.\\nFor example, the ability of some reinforcement learning methods to learn with parameter-\\nized approximators addresses the classical “curse of dimensionality” in operations research\\nand control theory. More distinctively, reinforcement learning has also interacted strongly\\nwith psychology and neuroscience, with substantial beneﬁts going both ways. Of all the\\nforms of machine learning, reinforcement learning is the closest to the kind of learning\\nthat humans and other animals do, and many of the core algorithms of reinforcement\\nlearning were originally inspired by biological learning systems. Reinforcement learning\\nhas also given back, both through a psychological model of animal learning that better\\nmatches some of the empirical data, and through an inﬂuential model of parts of the\\nbrain’s reward system. The body of this book develops the ideas of reinforcement learning\\nthat pertain to engineering and artiﬁcial intelligence, with connections to psychology and\\nneuroscience summarized in Chapters 14 and 15.\\nFinally, reinforcement learning is also part of a larger trend in artiﬁcial intelligence\\nback toward simple general principles. Since the late 1960s, many artiﬁcial intelligence re-\\nsearchers presumed that there are no general principles to be discovered, that intelligence is\\ninstead due to the possession of a vast number of special purpose tricks, procedures, and\\nheuristics. It was sometimes said that if we could just get enough relevant facts into a\\nmachine, say one million, or one billion, then it would become intelligent. Methods based\\non general principles, such as search or learning, were characterized as “weak methods,”\\nwhereas those based on speciﬁc knowledge were called “strong methods.” This view is\\nuncommon today. From our point of view, it was premature: too little e↵ort had been\\nput into the search for general principles to conclude that there were none. Modern\\nartiﬁcial intelligence now includes much research looking for general principles of learning,\\nsearch, and decision making. It is not clear how far back the pendulum will swing, but\\nreinforcement learning research is certainly part of the swing back toward simpler and\\nfewer general principles of artiﬁcial intelligence.\\n1.2 Examples\\nA good way to understand reinforcement learning is to consider some of the examples\\nand possible applications that have guided its development.\\n• A master chess player makes a move. The choice is informed both by planning—\\nanticipating possible replies and counterreplies—and by immediate, intuitive judg-\\nments of the desirability of particular positions and moves.\\n• An adaptive controller adjusts parameters of a petroleum reﬁnery’s operation in\\nreal time. The controller optimizes the yield/cost/quality trade-o↵ on the basis'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 26, 'page_label': '27'}, page_content='1.2. Examples 5\\nof speciﬁed marginal costs without sticking strictly to the set points originally\\nsuggested by engineers.\\n• A gazelle calf struggles to its feet minutes after being born. Half an hour later it is\\nrunning at 20 miles per hour.\\n• A mobile robot decides whether it should enter a new room in search of more trash\\nto collect or start trying to ﬁnd its way back to its battery recharging station. It\\nmakes its decision based on the current charge level of its battery and how quickly\\nand easily it has been able to ﬁnd the recharger in the past.\\n• Phil prepares his breakfast. Closely examined, even this apparently mundane\\nactivity reveals a complex web of conditional behavior and interlocking goal–subgoal\\nrelationships: walking to the cupboard, opening it, selecting a cereal box, then\\nreaching for, grasping, and retrieving the box. Other complex, tuned, interactive\\nsequences of behavior are required to obtain a bowl, spoon, and milk carton. Each\\nstep involves a series of eye movements to obtain information and to guide reaching\\nand locomotion. Rapid judgments are continually made about how to carry the\\nobjects or whether it is better to ferry some of them to the dining table before\\nobtaining others. Each step is guided by goals, such as grasping a spoon or getting\\nto the refrigerator, and is in service of other goals, such as having the spoon to eat\\nwith once the cereal is prepared and ultimately obtaining nourishment. Whether\\nhe is aware of it or not, Phil is accessing information about the state of his body\\nthat determines his nutritional needs, level of hunger, and food preferences.\\nThese examples share features that are so basic that they are easy to overlook. All\\ninvolve interaction between an active decision-making agent and its environment, within\\nwhich the agent seeks to achieve agoal despite uncertainty about its environment. The\\nagent’s actions are permitted to a↵ect the future state of the environment (e.g., the\\nnext chess position, the level of reservoirs of the reﬁnery, the robot’s next location and\\nthe future charge level of its battery), thereby a↵ecting the actions and opportunities\\navailable to the agent at later times. Correct choice requires taking into account indirect,\\ndelayed consequences of actions, and thus may require foresight or planning.\\nAt the same time, in all of these examples the e↵ects of actions cannot be fully predicted;\\nthus the agent must monitor its environment frequently and react appropriately. For\\nexample, Phil must watch the milk he pours into his cereal bowl to keep it from overﬂowing.\\nAll these examples involve goals that are explicit in the sense that the agent can judge\\nprogress toward its goal based on what it can sense directly. The chess player knows\\nwhether or not he wins, the reﬁnery controller knows how much petroleum is being\\nproduced, the gazelle calf knows when it falls, the mobile robot knows when its batteries\\nrun down, and Phil knows whether or not he is enjoying his breakfast.\\nIn all of these examples the agent can use its experience to improve its performance\\nover time. The chess player reﬁnes the intuition he uses to evaluate positions, thereby\\nimproving his play; the gazelle calf improves the e\\x00ciency with which it can run; Phil\\nlearns to streamline making his breakfast. The knowledge the agent brings to the task at\\nthe start—either from previous experience with related tasks or built into it by design or'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 27, 'page_label': '28'}, page_content='6C h a p t e r 1 : I n t r o d u c t i o n\\nevolution—inﬂuences what is useful or easy to learn, but interaction with the environment\\nis essential for adjusting behavior to exploit speciﬁc features of the task.\\n1.3 Elements of Reinforcement Learning\\nBeyond the agent and the environment, one can identify four main subelements of a\\nreinforcement learning system: apolicy,a reward signal,a value function, and, optionally,\\na model of the environment.\\nA policy deﬁnes the learning agent’s way of behaving at a given time. Roughly speaking,\\na policy is a mapping from perceived states of the environment to actions to be taken\\nwhen in those states. It corresponds to what in psychology would be called a set of\\nstimulus–response rules or associations. In some cases the policy may be a simple function\\nor lookup table, whereas in others it may involve extensive computation such as a search\\nprocess. The policy is the core of a reinforcement learning agent in the sense that it alone\\nis su\\x00cient to determine behavior. In general, policies may be stochastic, specifying\\nprobabilities for each action.\\nA reward signal deﬁnes the goal of a reinforcement learning problem. On each time\\nstep, the environment sends to the reinforcement learning agent a single number called\\nthe reward. The agent’s sole objective is to maximize the total reward it receives over\\nthe long run. The reward signal thus deﬁnes what are the good and bad events for the\\nagent. In a biological system, we might think of rewards as analogous to the experiences\\nof pleasure or pain. They are the immediate and deﬁning features of the problem faced\\nby the agent. The reward signal is the primary basis for altering the policy; if an action\\nselected by the policy is followed by low reward, then the policy may be changed to\\nselect some other action in that situation in the future. In general, reward signals may\\nbe stochastic functions of the state of the environment and the actions taken.\\nWhereas the reward signal indicates what is good in an immediate sense, avalue\\nfunction speciﬁes what is good in the long run. Roughly speaking, thevalue of a state is\\nthe total amount of reward an agent can expect to accumulate over the future, starting\\nfrom that state. Whereas rewards determine the immediate, intrinsic desirability of\\nenvironmental states, values indicate thelong-term desirability of states after taking into\\naccount the states that are likely to follow and the rewards available in those states. For\\nexample, a state might always yield a low immediate reward but still have a high value\\nbecause it is regularly followed by other states that yield high rewards. Or the reverse\\ncould be true. To make a human analogy, rewards are somewhat like pleasure (if high)\\nand pain (if low), whereas values correspond to a more reﬁned and farsighted judgment\\nof how pleased or displeased we are that our environment is in a particular state.\\nRewards are in a sense primary, whereas values, as predictions of rewards, are secondary.\\nWithout rewards there could be no values, and the only purpose of estimating values is to\\nachieve more reward. Nevertheless, it is values with which we are most concerned when\\nmaking and evaluating decisions. Action choices are made based on value judgments. We\\nseek actions that bring about states of highest value, not highest reward, because these\\nactions obtain the greatest amount of reward for us over the long run. Unfortunately, it\\nis much harder to determine values than it is to determine rewards. Rewards are basically\\ngiven directly by the environment, but values must be estimated and re-estimated from'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 28, 'page_label': '29'}, page_content='1.4. Limitations and Scope 7\\nthe sequences of observations an agent makes over its entire lifetime. In fact, the most\\nimportant component of almost all reinforcement learning algorithms we consider is a\\nmethod for e\\x00ciently estimating values. The central role of value estimation is arguably\\nthe most important thing that has been learned about reinforcement learning over the\\nlast six decades.\\nThe fourth and ﬁnal element of some reinforcement learning systems is amodel of\\nthe environment. This is something that mimics the behavior of the environment, or\\nmore generally, that allows inferences to be made about how the environment will behave.\\nFor example, given a state and action, the model might predict the resultant next state\\nand next reward. Models are used forplanning, by which we mean any way of deciding\\non a course of action by considering possible future situations before they are actually\\nexperienced. Methods for solving reinforcement learning problems that use models and\\nplanning are calledmodel-based methods, as opposed to simplermodel-free methods that\\nare explicitly trial-and-error learners—viewed as almost theopposite of planning. In\\nChapter 8 we explore reinforcement learning systems that simultaneously learn by trial\\nand error, learn a model of the environment, and use the model for planning. Modern\\nreinforcement learning spans the spectrum from low-level, trial-and-error learning to\\nhigh-level, deliberative planning.\\n1.4 Limitations and Scope\\nReinforcement learning relies heavily on the concept of state—as input to the policy and\\nvalue function, and as both input to and output from the model. Informally, we can\\nthink of the state as a signal conveying to the agent some sense of “how the environment\\nis” at a particular time. The formal deﬁnition of state as we use it here is given by\\nthe framework of Markov decision processes presented in Chapter 3. More generally,\\nhowever, we encourage the reader to follow the informal meaning and think of the state\\nas whatever information is available to the agent about its environment. In e↵ect, we\\nassume that the state signal is produced by some preprocessing system that is nominally\\npart of the agent’s environment. We do not address the issues of constructing, changing,\\nor learning the state signal in this book (other than brieﬂy in Section 17.3). We take this\\napproach not because we consider state representation to be unimportant, but in order\\nto focus fully on the decision-making issues. In other words, our concern in this book is\\nnot with designing the state signal, but with deciding what action to take as a function\\nof whatever state signal is available.\\nMost of the reinforcement learning methods we consider in this book are structured\\naround estimating value functions, but it is not strictly necessary to do this to solve\\nreinforcement learning problems. For example, solution methods such as genetic algo-\\nrithms, genetic programming, simulated annealing, and other optimization methods never\\nestimate value functions. These methods apply multiple static policies each interacting\\nover an extended period of time with a separate instance of the environment. The policies\\nthat obtain the most reward, and random variations of them, are carried over to the\\nnext generation of policies, and the process repeats. We call theseevolutionary methods\\nbecause their operation is analogous to the way biological evolution produces organisms'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 29, 'page_label': '30'}, page_content='8C h a p t e r 1 : I n t r o d u c t i o n\\nwith skilled behavior even if they do not learn during their individual lifetimes. If the\\nspace of policies is su\\x00ciently small, or can be structured so that good policies are\\ncommon or easy to ﬁnd—or if a lot of time is available for the search—then evolutionary\\nmethods can be e↵ective. In addition, evolutionary methods have advantages on problems\\nin which the learning agent cannot sense the complete state of its environment.\\nOur focus is on reinforcement learning methods that learn while interacting with the\\nenvironment, which evolutionary methods do not do. Methods able to take advantage\\nof the details of individual behavioral interactions can be much more e\\x00cient than\\nevolutionary methods in many cases. Evolutionary methods ignore much of the useful\\nstructure of the reinforcement learning problem: they do not use the fact that the policy\\nthey are searching for is a function from states to actions; they do not notice which states\\nan individual passes through during its lifetime, or which actions it selects. In some cases\\nsuch information can be misleading (e.g., when states are misperceived), but more often it\\nshould enable more e\\x00cient search. Although evolution and learning share many features\\nand naturally work together, we do not consider evolutionary methods by themselves to\\nbe especially well suited to reinforcement learning problems and, accordingly, we do not\\ncover them in this book.\\n1.5 An Extended Example: Tic-Tac-Toe\\nTo illustrate the general idea of reinforcement learning and contrast it with other ap-\\nproaches, we next consider a single example in more detail.\\nX\\nX\\nX\\nO O\\nXO\\nConsider the familiar child’s game of tic-tac-toe. Two players\\ntake turns playing on a three-by-three board. One player plays\\nXs and the other Os until one player wins by placing three marks\\nin a row, horizontally, vertically, or diagonally, as the X player\\nhas in the game shown to the right. If the board ﬁlls up with\\nneither player getting three in a row, then the game is a draw.\\nBecause a skilled player can play so as never to lose, let us assume\\nthat we are playing against an imperfect player, one whose play\\nis sometimes incorrect and allows us to win. For the moment, in\\nfact, let us consider draws and losses to be equally bad for us. How might we construct a\\nplayer that will ﬁnd the imperfections in its opponent’s play and learn to maximize its\\nchances of winning?\\nAlthough this is a simple problem, it cannot readily be solved in a satisfactory way\\nthrough classical techniques. For example, the classical “minimax” solution from game\\ntheory is not correct here because it assumes a particular way of playing by the opponent.\\nFor example, a minimax player would never reach a game state from which it could\\nlose, even if in fact it always won from that state because of incorrect play by the\\nopponent. Classical optimization methods for sequential decision problems, such as\\ndynamic programming, cancompute an optimal solution for any opponent, but require\\nas input a complete speciﬁcation of that opponent, including the probabilities with which\\nthe opponent makes each move in each board state. Let us assume that this information\\nis not available a priori for this problem, as it is not for the vast majority of problems of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 30, 'page_label': '31'}, page_content='1.5. An Extended Example: Tic-Tac-Toe 9\\npractical interest. On the other hand, such information can be estimated from experience,\\nin this case by playing many games against the opponent. About the best one can do\\non this problem is ﬁrst to learn a model of the opponent’s behavior, up to some level of\\nconﬁdence, and then apply dynamic programming to compute an optimal solution given\\nthe approximate opponent model. In the end, this is not that di↵erent from some of the\\nreinforcement learning methods we examine later in this book.\\nAn evolutionary method applied to this problem would directly search the space\\nof possible policies for one with a high probability of winning against the opponent.\\nHere, a policy is a rule that tells the player what move to make for every state of the\\ngame—every possible conﬁguration of Xs and Os on the three-by-three board. For each\\npolicy considered, an estimate of its winning probability would be obtained by playing\\nsome number of games against the opponent. This evaluation would then direct which\\npolicy or policies were considered next. A typical evolutionary method would hill-climb\\nin policy space, successively generating and evaluating policies in an attempt to obtain\\nincremental improvements. Or, perhaps, a genetic-style algorithm could be used that\\nwould maintain and evaluate a population of policies. Literally hundreds of di↵erent\\noptimization methods could be applied.\\nHere is how the tic-tac-toe problem would be approached with a method making use\\nof a value function. First we would set up a table of numbers, one for each possible state\\nof the game. Each number will be the latest estimate of the probability of our winning\\nfrom that state. We treat this estimate as the state’svalue, and the whole table is the\\nlearned value function. StateA has higher value than stateB, or is considered “better”\\nthan stateB, if the current estimate of the probability of our winning fromA is higher\\nthan it is fromB. Assuming we always play Xs, then for all states with three Xs in a row\\nthe probability of winning is 1, because we have already won. Similarly, for all states\\nwith three Os in a row, or that are ﬁlled up, the correct probability is 0, as we cannot\\nwin from them. We set the initial values of all the other states to 0.5, representing a\\nguess that we have a 50% chance of winning.\\nWe then play many games against the opponent. To select our moves we examine the\\nstates that would result from each of our possible moves (one for each blank space on the\\nboard) and look up their current values in the table. Most of the time we movegreedily,\\nselecting the move that leads to the state with greatest value, that is, with the highest\\nestimated probability of winning. Occasionally, however, we select randomly from among\\nthe other moves instead. These are calledexploratory moves because they cause us to\\nexperience states that we might otherwise never see. A sequence of moves made and\\nconsidered during a game can be diagrammed as in Figure 1.1.\\nWhile we are playing, we change the values of the states in which we ﬁnd ourselves\\nduring the game. We attempt to make them more accurate estimates of the probabilities\\nof winning. To do this, we “back up” the value of the state after each greedy move to\\nthe state before the move, as suggested by the arrows in Figure 1.1. More precisely, the\\ncurrent value of the earlier state is updated to be closer to the value of the later state.\\nThis can be done by moving the earlier state’s value a fraction of the way toward the\\nvalue of the later state. If we letSt denote the state before the greedy move, andSt+1\\nthe state after that move, then the update to the estimated value ofSt, denotedV (St),'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 31, 'page_label': '32'}, page_content=\"10 Chapter 1: Introduction\\n..\\n•\\nour move{\\nopponent's move{\\nour move{\\nstarting position\\n•\\n•\\n•\\na\\nb\\nc*\\nd\\nee*\\nopponent's move{\\nc\\n•f\\n•g*g\\nopponent's move{\\nour move{\\n.\\n•\\na\\ng*\\nc\\nstarting position\\nb\\nc*\\nd\\ne* e\\nf\\ng…\\nFigure 1.1: A sequence of tic-tac-toe moves. The solid black lines represent the moves taken\\nduring a game; the dashed lines represent moves that we (our reinforcement learning player)\\nconsidered but did not make. The * indicates the move currently estimated to be the best. Our\\nsecond move was an exploratory move, meaning that it was taken even though another sibling\\nmove, the one leading toe⇤, was ranked higher. Exploratory moves do not result in any learning,\\nbut each of our other moves does, causing updates as suggested by the red arrows in which\\nestimated values are moved up the tree from later nodes to earlier nodes as detailed in the text.\\ncan be written as\\nV (St)  V (St)+ ↵\\nh\\nV (St+1) \\x00 V (St)\\ni\\n,\\nwhere ↵ is a small positive fraction called thestep-size parameter,w h i c hi n ﬂ u e n c e s\\nthe rate of learning. This update rule is an example of atemporal-di↵erence learning\\nmethod, so called because its changes are based on a di↵erence,V (St+1)\\x00V (St), between\\nestimates at two successive times.\\nThe method described above performs quite well on this task. For example, if the\\nstep-size parameter is reduced properly over time, then this method converges, for any\\nﬁxed opponent, to the true probabilities of winning from each state given optimal play\\nby our player. Furthermore, the moves then taken (except on exploratory moves) are in\\nfact the optimal moves against this (imperfect) opponent. In other words, the method\\nconverges to an optimal policy for playing the game against this opponent. If the step-size\\nparameter is not reduced all the way to zero over time, then this player also plays well\\nagainst opponents that slowly change their way of playing.\"),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 32, 'page_label': '33'}, page_content='1.5. An Extended Example: Tic-Tac-Toe 11\\nThis example illustrates the di↵erences between evolutionary methods and methods\\nthat learn value functions. To evaluate a policy, an evolutionary method holds the policy\\nﬁxed and plays many games against the opponent or simulates many games using a model\\nof the opponent. The frequency of wins gives an unbiased estimate of the probability\\nof winning with that policy, and can be used to direct the next policy selection. But\\neach policy change is made only after many games, and only the ﬁnal outcome of each\\ngame is used: what happensduring the games is ignored. For example, if the player wins,\\nthen all of its behavior in the game is given credit, independently of how speciﬁc moves\\nmight have been critical to the win. Credit is even given to moves that never occurred!\\nValue function methods, in contrast, allow individual states to be evaluated. In the end,\\nevolutionary and value function methods both search the space of policies, but learning a\\nvalue function takes advantage of information available during the course of play.\\nThis simple example illustrates some of the key features of reinforcement learning\\nmethods. First, there is the emphasis on learning while interacting with an environment,\\nin this case with an opponent player. Second, there is a clear goal, and correct behavior\\nrequires planning or foresight that takes into account delayed e↵ects of one’s choices. For\\nexample, the simple reinforcement learning player would learn to set up multi-move traps\\nfor a shortsighted opponent. It is a striking feature of the reinforcement learning solution\\nthat it can achieve the e↵ects of planning and lookahead without using a model of the\\nopponent and without conducting an explicit search over possible sequences of future\\nstates and actions.\\nWhile this example illustrates some of the key features of reinforcement learning, it is\\nso simple that it might give the impression that reinforcement learning is more limited\\nthan it really is. Although tic-tac-toe is a two-person game, reinforcement learning\\nalso applies in the case in which there is no external adversary, that is, in the case of\\na “game against nature.” Reinforcement learning also is not restricted to problems in\\nwhich behavior breaks down into separate episodes, like the separate games of tic-tac-toe,\\nwith reward only at the end of each episode. It is just as applicable when behavior\\ncontinues indeﬁnitely and when rewards of various magnitudes can be received at any\\ntime. Reinforcement learning is also applicable to problems that do not even break down\\ninto discrete time steps like the plays of tic-tac-toe. The general principles apply to\\ncontinuous-time problems as well, although the theory gets more complicated and we\\nomit it from this introductory treatment.\\nTic-tac-toe has a relatively small, ﬁnite state set, whereas reinforcement learning can\\nbe used when the state set is very large, or even inﬁnite. For example, Gerry Tesauro\\n(1992, 1995) combined the algorithm described above with an artiﬁcial neural network to\\nlearn to play backgammon, which has approximately 1020 states. With this many states\\nit is impossible ever to experience more than a small fraction of them. Tesauro’s program\\nlearned to play far better than any previous program and eventually better than the\\nworld’s best human players (see Section 16.1). The artiﬁcial neural network provides the\\nprogram with the ability to generalize from its experience, so that in new states it selects\\nmoves based on information saved from similar states faced in the past, as determined\\nby the network. How well a reinforcement learning system can work in problems with\\nsuch large state sets is intimately tied to how appropriately it can generalize from past'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 33, 'page_label': '34'}, page_content='12 Chapter 1: Introduction\\nexperience. It is in this role that we have the greatest need for supervised learning\\nmethods within reinforcement learning. Artiﬁcial neural networks and deep learning\\n(Section 9.7) are not the only, or necessarily the best, way to do this.\\nIn this tic-tac-toe example, learning started with no prior knowledge beyond the\\nrules of the game, but reinforcement learning by no means entails a tabula rasa view of\\nlearning and intelligence. On the contrary, prior information can be incorporated into\\nreinforcement learning in a variety of ways that can be critical for e\\x00cient learning (e.g.,\\nsee Sections 9.5, 17.4, and 13.1). We also have access to the true state in the tic-tac-toe\\nexample, whereas reinforcement learning can also be applied when part of the state is\\nhidden, or when di↵erent states appear to the learner to be the same.\\nFinally, the tic-tac-toe player was able to look ahead and know the states that would\\nresult from each of its possible moves. To do this, it had to have a model of the game\\nthat allowed it to foresee how its environment would change in response to moves that it\\nmight never make. Many problems are like this, but in others even a short-term model\\nof the e↵ects of actions is lacking. Reinforcement learning can be applied in either case.\\nA model is not required, but models can easily be used if they are available or can be\\nlearned (Chapter 8).\\nOn the other hand, there are reinforcement learning methods that do not need any\\nkind of environment model at all. Model-free systems cannot even think about how\\ntheir environments will change in response to a single action. The tic-tac-toe player is\\nmodel-free in this sense with respect to its opponent: it has no model of its opponent\\nof any kind. Because models have to be reasonably accurate to be useful, model-free\\nmethods can have advantages over more complex methods when the real bottleneck in\\nsolving a problem is the di\\x00culty of constructing a su\\x00ciently accurate environment\\nmodel. Model-free methods are also important building blocks for model-based methods.\\nIn this book we devote several chapters to model-free methods before we discuss how\\nthey can be used as components of more complex model-based methods.\\nReinforcement learning can be used at both high and low levels in a system. Although\\nthe tic-tac-toe player learned only about the basic moves of the game, nothing prevents\\nreinforcement learning from working at higher levels where each of the “actions” may\\nitself be the application of a possibly elaborate problem-solving method. In hierarchical\\nlearning systems, reinforcement learning can work simultaneously on several levels.\\nExercise 1.1: Self-Play Suppose, instead of playing against a random opponent, the\\nreinforcement learning algorithm described above played against itself, with both sides\\nlearning. What do you think would happen in this case? Would it learn a di↵erent policy\\nfor selecting moves? ⇤\\nExercise 1.2: Symmetries Many tic-tac-toe positions appear di↵erent but are really\\nthe same because of symmetries. How might we amend the learning process described\\nabove to take advantage of this? In what ways would this change improve the learning\\nprocess? Now think again. Suppose the opponent did not take advantage of symmetries.\\nIn that case, should we? Is it true, then, that symmetrically equivalent positions should\\nnecessarily have the same value? ⇤\\nExercise 1.3: Greedy Play Suppose the reinforcement learning player wasgreedy, that is,\\nit always played the move that brought it to the position that it rated the best. Might it'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 34, 'page_label': '35'}, page_content='1.7. Early History of Reinforcement Learning 13\\nlearn to play better, or worse, than a nongreedy player? What problems might occur?⇤\\nExercise 1.4: Learning from ExplorationSuppose learning updates occurred afterall\\nmoves, including exploratory moves. If the step-size parameter is appropriately reduced\\nover time (but not the tendency to explore), then the state values would converge to\\na di↵erent set of probabilities. What (conceptually) are the two sets of probabilities\\ncomputed when we do, and when we do not, learn from exploratory moves? Assuming\\nthat we do continue to make exploratory moves, which set of probabilities might be better\\nto learn? Which would result in more wins? ⇤\\nExercise 1.5: Other Improvements Can you think of other ways to improve the reinforce-\\nment learning player? Can you think of any better way to solve the tic-tac-toe problem\\nas posed? ⇤\\n1.6 Summary\\nReinforcement learning is a computational approach to understanding and automating\\ngoal-directed learning and decision making. It is distinguished from other computational\\napproaches by its emphasis on learning by an agent from direct interaction with its\\nenvironment, without requiring exemplary supervision or complete models of the envi-\\nronment. In our opinion, reinforcement learning is the ﬁrst ﬁeld to seriously address the\\ncomputational issues that arise when learning from interaction with an environment in\\norder to achieve long-term goals.\\nReinforcement learning uses the formal framework of Markov decision processes to\\ndeﬁne the interaction between a learning agent and its environment in terms of states,\\nactions, and rewards. This framework is intended to be a simple way of representing\\nessential features of the artiﬁcial intelligence problem. These features include a sense of\\ncause and e↵ect, a sense of uncertainty and nondeterminism, and the existence of explicit\\ngoals.\\nThe concepts of value and value function are key to most of the reinforcement learning\\nmethods that we consider in this book. We take the position that value functions\\nare important for e\\x00cient search in the space of policies. The use of value functions\\ndistinguishes reinforcement learning methods from evolutionary methods that search\\ndirectly in policy space guided by evaluations of entire policies.\\n1.7 Early History of Reinforcement Learning\\nThe early history of reinforcement learning has two main threads, both long and rich, that\\nwere pursued independently before intertwining in modern reinforcement learning. One\\nthread concerns learning by trial and error, and originated in the psychology of animal\\nlearning. This thread runs through some of the earliest work in artiﬁcial intelligence\\nand led to the revival of reinforcement learning in the early 1980s. The second thread\\nconcerns the problem of optimal control and its solution using value functions and\\ndynamic programming. For the most part, this thread did not involve learning. The\\ntwo threads were mostly independent, but became interrelated to some extent around a'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 35, 'page_label': '36'}, page_content='14 Chapter 1: Introduction\\nthird, less distinct thread concerning temporal-di↵erence methods such as that used in\\nthe tic-tac-toe example in this chapter. All three threads came together in the late 1980s\\nto produce the modern ﬁeld of reinforcement learning as we present it in this book.\\nThe thread focusing on trial-and-error learning is the one with which we are most\\nfamiliar and about which we have the most to say in this brief history. Before doing that,\\nhowever, we brieﬂy discuss the optimal control thread.\\nThe term “optimal control” came into use in the late 1950s to describe the problem of\\ndesigning a controller to minimize or maximize a measure of a dynamical system’s behavior\\nover time. One of the approaches to this problem was developed in the mid-1950s by\\nRichard Bellman and others through extending a nineteenth century theory of Hamilton\\nand Jacobi. This approach uses the concepts of a dynamical system’s state and of a\\nvalue function, or “optimal return function,” to deﬁne a functional equation, now often\\ncalled the Bellman equation. The class of methods for solving optimal control problems\\nby solving this equation came to be known as dynamic programming (Bellman, 1957a).\\nBellman (1957b) also introduced the discrete stochastic version of the optimal control\\nproblem known as Markov decision processes (MDPs). Ronald Howard (1960) devised\\nthe policy iteration method for MDPs. All of these are essential elements underlying the\\ntheory and algorithms of modern reinforcement learning.\\nDynamic programming is widely considered the only feasible way of solving general\\nstochastic optimal control problems. It su↵ers from what Bellman called “the curse of\\ndimensionality,” meaning that its computational requirements grow exponentially with\\nthe number of state variables, but it is still far more e\\x00cient and more widely applicable\\nthan any other general method. Dynamic programming has been extensively developed\\nsince the late 1950s, including extensions to partially observable MDPs (surveyed by\\nLovejoy, 1991), many applications (surveyed by White, 1985, 1988, 1993), approximation\\nmethods (surveyed by Rust, 1996), and asynchronous methods (Bertsekas, 1982, 1983).\\nMany excellent modern treatments of dynamic programming are available (e.g., Bertsekas,\\n2005, 2012; Puterman, 1994; Ross, 1983; Whittle, 1982, 1983). Bryson (1996) provides\\nan authoritative history of optimal control.\\nConnections between optimal control and dynamic programming, on the one hand,\\nand learning, on the other, were slow to be recognized. We cannot be sure about what\\naccounted for this separation, but its main cause was likely the separation between\\nthe disciplines involved and their di↵erent goals. Also contributing may have been the\\nprevalent view of dynamic programming as an o↵-line computation depending essentially\\non accurate system models and analytic solutions to the Bellman equation. Further,\\nthe simplest form of dynamic programming is a computation that proceeds backwards\\nin time, making it di\\x00cult to see how it could be involved in a learning process that\\nmust proceed in a forward direction. Some of the earliest work in dynamic programming,\\nsuch as that by Bellman and Dreyfus (1959), might now be classiﬁed as following\\na learning approach. Witten’s (1977) work (discussed below) certainly qualiﬁes as a\\ncombination of learning and dynamic-programming ideas. Werbos (1987) argued explicitly\\nfor greater interrelation of dynamic programming and learning methods and for dynamic\\nprogramming’s relevance to understanding neural and cognitive mechanisms. For us the\\nfull integration of dynamic programming methods with online learning did not occur'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 36, 'page_label': '37'}, page_content='1.7. Early History of Reinforcement Learning 15\\nuntil the work of Chris Watkins in 1989, whose treatment of reinforcement learning using\\nthe MDP formalism has been widely adopted. Since then these relationships have been\\nextensively developed by many researchers, most particularly by Dimitri Bertsekas and\\nJohn Tsitsiklis (1996), who coined the term “neurodynamic programming” to refer to\\nthe combination of dynamic programming and artiﬁcial neural networks. Another term\\ncurrently in use is “approximate dynamic programming.” These various approaches\\nemphasize di↵erent aspects of the subject, but they all share with reinforcement learning\\nan interest in circumventing the classical shortcomings of dynamic programming.\\nWe consider all of the work in optimal control also to be, in a sense, work in reinforce-\\nment learning. We deﬁne a reinforcement learning method as any e↵ective way of solving\\nreinforcement learning problems, and it is now clear that these problems are closely\\nrelated to optimal control problems, particularly stochastic optimal control problems\\nsuch as those formulated as MDPs. Accordingly, we must consider the solution methods\\nof optimal control, such as dynamic programming, also to be reinforcement learning\\nmethods. Because almost all of the conventional methods require complete knowledge\\nof the system to be controlled, it feels a little unnatural to say that they are part of\\nreinforcement learning. On the other hand, many dynamic programming algorithms are\\nincremental and iterative. Like learning methods, they gradually reach the correct answer\\nthrough successive approximations. As we show in the rest of this book, these similarities\\nare far more than superﬁcial. The theories and solution methods for the cases of complete\\nand incomplete knowledge are so closely related that we feel they must be considered\\ntogether as part of the same subject matter.\\nLet us return now to the other major thread leading to the modern ﬁeld of reinforcement\\nlearning, the thread centered on the idea of trial-and-error learning. We only touch on\\nthe major points of contact here, taking up this topic in more detail in Section 14.3.\\nAccording to American psychologist R. S. Woodworth (1938) the idea of trial-and-error\\nlearning goes as far back as the 1850s to Alexander Bain’s discussion of learning by\\n“groping and experiment” and more explicitly to the British ethologist and psychologist\\nConway Lloyd Morgan’s 1894 use of the term to describe his observations of animal\\nbehavior. Perhaps the ﬁrst to succinctly express the essence of trial-and-error learning as\\na principle of learning was Edward Thorndike:\\nOf several responses made to the same situation, those which are accompanied\\nor closely followed by satisfaction to the animal will, other things being\\nequal, be more ﬁrmly connected with the situation, so that, when it recurs,\\nthey will be more likely to recur; those which are accompanied or closely\\nfollowed by discomfort to the animal will, other things being equal, have their\\nconnections with that situation weakened, so that, when it recurs, they will\\nbe less likely to occur. The greater the satisfaction or discomfort, the greater\\nthe strengthening or weakening of the bond. (Thorndike, 1911, p. 244)\\nThorndike called this the “Law of E↵ect” because it describes the e↵ect of reinforcing\\nevents on the tendency to select actions. Thorndike later modiﬁed the law to better\\naccount for subsequent data on animal learning (such as di↵erences between the e↵ects\\nof reward and punishment), and the law in its various forms has generated considerable\\ncontroversy among learning theorists (e.g., see Gallistel, 2005; Herrnstein, 1970; Kimble,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 37, 'page_label': '38'}, page_content='16 Chapter 1: Introduction\\n1961, 1967; Mazur, 1994). Despite this, the Law of E↵ect—in one form or another—is\\nwidely regarded as a basic principle underlying much behavior (e.g., Hilgard and Bower,\\n1975; Dennett, 1978; Campbell, 1960; Cziko, 1995). It is the basis of the inﬂuential\\nlearning theories of Clark Hull (1943, 1952) and the inﬂuential experimental methods of\\nB. F. Skinner (1938).\\nThe term “reinforcement” in the context of animal learning came into use well after\\nThorndike’s expression of the Law of E↵ect, ﬁrst appearing in this context (to the best of\\nour knowledge) in the 1927 English translation of Pavlov’s monograph on conditioned\\nreﬂexes. Pavlov described reinforcement as the strengthening of a pattern of behavior due\\nto an animal receiving a stimulus—a reinforcer—in an appropriate temporal relationship\\nwith another stimulus or with a response. Some psychologists extended the idea of\\nreinforcement to include weakening as well as strengthening of behavior, and extended\\nthe idea of a reinforcer to include possibly the omission or termination of stimulus. To be\\nconsidered a reinforcer, the strengthening or weakening must persist after the reinforcer\\nis withdrawn; a stimulus that merely attracts an animal’s attention or that energizes its\\nbehavior without producing lasting changes would not be considered a reinforcer.\\nThe idea of implementing trial-and-error learning in a computer appeared among the\\nearliest thoughts about the possibility of artiﬁcial intelligence. In a 1948 report, Alan\\nTuring described a design for a “pleasure-pain system” that worked along the lines of the\\nLaw of E↵ect:\\nWhen a conﬁguration is reached for which the action is undetermined, a\\nrandom choice for the missing data is made and the appropriate entry is made\\nin the description, tentatively, and is applied. When a pain stimulus occurs\\nall tentative entries are cancelled, and when a pleasure stimulus occurs they\\nare all made permanent. (Turing, 1948)\\nMany ingenious electro-mechanical machines were constructed that demonstrated trial-\\nand-error learning. The earliest may have been a machine built by Thomas Ross (1933)\\nthat was able to ﬁnd its way through a simple maze and remember the path through\\nthe settings of switches. In 1951 W. Grey Walter built a version of his “mechanical\\ntortoise” (Walter, 1950) capable of a simple form of learning. In 1952 Claude Shannon\\ndemonstrated a maze-running mouse named Theseus that used trial and error to ﬁnd\\nits way through a maze, with the maze itself remembering the successful directions\\nvia magnets and relays under its ﬂoor (see also Shannon, 1951). J. A. Deutsch (1954)\\ndescribed a maze-solving machine based on his behavior theory (Deutsch, 1953) that\\nhas some properties in common with model-based reinforcement learning (Chapter 8).\\nIn his PhD dissertation, Marvin Minsky (1954) discussed computational models of\\nreinforcement learning and described his construction of an analog machine composed of\\ncomponents he called SNARCs (Stochastic Neural-Analog Reinforcement Calculators)\\nmeant to resemble modiﬁable synaptic connections in the brain (Chapter 15). The\\nweb sitecyberneticzoo.com contains a wealth of information on these and many other\\nelectro-mechanical learning machines.\\nBuilding electro-mechanical learning machines gave way to programming digital com-\\nputers to perform various types of learning, some of which implemented trial-and-error\\nlearning. Farley and Clark (1954) described a digital simulation of a neural-network'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 38, 'page_label': '39'}, page_content='1.7. Early History of Reinforcement Learning 17\\nlearning machine that learned by trial and error. But their interests soon shifted from\\ntrial-and-error learning to generalization and pattern recognition, that is, from reinforce-\\nment learning to supervised learning (Clark and Farley, 1955). This began a pattern\\nof confusion about the relationship between these types of learning. Many researchers\\nseemed to believe that they were studying reinforcement learning when they were actually\\nstudying supervised learning. For example, artiﬁcial neural network pioneers such as\\nRosenblatt (1962) and Widrow and Ho↵ (1960) were clearly motivated by reinforcement\\nlearning—they used the language of rewards and punishments—but the systems they\\nstudied were supervised learning systems suitable for pattern recognition and perceptual\\nlearning. Even today, some researchers and textbooks minimize or blur the distinction\\nbetween these types of learning. For example, some textbooks have used the term “trial-\\nand-error” to describe artiﬁcial neural networks that learn from training examples. This\\nis an understandable confusion because these networks use error information to update\\nconnection weights, but this misses the essential character of trial-and-error learning as\\nselecting actions on the basis of evaluative feedback that does not rely on knowledge of\\nwhat the correct action should be.\\nPartly as a result of these confusions, research into genuine trial-and-error learning\\nbecame rare in the 1960s and 1970s, although there were notable exceptions. In the 1960s\\nthe terms “reinforcement” and “reinforcement learning” were used in the engineering\\nliterature for the ﬁrst time to describe engineering uses of trial-and-error learning (e.g.,\\nWaltz and Fu, 1965; Mendel, 1966; Fu, 1970; Mendel and McClaren, 1970). Particularly\\ninﬂuential was Minsky’s paper “Steps Toward Artiﬁcial Intelligence” (Minsky, 1961),\\nwhich discussed several issues relevant to trial-and-error learning, including prediction,\\nexpectation, and what he called thebasic credit-assignment problem for complex rein-\\nforcement learning systems: How do you distribute credit for success among the many\\ndecisions that may have been involved in producing it? All of the methods we discuss in\\nthis book are, in a sense, directed toward solving this problem. Minsky’s paper is well\\nworth reading today.\\nIn the next few paragraphs we discuss some of the other exceptions and partial\\nexceptions to the relative neglect of computational and theoretical study of genuine\\ntrial-and-error learning in the 1960s and 1970s.\\nOne exception was the work of the New Zealand researcher John Andreae, who\\ndeveloped a system called STeLLA that learned by trial and error in interaction with\\nits environment. This system included an internal model of the world and, later, an\\n“internal monologue” to deal with problems of hidden state (Andreae, 1963, 1969; Andreae\\nand Cashin, 1969). Andreae’s later work (1977) placed more emphasis on learning\\nfrom a teacher, but still included learning by trial and error, with the generation of\\nnovel events being one of the system’s goals. A feature of this work was a “leakback\\nprocess,” elaborated more fully in Andreae (1998), that implemented a credit-assignment\\nmechanism similar to the backing-up update operations that we describe. Unfortunately,\\nhis pioneering research was not well known and did not greatly impact subsequent\\nreinforcement learning research. Recent summaries are available (Andreae, 2017a,b).\\nMore inﬂuential was the work of Donald Michie. In 1961 and 1963 he described a\\nsimple trial-and-error learning system for learning how to play tic-tac-toe (or naughts'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 39, 'page_label': '40'}, page_content='18 Chapter 1: Introduction\\nand crosses) called MENACE (for Matchbox Educable Naughts and Crosses Engine). It\\nconsisted of a matchbox for each possible game position, each matchbox containing a\\nnumber of colored beads, a di↵erent color for each possible move from that position. By\\ndrawing a bead at random from the matchbox corresponding to the current game position,\\none could determine MENACE’s move. When a game was over, beads were added to\\nor removed from the boxes used during play to reward or punish MENACE’s decisions.\\nMichie and Chambers (1968) described another tic-tac-toe reinforcement learner called\\nGLEE (Game Learning Expectimaxing Engine) and a reinforcement learning controller\\ncalled BOXES. They applied BOXES to the task of learning to balance a pole hinged to\\na movable cart on the basis of a failure signal occurring only when the pole fell or the\\ncart reached the end of a track. This task was adapted from the earlier work of Widrow\\nand Smith (1964), who used supervised learning methods, assuming instruction from a\\nteacher already able to balance the pole. Michie and Chambers’s version of pole-balancing\\nis one of the best early examples of a reinforcement learning task under conditions of\\nincomplete knowledge. It inﬂuenced much later work in reinforcement learning, beginning\\nwith some of our own studies (Barto, Sutton, and Anderson, 1983; Sutton, 1984). Michie\\n(1974) consistently emphasized trial and error and learning as essential aspects of artiﬁcial\\nintelligence.\\nWidrow, Gupta, and Maitra (1973) modiﬁed the Least-Mean-Square (LMS) algorithm\\nof Widrow and Ho↵ (1960) to produce a reinforcement learning rule that could learn\\nfrom success and failure signals instead of from training examples. They called this form\\nof learning “selective bootstrap adaptation” and described it as “learning with a critic”\\ninstead of “learning with a teacher.” They analyzed this rule and showed how it could\\nlearn to play blackjack. This was an isolated foray into reinforcement learning by Widrow,\\nwhose contributions to supervised learning were much more inﬂuential. Our use of the\\nterm “critic” is derived from Widrow, Gupta, and Maitra’s paper. Buchanan, Mitchell,\\nSmith, and Johnson (1978) independently used the term critic in the context of machine\\nlearning (see also Dietterich and Buchanan, 1984), but for them a critic was an expert\\nsystem able to do more than evaluate performance.\\nResearch on learning automata had a more direct inﬂuence on the trial-and-error\\nthread leading to modern reinforcement learning research. These are methods for solving\\na nonassociative, purely selectional learning problem known as thek-armed bandit by\\nanalogy to a slot machine, or “one-armed bandit,” except withk levers (see Chapter 2).\\nLearning automata are simple, low-memory machines for improving the probability\\nof reward in these problems. Learning automata originated with work in the 1960s\\nof the Russian mathematician and physicist M. L. Tsetlin and colleagues (published\\nposthumously in Tsetlin, 1973) and has been extensively developed since then within\\nengineering (see Narendra and Thathachar, 1974, 1989). These developments included the\\nstudy ofstochastic learning automata, which are methods for updating action probabilities\\non the basis of reward signals. Although not developed in the tradition of stochastic\\nlearning automata, Harth and Tzanakou’s (1974) Alopex algorithm (forAlgorithm of\\npattern extraction) is a stochastic method for detecting correlations between actions and\\nreinforcement that inﬂuenced some of our early research (Barto, Sutton, and Brouwer,\\n1981). Stochastic learning automata were foreshadowed by earlier work in psychology,\\nbeginning with William Estes’ (1950) e↵ort toward a statistical theory of learning and\\nfurther developed by others (e.g., Bush and Mosteller, 1955; Sternberg, 1963).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 40, 'page_label': '41'}, page_content='1.7. Early History of Reinforcement Learning 19\\nThe statistical learning theories developed in psychology were adopted by researchers in\\neconomics, leading to a thread of research in that ﬁeld devoted to reinforcement learning.\\nThis work began in 1973 with the application of Bush and Mosteller’s learning theory to\\na collection of classical economic models (Cross, 1973). One goal of this research was to\\nstudy artiﬁcial agents that act more like real people than do traditional idealized economic\\nagents (Arthur, 1991). This approach expanded to the study of reinforcement learning\\nin the context of game theory. Reinforcement learning in economics developed largely\\nindependently of the early work in reinforcement learning in artiﬁcial intelligence, though\\ngame theory remains a topic of interest in both ﬁelds (beyond the scope of this book).\\nCamerer (2011) discusses the reinforcement learning tradition in economics, and Now´ e,\\nVrancx, and De Hauwere (2012) provide an overview of the subject from the point of view\\nof multi-agent extensions to the approach that we introduce in this book. Reinforcement\\nlearning in the context of game theory is a much di↵erent subject than reinforcement\\nlearning used in programs to play tic-tac-toe, checkers, and other recreational games. See,\\nfor example, Szita (2012) for an overview of this aspect of reinforcement learning and\\ngames.\\nJohn Holland (1975) outlined a general theory of adaptive systems based on selectional\\nprinciples. His early work concerned trial and error primarily in its nonassociative\\nform, as in evolutionary methods and thek-armed bandit. In 1976 and more fully in\\n1986, he introduced classiﬁer systems, true reinforcement learning systems including\\nassociation and value functions. A key component of Holland’s classiﬁer systems was\\nthe “bucket-brigade algorithm” for credit assignment, which is closely related to the\\ntemporal di↵erence algorithm used in our tic-tac-toe example and discussed in Chapter 6.\\nAnother key component was agenetic algorithm, an evolutionary method whose role was\\nto evolve useful representations. Classiﬁer systems have been extensively developed by\\nmany researchers to form a major branch of reinforcement learning research (reviewed by\\nUrbanowicz and Moore, 2009), but genetic algorithms—which we do not consider to be\\nreinforcement learning systems by themselves—have received much more attention, as\\nhave other approaches to evolutionary computation (e.g., Fogel, Owens and Walsh, 1966;\\nKoza, 1992).\\nThe individual most responsible for reviving the trial-and-error thread of reinforcement\\nlearning within artiﬁcial intelligence was Harry Klopf (1972, 1975, 1982). Klopf recognized\\nthat essential aspects of adaptive behavior were being lost as learning researchers came\\nto focus almost exclusively on supervised learning. What was missing, according to\\nKlopf, were the hedonic aspects of behavior: the drive to achieve some result from the\\nenvironment, to control the environment toward desired ends and away from undesired\\nends (see Section 15.9). This is the essential idea of trial-and-error learning. Klopf’s\\nideas were especially inﬂuential on the authors because our assessment of them (Barto\\nand Sutton, 1981a) led to our appreciation of the distinction between supervised and\\nreinforcement learning, and to our eventual focus on reinforcement learning. Much of\\nthe early work that we and colleagues accomplished was directed toward showing that\\nreinforcement learning and supervised learning were indeed di↵erent (Barto, Sutton, and\\nBrouwer, 1981; Barto and Sutton, 1981b; Barto and Anandan, 1985). Other studies\\nshowed how reinforcement learning could address important problems in artiﬁcial neural'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 41, 'page_label': '42'}, page_content='20 Chapter 1: Introduction\\nnetwork learning, in particular, how it could produce learning algorithms for multilayer\\nnetworks (Barto, Anderson, and Sutton, 1982; Barto and Anderson, 1985; Barto, 1985,\\n1986; Barto and Jordan, 1987; see Section 15.10).\\nWe turn now to the third thread to the history of reinforcement learning, that concerning\\ntemporal-di↵erence learning. Temporal-di↵erence learning methods are distinctive in\\nbeing driven by the di↵erence between temporally successive estimates of the same\\nquantity—for example, of the probability of winning in the tic-tac-toe example. This\\nthread is smaller and less distinct than the other two, but it has played a particularly\\nimportant role in the ﬁeld, in part because temporal-di↵erence methods seem to be new\\nand unique to reinforcement learning.\\nThe origins of temporal-di↵erence learning are in part in animal learning psychology,\\nin particular, in the notion ofsecondary reinforcers. A secondary reinforcer is a stimulus\\nthat has been paired with a primary reinforcer such as food or pain and, as a result, has\\ncome to take on similar reinforcing properties. Minsky (1954) may have been the ﬁrst to\\nrealize that this psychological principle could be important for artiﬁcial learning systems.\\nArthur Samuel (1959) was the ﬁrst to propose and implement a learning method that\\nincluded temporal-di↵erence ideas, as part of his celebrated checkers-playing program\\n(Section 16.2).\\nSamuel made no reference to Minsky’s work or to possible connections to animal\\nlearning. His inspiration apparently came from Claude Shannon’s (1950) suggestion that\\na computer could be programmed to use an evaluation function to play chess, and that it\\nmight be able to improve its play by modifying this function online. (It is possible that\\nthese ideas of Shannon’s also inﬂuenced Bellman, but we know of no evidence for this.)\\nMinsky (1961) extensively discussed Samuel’s work in his “Steps” paper, suggesting the\\nconnection to secondary reinforcement theories, both natural and artiﬁcial.\\nAs we have discussed, in the decade following the work of Minsky and Samuel, little\\ncomputational work was done on trial-and-error learning, and apparently no computational\\nwork at all was done on temporal-di↵erence learning. In 1972, Klopf brought trial-and-\\nerror learning together with an important component of temporal-di↵erence learning.\\nKlopf was interested in principles that would scale to learning in large systems, and thus\\nwas intrigued by notions of local reinforcement, whereby subcomponents of an overall\\nlearning system could reinforce one another. He developed the idea of “generalized\\nreinforcement,” whereby every component (nominally, every neuron) views all of its\\ninputs in reinforcement terms: excitatory inputs as rewards and inhibitory inputs as\\npunishments. This is not the same idea as what we now know as temporal-di↵erence\\nlearning, and in retrospect it is farther from it than was Samuel’s work. On the other\\nhand, Klopf linked the idea with trial-and-error learning and related it to the massive\\nempirical database of animal learning psychology.\\nSutton (1978a,b,c) developed Klopf’s ideas further, particularly the links to animal\\nlearning theories, describing learning rules driven by changes in temporally successive\\npredictions. He and Barto reﬁned these ideas and developed a psychological model of\\nclassical conditioning based on temporal-di↵erence learning (Sutton and Barto, 1981a;\\nBarto and Sutton, 1982). There followed several other inﬂuential psychological models of\\nclassical conditioning based on temporal-di↵erence learning (e.g., Klopf, 1988; Moore et al.,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 42, 'page_label': '43'}, page_content='1.7. Early History of Reinforcement Learning 21\\n1986; Sutton and Barto, 1987, 1990). Some neuroscience models developed at this time\\nare well interpreted in terms of temporal-di↵erence learning (Hawkins and Kandel, 1984;\\nByrne, Gingrich, and Baxter, 1990; Gelperin, Hopﬁeld, and Tank, 1985; Tesauro, 1986;\\nFriston et al., 1994), although in most cases there was no historical connection.\\nOur early work on temporal-di↵erence learning was strongly inﬂuenced by animal\\nlearning theories and by Klopf’s work. Relationships to Minsky’s “Steps” paper and to\\nSamuel’s checkers players were recognized only afterward. By 1981, however, we were\\nfully aware of all the prior work mentioned above as part of the temporal-di↵erence and\\ntrial-and-error threads. At this time we developed a method for using temporal-di↵erence\\nlearning combined with trial-and-error learning, known as theactor–critic architecture,\\nand applied this method to Michie and Chambers’s pole-balancing problem (Barto,\\nSutton, and Anderson, 1983). This method was extensively studied in Sutton’s (1984)\\nPhD dissertation and extended to use backpropagation neural networks in Anderson’s\\n(1986) PhD dissertation. Around this time, Holland (1986) incorporated temporal-\\ndi↵erence ideas explicitly into his classiﬁer systems in the form of his bucket-brigade\\nalgorithm. A key step was taken by Sutton (1988) by separating temporal-di↵erence\\nlearning from control, treating it as a general prediction method. That paper also\\nintroduced the TD(\\x00) algorithm and proved some of its convergence properties.\\nAs we were ﬁnalizing our work on the actor–critic architecture in 1981, we discovered\\na paper by Ian Witten (1977, 1976a) which appears to be the earliest publication of a\\ntemporal-di↵erence learning rule. He proposed the method that we now call tabular TD(0)\\nfor use as part of an adaptive controller for solving MDPs. This work was ﬁrst submitted\\nfor journal publication in 1974 and also appeared in Witten’s 1976 PhD dissertation.\\nWitten’s work was a descendant of Andreae’s early experiments with STeLLA and other\\ntrial-and-error learning systems. Thus, Witten’s 1977 paper spanned both major threads\\nof reinforcement learning research—trial-and-error learning and optimal control—while\\nmaking a distinct early contribution to temporal-di↵erence learning.\\nThe temporal-di↵erence and optimal control threads were fully brought together\\nin 1989 with Chris Watkins’s development of Q-learning. This work extended and\\nintegrated prior work in all three threads of reinforcement learning research. Paul Werbos\\n(1987) contributed to this integration by arguing for the convergence of trial-and-error\\nlearning and dynamic programming since 1977. By the time of Watkins’s work there had\\nbeen tremendous growth in reinforcement learning research, primarily in the machine\\nlearning subﬁeld of artiﬁcial intelligence, but also in artiﬁcial neural networks and artiﬁcial\\nintelligence more broadly. In 1992, the remarkable success of Gerry Tesauro’s backgammon\\nplaying program, TD-Gammon, brought additional attention to the ﬁeld.\\nIn the time since publication of the ﬁrst edition of this book, a ﬂourishing subﬁeld of\\nneuroscience developed that focuses on the relationship between reinforcement learning\\nalgorithms and reinforcement learning in the nervous system. Most responsible for this is\\nan uncanny similarity between the behavior of temporal-di↵erence algorithms and the\\nactivity of dopamine producing neurons in the brain, as pointed out by a number of\\nresearchers (Friston et al., 1994; Barto, 1995a; Houk, Adams, and Barto, 1995; Montague,\\nDayan, and Sejnowski, 1996; and Schultz, Dayan, and Montague, 1997). Chapter 15\\nprovides an introduction to this exciting aspect of reinforcement learning. Other important'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 43, 'page_label': '44'}, page_content='22 Chapter 1: Introduction\\ncontributions made in the recent history of reinforcement learning are too numerous to\\nmention in this brief account; we cite many of these at the end of the individual chapters\\nin which they arise.\\nBibliographical Remarks\\nFor additional general coverage of reinforcement learning, we refer the reader to the\\nbooks by Szepesv´ ari (2010), Bertsekas and Tsitsiklis (1996), Kaelbling (1993a), and\\nSugiyama, Hachiya, and Morimura (2013). Books that take a control or operations research\\nperspective include those of Si, Barto, Powell, and Wunsch (2004), Powell (2011), Lewis\\nand Liu (2012), and Bertsekas (2012). Cao’s (2009) review places reinforcement learning\\nin the context of other approaches to learning and optimization of stochastic dynamic\\nsystems. Three special issues of the journalMachine Learning focus on reinforcement\\nlearning: Sutton (1992a), Kaelbling (1996), and Singh (2002). Useful surveys are provided\\nby Barto (1995b); Kaelbling, Littman, and Moore (1996); and Keerthi and Ravindran\\n(1997). The volume edited by Weiring and van Otterlo (2012) provides an excellent\\noverview of recent developments.\\n1.2 The example of Phil’s breakfast in this chapter was inspired by Agre (1988).\\n1.5 The temporal-di↵erence method used in the tic-tac-toe example is developed in\\nChapter 6.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 44, 'page_label': '45'}, page_content='Part I: Tabular Solution Methods\\nIn this part of the book we describe almost all the core ideas of reinforcement learning\\nalgorithms in their simplest forms: that in which the state and action spaces are small\\nenough for the approximate value functions to be represented as arrays, ortables.I n\\nthis case, the methods can often ﬁnd exact solutions, that is, they can often ﬁnd exactly\\nthe optimal value function and the optimal policy. This contrasts with the approximate\\nmethods described in the next part of the book, which only ﬁnd approximate solutions,\\nbut which in return can be applied e↵ectively to much larger problems.\\nThe ﬁrst chapter of this part of the book describes solution methods for the special\\ncase of the reinforcement learning problem in which there is only a single state, called\\nbandit problems. The second chapter describes the general problem formulation that we\\ntreat throughout the rest of the book—ﬁnite Markov decision processes—and its main\\nideas including Bellman equations and value functions.\\nThe next three chapters describe three fundamental classes of methods for solving ﬁnite\\nMarkov decision problems: dynamic programming, Monte Carlo methods, and temporal-\\ndi↵erence learning. Each class of methods has its strengths and weaknesses. Dynamic\\nprogramming methods are well developed mathematically, but require a complete and\\naccurate model of the environment. Monte Carlo methods don’t require a model and are\\nconceptually simple, but are not well suited for step-by-step incremental computation.\\nFinally, temporal-di↵erence methods require no model and are fully incremental, but are\\nmore complex to analyze. The methods also di↵er in several ways with respect to their\\ne\\x00ciency and speed of convergence.\\nThe remaining two chapters describe how these three classes of methods can be\\ncombined to obtain the best features of each of them. In one chapter we describe how\\nthe strengths of Monte Carlo methods can be combined with the strengths of temporal-\\ndi↵erence methods via multi-step bootstrapping methods. In the ﬁnal chapter of this part\\nof the book we show how temporal-di↵erence learning methods can be combined with\\nmodel learning and planning methods (such as dynamic programming) for a complete\\nand uniﬁed solution to the tabular reinforcement learning problem.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 45, 'page_label': '46'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 46, 'page_label': '47'}, page_content='Chapter 2\\nMulti-armed Bandits\\nThe most important feature distinguishing reinforcement learning from other types of\\nlearning is that it uses training information thatevaluates the actions taken rather\\nthan instructs by giving correct actions. This is what creates the need for active\\nexploration, for an explicit search for good behavior. Purely evaluative feedback indicates\\nhow good the action taken was, but not whether it was the best or the worst action\\npossible. Purely instructive feedback, on the other hand, indicates the correct action to\\ntake, independently of the action actually taken. This kind of feedback is the basis of\\nsupervised learning, which includes large parts of pattern classiﬁcation, artiﬁcial neural\\nnetworks, and system identiﬁcation. In their pure forms, these two kinds of feedback\\nare quite distinct: evaluative feedback depends entirely on the action taken, whereas\\ninstructive feedback is independent of the action taken.\\nIn this chapter we study the evaluative aspect of reinforcement learning in a simpliﬁed\\nsetting, one that does not involve learning to act in more than one situation. This\\nnonassociative setting is the one in which most prior work involving evaluative feedback\\nhas been done, and it avoids much of the complexity of the full reinforcement learning\\nproblem. Studying this case enables us to see most clearly how evaluative feedback di↵ers\\nfrom, and yet can be combined with, instructive feedback.\\nThe particular nonassociative, evaluative feedback problem that we explore is a simple\\nversion of thek-armed bandit problem. We use this problem to introduce a number\\nof basic learning methods which we extend in later chapters to apply to the full rein-\\nforcement learning problem. At the end of this chapter, we take a step closer to the full\\nreinforcement learning problem by discussing what happens when the bandit problem\\nbecomes associative, that is, when the best action depends on the situation.\\n2.1 A k-armed Bandit Problem\\nConsider the following learning problem. You are faced repeatedly with a choice among\\nk di↵erent options, or actions. After each choice you receive a numerical reward chosen\\nfrom a stationary probability distribution that depends on the action you selected. Your'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 47, 'page_label': '48'}, page_content='26 Chapter 2: Multi-armed Bandits\\nobjective is to maximize the expected total reward over some time period, for example,\\nover 1000 action selections, ortime steps.\\nThis is the original form of thek-armed bandit problem, so named by analogy to a slot\\nmachine, or “one-armed bandit,” except that it hask levers instead of one. Each action\\nselection is like a play of one of the slot machine’s levers, and the rewards are the payo↵s\\nfor hitting the jackpot. Through repeated action selections you are to maximize your\\nwinnings by concentrating your actions on the best levers. Another analogy is that of\\na doctor choosing between experimental treatments for a series of seriously ill patients.\\nEach action is the selection of a treatment, and each reward is the survival or well-being\\nof the patient. Today the term “bandit problem” is sometimes used for a generalization\\nof the problem described above, but in this book we use it to refer just to this simple\\ncase.\\nIn ourk-armed bandit problem, each of thek actions has an expected or mean reward\\ngiven that that action is selected; let us call this thevalue of that action. We denote the\\naction selected on time stept as At, and the corresponding reward asRt. The value then\\nof an arbitrary actiona, denotedq⇤(a), is the expected reward given thata is selected:\\nq⇤(a) .= E[Rt | At =a] .\\nIf you knew the value of each action, then it would be trivial to solve thek-armed bandit\\nproblem: you would always select the action with highest value. We assume that you do\\nnot know the action values with certainty, although you may have estimates. We denote\\nthe estimated value of actiona at time stept as Qt(a). We would likeQt(a) to be close\\nto q⇤(a).\\nIf you maintain estimates of the action values, then at any time step there is at least\\none action whose estimated value is greatest. We call these thegreedy actions. When you\\nselect one of these actions, we say that you areexploiting your current knowledge of the\\nvalues of the actions. If instead you select one of the nongreedy actions, then we say you\\nare exploring, because this enables you to improve your estimate of the nongreedy action’s\\nvalue. Exploitation is the right thing to do to maximize the expected reward on the one\\nstep, but exploration may produce the greater total reward in the long run. For example,\\nsuppose a greedy action’s value is known with certainty, while several other actions are\\nestimated to be nearly as good but with substantial uncertainty. The uncertainty is\\nsuch that at least one of these other actions probably is actually better than the greedy\\naction, but you don’t know which one. If you have many time steps ahead on which\\nto make action selections, then it may be better to explore the nongreedy actions and\\ndiscover which of them are better than the greedy action. Reward is lower in the short\\nrun, during exploration, but higher in the long run because after you have discovered\\nthe better actions, you can exploit them many times. Because it is not possible both to\\nexplore and to exploit with any single action selection, one often refers to the “conﬂict”\\nbetween exploration and exploitation.\\nIn any speciﬁc case, whether it is better to explore or exploit depends in a complex\\nway on the precise values of the estimates, uncertainties, and the number of remaining\\nsteps. There are many sophisticated methods for balancing exploration and exploitation\\nfor particular mathematical formulations of thek-armed bandit and related problems.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 48, 'page_label': '49'}, page_content='2.2. Action-value Methods 27\\nHowever, most of these methods make strong assumptions about stationarity and prior\\nknowledge that are either violated or impossible to verify in most applications and in\\nthe full reinforcement learning problem that we consider in subsequent chapters. The\\nguarantees of optimality or bounded loss for these methods are of little comfort when the\\nassumptions of their theory do not apply.\\nIn this book we do not worry about balancing exploration and exploitation in a\\nsophisticated way; we worry only about balancing them at all. In this chapter we present\\nseveral simple balancing methods for thek-armed bandit problem and show that they\\nwork much better than methods that always exploit. The need to balance exploration\\nand exploitation is a distinctive challenge that arises in reinforcement learning; the\\nsimplicity of our version of thek-armed bandit problem enables us to show this in a\\nparticularly clear form.\\n2.2 Action-value Methods\\nWe begin by looking more closely at methods for estimating the values of actions and\\nfor using the estimates to make action selection decisions, which we collectively call\\naction-value methods. Recall that the true value of an action is the mean reward when\\nthat action is selected. One natural way to estimate this is by averaging the rewards\\nactually received:\\nQt(a) .= sum of rewards whena taken prior tot\\nnumber of timesa taken prior tot =\\nPt\\x001\\ni=1 Ri ·\\n Ai=a\\nPt\\x001\\ni=1\\n Ai=a\\n, (2.1)\\nwhere\\n predicate denotes the random variable that is 1 ifpredicate is true and 0 if it is not.\\nIf the denominator is zero, then we instead deﬁneQt(a) as some default value, such as\\n0. As the denominator goes to inﬁnity, by the law of large numbers,Qt(a) converges to\\nq⇤(a). We call this thesample-average method for estimating action values because each\\nestimate is an average of the sample of relevant rewards. Of course this is just one way\\nto estimate action values, and not necessarily the best one. Nevertheless, for now let us\\nstay with this simple estimation method and turn to the question of how the estimates\\nmight be used to select actions.\\nThe simplest action selection rule is to select one of the actions with the highest\\nestimated value, that is, one of the greedy actions as deﬁned in the previous section.\\nIf there is more than one greedy action, then a selection is made among them in some\\narbitrary way, perhaps randomly. We write thisgreedy action selection method as\\nAt\\n.= argmax\\na\\nQt(a), (2.2)\\nwhere argmaxa denotes the actiona for which the expression that follows is maximized\\n(with ties broken arbitrarily). Greedy action selection always exploits current knowledge to\\nmaximize immediate reward; it spends no time at all sampling apparently inferior actions\\nto see if they might really be better. A simple alternative is to behave greedily most of\\nthe time, but every once in a while, say with small probability\", instead select randomly'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 49, 'page_label': '50'}, page_content='28 Chapter 2: Multi-armed Bandits\\nfrom among all the actions with equal probability, independently of the action-value\\nestimates. We call methods using this near-greedy action selection rule\"-greedy methods.\\nAn advantage of these methods is that, in the limit as the number of steps increases,\\nevery action will be sampled an inﬁnite number of times, thus ensuring that all theQt(a)\\nconverge to their respectiveq⇤(a). This of course implies that the probability of selecting\\nthe optimal action converges to greater than 1\\x00 \", that is, to near certainty. These are\\njust asymptotic guarantees, however, and say little about the practical e↵ectiveness of\\nthe methods.\\nExercise 2.1 In \"-greedy action selection, for the case of two actions and\" =0 .5, what is\\nthe probability that the greedy action is selected? ⇤\\n2.3 The 10-armed Testbed\\nTo roughly assess the relative e↵ectiveness of the greedy and\"-greedy action-value\\nmethods, we compared them numerically on a suite of test problems. This was a set\\nof 2000 randomly generatedk-armed bandit problems withk = 10. For each bandit\\nproblem, such as the one shown in Figure 2.1, the action values,q⇤(a), a =1 ,..., 10,\\n0\\n1\\n2\\n3\\n-3\\n-2\\n-1\\nq⇤(1)\\nq⇤(2)\\nq⇤(3)\\nq⇤(4)\\nq⇤(5)\\nq⇤(6)\\nq⇤(7)\\nq⇤(8)\\nq⇤(9)\\nq⇤(10)\\nReward\\ndistribution\\n1 2 63 54 7 8 9 10\\nAction\\nFigure 2.1: An example bandit problem from the 10-armed testbed. The true valueq⇤(a)o f\\neach of the ten actions was selected according to a normal distribution with mean zero and unit\\nvariance, and then the actual rewards were selected according to a meanq⇤(a), unit-variance\\nnormal distribution, as suggested by these gray distributions.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 50, 'page_label': '51'}, page_content='2.3. The 10-armed Testbed 29\\nwere selected according to a normal (Gaussian) distribution with mean 0 and variance 1.\\nThen, when a learning method applied to that problem selected actionAt at time stept,\\nthe actual reward,Rt, was selected from a normal distribution with meanq⇤(At) and\\nvariance 1. These distributions are shown in gray in Figure 2.1. We call this suite of test\\ntasks the10-armed testbed. For any learning method, we can measure its performance\\nand behavior as it improves with experience over 1000 time steps when applied to one of\\nthe bandit problems. This makes up onerun. Repeating this for 2000 independent runs,\\neach with a di↵erent bandit problem, we obtained measures of the learning algorithm’s\\naverage behavior.\\nFigure 2.2 compares a greedy method with two\"-greedy methods (\"=0.01 and\"=0.1),\\nas described above, on the 10-armed testbed. All the methods formed their action-value\\nestimates using the sample-average technique (with an initial estimate of 0). The upper\\ngraph shows the increase in expected reward with experience. The greedy method\\nimproved slightly faster than the other methods at the very beginning, but then leveled\\no↵ at a lower level. It achieved a reward-per-step of only about 1, compared with the best\\npossible of about 1.54 on this testbed. The greedy method performed signiﬁcantly worse\\nin the long run because it often got stuck performing suboptimal actions. The lower graph\\n (greedy)\\n0\\n0.5\\n1\\n1.5\\nAveragereward\\n0 250 500 750 1000\\nSteps\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\n%Optimalaction\\n0 250 500 750 1000\\nSteps\\n \\n \\n1\\n1\\n\"=0.1\\n\"=0.01\\n\"=0.1\\n\"=0.01\\n\"=0\\n (greedy)\"=0\\nFigure 2.2: Average performance of\"-greedy action-value methods on the 10-armed testbed.\\nThese data are averages over 2000 runs with di↵erent bandit problems. All methods used sample\\naverages as their action-value estimates.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 51, 'page_label': '52'}, page_content='30 Chapter 2: Multi-armed Bandits\\nshows that the greedy method found the optimal action in only approximately one-third\\nof the tasks. In the other two-thirds, its initial samples of the optimal action were\\ndisappointing, and it never returned to it. The\"-greedy methods eventually performed\\nbetter because they continued to explore and to improve their chances of recognizing\\nthe optimal action. The\" =0 .1 method explored more, and usually found the optimal\\naction earlier, but it never selected that action more than 91% of the time. The\" =0 .01\\nmethod improved more slowly, but eventually would perform better than the\" =0 .1\\nmethod on both performance measures shown in the ﬁgure. It is also possible to reduce\"\\nover time to try to get the best of both high and low values.\\nThe advantage of\"-greedy over greedy methods depends on the task. For example,\\nsuppose the reward variance had been larger, say 10 instead of 1. With noisier rewards\\nit takes more exploration to ﬁnd the optimal action, and\"-greedy methods should fare\\neven better relative to the greedy method. On the other hand, if the reward variances\\nwere zero, then the greedy method would know the true value of each action after trying\\nit once. In this case the greedy method might actually perform best because it would\\nsoon ﬁnd the optimal action and then never explore. But even in the deterministic case\\nthere is a large advantage to exploring if we weaken some of the other assumptions. For\\nexample, suppose the bandit task were nonstationary, that is, the true values of the\\nactions changed over time. In this case exploration is needed even in the deterministic\\ncase to make sure one of the nongreedy actions has not changed to become better than\\nthe greedy one. As we shall see in the next few chapters, nonstationarity is the case\\nmost commonly encountered in reinforcement learning. Even if the underlying task is\\nstationary and deterministic, the learner faces a set of banditlike decision tasks each of\\nwhich changes over time as learning proceeds and the agent’s decision-making policy\\nchanges. Reinforcement learning requires a balance between exploration and exploitation.\\nExercise 2.2: Bandit example Consider ak-armed bandit problem withk = 4 actions,\\ndenoted 1, 2, 3, and 4. Consider applying to this problem a bandit algorithm using\\n\"-greedy action selection, sample-average action-value estimates, and initial estimates\\nof Q1(a) = 0, for alla. Suppose the initial sequence of actions and rewards isA1 = 1,\\nR1 = \\x001, A2 = 2,R2 = 1,A3 = 2,R3 = \\x002, A4 = 2,R4 = 2,A5 = 3,R5 = 0. On some\\nof these time steps the\" case may have occurred, causing an action to be selected at\\nrandom. On which time steps did this deﬁnitely occur? On which time steps could this\\npossibly have occurred? ⇤\\nExercise 2.3 In the comparison shown in Figure 2.2, which method will perform best in\\nthe long run in terms of cumulative reward and probability of selecting the best action?\\nHow much better will it be? Express your answer quantitatively. ⇤\\n2.4 Incremental Implementation\\nThe action-value methods we have discussed so far all estimate action values as sample\\naverages of observed rewards. We now turn to the question of how these averages can be\\ncomputed in a computationally e\\x00cient manner, in particular, with constant memory\\nand constant per-time-step computation.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 52, 'page_label': '53'}, page_content='2.4. Incremental Implementation 31\\nTo simplify notation we concentrate on a single action. LetRi now denote the reward\\nreceived after theith selectionof this action, and letQn denote the estimate of its action\\nvalue after it has been selectedn \\x00 1 times, which we can now write simply as\\nQn\\n.= R1 + R2 + ··· + Rn\\x001\\nn \\x00 1 .\\nThe obvious implementation would be to maintain a record of all the rewards and then\\nperform this computation whenever the estimated value was needed. However, if this is\\ndone, then the memory and computational requirements would grow over time as more\\nrewards are seen. Each additional reward would require additional memory to store it\\nand additional computation to compute the sum in the numerator.\\nAs you might suspect, this is not really necessary. It is easy to devise incremental\\nformulas for updating averages with small, constant computation required to process\\neach new reward. GivenQn and thenth reward,Rn, the new average of alln rewards\\ncan be computed by\\nQn+1 = 1\\nn\\nnX\\ni=1\\nRi\\n= 1\\nn\\n \\nRn +\\nn\\x001X\\ni=1\\nRi\\n!\\n= 1\\nn\\n \\nRn +( n \\x00 1) 1\\nn \\x00 1\\nn\\x001X\\ni=1\\nRi\\n!\\n= 1\\nn\\n⇣\\nRn +( n \\x00 1)Qn\\n⌘\\n= 1\\nn\\n⇣\\nRn + nQn \\x00 Qn\\n⌘\\n= Qn + 1\\nn\\nh\\nRn \\x00 Qn\\ni\\n, (2.3)\\nwhich holds even forn = 1, obtainingQ2 = R1 for arbitraryQ1. This implementation\\nrequires memory only forQn and n, and only the small computation (2.3) for each new\\nreward.\\nThis update rule(2.3) is of a form that occurs frequently throughout this book. The\\ngeneral form is\\nNewEstimate  OldEstimate + StepSize\\nh\\nTarget \\x00 OldEstimate\\ni\\n. (2.4)\\nThe expression\\n⇥\\nTarget\\x00OldEstimate\\n⇤\\nis anerror in the estimate. It is reduced by taking\\na step toward the “Target.” The target is presumed to indicate a desirable direction in\\nwhich to move, though it may be noisy. In the case above, for example, the target is the\\nnth reward.\\nNote that the step-size parameter (StepSize ) used in the incremental method(2.3)\\nchanges from time step to time step. In processing thenth reward for actiona,t h e'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 53, 'page_label': '54'}, page_content='32 Chapter 2: Multi-armed Bandits\\nmethod uses the step-size parameter1\\nn . In this book we denote the step-size parameter\\nby ↵ or, more generally, by↵t(a).\\nPseudocode for a complete bandit algorithm using incrementally computed sample\\naverages and\"-greedy action selection is shown in the box below. The functionbandit(a)\\nis assumed to take an action and return a corresponding reward.\\nA simple bandit algorithm\\nInitialize, fora =1t o k:\\nQ(a)  0\\nN(a)  0\\nLoop forever:\\nA  \\n⇢ argmaxa Q(a) with probability 1 \\x00 \" (breaking ties randomly)\\na random action with probability \"\\nR  bandit(A)\\nN(A)  N(A)+1\\nQ(A)  Q(A)+ 1\\nN(A)\\n⇥\\nR \\x00 Q(A)\\n⇤\\n2.5 Tracking a Nonstationary Problem\\nThe averaging methods discussed so far are appropriate for stationary bandit problems,\\nthat is, for bandit problems in which the reward probabilities do not change over time.\\nAs noted earlier, we often encounter reinforcement learning problems that are e↵ectively\\nnonstationary. In such cases it makes sense to give more weight to recent rewards than\\nto long-past rewards. One of the most popular ways of doing this is to use a constant\\nstep-size parameter. For example, the incremental update rule (2.3) for updating an\\naverage Qn of then \\x00 1 past rewards is modiﬁed to be\\nQn+1\\n.= Qn + ↵\\nh\\nRn \\x00 Qn\\ni\\n, (2.5)\\nwhere the step-size parameter↵ 2 (0, 1] is constant. This results inQn+1 being a weighted\\naverage of past rewards and the initial estimateQ1:\\nQn+1 = Qn + ↵\\nh\\nRn \\x00 Qn\\ni\\n= ↵Rn +( 1\\x00 ↵)Qn\\n= ↵Rn +( 1\\x00 ↵)[ ↵Rn\\x001 +( 1\\x00 ↵)Qn\\x001]\\n= ↵Rn +( 1\\x00 ↵)↵Rn\\x001 +( 1\\x00 ↵)2Qn\\x001\\n= ↵Rn +( 1\\x00 ↵)↵Rn\\x001 +( 1\\x00 ↵)2↵Rn\\x002 +\\n··· +( 1\\x00 ↵)n\\x001↵R1 +( 1\\x00 ↵)nQ1\\n=( 1 \\x00 ↵)nQ1 +\\nnX\\ni=1\\n↵(1 \\x00 ↵)n\\x00iRi. (2.6)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 54, 'page_label': '55'}, page_content='2.5. Tracking a Nonstationary Problem 33\\nWe call this a weighted average because the sum of the weights is (1\\x00 ↵)n + Pn\\ni=1 ↵(1 \\x00\\n↵)n\\x00i = 1, as you can check for yourself. Note that the weight,↵(1 \\x00 ↵)n\\x00i, given to the\\nreward Ri depends on how many rewards ago,n \\x00i, it was observed. The quantity 1\\x00↵\\nis less than 1, and thus the weight given toRi decreases as the number of intervening\\nrewards increases. In fact, the weight decays exponentially according to the exponent\\non 1\\x00 ↵.( I f 1\\x00 ↵ = 0, then all the weight goes on the very last reward,Rn, because\\nof the convention that 00 = 1.) Accordingly, this is sometimes called anexponential\\nrecency-weighted average.\\nSometimes it is convenient to vary the step-size parameter from step to step. Let↵n(a)\\ndenote the step-size parameter used to process the reward received after thenth selection\\nof actiona. As we have noted, the choice↵n(a)= 1\\nn results in the sample-average method,\\nwhich is guaranteed to converge to the true action values by the law of large numbers.\\nBut of course convergence is not guaranteed for all choices of the sequence{↵n(a)}.A\\nwell-known result in stochastic approximation theory gives us the conditions required to\\nassure convergence with probability 1:\\n1X\\nn=1\\n↵n(a)= 1 and\\n1X\\nn=1\\n↵2\\nn(a) < 1. (2.7)\\nThe ﬁrst condition is required to guarantee that the steps are large enough to eventually\\novercome any initial conditions or random ﬂuctuations. The second condition guarantees\\nthat eventually the steps become small enough to assure convergence.\\nNote that both convergence conditions are met for the sample-average case,↵n(a)= 1\\nn ,\\nbut not for the case of constant step-size parameter,↵n(a)= ↵. In the latter case, the\\nsecond condition is not met, indicating that the estimates never completely converge but\\ncontinue to vary in response to the most recently received rewards. As we mentioned\\nabove, this is actually desirable in a nonstationary environment, and problems that are\\ne↵ectively nonstationary are the most common in reinforcement learning. In addition,\\nsequences of step-size parameters that meet the conditions (2.7) often converge very slowly\\nor need considerable tuning in order to obtain a satisfactory convergence rate. Although\\nsequences of step-size parameters that meet these convergence conditions are often used\\nin theoretical work, they are seldom used in applications and empirical research.\\nExercise 2.4 If the step-size parameters,↵n, are not constant, then the estimateQn is\\na weighted average of previously received rewards with a weighting di↵erent from that\\ngiven by (2.6). What is the weighting on each prior reward for the general case, analogous\\nto (2.6), in terms of the sequence of step-size parameters? ⇤\\nExercise 2.5 (programming) Design and conduct an experiment to demonstrate the\\ndi\\x00culties that sample-average methods have for nonstationary problems. Use a modiﬁed\\nversion of the 10-armed testbed in which all theq⇤(a) start out equal and then take\\nindependent random walks (say by adding a normally distributed increment with mean 0\\nand standard deviation 0.01 to all theq⇤(a) on each step). Prepare plots like Figure 2.2\\nfor an action-value method using sample averages, incrementally computed, and another\\naction-value method using a constant step-size parameter,↵ =0 .1. Use \" =0 .1 and\\nlonger runs, say of 10,000 steps. ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 55, 'page_label': '56'}, page_content='34 Chapter 2: Multi-armed Bandits\\n2.6 Optimistic Initial Values\\nAll the methods we have discussed so far are dependent to some extent on the initial\\naction-value estimates, Q1(a). In the language of statistics, these methods arebiased\\nby their initial estimates. For the sample-average methods, the bias disappears once all\\nactions have been selected at least once, but for methods with constant↵, the bias is\\npermanent, though decreasing over time as given by (2.6). In practice, this kind of bias\\nis usually not a problem and can sometimes be very helpful. The downside is that the\\ninitial estimates become, in e↵ect, a set of parameters that must be picked by the user, if\\nonly to set them all to zero. The upside is that they provide an easy way to supply some\\nprior knowledge about what level of rewards can be expected.\\nInitial action values can also be used as a simple way to encourage exploration. Suppose\\nthat instead of setting the initial action values to zero, as we did in the 10-armed testbed,\\nwe set them all to +5. Recall that theq⇤(a) in this problem are selected from a normal\\ndistribution with mean 0 and variance 1. An initial estimate of +5 is thus wildly optimistic.\\nBut this optimism encourages action-value methods to explore. Whichever actions are\\ninitially selected, the reward is less than the starting estimates; the learner switches to\\nother actions, being “disappointed” with the rewards it is receiving. The result is that all\\nactions are tried several times before the value estimates converge. The system does a\\nfair amount of exploration even if greedy actions are selected all the time.\\nFigure 2.3 shows the performance on the 10-armed bandit testbed of a greedy method\\nusing Q1(a) = +5, for alla. For comparison, also shown is an\"-greedy method with\\nQ1(a) = 0. Initially, the optimistic method performs worse because it explores more,\\nbut eventually it performs better because its exploration decreases with time. We call\\nthis technique for encouraging explorationoptimistic initial values. We regard it as\\na simple trick that can be quite e↵ective on stationary problems, but it is far from\\nbeing a generally useful approach to encouraging exploration. For example, it is not\\nwell suited to nonstationary problems because its drive for exploration is inherently\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\n%Optimalaction\\n0 200 400 600 800 1000\\nPlays\\noptimistic, greedy\\nQ0 = 5,  \\x04\\x04= 0\\nrealistic, \\x04-greedy\\nQ0 = 0,  \\x04\\x04= 0.11\\n1\\nSteps\\n1\\nOptimistic, greedyQ1=5, \"=0\\nRealistic,   -greedy\"\\nQ1=0, \"=0.1\\nFigure 2.3: The e↵ect of optimistic initial action-value estimates on the 10-armed testbed.\\nBoth methods used a constant step-size parameter,↵ =0 .1.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 56, 'page_label': '57'}, page_content='2.7. Upper-Conﬁdence-Bound Action Selection 35\\ntemporary. If the task changes, creating a renewed need for exploration, this method\\ncannot help. Indeed, any method that focuses on the initial conditions in any special way\\nis unlikely to help with the general nonstationary case. The beginning of time occurs\\nonly once, and thus we should not focus on it too much. This criticism applies as well to\\nthe sample-average methods, which also treat the beginning of time as a special event,\\naveraging all subsequent rewards with equal weights. Nevertheless, all of these methods\\nare very simple, and one of them—or some simple combination of them—is often adequate\\nin practice. In the rest of this book we make frequent use of several of these simple\\nexploration techniques.\\nExercise 2.6: Mysterious Spikes The results shown in Figure 2.3 should be quite reliable\\nbecause they are averages over 2000 individual, randomly chosen 10-armed bandit tasks.\\nWhy, then, are there oscillations and spikes in the early part of the curve for the optimistic\\nmethod? In other words, what might make this method perform particularly better or\\nworse, on average, on particular early steps? ⇤\\nExercise 2.7: Unbiased Constant-Step-Size Trick In most of this chapter we have used\\nsample averages to estimate action values because sample averages do not produce the\\ninitial bias that constant step sizes do (see the analysis leading to(2.6)). However, sample\\naverages are not a completely satisfactory solution because they may perform poorly\\non nonstationary problems. Is it possible to avoid the bias of constant step sizes while\\nretaining their advantages on nonstationary problems? One way is to use a step size of\\n\\x00n\\n.= ↵/¯on, (2.8)\\nto process thenth reward for a particular action, where↵> 0 is a conventional constant\\nstep size, and ¯on is a trace of one that starts at 0:\\n¯on\\n.=¯ on\\x001 + ↵(1 \\x00 ¯on\\x001), for n> 0, with ¯o0\\n.=0 . (2.9)\\nCarry out an analysis like that in(2.6) to show thatQn is an exponential recency-weighted\\naverage without initial bias. ⇤\\n2.7 Upper-Conﬁdence-Bound Action Selection\\nExploration is needed because there is always uncertainty about the accuracy of the\\naction-value estimates. The greedy actions are those that look best at present, but some of\\nthe other actions may actually be better.\"-greedy action selection forces the non-greedy\\nactions to be tried, but indiscriminately, with no preference for those that are nearly\\ngreedy or particularly uncertain. It would be better to select among the non-greedy\\nactions according to their potential for actually being optimal, taking into account both\\nhow close their estimates are to being maximal and the uncertainties in those estimates.\\nOne e↵ective way of doing this is to select actions according to\\nAt\\n.= argmax\\na\\n\"\\nQt(a)+ c\\ns\\nln t\\nNt(a)\\n#\\n, (2.10)\\nwhere ln t denotes the natural logarithm oft (the number thate ⇡ 2.71828 would have\\nto be raised to in order to equalt), Nt(a) denotes the number of times that actiona has'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 57, 'page_label': '58'}, page_content='36 Chapter 2: Multi-armed Bandits\\nbeen selected prior to timet (the denominator in(2.1)), and the numberc> 0 controls\\nthe degree of exploration. IfNt(a) = 0, thena is considered to be a maximizing action.\\nThe idea of thisupper conﬁdence bound(UCB) action selection is that the square-root\\nterm is a measure of the uncertainty or variance in the estimate ofa’s value. The quantity\\nbeing max’ed over is thus a sort of upper bound on the possible true value of actiona,w i t h\\nc determining the conﬁdence level. Each timea is selected the uncertainty is presumably\\nreduced: Nt(a) increments, and, as it appears in the denominator, the uncertainty term\\ndecreases. On the other hand, each time an action other thana is selected,t increases but\\nNt(a) does not; becauset appears in the numerator, the uncertainty estimate increases.\\nThe use of the natural logarithm means that the increases get smaller over time, but are\\nunbounded; all actions will eventually be selected, but actions with lower value estimates,\\nor that have already been selected frequently, will be selected with decreasing frequency\\nover time.\\nResults with UCB on the 10-armed testbed are shown in Figure 2.4. UCB often\\nperforms well, as shown here, but is more di\\x00cult than\"-greedy to extend beyond bandits\\nto the more general reinforcement learning settings considered in the rest of this book.\\nOne di\\x00culty is in dealing with nonstationary problems; methods more complex than\\nthose presented in Section 2.5 would be needed. Another di\\x00culty is dealing with large\\nstate spaces, particularly when using function approximation as developed in Part II of\\nthis book. In these more advanced settings the idea of UCB action selection is usually\\nnot practical.\\n1 250 500 750 1000\\n0\\n0.5\\n1\\n1.5\\n\\x00-greedy  \\x00 = 0.1\\nUCB  c = 2\\nAverage\\nreward\\nSteps\\nFigure 2.4: Average performance of UCB action selection on the 10-armed testbed. As shown,\\nUCB generally performs better than\"-greedy action selection, except in the ﬁrstk steps, when\\nit selects randomly among the as-yet-untried actions.\\nExercise 2.8: UCB Spikes In Figure 2.4 the UCB algorithm shows a distinct spike\\nin performance on the 11th step. Why is this? Note that for your answer to be fully\\nsatisfactory it must explain both why the reward increases on the 11th step and why it\\ndecreases on the subsequent steps. Hint: Ifc = 1, then the spike is less prominent.⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 58, 'page_label': '59'}, page_content='2.8. Gradient Bandit Algorithms 37\\n2.8 Gradient Bandit Algorithms\\nSo far in this chapter we have considered methods that estimate action values and use\\nthose estimates to select actions. This is often a good approach, but it is not the only\\none possible. In this section we consider learning a numericalpreference for each action\\na, which we denoteHt(a) 2 R. The larger the preference, the more often that action is\\ntaken, but the preference has no interpretation in terms of reward. Only the relative\\npreference of one action over another is important; if we add 1000 to all the action\\npreferences there is no e↵ect on the action probabilities, which are determined according\\nto asoft-max distribution (i.e., Gibbs or Boltzmann distribution) as follows:\\nPr{At =a} .= eHt(a)\\nPk\\nb=1 eHt(b)\\n.= ⇡t(a), (2.11)\\nwhere here we have also introduced a useful new notation,⇡t(a), for the probability of\\ntaking actiona at timet. Initially all action preferences are the same (e.g.,H1(a) = 0,\\nfor alla) so that all actions have an equal probability of being selected.\\nExercise 2.9 Show that in the case of two actions, the soft-max distribution is the same\\nas that given by the logistic, or sigmoid, function often used in statistics and artiﬁcial\\nneural networks. ⇤\\nThere is a natural learning algorithm for soft-max action preferences based on the idea\\nof stochastic gradient ascent. On each step, after selecting actionAt and receiving the\\nreward Rt, the action preferences are updated by:\\nHt+1(At) .= Ht(At)+ ↵\\n\\x00\\nRt \\x00 ¯Rt\\n\\x00\\x00\\n1 \\x00 ⇡t(At)\\n\\x00\\n, and\\nHt+1(a) .= Ht(a) \\x00 ↵\\n\\x00\\nRt \\x00 ¯Rt\\n\\x00\\n⇡t(a), for alla 6= At, (2.12)\\nwhere ↵> 0 is a step-size parameter, and¯Rt 2 R is the average of the rewards up to but\\nnot including timet (with ¯R1\\n.= R1), which can be computed incrementally as described\\nin Section 2.4 (or Section 2.5 if the problem is nonstationary).1 The ¯Rt term serves as a\\nbaseline with which the reward is compared. If the reward is higher than the baseline,\\nthen the probability of takingAt in the future is increased, and if the reward is below\\nbaseline, then the probability is decreased. The non-selected actions move in the opposite\\ndirection.\\nFigure 2.5 shows results with the gradient bandit algorithm on a variant of the 10-\\narmed testbed in which the true expected rewards were selected according to a normal\\ndistribution with a mean of +4 instead of zero (and with unit variance as before). This\\nshifting up of all the rewards has absolutely no e↵ect on the gradient bandit algorithm\\nbecause of the reward baseline term, which instantaneously adapts to the new level. But\\nif the baseline were omitted (that is, if¯Rt was taken to be constant zero in(2.12)), then\\nperformance would be signiﬁcantly degraded, as shown in the ﬁgure.\\n1In the empirical results in this chapter, the baseline¯Rt also includedRt.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 59, 'page_label': '60'}, page_content='38 Chapter 2: Multi-armed Bandits\\n%\\nOptimal\\naction\\nSteps\\nα = 0.1\\n100%\\n80%\\n60%\\n40%\\n20%\\n0%\\nα = 0.4\\nα = 0.1\\nα = 0.4\\nwithout baseline\\nwith baseline\\n1 250 500 750 1000\\nFigure 2.5: Average performance of the gradient bandit algorithm with and without a reward\\nbaseline on the 10-armed testbed when theq⇤(a) are chosen to be near +4 rather than near zero.\\nThe Bandit Gradient Algorithm as Stochastic Gradient Ascent\\nOne can gain a deeper insight into the gradient bandit algorithm by understanding\\nit as a stochastic approximation to gradient ascent. In exactgradient ascent, each\\naction preference Ht(a) would be incremented in proportion to the increment’s\\ne↵ect on performance:\\nHt+1(a) .= Ht(a)+ ↵@ E[Rt]\\n@Ht(a) , (2.13)\\nwhere the measure of performance here is the expected reward:\\nE[Rt]=\\nX\\nx\\n⇡t(x)q⇤(x),\\nand the measure of the increment’s e↵ect is thepartial derivativeof this performance\\nmeasure with respect to the action preference. Of course, it is not possible to\\nimplement gradient ascent exactly in our case because by assumption we do not\\nknow theq⇤(x), but in fact the updates of our algorithm(2.12) are equal to(2.13)\\nin expected value, making the algorithm an instance ofstochastic gradient ascent.\\nThe calculations showing this require only beginning calculus, but take several'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 60, 'page_label': '61'}, page_content='2.8. Gradient Bandit Algorithms 39\\nsteps. First we take a closer look at the exact performance gradient:\\n@ E[Rt]\\n@Ht(a) = @\\n@Ht(a)\\n\"X\\nx\\n⇡t(x)q⇤(x)\\n#\\n=\\nX\\nx\\nq⇤(x) @⇡ t(x)\\n@Ht(a)\\n=\\nX\\nx\\n\\x00\\nq⇤(x) \\x00 Bt\\n\\x00 @⇡ t(x)\\n@Ht(a),\\nwhere Bt, called thebaseline, can be any scalar that does not depend onx. We can\\ninclude a baseline here without changing the equality because the gradient sums\\nto zero over all the actions,P\\nx\\n@⇡ t(x)\\n@Ht(a) = 0. As Ht(a) is changed, some actions’\\nprobabilities go up and some go down, but the sum of the changes must be zero\\nbecause the sum of the probabilities is always one.\\nNext we multiply each term of the sum by⇡t(x)/⇡t(x):\\n@ E[Rt]\\n@Ht(a) =\\nX\\nx\\n⇡t(x)\\n\\x00\\nq⇤(x) \\x00 Bt\\n\\x00 @⇡ t(x)\\n@Ht(a)/⇡t(x).\\nThe equation is now in the form of an expectation, summing over all possible values\\nx of the random variableAt, then multiplying by the probability of taking those\\nvalues. Thus:\\n= E\\n\\uf8ff\\x00\\nq⇤(At) \\x00 Bt\\n\\x00@⇡ t(At)\\n@Ht(a) /⇡t(At)\\n\\x00\\n= E\\n\\uf8ff\\x00\\nRt \\x00 ¯Rt\\n\\x00@⇡ t(At)\\n@Ht(a) /⇡t(At)\\n\\x00\\n,\\nwhere here we have chosen the baselineBt = ¯Rt and substituted Rt for q⇤(At),\\nwhich is permitted because E[Rt|At] = q⇤(At). Shortly we will establish that\\n@⇡ t(x)\\n@Ht(a) = ⇡t(x)\\n\\x00\\na=x \\x00 ⇡t(a)\\n\\x00\\n,w h e r e\\na=x is deﬁned to be 1 ifa = x, else 0.\\nAssuming that for now, we have\\n= E\\n⇥\\x00\\nRt \\x00 ¯Rt\\n\\x00\\n⇡t(At)\\n\\x00\\na=At \\x00 ⇡t(a)\\n\\x00\\n/⇡t(At)\\n⇤\\n= E\\n⇥\\x00\\nRt \\x00 ¯Rt\\n\\x00\\x00\\na=At \\x00 ⇡t(a)\\n\\x00⇤\\n.\\nRecall that our plan has been to write the performance gradient as an expectation\\nof something that we can sample on each step, as we have just done, and then\\nupdate on each step in proportion to the sample. Substituting a sample of the\\nexpectation above for the performance gradient in (2.13) yields:\\nHt+1(a)= Ht(a)+ ↵\\n\\x00\\nRt \\x00 ¯Rt\\n\\x00\\x00\\na=At \\x00 ⇡t(a)\\n\\x00\\n, for alla,\\nwhich you may recognize as being equivalent to our original algorithm (2.12).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 61, 'page_label': '62'}, page_content='40 Chapter 2: Multi-armed Bandits\\nThus it remains only to show that@⇡ t(x)\\n@Ht(a) = ⇡t(x)\\n\\x00\\na=x \\x00 ⇡t(a)\\n\\x00\\n, as we assumed.\\nRecall the standard quotient rule for derivatives:\\n@\\n@x\\n\\uf8fff(x)\\ng(x)\\n\\x00\\n=\\n@f (x)\\n@x g(x) \\x00 f(x)@g(x)\\n@x\\ng(x)2 .\\nUsing this, we can write\\n@⇡ t(x)\\n@Ht(a) = @\\n@Ht(a) ⇡t(x)\\n= @\\n@Ht(a)\\n\"\\neHt(x)\\nPk\\ny=1 eHt(y)\\n#\\n=\\n@eHt(x)\\n@Ht(a)\\nPk\\ny=1 eHt(y) \\x00 eHt(x) @ Pk\\ny=1 eHt(y)\\n@Ht(a)\\n⇣Pk\\ny=1 eHt(y)\\n⌘2 (by the quotient rule)\\n=\\na=xeHt(x) Pk\\ny=1 eHt(y) \\x00 eHt(x)eHt(a)\\n⇣Pk\\ny=1 eHt(y)\\n⌘2 (because @ex\\n@x = ex)\\n=\\n a=xeHt(x)\\nPk\\ny=1 eHt(y)\\n\\x00 eHt(x)eHt(a)\\n⇣Pk\\ny=1 eHt(y)\\n⌘2\\n=\\n a=x⇡t(x) \\x00 ⇡t(x)⇡t(a)\\n= ⇡t(x)\\n\\x00\\na=x \\x00 ⇡t(a)\\n\\x00\\n. Q.E.D.\\nWe have just shown that the expected update of the gradient bandit algorithm\\nis equal to the gradient of expected reward, and thus that the algorithm is an\\ninstance of stochastic gradient ascent. This assures us that the algorithm has robust\\nconvergence properties.\\nNote that we did not require any properties of the reward baseline other than\\nthat it does not depend on the selected action. For example, we could have set\\nit to zero, or to 1000, and the algorithm would still be an instance of stochastic\\ngradient ascent. The choice of the baseline does not a↵ect the expected update\\nof the algorithm, but it does a↵ect the variance of the update and thus the rate of\\nconvergence (as shown, for example, in Figure 2.5). Choosing it as the average of\\nthe rewards may not be the very best, but it is simple and works well in practice.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 62, 'page_label': '63'}, page_content='2.9. Associative Search (Contextual Bandits) 41\\n2.9 Associative Search (Contextual Bandits)\\nSo far in this chapter we have considered only nonassociative tasks, that is, tasks in which\\nthere is no need to associate di↵erent actions with di↵erent situations. In these tasks\\nthe learner either tries to ﬁnd a single best action when the task is stationary, or tries to\\ntrack the best action as it changes over time when the task is nonstationary. However,\\nin a general reinforcement learning task there is more than one situation, and the goal\\nis to learn a policy: a mapping from situations to the actions that are best in those\\nsituations. To set the stage for the full problem, we brieﬂy discuss the simplest way in\\nwhich nonassociative tasks extend to the associative setting.\\nAs an example, suppose there are several di↵erentk-armed bandit tasks, and that on\\neach step you confront one of these chosen at random. Thus, the bandit task changes\\nrandomly from step to step. If the probabilities with which each task is selected for you\\ndo not change over time, this would appear as a single stationaryk-armed bandit task,\\nand you could use one of the methods described in this chapter. Now suppose, however,\\nthat when a bandit task is selected for you, you are given some distinctive clue about its\\nidentity (but not its action values). Maybe you are facing an actual slot machine that\\nchanges the color of its display as it changes its action values. Now you can learn a policy\\nassociating each task, signaled by the color you see, with the best action to take when\\nfacing that task—for instance, if red, select arm 1; if green, select arm 2. With the right\\npolicy you can usually do much better than you could in the absence of any information\\ndistinguishing one bandit task from another.\\nThis is an example of anassociative searchtask, so called because it involves both\\ntrial-and-error learning tosearch for the best actions, andassociation of these actions\\nwith the situations in which they are best. Associative search tasks are often now called\\ncontextual banditsin the literature. Associative search tasks are intermediate between\\nthe k-armed bandit problem and the full reinforcement learning problem. They are like\\nthe full reinforcement learning problem in that they involve learning a policy, but they\\nare also like our version of thek-armed bandit problem in that each action a↵ects only\\nthe immediate reward. If actions are allowed to a↵ect thenext situation as well as the\\nreward, then we have the full reinforcement learning problem. We present this problem\\nin the next chapter and consider its ramiﬁcations throughout the rest of the book.\\nExercise 2.10 Suppose you face a 2-armed bandit task whose true action values change\\nrandomly from time step to time step. Speciﬁcally, suppose that, for any time step,\\nthe true values of actions 1 and 2 are respectively 10 and 20 with probability 0.5 (case\\nA), and 90 and 80 with probability 0.5 (case B). If you are not able to tell which case\\nyou face at any step, what is the best expected reward you can achieve and how should\\nyou behave to achieve it? Now suppose that on each step you are told whether you are\\nfacing case A or case B (although you still don’t know the true action values). This is an\\nassociative search task. What is the best expected reward you can achieve in this task,\\nand how should you behave to achieve it? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 63, 'page_label': '64'}, page_content='42 Chapter 2: Multi-armed Bandits\\n2.10 Summary\\nWe have presented in this chapter several simple ways of balancing exploration and\\nexploitation. The \"-greedy methods choose randomly a small fraction of the time, whereas\\nUCB methods choose deterministically but achieve exploration by subtly favoring at each\\nstep the actions that have so far received fewer samples. Gradient bandit algorithms\\nestimate not action values, but action preferences, and favor the more preferred actions\\nin a graded, probabilistic manner using a soft-max distribution. The simple expedient of\\ninitializing estimates optimistically causes even greedy methods to explore signiﬁcantly.\\nIt is natural to ask which of these methods is best. Although this is a di\\x00cult question\\nto answer in general, we can certainly run them all on the 10-armed testbed that we\\nhave used throughout this chapter and compare their performances. A complication is\\nthat they all have a parameter; to get a meaningful comparison we have to consider\\ntheir performance as a function of their parameter. Our graphs so far have shown the\\ncourse of learning over time for each algorithm and parameter setting, to produce a\\nlearning curve for that algorithm and parameter setting. If we plotted learning curves\\nfor all algorithms and all parameter settings, then the graph would be too complex and\\ncrowded to make clear comparisons. Instead we summarize a complete learning curve\\nby its average value over the 1000 steps; this value is proportional to the area under the\\nlearning curve. Figure 2.6 shows this measure for the various bandit algorithms from\\nthis chapter, each as a function of its own parameter shown on a single scale on the\\nx-axis. This kind of graph is called aparameter study. Note that the parameter values\\nare varied by factors of two and presented on a log scale. Note also the characteristic\\ninverted-U shapes of each algorithm’s performance; all the algorithms perform best at\\nan intermediate value of their parameter, neither too large nor too small. In assessing\\nAverage\\nreward\\nover ﬁrst \\n1000 steps\\n1.5\\n1.4\\n1.3\\n1.2\\n1.1\\n1\\n\\x00-greedy\\nUCB\\ngradient\\nbandit\\ngreedy with\\noptimistic\\ninitialization\\nα = 0.1\\n1 2 41/21/41/81/161/321/641/128\\n\" ↵ c Q 0\\nFigure 2.6: A parameter study of the various bandit algorithms presented in this chapter.\\nEach point is the average reward obtained over 1000 steps with a particular algorithm at a\\nparticular setting of its parameter.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 64, 'page_label': '65'}, page_content='2.10. Summary 43\\na method, we should attend not just to how well it does at its best parameter setting,\\nbut also to how sensitive it is to its parameter value. All of these algorithms are fairly\\ninsensitive, performing well over a range of parameter values varying by about an order\\nof magnitude. Overall, on this problem, UCB seems to perform best.\\nDespite their simplicity, in our opinion the methods presented in this chapter can\\nfairly be considered the state of the art. There are more sophisticated methods, but their\\ncomplexity and assumptions make them impractical for the full reinforcement learning\\nproblem that is our real focus. Starting in Chapter 5 we present learning methods for\\nsolving the full reinforcement learning problem that use in part the simple methods\\nexplored in this chapter.\\nAlthough the simple methods explored in this chapter may be the best we can do\\nat present, they are far from a fully satisfactory solution to the problem of balancing\\nexploration and exploitation.\\nOne well-studied approach to balancing exploration and exploitation ink-armed bandit\\nproblems is to compute a special kind of action value called aGittins index. In certain\\nimportant special cases, this computation is tractable and leads directly to optimal\\nsolutions, although it does require complete knowledge of the prior distribution of possible\\nproblems, which we generally assume is not available. In addition, neither the theory\\nnor the computational tractability of this approach appear to generalize to the full\\nreinforcement learning problem that we consider in the rest of the book.\\nThe Gittins-index approach is an instance ofBayesian methods, which assume a known\\ninitial distribution over the action values and then update the distribution exactly after\\neach step (assuming that the true action values are stationary). In general, the update\\ncomputations can be very complex, but for certain special distributions (calledconjugate\\npriors) they are easy. One possibility is to then select actions at each step according\\nto their posterior probability of being the best action. This method, sometimes called\\nposterior sampling or Thompson sampling, often performs similarly to the best of the\\ndistribution-free methods we have presented in this chapter.\\nIn the Bayesian setting it is even conceivable to compute theoptimal balance between\\nexploration and exploitation. One can compute for any possible action the probability\\nof each possible immediate reward and the resultant posterior distributions over action\\nvalues. This evolving distribution becomes theinformation state of the problem. Given\\na horizon, say of 1000 steps, one can consider all possible actions, all possible resulting\\nrewards, all possible next actions, all next rewards, and so on for all 1000 steps. Given\\nthe assumptions, the rewards and probabilities of each possible chain of events can be\\ndetermined; one need only pick the best. But the tree of possibilities grows extremely\\nrapidly; even if there were only two actions and two rewards, the tree would have 22000\\nleaves. It is generally not feasible to perform this immense computation exactly, but\\nperhaps it could be approximated e\\x00ciently. This approach would e↵ectively turn the\\nbandit problem into an instance of the full reinforcement learning problem. In the end, we\\nmay be able to use approximate reinforcement learning methods such as those presented\\nin Part II of this book to approach this optimal solution. But that is a topic for research\\nand beyond the scope of this introductory book.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 65, 'page_label': '66'}, page_content='44 Chapter 2: Multi-armed Bandits\\nExercise 2.11 (programming)Make a ﬁgure analogous to Figure 2.6 for the nonstationary\\ncase outlined in Exercise 2.5. Include the constant-step-size\"-greedy algorithm with\\n↵=0.1. Use runs of 200,000 steps and, as a performance measure for each algorithm and\\nparameter setting, use the average reward over the last 100,000 steps. ⇤\\nBibliographical and Historical Remarks\\n2.1 Bandit problems have been studied in statistics, engineering, and psychology. In\\nstatistics, bandit problems fall under the heading “sequential design of experi-\\nments,” introduced by Thompson (1933, 1934) and Robbins (1952), and studied\\nby Bellman (1956). Berry and Fristedt (1985) provide an extensive treatment of\\nbandit problems from the perspective of statistics. Narendra and Thathachar\\n(1989) treat bandit problems from the engineering perspective, providing a good\\ndiscussion of the various theoretical traditions that have focused on them. In\\npsychology, bandit problems have played roles in statistical learning theory (e.g.,\\nBush and Mosteller, 1955; Estes, 1950).\\nThe termgreedy is often used in the heuristic search literature (e.g., Pearl, 1984).\\nThe conﬂict between exploration and exploitation is known in control engineering\\nas the conﬂict between identiﬁcation (or estimation) and control (e.g., Witten,\\n1976b). Feldbaum (1965) called it thedual control problem, referring to the\\nneed to solve the two problems of identiﬁcation and control simultaneously when\\ntrying to control a system under uncertainty. In discussing aspects of genetic\\nalgorithms, Holland (1975) emphasized the importance of this conﬂict, referring\\nto it as the conﬂict between the need to exploit and the need for new information.\\n2.2 Action-value methods for ourk-armed bandit problem were ﬁrst proposed by\\nThathachar and Sastry (1985). These are often calledestimator algorithms in the\\nlearning automata literature. The termaction value is due to Watkins (1989).\\nThe ﬁrst to use\"-greedy methods may also have been Watkins (1989, p. 187),\\nbut the idea is so simple that some earlier use seems likely.\\n2.4–5 This material falls under the general heading of stochastic iterative algorithms,\\nwhich is well covered by Bertsekas and Tsitsiklis (1996).\\n2.6 Optimistic initialization was used in reinforcement learning by Sutton (1996).\\n2.7 Early work on using estimates of the upper conﬁdence bound to select actions\\nwas done by Lai and Robbins (1985), Kaelbling (1993b), and Agrawal (1995).\\nThe UCB algorithm we present here is called UCB1 in the literature and was\\nﬁrst developed by Auer, Cesa-Bianchi and Fischer (2002).\\n2.8 Gradient bandit algorithms are a special case of the gradient-based reinforcement\\nlearning algorithms introduced by Williams (1992) that later developed into the\\nactor–critic and policy-gradient algorithms that we treat later in this book. Our\\ndevelopment here was inﬂuenced by that by Balaraman Ravindran (personal'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 66, 'page_label': '67'}, page_content='2.10. Summary 45\\ncommunication). Further discussion of the choice of baseline is provided by\\nGreensmith, Bartlett, and Baxter (2002, 2004) and by Dick (2015). Early\\nsystematic studies of algorithms like this were done by Sutton (1984).\\nThe term soft-max for the action selection rule(2.11) is due to Bridle (1990).\\nThis rule appears to have been ﬁrst proposed by Luce (1959).\\n2.9 The termassociative searchand the corresponding problem were introduced by\\nBarto, Sutton, and Brouwer (1981). The termassociative reinforcement learning\\nhas also been used for associative search (Barto and Anandan, 1985), but we\\nprefer to reserve that term as a synonym for the full reinforcement learning\\nproblem (as in Sutton, 1984). (And, as we noted, the modern literature also\\nuses the term “contextual bandits” for this problem.) We note that Thorndike’s\\nLaw of E↵ect (quoted in Chapter 1) describes associative search by referring\\nto the formation of associative links between situations (states) and actions.\\nAccording to the terminology of operant, or instrumental, conditioning (e.g.,\\nSkinner, 1938), a discriminative stimulus is a stimulus that signals the presence\\nof a particular reinforcement contingency. In our terms, di↵erent discriminative\\nstimuli correspond to di↵erent states.\\n2.10 Bellman (1956) was the ﬁrst to show how dynamic programming could be used\\nto compute the optimal balance between exploration and exploitation within a\\nBayesian formulation of the problem. The Gittins index approach is due to Gittins\\nand Jones (1974). Du↵ (1995) showed how it is possible to learn Gittins indices\\nfor bandit problems through reinforcement learning. The survey by Kumar (1985)\\nprovides a good discussion of Bayesian and non-Bayesian approaches to these\\nproblems. The term information state comes from the literature on partially\\nobservable MDPs; see, for example, Lovejoy (1991).\\nOther theoretical research focuses on the e\\x00ciency of exploration, usually ex-\\npressed as how quickly an algorithm can approach an optimal decision-making\\npolicy. One way to formalize exploration e\\x00ciency is by adapting to reinforcement\\nlearning the notion ofsample complexity for a supervised learning algorithm,\\nwhich is the number of training examples the algorithm needs to attain a desired\\ndegree of accuracy in learning the target function. A deﬁnition of the sample\\ncomplexity of exploration for a reinforcement learning algorithm is the number of\\ntime steps in which the algorithm does not select near-optimal actions (Kakade,\\n2003). Li (2012) discusses this and several other approaches in a survey of theo-\\nretical approaches to exploration e\\x00ciency in reinforcement learning. A thorough\\nmodern treatment of Thompson sampling is provided by Russo et al. (2018).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 67, 'page_label': '68'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 68, 'page_label': '69'}, page_content='Chapter 3\\nFinite Markov Decision\\nProcesses\\nIn this chapter we introduce the formal problem of ﬁnite Markov decision processes, or\\nﬁnite MDPs, which we try to solve in the rest of the book. This problem involves evaluative\\nfeedback, as in bandits, but also an associative aspect—choosing di↵erent actions in\\ndi↵erent situations. MDPs are a classical formalization of sequential decision making,\\nwhere actions inﬂuence not just immediate rewards, but also subsequent situations,\\nor states, and through those future rewards. Thus MDPs involve delayed reward and\\nthe need to trade o↵ immediate and delayed reward. Whereas in bandit problems we\\nestimated the valueq⇤(a) of each actiona, in MDPs we estimate the valueq⇤(s, a) of\\neach actiona in each states, or we estimate the valuev⇤(s) of each state given optimal\\naction selections. These state-dependent quantities are essential to accurately assigning\\ncredit for long-term consequences to individual action selections.\\nMDPs are a mathematically idealized form of the reinforcement learning problem\\nfor which precise theoretical statements can be made. We introduce key elements of\\nthe problem’s mathematical structure, such as returns, value functions, and Bellman\\nequations. We try to convey the wide range of applications that can be formulated as\\nﬁnite MDPs. As in all of artiﬁcial intelligence, there is a tension between breadth of\\napplicability and mathematical tractability. In this chapter we introduce this tension\\nand discuss some of the trade-o↵s and challenges that it implies. Some ways in which\\nreinforcement learning can be taken beyond MDPs are treated in Chapter 17.\\n3.1 The Agent–Environment Interface\\nMDPs are meant to be a straightforward framing of the problem of learning from\\ninteraction to achieve a goal. The learner and decision maker is called theagent.T h e\\nthing it interacts with, comprising everything outside the agent, is called theenvironment.\\nThese interact continually, the agent selecting actions and the environment responding to'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 69, 'page_label': '70'}, page_content='48 Chapter 3: Finite Markov Decision Processes\\nthese actions and presenting new situations to the agent.1 The environment also gives\\nrise to rewards, special numerical values that the agent seeks to maximize over time\\nthrough its choice of actions.\\nAgent\\nEnvironment\\nactionAt\\nrewardRt\\nstateSt Rt+1\\nSt+1\\nFigure 3.1: The agent–environment interaction in a Markov decision process.\\nMore speciﬁcally, the agent and environment interact at each of a sequence of discrete\\ntime steps, t =0 , 1, 2, 3,... .2 At each time stept, the agent receives some representation\\nof the environment’sstate, St 2 S, and on that basis selects anaction, At 2 A(s).3 One\\ntime step later, in part as a consequence of its action, the agent receives a numerical\\nreward, Rt+1 2 R ⇢ R, and ﬁnds itself in a new state,St+1.4 The MDP and agent\\ntogether thereby give rise to a sequence ortrajectory that begins like this:\\nS0,A 0,R 1,S 1,A 1,R 2,S 2,A 2,R 3,... (3.1)\\nIn aﬁnite MDP, the sets of states, actions, and rewards (S, A, andR) all have a ﬁnite\\nnumber of elements. In this case, the random variablesRt and St have well deﬁned\\ndiscrete probability distributions dependent only on the preceding state and action. That\\nis, for particular values of these random variables,s0 2 S and r 2 R, there is a probability\\nof those values occurring at timet, given particular values of the preceding state and\\naction:\\np(s0,r |s, a) .=P r{St =s0,R t =r | St\\x001 =s, At\\x001 =a}, (3.2)\\nfor alls0,s 2 S, r 2 R, anda 2 A(s). The functionp deﬁnes thedynamics of the MDP.\\nThe dot over the equals sign in the equation reminds us that it is a deﬁnition (in this\\ncase of the functionp) rather than a fact that follows from previous deﬁnitions. The\\ndynamics functionp : S ⇥ R ⇥ S ⇥ A ! [0, 1] is an ordinary deterministic function of four\\narguments. The ‘|’ in the middle of it comes from the notation for conditional probability,\\n1We use the termsagent, environment,a n daction instead of the engineers’ termscontroller, controlled\\nsystem (or plant), andcontrol signal because they are meaningful to a wider audience.\\n2We restrict attention to discrete time to keep things as simple as possible, even though many of the\\nideas can be extended to the continuous-time case (e.g., see Bertsekas and Tsitsiklis, 1996; Doya, 1996).\\n3To simplify notation, we sometimes assume the special case in which the action set is the same in all\\nstates and write it simply asA.\\n4We useRt+1 instead of Rt to denote the reward due toAt because it emphasizes that the next\\nreward and next state,Rt+1 and St+1,a r ej o i n t l yd e t e r m i n e d . U n f o r t u n a t e l y ,b o t hc o n v e n t i o n sa r e\\nwidely used in the literature.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 70, 'page_label': '71'}, page_content='3.1. The Agent–Environment Interface 49\\nbut here it just reminds us thatp speciﬁes a probability distribution for each choice ofs\\nand a, that is, that\\nX\\ns02S\\nX\\nr2R\\np(s0,r |s, a)=1 , for alls 2 S,a 2 A(s). (3.3)\\nIn aMarkov decision process, the probabilities given byp completely characterize the\\nenvironment’s dynamics. That is, the probability of each possible value forSt and Rt\\ndepends on the immediately preceding state and action,St\\x001 and At\\x001, and, given them,\\nnot at all on earlier states and actions. This is best viewed as a restriction not on the\\ndecision process, but on thestate. The state must include information about all aspects\\nof the past agent–environment interaction that make a di↵erence for the future. If it\\ndoes, then the state is said to have theMarkov property. We will assume the Markov\\nproperty throughout this book, though starting in Part II we will consider approximation\\nmethods that do not rely on it, and in Chapter 17 we consider how a Markov state can\\nbe e\\x00ciently learned and constructed from non-Markov observations.\\nFrom the four-argument dynamics function,p, one can compute anything else one might\\nwant to know about the environment, such as thestate-transition probabilities(which we\\ndenote, with a slight abuse of notation, as a three-argument functionp : S⇥S⇥A ! [0, 1]),\\np(s0|s, a) .=P r{St =s0 | St\\x001 =s, At\\x001 =a} =\\nX\\nr2R\\np(s0,r |s, a). (3.4)\\nWe can also compute the expected rewards for state–action pairs as a two-argument\\nfunction r : S ⇥ A ! R:\\nr(s, a) .= E[Rt | St\\x001 =s, At\\x001 =a]=\\nX\\nr2R\\nr\\nX\\ns02S\\np(s0,r |s, a), (3.5)\\nand the expected rewards for state–action–next-state triples as a three-argument function\\nr : S ⇥ A ⇥ S ! R,\\nr(s, a, s0) .= E[Rt | St\\x001 =s, At\\x001 =a, St = s0]=\\nX\\nr2R\\nr p(s0,r |s, a)\\np(s0|s, a) . (3.6)\\nIn this book, we usually use the four-argumentp function (3.2), but each of these other\\nnotations are also occasionally convenient.\\nThe MDP framework is abstract and ﬂexible and can be applied to many di↵erent\\nproblems in many di↵erent ways. For example, the time steps need not refer to ﬁxed\\nintervals of real time; they can refer to arbitrary successive stages of decision making\\nand acting. The actions can be low-level controls, such as the voltages applied to the\\nmotors of a robot arm, or high-level decisions, such as whether or not to have lunch or\\nto go to graduate school. Similarly, the states can take a wide variety of forms. They\\ncan be completely determined by low-level sensations, such as direct sensor readings, or\\nthey can be more high-level and abstract, such as symbolic descriptions of objects in a\\nroom. Some of what makes up a state could be based on memory of past sensations or'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 71, 'page_label': '72'}, page_content='50 Chapter 3: Finite Markov Decision Processes\\neven be entirely mental or subjective. For example, an agent could be in the state of not\\nbeing sure where an object is, or of having just been surprised in some clearly deﬁned\\nsense. Similarly, some actions might be totally mental or computational. For example,\\nsome actions might control what an agent chooses to think about, or where it focuses its\\nattention. In general, actions can be any decisions we want to learn how to make, and\\nstates can be anything we can know that might be useful in making them.\\nIn particular, the boundary between agent and environment is typically not the same\\nas the physical boundary of a robot’s or an animal’s body. Usually, the boundary is\\ndrawn closer to the agent than that. For example, the motors and mechanical linkages of\\na robot and its sensing hardware should usually be considered parts of the environment\\nrather than parts of the agent. Similarly, if we apply the MDP framework to a person\\nor animal, the muscles, skeleton, and sensory organs should be considered part of the\\nenvironment. Rewards, too, presumably are computed inside the physical bodies of\\nnatural and artiﬁcial learning systems, but are considered external to the agent.\\nThe general rule we follow is that anything that cannot be changed arbitrarily by\\nthe agent is considered to be outside of it and thus part of its environment. We do\\nnot assume that everything in the environment is unknown to the agent. For example,\\nthe agent often knows quite a bit about how its rewards are computed as a function of\\nits actions and the states in which they are taken. But we always consider the reward\\ncomputation to be external to the agent because it deﬁnes the task facing the agent and\\nthus must be beyond its ability to change arbitrarily. In fact, in some cases the agent may\\nknow everything about how its environment works and still face a di\\x00cult reinforcement\\nlearning task, just as we may know exactly how a puzzle like Rubik’s cube works, but\\nstill be unable to solve it. The agent–environment boundary represents the limit of the\\nagent’sabsolute control, not of its knowledge.\\nThe agent–environment boundary can be located at di↵erent places for di↵erent\\npurposes. In a complicated robot, many di↵erent agents may be operating at once, each\\nwith its own boundary. For example, one agent may make high-level decisions which form\\npart of the states faced by a lower-level agent that implements the high-level decisions. In\\npractice, the agent–environment boundary is determined once one has selected particular\\nstates, actions, and rewards, and thus has identiﬁed a speciﬁc decision-making task of\\ninterest.\\nThe MDP framework is a considerable abstraction of the problem of goal-directed\\nlearning from interaction. It proposes that whatever the details of the sensory, memory,\\nand control apparatus, and whatever objective one is trying to achieve, any problem of\\nlearning goal-directed behavior can be reduced to three signals passing back and forth\\nbetween an agent and its environment: one signal to represent the choices made by the\\nagent (the actions), one signal to represent the basis on which the choices are made (the\\nstates), and one signal to deﬁne the agent’s goal (the rewards). This framework may not\\nbe su\\x00cient to represent all decision-learning problems usefully, but it has proved to be\\nwidely useful and applicable.\\nOf course, the particular states and actions vary greatly from task to task, and how\\nthey are represented can strongly a↵ect performance. In reinforcement learning, as in\\nother kinds of learning, such representational choices are at present more art than science.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 72, 'page_label': '73'}, page_content='3.1. The Agent–Environment Interface 51\\nIn this book we o↵er some advice and examples regarding good ways of representing\\nstates and actions, but our primary focus is on general principles for learning how to\\nbehave once the representations have been selected.\\nExample 3.1: BioreactorSuppose reinforcement learning is being applied to determine\\nmoment-by-moment temperatures and stirring rates for a bioreactor (a large vat of\\nnutrients and bacteria used to produce useful chemicals). The actions in such an\\napplication might be target temperatures and target stirring rates that are passed to\\nlower-level control systems that, in turn, directly activate heating elements and motors to\\nattain the targets. The states are likely to be thermocouple and other sensory readings,\\nperhaps ﬁltered and delayed, plus symbolic inputs representing the ingredients in the\\nvat and the target chemical. The rewards might be moment-by-moment measures of the\\nrate at which the useful chemical is produced by the bioreactor. Notice that here each\\nstate is a list, or vector, of sensor readings and symbolic inputs, and each action is a\\nvector consisting of a target temperature and a stirring rate. It is typical of reinforcement\\nlearning tasks to have states and actions with such structured representations. Rewards,\\non the other hand, are always single numbers.\\nExample 3.2: Pick-and-Place Robot Consider using reinforcement learning to\\ncontrol the motion of a robot arm in a repetitive pick-and-place task. If we want to learn\\nmovements that are fast and smooth, the learning agent will have to control the motors\\ndirectly and have low-latency information about the current positions and velocities\\nof the mechanical linkages. The actions in this case might be the voltages applied to\\neach motor at each joint, and the states might be the latest readings of joint angles and\\nvelocities. The reward might be +1 for each object successfully picked up and placed. To\\nencourage smooth movements, on each time step a small, negative reward could be given\\nas a function of the moment-to-moment jerkiness of the motion.\\nExercise 3.1 Devise three example tasks of your own that ﬁt into the MDP framework,\\nidentifying for each its states, actions, and rewards. Make the three examples asdi↵erent\\nfrom each other as possible. The framework is abstract and ﬂexible and can be applied in\\nmany di↵erent ways. Stretch its limits in some way in at least one of your examples.⇤\\nExercise 3.2 Is the MDP framework adequate to usefully representall goal-directed\\nlearning tasks? Can you think of any clear exceptions? ⇤\\nExercise 3.3 Consider the problem of driving. You could deﬁne the actions in terms of\\nthe accelerator, steering wheel, and brake, that is, where your body meets the machine.\\nOr you could deﬁne them farther out—say, where the rubber meets the road, considering\\nyour actions to be tire torques. Or you could deﬁne them farther in—say, where your\\nbrain meets your body, the actions being muscle twitches to control your limbs. Or you\\ncould go to a really high level and say that your actions are your choices ofwhere to drive.\\nWhat is the right level, the right place to draw the line between agent and environment?\\nOn what basis is one location of the line to be preferred over another? Is there any\\nfundamental reason for preferring one location over another, or is it a free choice?⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 73, 'page_label': '74'}, page_content='52 Chapter 3: Finite Markov Decision Processes\\nExample 3.3 Recycling Robot\\nA mobile robot has the job of collecting empty soda cans in an o\\x00ce environment. It\\nhas sensors for detecting cans, and an arm and gripper that can pick them up and place\\nthem in an onboard bin; it runs on a rechargeable battery. The robot’s control system\\nhas components for interpreting sensory information, for navigating, and for controlling\\nthe arm and gripper. High-level decisions about how to search for cans are made by a\\nreinforcement learning agent based on the current charge level of the battery. To make a\\nsimple example, we assume that only two charge levels can be distinguished, comprising\\nas m a l ls t a t es e tS = {high, low}.I n e a c h s t a t e , t h e a g e n t c a n d e c i d e w h e t h e r t o ( 1 )\\nactively search for a can for a certain period of time, (2) remain stationary andwait\\nfor someone to bring it a can, or (3) head back to its home base torecharge its battery.\\nWhen the energy level ishigh, recharging would always be foolish, so we do not include it\\nin the action set for this state. The action sets are thenA(high)= {search, wait} and\\nA(low)= {search, wait, recharge}.\\nThe rewards are zero most of the time, but become positive when the robot secures an\\nempty can, or large and negative if the battery runs all the way down. The best way to\\nﬁnd cans is to actively search for them, but this runs down the robot’s battery, whereas\\nwaiting does not. Whenever the robot is searching, the possibility exists that its battery\\nwill become depleted. In this case the robot must shut down and wait to be rescued\\n(producing a low reward). If the energy level ishigh, then a period of active search can\\nalways be completed without risk of depleting the battery. A period of searching that\\nbegins with ahigh energy level leaves the energy levelhigh with probability↵ and reduces\\nit to low with probability 1\\x00 ↵. On the other hand, a period of searching undertaken\\nwhen the energy level islow leaves itlow with probability\\x00 and depletes the battery with\\nprobability 1\\x00 \\x00. In the latter case, the robot must be rescued, and the battery is then\\nrecharged back tohigh. Each can collected by the robot counts as a unit reward, whereas\\na reward of\\x003 results whenever the robot has to be rescued. Letrsearch and rwait,w i t h\\nrsearch >r wait,d e n o t et h ee x p e c t e dn u m b e ro fc a n st h er o b o tw i l lc o l l e c t( a n dh e n c et h e\\nexpected reward) while searching and while waiting respectively. Finally, suppose that no\\ncans can be collected during a run home for recharging, and that no cans can be collected\\non a step in which the battery is depleted. This system is then a ﬁnite MDP, and we\\ncan write down the transition probabilities and the expected rewards, with dynamics as\\nindicated in the table on the left:\\nsa s 0 p(s0 | s, a) r(s, a, s0)\\nhigh search high ↵ rsearch\\nhigh search low 1 \\x00 ↵ rsearch\\nlow search high 1 \\x00 \\x00 \\x003\\nlow search low \\x00 rsearch\\nhigh wait high 1 rwait\\nhigh wait low 0 -\\nlow wait high 0 -\\nlow wait low 1 rwait\\nlow recharge high 1 0\\nlow recharge low 0 -\\nsearch\\nhigh low1,  0\\nsearch\\nrecharge\\nwait\\nwait\\n\\x00,rsearch\\n↵,rsearch 1\\x00↵,rsearch\\n1\\x00\\x00,\\x003\\n1,rwait\\n1,rwait\\nNote that there is a row in the table for each possible combination of current state,s,\\naction, a 2 A(s), and next state,s0. Some transitions have zero probability of occurring,\\nso no expected reward is speciﬁed for them. Shown on the right is another useful way of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 74, 'page_label': '75'}, page_content='3.2. Goals and Rewards 53\\nsummarizing the dynamics of a ﬁnite MDP, as atransition graph. There are two kinds of\\nnodes: state nodes and action nodes. There is a state node for each possible state (a large\\nopen circle labeled by the name of the state), and an action node for each state–action\\npair (a small solid circle labeled by the name of the action and connected by a line to the\\nstate node). Starting in states and taking actiona moves you along the line from state\\nnode s to action node (s, a). Then the environment responds with a transition to the next\\nstate’s node via one of the arrows leaving action node (s, a). Each arrow corresponds to\\na triple (s, s0,a ), where s0 is the next state, and we label the arrow with the transition\\nprobability, p(s0 |s, a), and the expected reward for that transition,r(s, a, s0). Note that\\nthe transition probabilities labeling the arrows leaving an action node always sum to 1.\\nExercise 3.4 Give a table analogous to that in Example 3.3, but forp(s0,r |s, a). It\\nshould have columns fors, a, s0, r, andp(s0,r |s, a), and a row for every 4-tuple for which\\np(s0,r |s, a) > 0. ⇤\\n3.2 Goals and Rewards\\nIn reinforcement learning, the purpose or goal of the agent is formalized in terms of a\\nspecial signal, called thereward, passing from the environment to the agent. At each time\\nstep, the reward is a simple number,Rt 2 R. Informally, the agent’s goal is to maximize\\nthe total amount of reward it receives. This means maximizing not immediate reward,\\nbut cumulative reward in the long run. We can clearly state this informal idea as the\\nreward hypothesis:\\nThat all of what we mean by goals and purposes can be well thought of as\\nthe maximization of the expected value of the cumulative sum of a received\\nscalar signal (called reward).\\nThe use of a reward signal to formalize the idea of a goal is one of the most distinctive\\nfeatures of reinforcement learning.\\nAlthough formulating goals in terms of reward signals might at ﬁrst appear limiting,\\nin practice it has proved to be ﬂexible and widely applicable. The best way to see this is\\nto consider examples of how it has been, or could be, used. For example, to make a robot\\nlearn to walk, researchers have provided reward on each time step proportional to the\\nrobot’s forward motion. In making a robot learn how to escape from a maze, the reward\\nis often\\x001 for every time step that passes prior to escape; this encourages the agent to\\nescape as quickly as possible. To make a robot learn to ﬁnd and collect empty soda cans\\nfor recycling, one might give it a reward of zero most of the time, and then a reward of\\n+1 for each can collected. One might also want to give the robot negative rewards when\\nit bumps into things or when somebody yells at it. For an agent to learn to play checkers\\nor chess, the natural rewards are +1 for winning,\\x001 for losing, and 0 for drawing and\\nfor all nonterminal positions.\\nYou can see what is happening in all of these examples. The agent always learns to\\nmaximize its reward. If we want it to do something for us, we must provide rewards\\nto it in such a way that in maximizing them the agent will also achieve our goals. It'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 75, 'page_label': '76'}, page_content='54 Chapter 3: Finite Markov Decision Processes\\nis thus critical that the rewards we set up truly indicate what we want accomplished.\\nIn particular, the reward signal is not the place to impart to the agent prior knowledge\\nabout how to achieve what we want it to do.5 For example, a chess-playing agent should\\nbe rewarded only for actually winning, not for achieving subgoals such as taking its\\nopponent’s pieces or aining control of the center of the board. If achieving these sorts\\nof subgoals were rewarded, then the agent might ﬁnd a way to achieve them without\\nachieving the real goal. For example, it might ﬁnd a way to take the opponent’s pieces\\neven at the cost of losing the game. The reward signal is your way of communicating to\\nthe agentwhat you want achieved, nothow you want it achieved.6\\n3.3 Returns and Episodes\\nSo far we have discussed informally the objective of learning. We have said that the\\nagent’s goal is to maximize the cumulative reward it receives in the long run. How might\\nthis be deﬁned formally? If the sequence of rewards received after time stept is denoted\\nRt+1,R t+2,R t+3,... , then what precise aspect of this sequence do we wish to maximize?\\nIn general, we seek to maximize theexpected return, where the return, denotedGt,i s\\ndeﬁned as some speciﬁc function of the reward sequence. In the simplest case the return\\nis the sum of the rewards:\\nGt\\n.= Rt+1 + Rt+2 + Rt+3 + ··· + RT , (3.7)\\nwhere T is a ﬁnal time step. This approach makes sense in applications in which there\\nis a natural notion of ﬁnal time step, that is, when the agent–environment interaction\\nbreaks naturally into subsequences, which we callepisodes,7 such as plays of a game,\\ntrips through a maze, or any sort of repeated interaction. Each episode ends in a special\\nstate called theterminal state, followed by a reset to a standard starting state or to a\\nsample from a standard distribution of starting states. Even if you think of episodes as\\nending in di↵erent ways, such as winning and losing a game, the next episode begins\\nindependently of how the previous one ended. Thus the episodes can all be considered to\\nend in the same terminal state, with di↵erent rewards for the di↵erent outcomes. Tasks\\nwith episodes of this kind are calledepisodic tasks. In episodic tasks we sometimes need\\nto distinguish the set of all nonterminal states, denotedS, from the set of all states plus\\nthe terminal state, denotedS+. The time of termination,T, is a random variable that\\nnormally varies from episode to episode.\\nOn the other hand, in many cases the agent–environment interaction does not break\\nnaturally into identiﬁable episodes, but goes on continually without limit. For example,\\nthis would be the natural way to formulate an on-going process-control task, or an\\napplication to a robot with a long life span. We call thesecontinuing tasks.T h er e t u r n\\nformulation (3.7) is problematic for continuing tasks because the ﬁnal time step would be\\nT = 1, and the return, which is what we are trying to maximize, could easily be inﬁnite.\\n5Better places for imparting this kind of prior knowledge are the initial policy or initial value function.\\n6Section 17.4 delves further into the issue of designing e↵ective reward signals.\\n7Episodes are sometimes called “trials” in the literature.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 76, 'page_label': '77'}, page_content='3.3. Returns and Episodes 55\\n(For example, suppose the agent receives a reward of +1 at each time step.) Thus, in this\\nbook we usually use a deﬁnition of return that is slightly more complex conceptually but\\nmuch simpler mathematically.\\nThe additional concept that we need is that ofdiscounting. According to this approach,\\nthe agent tries to select actions so that the sum of the discounted rewards it receives over\\nthe future is maximized. In particular, it choosesAt to maximize the expecteddiscounted\\nreturn:\\nGt\\n.= Rt+1 + \\x00Rt+2 + \\x002Rt+3 + ··· =\\n1X\\nk=0\\n\\x00kRt+k+1, (3.8)\\nwhere \\x00 is a parameter, 0\\uf8ff \\x00 \\uf8ff 1, called thediscount rate.\\nThe discount rate determines the present value of future rewards: a reward received\\nk time steps in the future is worth only\\x00k\\x001 times what it would be worth if it were\\nreceived immediately. If\\x00< 1, the inﬁnite sum in(3.8) has a ﬁnite value as long as the\\nreward sequence{Rk} is bounded. If\\x00 = 0, the agent is “myopic” in being concerned\\nonly with maximizing immediate rewards: its objective in this case is to learn how to\\nchoose At so as to maximize onlyRt+1. If each of the agent’s actions happened to\\ninﬂuence only the immediate reward, not future rewards as well, then a myopic agent\\ncould maximize (3.8) by separately maximizing each immediate reward. But in general,\\nacting to maximize immediate reward can reduce access to future rewards so that the\\nreturn is reduced. As\\x00 approaches 1, the return objective takes future rewards into\\naccount more strongly; the agent becomes more farsighted.\\nReturns at successive time steps are related to each other in a way that is important\\nfor the theory and algorithms of reinforcement learning:\\nGt\\n.= Rt+1 + \\x00Rt+2 + \\x002Rt+3 + \\x003Rt+4 + ···\\n= Rt+1 + \\x00\\n\\x00\\nRt+2 + \\x00Rt+3 + \\x002Rt+4 + ···\\n\\x00\\n= Rt+1 + \\x00Gt+1 (3.9)\\nNote that this works for all time stepst<T , even if termination occurs att + 1, provided\\nwe deﬁneGT = 0. This often makes it easy to compute returns from reward sequences.\\nNote that although the return(3.8) is a sum of an inﬁnite number of terms, it is still\\nﬁnite if the reward is nonzero and constant—if\\x00< 1. For example, if the reward is a\\nconstant +1, then the return is\\nGt =\\n1X\\nk=0\\n\\x00k = 1\\n1 \\x00 \\x00 . (3.10)\\nExercise 3.5 The equations in Section 3.1 are for the continuing case and need to be\\nmodiﬁed (very slightly) to apply to episodic tasks. Show that you know the modiﬁcations\\nneeded by giving the modiﬁed version of (3.3). ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 77, 'page_label': '78'}, page_content='56 Chapter 3: Finite Markov Decision Processes\\nExample 3.4: Pole-Balancing\\nThe objective in this task is to apply\\nforces to a cart moving along a track\\nso as to keep a pole hinged to the cart\\nfrom falling over: A failure is said to\\noccur if the pole falls past a given angle\\nfrom vertical or if the cart runs o↵ the\\ntrack. The pole is reset to vertical\\nafter each failure. This task could be\\ntreated as episodic, where the natural\\nepisodes are the repeated attempts to balance the pole. The reward in this case could be\\n+1 for every time step on which failure did not occur, so that the return at each time\\nwould be the number of steps until failure. In this case, successful balancing forever would\\nmean a return of inﬁnity. Alternatively, we could treat pole-balancing as a continuing\\ntask, using discounting. In this case the reward would be\\x001 on each failure and zero at\\nall other times. The return at each time would then be related to\\x00\\x00K\\x001,w h e r eK is\\nthe number of time steps before failure (as well as to the times of later failures). In either\\ncase, the return is maximized by keeping the pole balanced for as long as possible.\\nExercise 3.6 Suppose you treated pole-balancing as an episodic task but also used\\ndiscounting, with all rewards zero except for\\x001 upon failure. What then would the\\nreturn be at each time? How does this return di↵er from that in the discounted, continuing\\nformulation of this task? ⇤\\nExercise 3.7 Imagine that you are designing a robot to run a maze. You decide to give it a\\nreward of +1 for escaping from the maze and a reward of zero at all other times. The task\\nseems to break down naturally into episodes—the successive runs through the maze—so\\nyou decide to treat it as an episodic task, where the goal is to maximize expected total\\nreward (3.7). After running the learning agent for a while, you ﬁnd that it is showing\\nno improvement in escaping from the maze. What is going wrong? Have you e↵ectively\\ncommunicated to the agent what you want it to achieve? ⇤\\nExercise 3.8 Suppose \\x00 =0 .5 and the following sequence of rewards is receivedR1 = \\x001,\\nR2 = 2, R3 = 6, R4 = 3, andR5 = 2, withT = 5. What areG0, G1, ... , G5? Hint:\\nWork backwards. ⇤\\nExercise 3.9 Suppose \\x00 =0 .9 and the reward sequence isR1 = 2 followed by an inﬁnite\\nsequence of 7s. What areG1 and G0? ⇤\\nExercise 3.10 Prove the second equality in (3.10). ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 78, 'page_label': '79'}, page_content='3.4. Uniﬁed Notation for Episodic and Continuing Tasks 57\\n3.4 Uniﬁed Notation for Episodic and Continuing Tasks\\nIn the preceding section we described two kinds of reinforcement learning tasks, one\\nin which the agent–environment interaction naturally breaks down into a sequence of\\nseparate episodes (episodic tasks), and one in which it does not (continuing tasks). The\\nformer case is mathematically easier because each action a↵ects only the ﬁnite number of\\nrewards subsequently received during the episode. In this book we consider sometimes\\none kind of problem and sometimes the other, but often both. It is therefore useful to\\nestablish one notation that enables us to talk precisely about both cases simultaneously.\\nTo be precise about episodic tasks requires some additional notation. Rather than one\\nlong sequence of time steps, we need to consider a series of episodes, each of which consists\\nof a ﬁnite sequence of time steps. We number the time steps of each episode starting\\nanew from zero. Therefore, we have to refer not just toSt, the state representation at\\ntime t,b u tt oSt,i, the state representation at timet of episode i (and similarly forAt,i,\\nRt,i, ⇡t,i, Ti, etc.). However, it turns out that when we discuss episodic tasks we almost\\nnever have to distinguish between di↵erent episodes. We are almost always considering\\na particular episode, or stating something that is true for all episodes. Accordingly, in\\npractice we almost always abuse notation slightly by dropping the explicit reference to\\nepisode number. That is, we writeSt to refer toSt,i, and so on.\\nWe need one other convention to obtain a single notation that covers both episodic\\nand continuing tasks. We have deﬁned the return as a sum over a ﬁnite number of terms\\nin one case (3.7) and as a sum over an inﬁnite number of terms in the other (3.8). These\\ntwo can be uniﬁed by considering episode termination to be the entering of a special\\nabsorbing state that transitions only to itself and that generates only rewards of zero. For\\nexample, consider the state transition diagram:\\nR1 = +1S0 S1 R2 = +1S2 R3 = +1 R4 = 0R5 = 0. . .\\nHere the solid square represents the special absorbing state corresponding to the end of an\\nepisode. Starting fromS0, we get the reward sequence +1, +1, +1, 0, 0, 0,... .S u m m i n g\\nthese, we get the same return whether we sum over the ﬁrstT rewards (hereT = 3) or\\nover the full inﬁnite sequence. This remains true even if we introduce discounting. Thus,\\nwe can deﬁne the return, in general, according to (3.8), using the convention of omitting\\nepisode numbers when they are not needed, and including the possibility that\\x00 =1i ft h e\\nsum remains deﬁned (e.g., because all episodes terminate). Alternatively, we can write\\nGt\\n.=\\nTX\\nk=t+1\\n\\x00k\\x00t\\x001Rk, (3.11)\\nincluding the possibility thatT = 1 or \\x00 = 1 (but not both). We use these conventions\\nthroughout the rest of the book to simplify notation and to express the close parallels'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 79, 'page_label': '80'}, page_content='58 Chapter 3: Finite Markov Decision Processes\\nbetween episodic and continuing tasks. (Later, in Chapter 10, we will introduce a\\nformulation that is both continuing and undiscounted.)\\n3.5 Policies and Value Functions\\nAlmost all reinforcement learning algorithms involve estimatingvalue functions—functions\\nof states (or of state–action pairs) that estimatehow good it is for the agent to be in a\\ngiven state (or how good it is to perform a given action in a given state). The notion\\nof “how good” here is deﬁned in terms of future rewards that can be expected, or, to\\nbe precise, in terms of expected return. Of course the rewards the agent can expect to\\nreceive in the future depend on what actions it will take. Accordingly, value functions\\nare deﬁned with respect to particular ways of acting, called policies.\\nFormally, apolicy is a mapping from states to probabilities of selecting each possible\\naction. If the agent is following policy⇡ at time t,t h e n⇡(a|s) is the probability that\\nAt = a if St = s. Like p, ⇡ is an ordinary function; the “|” in the middle of⇡(a|s)\\nmerely reminds us that it deﬁnes a probability distribution overa 2 A(s) for eachs 2 S.\\nReinforcement learning methods specify how the agent’s policy is changed as a result of\\nits experience.\\nExercise 3.11 If the current state isSt, and actions are selected according to a stochastic\\npolicy ⇡, then what is the expectation ofRt+1 in terms of⇡ and the four-argument\\nfunction p (3.2)? ⇤\\nThe value function of a states under a policy⇡, denotedv⇡(s), is the expected return\\nwhen starting ins and following⇡ thereafter. For MDPs, we can deﬁnev⇡ formally by\\nv⇡(s) .= E⇡[Gt | St =s]= E⇡\\n\" 1X\\nk=0\\n\\x00kRt+k+1\\n\\x00\\x00\\x00\\x00\\x00 St =s\\n#\\n, for alls 2 S, (3.12)\\nwhere E⇡[· ] denotes the expected value of a random variable given that the agent follows\\npolicy ⇡, and t is any time step. Note that the value of the terminal state, if any, is\\nalways zero. We call the functionv⇡ the state-value function for policy⇡.\\nSimilarly, we deﬁne the value of taking actiona in states under a policy⇡, denoted\\nq⇡(s, a), as the expected return starting froms, taking the actiona, and thereafter\\nfollowing policy⇡:\\nq⇡(s, a) .= E⇡[Gt | St =s, At = a]= E⇡\\n\" 1X\\nk=0\\n\\x00kRt+k+1\\n\\x00\\x00\\x00\\x00\\x00 St =s, At =a\\n#\\n. (3.13)\\nWe callq⇡ the action-value function for policy⇡.\\nExercise 3.12 Give an equation forv⇡ in terms ofq⇡ and ⇡. ⇤\\nExercise 3.13 Give an equation forq⇡ in terms ofv⇡ and the four-argumentp. ⇤\\nThe value functionsv⇡ and q⇡ can be estimated from experience. For example, if an\\nagent follows policy⇡ and maintains an average, for each state encountered, of the actual\\nreturns that have followed that state, then the average will converge to the state’s value,\\nv⇡(s), as the number of times that state is encountered approaches inﬁnity. If separate'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 80, 'page_label': '81'}, page_content='3.5. Policies and Value Functions 59\\naverages are kept for each action taken in each state, then these averages will similarly\\nconverge to the action values,q⇡(s, a). We call estimation methods of this kindMonte\\nCarlo methodsbecause they involve averaging over many random samples of actual returns.\\nThese kinds of methods are presented in Chapter 5. Of course, if there are very many\\nstates, then it may not be practical to keep separate averages for each state individually.\\nInstead, the agent would have to maintainv⇡ and q⇡ as parameterized functions (with\\nfewer parameters than states) and adjust the parameters to better match the observed\\nreturns. This can also produce accurate estimates, although much depends on the nature\\nof the parameterized function approximator. These possibilities are discussed in Part II\\nof the book.\\nA fundamental property of value functions used throughout reinforcement learning and\\ndynamic programming is that they satisfy recursive relationships similar to that which\\nwe have already established for the return(3.9). For any policy⇡ and any states,t h e\\nfollowing consistency condition holds between the value ofs and the value of its possible\\nsuccessor states:\\nv⇡(s) .= E⇡[Gt | St =s]\\n= E⇡[Rt+1 + \\x00Gt+1 | St =s] (by (3.9))\\n=\\nX\\na\\n⇡(a|s)\\nX\\ns0\\nX\\nr\\np(s0,r |s, a)\\nh\\nr + \\x00E⇡[Gt+1|St+1 =s0]\\ni\\n=\\nX\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇡(s0)\\ni\\n, for alls 2 S, (3.14)\\nwhere it is implicit that the actions,a, are taken from the setA(s), that the next states,\\ns0, are taken from the setS (or from S+ in the case of an episodic problem), and that\\nthe rewards, r, are taken from the setR. Note also how in the last equation we have\\nmerged the two sums, one over all the values ofs0 and the other over all the values ofr,\\ninto one sum over all the possible values of both. We use this kind of merged sum often\\nto simplify formulas. Note how the ﬁnal expression can be read easily as an expected\\nvalue. It is really a sum over all values of the three variables,a, s0, andr. For each triple,\\nwe compute its probability,⇡(a|s)p(s0,r |s, a), weight the quantity in brackets by that\\nprobability, then sum over all possibilities to get an expected value.\\n⇡\\ns\\ns0\\n⇡\\nrp\\na\\nBackup diagram forv⇡\\nEquation (3.14) is theBellman equation forv⇡.I te x p r e s s e s\\na relationship between the value of a state and the values of\\nits successor states. Think of looking ahead from a state to its\\npossible successor states, as suggested by the diagram to the\\nright. Each open circle represents a state and each solid circle\\nrepresents a state–action pair. Starting from states, the root\\nnode at the top, the agent could take any of some set of actions—\\nthree are shown in the diagram—based on its policy⇡. From\\neach of these, the environment could respond with one of several next states,s0 (two are\\nshown in the ﬁgure), along with a reward,r, depending on its dynamics given by the\\nfunction p. The Bellman equation (3.14) averages over all the possibilities, weighting each\\nby its probability of occurring. It states that the value of the start state must equal the\\n(discounted) value of the expected next state, plus the reward expected along the way.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 81, 'page_label': '82'}, page_content=\"60 Chapter 3: Finite Markov Decision Processes\\nThe value functionv⇡ is the unique solution to its Bellman equation. We show in\\nsubsequent chapters how this Bellman equation forms the basis of a number of ways to\\ncompute, approximate, and learnv⇡. We call diagrams like that abovebackup diagrams\\nbecause they diagram relationships that form the basis of the update orbackup operations\\nthat are at the heart of reinforcement learning methods. These operations transfer\\nvalue informationback to a state (or a state–action pair) from its successor states (or\\nstate–action pairs). We use backup diagrams throughout the book to provide graphical\\nsummaries of the algorithms we discuss. (Note that, unlike transition graphs, the state\\nnodes of backup diagrams do not necessarily represent distinct states; for example, a\\nstate might be its own successor.)\\nExample 3.5: GridworldFigure 3.2 (left) shows a rectangular gridworld representation\\nof a simple ﬁnite MDP. The cells of the grid correspond to the states of the environment. At\\neach cell, four actions are possible:north, south, east, andwest, which deterministically\\ncause the agent to move one cell in the respective direction on the grid. Actions that\\nwould take the agent o↵ the grid leave its location unchanged, but also result in a reward\\nof \\x001. Other actions result in a reward of 0, except those that move the agent out of the\\nspecial states A and B. From stateA, all four actions yield a reward of +10 and take the\\nagent toA0. From stateB, all actions yield a reward of +5 and take the agent toB0.\\n3.7. VALUE FUNCTIONS 63\\ns,as\\na\\ns'\\nr\\na'\\ns'\\nr\\n(b)(a)\\nFigure 3.4: Backup diagrams for (a)v⇡ and (b) q⇡.\\nthe states of the environment. At each cell, four actions are possible:north,\\nsouth, east,a n dwest, which deterministically cause the agent to move one\\ncell in the respective direction on the grid. Actions that would take the agent\\no\\x00 the grid leave its location unchanged, but also result in a reward of\\x001.\\nOther actions result in a reward of 0, except those that move the agent out\\nof the special states A and B. From state A, all four actions yield a reward of\\n+10 and take the agent to A\\x00. From state B, all actions yield a reward of +5\\nand take the agent to B\\x00.\\nSuppose the agent selects all four actions with equal probability in all\\nstates. Figure 3.5b shows the value function,v⇡, for this policy, for the dis-\\ncounted reward case with\\x00 =0 .9. This value function was computed by solv-\\ning the system of equations (3.10). Notice the negative values near the lower\\nedge; these are the result of the high probability of hitting the edge of the grid\\nthere under the random policy. State A is the best state to be in under this pol-\\nicy, but its expected return is less than 10, its immediate reward, because from\\nA the agent is taken to A\\x00, from which it is likely to run into the edge of the\\ngrid. State B, on the other hand, is valued more than 5, its immediate reward,\\nbecause from B the agent is taken to B\\x00, which has a positive value. From B\\x00 the\\nexpected penalty (negative reward) for possibly running into an edge is more\\n3.3 8.8 4.4 5.3 1.5\\n1.5 3.0 2.3 1.9 0.5\\n0.1 0.7 0.7 0.4 -0.4\\n-1.0-0.4-0.4-0.6-1.2\\n-1.9-1.3-1.2-1.4-2.0\\nA B\\nA'\\nB'+10\\n+5\\nActions\\n(a) (b)\\nFigure 3.5: Grid example: (a) exceptional reward dynamics; (b) state-value\\nfunction for the equiprobable random policy.\\nFigure 3.2: Gridworld example: exceptional reward dynamics (left) and state-value function\\nfor the equiprobable random policy (right).\\nSuppose the agent selects all four actions with equal probability in all states. Figure 3.2\\n(right) shows the value function,v⇡, for this policy, for the discounted reward case with\\n\\x00 =0 .9. This value function was computed by solving the system of linear equations\\n(3.14). Notice the negative values near the lower edge; these are the result of the high\\nprobability of hitting the edge of the grid there under the random policy. StateA is\\nthe best state to be in under this policy. Note thatA’s expected return isless than its\\nimmediate reward of 10, because fromA the agent is taken to stateA0 from which it is\\nlikely to run into the edge of the grid. StateB, on the other hand, is valuedmore than\\nits immediate reward of 5, because fromB the agent is taken toB0 which has a positive\\nvalue. FromB0 the expected penalty (negative reward) for possibly running into an edge\\nis more than compensated for by the expected gain for possibly stumbling ontoA or B.\\nExercise 3.14 The Bellman equation (3.14) must hold for each state for the value function\\nv⇡ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds\\nfor the center state, valued at +0.7, with respect to its four neighboring states, valued at\\n+2.3, +0.4, \\x000.4, and +0.7. (These numbers are accurate only to one decimal place.)⇤\"),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 82, 'page_label': '83'}, page_content='3.5. Policies and Value Functions 61\\nExercise 3.15 In the gridworld example, rewards are positive for goals, negative for\\nrunning into the edge of the world, and zero the rest of the time. Are the signs of these\\nrewards important, or only the intervals between them? Prove, using (3.8), that adding a\\nconstant c to all the rewards adds a constant,vc, to the values of all states, and thus\\ndoes not a↵ect the relative values of any states under any policies. What isvc in terms\\nof c and \\x00? ⇤\\nExercise 3.16 Now consider adding a constantc to all the rewards in an episodic task,\\nsuch as maze running. Would this have any e↵ect, or would it leave the task unchanged\\nas in the continuing task above? Why or why not? Give an example. ⇤\\nExample 3.6: Golf To formulate playing a hole of golf as a reinforcement learning\\ntask, we count a penalty (negative reward) of\\x001 for each stroke until we hit the ball\\ninto the hole. The state is the location of the ball. The value of a state is the negative of\\nthe number of strokes to the hole from that location. Our actions are how we aim and\\nswing at the ball, of course, and which club we select. Let us take the former as given\\nand consider just the choice of club, which we assume is either a putter or a driver. The\\nupper part of Figure 3.3 shows a possible state-value function,vputt(s), for the policy that\\nQ*(s,driver)\\nVputt\\nsand\\ngreen\\n!1\\nsand\\n!2!2!3\\n!4\\n!1\\n!5!6\\n!4\\n!3\\n!3 !2\\n!4\\nsand\\ngreen\\n!1\\nsand\\n!2\\n!3\\n!2\\n0\\n0\\n!\"\\n!\"\\nvputt\\nq⇤(s,driver)\\nFigure 3.3: A golf example: the state-value func-\\ntion for putting (upper) and the optimal action-\\nvalue function for using the driver (lower).\\nalways uses the putter. The terminal\\nstate in-the-hole has a value of 0. From\\nanywhere on the green we assume we can\\nmake a putt; these states have value\\x001.\\nO↵ the green we cannot reach the hole by\\nputting, and the value is lower. If we can\\nreach the green from a state by putting,\\nthen that state must have value one less\\nthan the green’s value, that is,\\x002. For\\nsimplicity, let us assume we can putt very\\nprecisely and deterministically, but with\\na limited range. This gives us the sharp\\ncontour line labeled\\x002 in the ﬁgure; all\\nlocations between that line and the green\\nrequire exactly two strokes to complete\\nthe hole. Similarly, any location within\\nputting range of the\\x002 contour line must\\nhave a value of\\x003, and so on to get all the\\ncontour lines shown in the ﬁgure. Putting\\ndoesn’t get us out of sand traps, so they\\nhave a value of\\x001. Overall, it takes us\\nsix strokes to get from the tee to the hole\\nby putting.\\nr s0\\ns, a\\na0⇡\\np\\nq⇡backup diagram\\nExercise 3.17 What is the Bellman equation for action values, that\\nis, forq⇡? It must give the action valueq⇡(s, a) in terms of the action\\nvalues, q⇡(s0,a 0), of possible successors to the state–action pair (s, a).\\nHint: The backup diagram to the right corresponds to this equation.\\nShow the sequence of equations analogous to (3.14), but for action\\nvalues. ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 83, 'page_label': '84'}, page_content='62 Chapter 3: Finite Markov Decision Processes\\nExercise 3.18 The value of a state depends on the values of the actions possible in that\\nstate and on how likely each action is to be taken under the current policy. We can\\nthink of this in terms of a small backup diagram rooted at the state and considering each\\npossible action:\\ns\\ntaken with\\nprobability⇡(a|s)\\nv⇡(s)\\nq⇡(s, a)\\na1 a2 a3\\nGive the equation corresponding to this intuition and diagram for the value at the root\\nnode, v⇡(s), in terms of the value at the expected leaf node,q⇡(s, a), givenSt = s.T h i s\\nequation should include an expectation conditioned on following the policy,⇡. Then give\\na second equation in which the expected value is written out explicitly in terms of⇡(a|s)\\nsuch that no expected value notation appears in the equation. ⇤\\nExercise 3.19 The value of an action,q⇡(s, a), depends on the expected next reward and\\nthe expected sum of the remaining rewards. Again we can think of this in terms of a\\nsmall backup diagram, this one rooted at an action (state–action pair) and branching to\\nthe possible next states:\\ns, a q⇡(s, a)\\ns0\\n3s0\\n2s0\\n1\\nr1 r2 r3\\nv⇡(s0)\\nexpected\\nrewards\\nGive the equation corresponding to this intuition and diagram for the action value,\\nq⇡(s, a), in terms of the expected next reward,Rt+1, and the expected next state value,\\nv⇡(St+1), given thatSt =s and At =a. This equation should include an expectation but\\nnot one conditioned on following the policy. Then give a second equation, writing out the\\nexpected value explicitly in terms ofp(s0,r |s, a)d e ﬁ n e db y(3.2), such that no expected\\nvalue notation appears in the equation. ⇤\\n3.6 Optimal Policies and Optimal Value Functions\\nSolving a reinforcement learning task means, roughly, ﬁnding a policy that achieves a lot\\nof reward over the long run. For ﬁnite MDPs, we can precisely deﬁne an optimal policy\\nin the following way. Value functions deﬁne a partial ordering over policies. A policy⇡ is\\ndeﬁned to be better than or equal to a policy⇡0 if its expected return is greater than\\nor equal to that of⇡0 for all states. In other words,⇡ \\x00 ⇡0 if and only ifv⇡(s) \\x00 v⇡0 (s)\\nfor alls 2 S. There is always at least one policy that is better than or equal to all other\\npolicies. This is anoptimal policy. Although there may be more than one, we denote all\\nthe optimal policies by⇡⇤. They share the same state-value function, called theoptimal\\nstate-value function, denotedv⇤, and deﬁned as\\nv⇤(s) .= max\\n⇡\\nv⇡(s), (3.15)\\nfor alls 2 S.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 84, 'page_label': '85'}, page_content='3.6. Optimal Policies and Optimal Value Functions 63\\nOptimal policies also share the sameoptimal action-value function, denotedq⇤, and\\ndeﬁned as\\nq⇤(s, a) .= max\\n⇡\\nq⇡(s, a), (3.16)\\nfor all s 2 S and a 2 A(s). For the state–action pair (s, a), this function gives the\\nexpected return for taking actiona in states and thereafter following an optimal policy.\\nThus, we can writeq⇤ in terms ofv⇤ as follows:\\nq⇤(s, a)= E[Rt+1 + \\x00v⇤(St+1) | St =s, At =a] . (3.17)\\nExample 3.7: Optimal Value Functions for GolfThe lower part of Figure 3.3\\nshows the contours of a possible optimal action-value functionq⇤(s, driver). These are\\nthe values of each state if we ﬁrst play a stroke with the driver and afterward select either\\nthe driver or the putter, whichever is better. The driver enables us to hit the ball farther,\\nbut with less accuracy. We can reach the hole in one shot using the driver only if we\\nare already very close; thus the\\x001 contour forq⇤(s, driver) covers only a small portion\\nof the green. If we have two strokes, however, then we can reach the hole from much\\nfarther away, as shown by the\\x002 contour. In this case we don’t have to drive all the way\\nto within the small\\x001 contour, but only to anywhere on the green; from there we can\\nuse the putter. The optimal action-value function gives the values after committing to a\\nparticular ﬁrst action, in this case, to the driver, but afterward using whichever actions\\nare best. The\\x003 contour is still farther out and includes the starting tee. From the tee,\\nthe best sequence of actions is two drives and one putt, sinking the ball in three strokes.\\nBecause v⇤ is the value function for a policy, it must satisfy the self-consistency\\ncondition given by the Bellman equation for state values (3.14). Because it is the optimal\\nvalue function, however,v⇤’s consistency condition can be written in a special form\\nwithout reference to any speciﬁc policy. This is the Bellman equation forv⇤, or the\\nBellman optimality equation. Intuitively, the Bellman optimality equation expresses the\\nfact that the value of a state under an optimal policy must equal the expected return for\\nthe best action from that state:\\nv⇤(s) = max\\na2A(s)\\nq⇡⇤ (s, a)\\n= max\\na\\nE⇡⇤[Gt | St =s, At =a]\\n= max\\na\\nE⇡⇤[Rt+1 + \\x00Gt+1 | St =s, At =a] (by (3.9))\\n= max\\na\\nE[Rt+1 + \\x00v⇤(St+1) | St =s, At =a] (3.18)\\n= max\\na\\nX\\ns0,r\\np(s0,r |s, a)\\n⇥\\nr + \\x00v⇤(s0)\\n⇤\\n. (3.19)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 85, 'page_label': '86'}, page_content='64 Chapter 3: Finite Markov Decision Processes\\nThe last two equations are two forms of the Bellman optimality equation forv⇤.T h e\\nBellman optimality equation forq⇤ is\\nq⇤(s, a)= E\\nh\\nRt+1 + \\x00 max\\na0\\nq⇤(St+1,a 0)\\n\\x00\\x00\\x00 St = s, At = a\\ni\\n=\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00 max\\na0\\nq⇤(s0,a 0)\\ni\\n. (3.20)\\nThe backup diagrams in the ﬁgure below show graphically the spans of future states\\nand actions considered in the Bellman optimality equations forv⇤ and q⇤. These are the\\nsame as the backup diagrams forv⇡ and q⇡ presented earlier except that arcs have been\\nadded at the agent’s choice points to represent that the maximum over that choice is\\ntaken rather than the expected value given some policy. The backup diagram on the left\\ngraphically represents the Bellman optimality equation(3.19) and the backup diagram\\non the right graphically represents (3.20).\\ns\\ns0\\na\\nr\\nr\\ns0\\ns, a\\na0\\nmax\\nmax\\n(v⇤) (q⇤)\\nFigure 3.4: Backup diagrams forv⇤ and q⇤\\nFor ﬁnite MDPs, the Bellman optimality equation forv⇤ (3.19) has a unique solution.\\nThe Bellman optimality equation is actually a system of equations, one for each state, so\\nif there aren states, then there aren equations in n unknowns. If the dynamicsp of the\\nenvironment are known, then in principle one can solve this system of equations forv⇤\\nusing any one of a variety of methods for solving systems of nonlinear equations. One\\ncan solve a related set of equations forq⇤.\\nOnce one hasv⇤, it is relatively easy to determine an optimal policy. For each state\\ns, there will be one or more actions at which the maximum is obtained in the Bellman\\noptimality equation. Any policy that assigns nonzero probability only to these actions is\\nan optimal policy. You can think of this as a one-step search. If you have the optimal\\nvalue function,v⇤, then the actions that appear best after a one-step search will be optimal\\nactions. Another way of saying this is that any policy that isgreedy with respect to the\\noptimal evaluation functionv⇤ is an optimal policy. The term greedy is used in computer\\nscience to describe any search or decision procedure that selects alternatives based only\\non local or immediate considerations, without considering the possibility that such a\\nselection may prevent future access to even better alternatives. Consequently, it describes\\npolicies that select actions based only on their short-term consequences. The beauty ofv⇤\\nis that if one uses it to evaluate the short-term consequences of actions—speciﬁcally, the\\none-step consequences—then a greedy policy is actually optimal in the long-term sense in\\nwhich we are interested becausev⇤ already takes into account the reward consequences of\\nall possible future behavior. By means ofv⇤, the optimal expected long-term return is'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 86, 'page_label': '87'}, page_content=\"3.6. Optimal Policies and Optimal Value Functions 65\\nturned into a quantity that is locally and immediately available for each state. Hence, a\\none-step-ahead search yields the long-term optimal actions.\\nHaving q⇤ makes choosing optimal actions even easier. Withq⇤, the agent does not\\neven have to do a one-step-ahead search: for any states, it can simply ﬁnd any action\\nthat maximizes q⇤(s, a). The action-value function e↵ectively caches the results of all\\none-step-ahead searches. It provides the optimal expected long-term return as a value\\nthat is locally and immediately available for each state–action pair. Hence, at the cost of\\nrepresenting a function of state–action pairs, instead of just of states, the optimal action-\\nvalue function allows optimal actions to be selected without having to know anything\\nabout possible successor states and their values, that is, without having to know anything\\nabout the environment’s dynamics.\\nExample 3.8: Solving the GridworldSuppose we solve the Bellman equation forv⇤\\nfor the simple grid task introduced in Example 3.5 and shown again in Figure 3.5 (left).\\nRecall that stateA is followed by a reward of +10 and transition to stateA0, while state\\nB is followed by a reward of +5 and transition to stateB0. Figure 3.5 (middle) shows the\\noptimal value function, and Figure 3.5 (right) shows the corresponding optimal policies.\\nWhere there are multiple arrows in a cell, all of the corresponding actions are optimal.\\na) gridworld b) V* c) !*\\n22.024.422.019.417.5\\n19.822.019.817.816.0\\n17.819.817.816.014.4\\n16.017.816.014.413.0\\n14.416.014.413.011.7\\nA B\\nA'\\nB'+10\\n+5\\nv* π*Gridworld v ⇤ ⇡ ⇤\\nFigure 3.5: Optimal solutions to the gridworld example.\\nExample 3.9: Bellman Optimality Equations for the Recycling RobotUsing\\n(3.19), we can explicitly give the Bellman optimality equation for the recycling robot\\nexample. To make things more compact, we abbreviate the stateshigh and low, and the\\nactions search, wait, andrecharge respectively byh, l, s, w, andre. Because there are\\nonly two states, the Bellman optimality equation consists of two equations. The equation\\nfor v⇤(h) can be written as follows:\\nv⇤(h) = max\\n⇢ p(h|h, s)[r(h, s, h)+ \\x00v⇤(h)] +p(l|h, s)[r(h, s, l)+ \\x00v⇤(l)],\\np(h|h, w)[r(h, w, h)+ \\x00v⇤(h)] +p(l|h, w)[r(h, w, l)+ \\x00v⇤(l)]\\n\\x00\\n= max\\n⇢\\n↵[rs + \\x00v⇤(h)] + (1\\x00 ↵)[rs + \\x00v⇤(l)],\\n1[rw + \\x00v⇤(h)] + 0[rw + \\x00v⇤(l)]\\n\\x00\\n= max\\n⇢ rs + \\x00[↵v⇤(h)+( 1\\x00 ↵)v⇤(l)],\\nrw + \\x00v⇤(h)\\n\\x00\\n.\"),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 87, 'page_label': '88'}, page_content='66 Chapter 3: Finite Markov Decision Processes\\nFollowing the same procedure forv⇤(l) yields the equation\\nv⇤(l) = max\\n8\\n<\\n:\\n\\x00rs \\x00 3(1 \\x00 \\x00)+ \\x00[(1 \\x00 \\x00)v⇤(h)+ \\x00v⇤(l)],\\nrw + \\x00v⇤(l),\\n\\x00v⇤(h)\\n9\\n=\\n;.\\nFor any choice ofrs, rw, ↵, \\x00, and \\x00,w i t h0\\uf8ff \\x00< 1, 0 \\uf8ff ↵, \\x00\\uf8ff 1, there is exactly\\none pair of numbers,v⇤(h) and v⇤(l), that simultaneously satisfy these two nonlinear\\nequations.\\nExplicitly solving the Bellman optimality equation provides one route to ﬁnding an\\noptimal policy, and thus to solving the reinforcement learning problem. However, this\\nsolution is rarely directly useful. It is akin to an exhaustive search, looking ahead at\\nall possibilities, computing their probabilities of occurrence and their desirabilities in\\nterms of expected rewards. This solution relies on at least three assumptions that are\\nrarely true in practice: (1) the dynamics of the environment are accurately known; (2)\\ncomputational resources are su\\x00cient to complete the calculation; and (3) the states\\nhave the Markov property. For the kinds of tasks in which we are interested, one is\\ngenerally not able to implement this solution exactly because various combinations of\\nthese assumptions are violated. For example, although the ﬁrst and third assumptions\\npresent no problems for the game of backgammon, the second is a major impediment.\\nBecause the game has about 1020 states, it would take thousands of years on today’s\\nfastest computers to solve the Bellman equation forv⇤, and the same is true for ﬁnding\\nq⇤. In reinforcement learning one typically has to settle for approximate solutions.\\nMany di↵erent decision-making methods can be viewed as ways of approximately\\nsolving the Bellman optimality equation. For example, heuristic search methods can be\\nviewed as expanding the right-hand side of (3.19) several times, up to some depth, forming\\na “tree” of possibilities, and then using a heuristic evaluation function to approximate\\nv⇤ at the “leaf” nodes. (Heuristic search methods such as A⇤ are almost always based\\non the episodic case.) The methods of dynamic programming can be related even more\\nclosely to the Bellman optimality equation. Many reinforcement learning methods can\\nbe clearly understood as approximately solving the Bellman optimality equation, using\\nactual experienced transitions in place of knowledge of the expected transitions. We\\nconsider a variety of such methods in the following chapters.\\nExercise 3.20 Draw or describe the optimal state-value function for the golf example.⇤\\nExercise 3.21 Draw or describe the contours of the optimal action-value function for\\nputting, q⇤(s, putter), for the golf example. ⇤\\n+20 0+1\\nleft right\\nExercise 3.22 Consider the continuing MDP shown to the\\nright. The only decision to be made is that in the top state,\\nwhere two actions are available,left and right.T h e n u m b e r s\\nshow the rewards that are received deterministically after\\neach action. There are exactly two deterministic policies,\\n⇡left and ⇡right. What policy is optimal if\\x00 = 0? If\\x00 =0 .9?\\nIf \\x00 =0 .5? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 88, 'page_label': '89'}, page_content='3.7. Optimality and Approximation 67\\nExercise 3.23 Give the Bellman equation forq⇤ for the recycling robot. ⇤\\nExercise 3.24 Figure 3.5 gives the optimal value of the best state of the gridworld as\\n24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express\\nthis value symbolically, and then to compute it to three decimal places. ⇤\\nExercise 3.25 Give an equation forv⇤ in terms ofq⇤. ⇤\\nExercise 3.26 Give an equation forq⇤ in terms ofv⇤ and the four-argumentp. ⇤\\nExercise 3.27 Give an equation for⇡⇤ in terms ofq⇤. ⇤\\nExercise 3.28 Give an equation for⇡⇤ in terms ofv⇤ and the four-argumentp. ⇤\\nExercise 3.29 Rewrite the four Bellman equations for the four value functions (v⇡, v⇤, q⇡,\\nand q⇤) in terms of the three argument functionp (3.4) and the two-argument functionr\\n(3.5). ⇤\\n3.7 Optimality and Approximation\\nWe have deﬁned optimal value functions and optimal policies. Clearly, an agent that\\nlearns an optimal policy has done very well, but in practice this rarely happens. For\\nthe kinds of tasks in which we are interested, optimal policies can be generated only\\nwith extreme computational cost. A well-deﬁned notion of optimality organizes the\\napproach to learning we describe in this book and provides a way to understand the\\ntheoretical properties of various learning algorithms, but it is an ideal that agents can\\nonly approximate. As we discussed above, even if we have a complete and accurate model\\nof the environment’s dynamics, it is usually not possible to simply compute an optimal\\npolicy by solving the Bellman optimality equation. For example, board games such as\\nchess are a tiny fraction of human experience, yet large, custom-designed computers still\\ncannot compute the optimal moves. A critical aspect of the problem facing the agent is\\nalways the computational power available to it, in particular, the amount of computation\\nit can perform in a single time step.\\nThe memory available is also an important constraint. A large amount of memory\\nis often required to build up approximations of value functions, policies, and models.\\nIn tasks with small, ﬁnite state sets, it is possible to form these approximations using\\narrays or tables with one entry for each state (or state–action pair). This we call the\\ntabular case, and the corresponding methods we call tabular methods. In many cases\\nof practical interest, however, there are far more states than could possibly be entries\\nin a table. In these cases the functions must be approximated, using some sort of more\\ncompact parameterized function representation.\\nOur framing of the reinforcement learning problem forces us to settle for approxi-\\nmations. However, it also presents us with some unique opportunities for achieving\\nuseful approximations. For example, in approximating optimal behavior, there may be\\nmany states that the agent faces with such a low probability that selecting suboptimal\\nactions for them has little impact on the amount of reward the agent receives. Tesauro’s\\nbackgammon player, for example, plays with exceptional skill even though it might make'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 89, 'page_label': '90'}, page_content='68 Chapter 3: Finite Markov Decision Processes\\nvery bad decisions on board conﬁgurations that never occur in games against experts. In\\nfact, it is possible that TD-Gammon makes bad decisions for a large fraction of the game’s\\nstate set. The online nature of reinforcement learning makes it possible to approximate\\noptimal policies in ways that put more e↵ort into learning to make good decisions for\\nfrequently encountered states, at the expense of less e↵ort for infrequently encountered\\nstates. This is one key property that distinguishes reinforcement learning from other\\napproaches to approximately solving MDPs.\\n3.8 Summary\\nLet us summarize the elements of the reinforcement learning problem that we have\\npresented in this chapter. Reinforcement learning is about learning from interaction\\nhow to behave in order to achieve a goal. The reinforcement learningagent and its\\nenvironment interact over a sequence of discrete time steps. The speciﬁcation of their\\ninterface deﬁnes a particular task: theactions are the choices made by the agent; the\\nstates are the basis for making the choices; and therewards are the basis for evaluating\\nthe choices. Everything inside the agent is known and controllable. Its environment, on\\nthe other hand, is incompletely controllable and may or may not be completely known.\\nA policy is a stochastic rule by which the agent selects actions as a function of states.\\nThe agent’s objective is to maximize the amount of reward it receives over time.\\nWhen the reinforcement learning setup described above is formulated with well deﬁned\\ntransition probabilities it constitutes aMarkov decision process(MDP). A ﬁnite MDP is\\nan MDP with ﬁnite state, action, and (as we formulate it here) reward sets. Much of the\\ncurrent theory of reinforcement learning is restricted to ﬁnite MDPs, but the methods\\nand ideas apply more generally.\\nThe return is the function of future rewards that the agent seeks to maximize (in\\nexpected value). It has several di↵erent deﬁnitions depending upon the nature of the\\ntask and whether one wishes todiscount delayed reward. The undiscounted formulation\\nis appropriate for episodic tasks, in which the agent–environment interaction breaks\\nnaturally intoepisodes; the discounted formulation is appropriate for tabularcontinuing\\ntasks, in which the interaction does not naturally break into episodes but continues\\nwithout limit (but see Sections 10.3–4). We try to deﬁne the returns for the two kinds of\\ntasks such that one set of equations can apply to both the episodic and continuing cases.\\nA policy’svalue functions (v⇡ and q⇡) assign to each state, or state–action pair, the\\nexpected return from that state, or state–action pair, given that the agent uses the\\npolicy. The optimal value functions(v⇤ and q⇤) assign to each state, or state–action pair,\\nthe largest expected return achievable by any policy. A policy whose value functions\\nare optimal is anoptimal policy. Whereas the optimal value functions for states and\\nstate–action pairs are unique for a given MDP, there can be many optimal policies. Any\\npolicy that isgreedy with respect to the optimal value functions must be an optimal\\npolicy. The Bellman optimality equations are special consistency conditions that the\\noptimal value functions must satisfy and that can, in principle, be solved for the optimal\\nvalue functions, from which an optimal policy can be determined with relative ease.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 90, 'page_label': '91'}, page_content='3.8. Summary 69\\nA reinforcement learning problem can be posed in a variety of di↵erent ways depending\\non assumptions about the level of knowledge initially available to the agent. In problems\\nof complete knowledge, the agent has a complete and accurate model of the environment’s\\ndynamics. If the environment is an MDP, then such a model consists of the complete four-\\nargument dynamics functionp (3.2). In problems ofincomplete knowledge, a complete\\nand perfect model of the environment is not available.\\nEven if the agent had a complete and accurate environment model, the agent would\\ntypically be unable to fully use it because of limitations on its memory and computation\\nper time step. In particular, extensive memory may be required to build up accurate\\napproximations of value functions, policies, and models. In most cases of practical interest\\nthere are far more states than could possibly be entries in a table, and approximations\\nmust be made.\\nA well-deﬁned notion of optimality organizes the approach to learning we describe in\\nthis book and provides a way to understand the theoretical properties of various learning\\nalgorithms, but it is an ideal that reinforcement learning agents can only approximate\\nto varying degrees. In reinforcement learning we are very much concerned with cases in\\nwhich optimal solutions cannot be found but must be approximated in some way.\\nBibliographical and Historical Remarks\\nThe reinforcement learning problem is deeply indebted to the idea of Markov decision\\nprocesses (MDPs) from the ﬁeld of optimal control. These historical inﬂuences and other\\nmajor inﬂuences from psychology are described in the brief history given in Chapter 1.\\nReinforcement learning adds to MDPs a focus on approximation and incomplete infor-\\nmation for realistically large problems. MDPs and the reinforcement learning problem\\nare only weakly linked to traditional learning and decision-making problems in artiﬁcial\\nintelligence. However, artiﬁcial intelligence is now vigorously exploring MDP formulations\\nfor planning and decision making from a variety of perspectives. MDPs are more general\\nthan previous formulations used in artiﬁcial intelligence in that they permit more general\\nkinds of goals and uncertainty.\\nThe theory of MDPs is treated by, for example, Bertsekas (2005), White (1969), Whittle\\n(1982, 1983), and Puterman (1994). A particularly compact treatment of the ﬁnite case\\nis given by Ross (1983). MDPs are also studied under the heading of stochastic optimal\\ncontrol, whereadaptive optimal control methods are most closely related to reinforcement\\nlearning (e.g., Kumar, 1985; Kumar and Varaiya, 1986).\\nThe theory of MDPs evolved from e↵orts to understand the problem of making sequences\\nof decisions under uncertainty, where each decision can depend on the previous decisions\\nand their outcomes. It is sometimes called the theory of multistage decision processes,\\nor sequential decision processes, and has roots in the statistical literature on sequential\\nsampling beginning with the papers by Thompson (1933, 1934) and Robbins (1952) that\\nwe cited in Chapter 2 in connection with bandit problems (which are prototypical MDPs\\nif formulated as multiple-situation problems).\\nThe earliest instance (that we are aware of) in which reinforcement learning was\\ndiscussed using the MDP formalism is Andreae’s (1969) description of a uniﬁed view of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 91, 'page_label': '92'}, page_content='70 Chapter 3: Finite Markov Decision Processes\\nlearning machines. Witten and Corbin (1973) experimented with a reinforcement learning\\nsystem later analyzed by Witten (1977, 1976a) using the MDP formalism. Although\\nhe did not explicitly mention MDPs, Werbos (1977) suggested approximate solution\\nmethods for stochastic optimal control problems that are related to modern reinforcement\\nlearning methods (see also Werbos, 1982, 1987, 1988, 1989, 1992). Although Werbos’s\\nideas were not widely recognized at the time, they were prescient in emphasizing the\\nimportance of approximately solving optimal control problems in a variety of domains,\\nincluding artiﬁcial intelligence. The most inﬂuential integration of reinforcement learning\\nand MDPs is due to Watkins (1989).\\n3.1 Our characterization of the dynamics of an MDP in terms ofp(s0,r |s, a)i s\\nslightly unusual. It is more common in the MDP literature to describe the\\ndynamics in terms of the state transition probabilitiesp(s0|s, a) and expected\\nnext rewards r(s, a). In reinforcement learning, however, we more often have\\nto refer to individual actual or sample rewards (rather than just their expected\\nvalues). Our notation also makes it plainer thatSt and Rt are in general jointly\\ndetermined, and thus must have the same time index. In teaching reinforcement\\nlearning, we have found our notation to be more straightforward conceptually\\nand easier to understand.\\nFor a good intuitive discussion of the system-theoretic concept of state, see\\nMinsky (1967).\\nThe bioreactor example is based on the work of Ungar (1990) and Miller and\\nWilliams (1992). The recycling robot example was inspired by the can-collecting\\nrobot built by Jonathan Connell (1989). Kober and Peters (2012) present a\\ncollection of robotics applications of reinforcement learning.\\n3.2 An explicit statement of the reward hypothesis was suggested by Michael Littman\\n(personal communication).\\n3.3–4 The terminology ofepisodic and continuing tasks is di↵erent from that usually\\nused in the MDP literature. In that literature it is common to distinguish\\nthree types of tasks: (1) ﬁnite-horizon tasks, in which interaction terminates\\nafter a particularﬁxed number of time steps; (2) indeﬁnite-horizon tasks, in\\nwhich interaction can last arbitrarily long but must eventually terminate; and\\n(3) inﬁnite-horizon tasks, in which interaction does not terminate. Our episodic\\nand continuing tasks are similar to indeﬁnite-horizon and inﬁnite-horizon tasks,\\nrespectively, but we prefer to emphasize the di↵erence in the nature of the\\ninteraction. This di↵erence seems more fundamental than the di↵erence in the\\nobjective functions emphasized by the usual terms. Often episodic tasks use\\nan indeﬁnite-horizon objective function and continuing tasks an inﬁnite-horizon\\nobjective function, but we see this as a common coincidence rather than a\\nfundamental di↵erence.\\nThe pole-balancing example is from Michie and Chambers (1968) and Barto,\\nSutton, and Anderson (1983).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 92, 'page_label': '93'}, page_content='3.8. Summary 71\\n3.5–6 Assigning value on the basis of what is good or bad in the long run has ancient\\nroots. In control theory, mapping states to numerical values representing the\\nlong-term consequences of control decisions is a key part of optimal control theory,\\nwhich was developed in the 1950s by extending nineteenth century state-function\\ntheories of classical mechanics (see, for example, Schultz and Melsa, 1967). In\\ndescribing how a computer could be programmed to play chess, Shannon (1950)\\nsuggested using an evaluation function that took into account the long-term\\nadvantages and disadvantages of chess positions.\\nWatkins’s (1989) Q-learning algorithm for estimatingq⇤ (Chapter 6) made action-\\nvalue functions an important part of reinforcement learning, and consequently\\nthese functions are often called “Q-functions.” But the idea of an action-value\\nfunction is much older than this. Shannon (1950) suggested that a function\\nh(P,M ) could be used by a chess-playing program to decide whether a moveM\\nin position P is worth exploring. Michie’s (1961, 1963) MENACE system and\\nMichie and Chambers’s (1968) BOXES system can be understood as estimating\\naction-value functions. In classical physics, Hamilton’s principal function is\\nan action-value function; Newtonian dynamics are greedy with respect to this\\nfunction (e.g., Goldstein, 1957). Action-value functions also played a central role\\nin Denardo’s (1967) theoretical treatment of dynamic programming in terms of\\ncontraction mappings.\\nThe Bellman optimality equation (forv⇤) was popularized by Richard Bellman\\n(1957a), who called it the “basic functional equation.” The counterpart of the\\nBellman optimality equation for continuous time and state problems is known\\nas the Hamilton–Jacobi–Bellman equation (or often just the Hamilton–Jacobi\\nequation), indicating its roots in classical physics (e.g., Schultz and Melsa, 1967).\\nThe golf example was suggested by Chris Watkins.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 93, 'page_label': '94'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 94, 'page_label': '95'}, page_content='Chapter 4\\nDynamic Programming\\nThe term dynamic programming (DP) refers to a collection of algorithms that can be\\nused to compute optimal policies given a perfect model of the environment as a Markov\\ndecision process (MDP). Classical DP algorithms are of limited utility in reinforcement\\nlearning both because of their assumption of a perfect model and because of their great\\ncomputational expense, but they are still important theoretically. DP provides an essential\\nfoundation for the understanding of the methods presented in the rest of this book. In\\nfact, all of these methods can be viewed as attempts to achieve much the same e↵ect as\\nDP, only with less computation and without assuming a perfect model of the environment.\\nStarting with this chapter, we usually assume that the environment is a ﬁnite MDP.\\nThat is, we assume that its state, action, and reward sets,S, A, andR, are ﬁnite, and\\nthat its dynamics are given by a set of probabilitiesp(s0,r |s, a), for alls 2 S, a 2 A(s),\\nr 2 R, ands0 2 S+ (S+ is S plus a terminal state if the problem is episodic). Although\\nDP ideas can be applied to problems with continuous state and action spaces, exact\\nsolutions are possible only in special cases. A common way of obtaining approximate\\nsolutions for tasks with continuous states and actions is to quantize the state and action\\nspaces and then apply ﬁnite-state DP methods. The methods we explore in Part II are\\napplicable to continuous problems and are a signiﬁcant extension of that approach.\\nThe key idea of DP, and of reinforcement learning generally, is the use of value functions\\nto organize and structure the search for good policies. In this chapter we show how DP\\ncan be used to compute the value functions deﬁned in Chapter 3. As discussed there, we\\ncan easily obtain optimal policies once we have found the optimal value functions,v⇤ or\\nq⇤, which satisfy the Bellman optimality equations:\\nv⇤(s) = max\\na\\nE[Rt+1 + \\x00v⇤(St+1) | St =s, At =a]\\n= max\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇤(s0)\\ni\\n, (4.1)\\nor\\nq⇤(s, a)= E\\nh\\nRt+1 + \\x00 max\\na0\\nq⇤(St+1,a 0)\\n\\x00\\x00\\x00 St =s, At =a\\ni\\n=\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00 max\\na0\\nq⇤(s0,a 0)\\ni\\n, (4.2)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 95, 'page_label': '96'}, page_content='74 Chapter 4: Dynamic Programming\\nfor all s 2 S, a 2 A(s), and s0 2 S+. As we shall see, DP algorithms are obtained by\\nturning Bellman equations such as these into assignments, that is, into update rules for\\nimproving approximations of the desired value functions.\\n4.1 Policy Evaluation (Prediction)\\nFirst we consider how to compute the state-value functionv⇡ for an arbitrary policy⇡.\\nThis is calledpolicy evaluationin the DP literature. We also refer to it as theprediction\\nproblem. Recall from Chapter 3 that, for alls 2 S,\\nv⇡(s) .= E⇡[Gt | St =s]\\n= E⇡[Rt+1 + \\x00Gt+1 | St =s] (from (3.9))\\n= E⇡[Rt+1 + \\x00v⇡(St+1) | St =s] (4.3)\\n=\\nX\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇡(s0)\\ni\\n, (4.4)\\nwhere ⇡(a|s) is the probability of taking actiona in state s under policy ⇡, and the\\nexpectations are subscripted by⇡ to indicate that they are conditional on⇡ being followed.\\nThe existence and uniqueness ofv⇡ are guaranteed as long as either\\x00< 1 or eventual\\ntermination is guaranteed from all states under the policy⇡.\\nIf the environment’s dynamics are completely known, then (4.4) is a system of|S|\\nsimultaneous linear equations in|S| unknowns (thev⇡(s), s 2 S). In principle, its solution\\nis a straightforward, if tedious, computation. For our purposes, iterative solution methods\\nare most suitable. Consider a sequence of approximate value functionsv0,v 1,v 2,... ,\\neach mapping S+ to R (the real numbers). The initial approximation,v0, is chosen\\narbitrarily (except that the terminal state, if any, must be given value 0), and each\\nsuccessive approximation is obtained by using the Bellman equation forv⇡ (4.4) as an\\nupdate rule:\\nvk+1(s) .= E⇡[Rt+1 + \\x00vk(St+1) | St =s]\\n=\\nX\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00vk(s0)\\ni\\n, (4.5)\\nfor alls 2 S. Clearly, vk = v⇡ is a ﬁxed point for this update rule because the Bellman\\nequation for v⇡ assures us of equality in this case. Indeed, the sequence{vk} can be\\nshown in general to converge tov⇡ as k !1 under the same conditions that guarantee\\nthe existence ofv⇡. This algorithm is callediterative policy evaluation.\\nTo produce each successive approximation,vk+1 from vk, iterative policy evaluation\\napplies the same operation to each states: it replaces the old value ofs with a new value\\nobtained from the old values of the successor states ofs, and the expected immediate\\nrewards, along all the one-step transitions possible under the policy being evaluated. We\\ncall this kind of operation anexpected update. Each iteration of iterative policy evaluation\\nupdates the value of every state once to produce the new approximate value function'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 96, 'page_label': '97'}, page_content='4.1. Policy Evaluation (Prediction) 75\\nvk+1. There are several di↵erent kinds of expected updates, depending on whether a\\nstate (as here) or a state–action pair is being updated, and depending on the precise way\\nthe estimated values of the successor states are combined. All the updates done in DP\\nalgorithms are calledexpected updates because they are based on an expectation over all\\npossible next states rather than on a sample next state. The nature of an update can\\nbe expressed in an equation, as above, or in a backup diagram like those introduced in\\nChapter 3. For example, the backup diagram corresponding to the expected update used\\nin iterative policy evaluation is shown on page 59.\\nTo write a sequential computer program to implement iterative policy evaluation as\\ngiven by (4.5) you would have to use two arrays, one for the old values,vk(s), and one\\nfor the new values,vk+1(s). With two arrays, the new values can be computed one by\\none from the old values without the old values being changed. Alternatively, you could\\nuse one array and update the values “in place,” that is, with each new value immediately\\noverwriting the old one. Then, depending on the order in which the states are updated,\\nsometimes new values are used instead of old ones on the right-hand side of (4.5). This\\nin-place algorithm also converges tov⇡; in fact, it usually converges faster than the\\ntwo-array version, as you might expect, because it uses new data as soon as they are\\navailable. We think of the updates as being done in asweep through the state space. For\\nthe in-place algorithm, the order in which states have their values updated during the\\nsweep has a signiﬁcant inﬂuence on the rate of convergence. We usually have the in-place\\nversion in mind when we think of DP algorithms.\\nA complete in-place version of iterative policy evaluation is shown in pseudocode in\\nthe box below. Note how it handles termination. Formally, iterative policy evaluation\\nconverges only in the limit, but in practice it must be halted short of this. The pseudocode\\ntests the quantitymaxs2S |vk+1(s)\\x00vk(s)| after each sweep and stops when it is su\\x00ciently\\nsmall.\\nIterative Policy Evaluation, for estimatingV ⇡ v⇡\\nInput ⇡, the policy to be evaluated\\nAlgorithm parameter: a small threshold✓> 0 determining accuracy of estimation\\nInitialize V (s) arbitrarily, fors 2 S, andV (terminal)t o0\\nLoop:\\n\\x00  0\\nLoop for eachs 2 S:\\nv  V (s)\\nV (s)  P\\na ⇡(a|s) P\\ns0,r p(s0,r |s, a)\\n⇥\\nr + \\x00V (s0)\\n⇤\\n\\x00  max(\\x00, |v \\x00 V (s)|)\\nuntil \\x00<✓'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 97, 'page_label': '98'}, page_content='76 Chapter 4: Dynamic Programming\\nExample 4.1 Consider the 4⇥4 gridworld shown below.\\nactions\\nr  =  !1\\non all transitions\\n1 2 3\\n4 5 6 7\\n8 9 10 11\\n12 13 14\\nRt = \\x001\\nThe nonterminal states areS = {1, 2,..., 14}. There are four actions possible in each\\nstate, A = {up, down, right, left}, which deterministically cause the corresponding\\nstate transitions, except that actions that would take the agent o↵ the grid in fact leave\\nthe state unchanged. Thus, for instance,p(6, \\x001|5, right) = 1,p(7, \\x001|7, right) = 1,\\nand p(10,r |5, right) = 0 for allr 2 R. This is an undiscounted, episodic task. The\\nreward is\\x001 on all transitions until the terminal state is reached. The terminal state is\\nshaded in the ﬁgure (although it is shown in two places, it is formally one state). The\\nexpected reward function is thusr(s, a, s0)= \\x001 for all statess, s0 and actionsa. Suppose\\nthe agent follows the equiprobable random policy (all actions equally likely). The left side\\nof Figure 4.1 shows the sequence of value functions{vk} computed by iterative policy\\nevaluation. The ﬁnal estimate is in factv⇡, which in this case gives for each state the\\nnegation of the expected number of steps from that state until termination.\\nExercise 4.1 In Example 4.1, if⇡ is the equiprobable random policy, what isq⇡(11, down)?\\nWhat isq⇡(7, down)? ⇤\\nExercise 4.2 In Example 4.1, suppose a new state 15 is added to the gridworld just below\\nstate 13, and its actions,left, up, right, anddown, take the agent to states 12, 13, 14,\\nand 15, respectively. Assume that the transitionsfrom the original states are unchanged.\\nWhat, then, isv⇡(15) for the equiprobable random policy? Now suppose the dynamics of\\nstate 13 are also changed, such that actiondown from state 13 takes the agent to the new\\nstate 15. What isv⇡(15) for the equiprobable random policy in this case? ⇤\\nExercise 4.3 What are the equations analogous to (4.3), (4.4), and (4.5), but foraction-\\nvalue functions instead of state-value functions? ⇤\\n4.2 Policy Improvement\\nOur reason for computing the value function for a policy is to help ﬁnd better policies.\\nSuppose we have determined the value functionv⇡ for an arbitrary deterministic policy\\n⇡. For some states we would like to know whether or not we should change the policy\\nto deterministically choose an actiona 6= ⇡(s). We know how good it is to follow the\\ncurrent policy froms—that is v⇡(s)—but would it be better or worse to change to the\\nnew policy? One way to answer this question is to consider selectinga in s and thereafter'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 98, 'page_label': '99'}, page_content='4.2. Policy Improvement 77\\n 0.0 0.0 0.0\\n 0.0 0.0 0.0 0.0\\n 0.0 0.0 0.0 0.0\\n 0.0 0.0 0.0\\n-1.0-1.0-1.0\\n-1.0-1.0-1.0-1.0\\n-1.0-1.0-1.0-1.0\\n-1.0-1.0-1.0\\n-1.7-2.0-2.0\\n-1.7-2.0-2.0-2.0\\n-2.0-2.0-2.0-1.7\\n-2.0-2.0-1.7\\n-2.4-2.9-3.0\\n-2.4-2.9-3.0-2.9\\n-2.9-3.0-2.9-2.4\\n-3.0-2.9-2.4\\n-6.1-8.4-9.0\\n-6.1-7.7-8.4-8.4\\n-8.4-8.4-7.7-6.1\\n-9.0-8.4-6.1\\n-14. -20. -22.\\n-14. -18. -20. -20.\\n-20. -20. -18. -14.\\n-22. -20. -14.\\nVk  for the\\nRandom Policy\\nGreedy Policyw.r.t. Vk\\nk = 0\\nk = 1\\nk = 2\\nk = 10\\nk = !\\nk = 3\\noptimal policy\\nrandom policy\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\n 0.0\\nvk      for therandom policyvk greedy policy    w.r.t.vk\\nFigure 4.1: Convergence of iterative policy evaluation on a small gridworld. The left column is\\nthe sequence of approximations of the state-value function for the random policy (all actions\\nequally likely). The right column is the sequence of greedy policies corresponding to the value\\nfunction estimates (arrows are shown for all actions achieving the maximum, and the numbers\\nshown are rounded to two signiﬁcant digits). The last policy is guaranteed only to be an\\nimprovement over the random policy, but in this case it, and all policies after the third iteration,\\nare optimal.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 99, 'page_label': '100'}, page_content='78 Chapter 4: Dynamic Programming\\nfollowing the existing policy,⇡. The value of this way of behaving is\\nq⇡(s, a) .= E[Rt+1 + \\x00v⇡(St+1) | St =s, At =a] (4.6)\\n=\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇡(s0)\\ni\\n.\\nThe key criterion is whether this is greater than or less thanv⇡(s). If it is greater—that\\nis, if it is better to selecta once ins and thereafter follow⇡ than it would be to follow\\n⇡ all the time—then one would expect it to be better still to selecta every time s is\\nencountered, and that the new policy would in fact be a better one overall.\\nThat this is true is a special case of a general result called thepolicy improvement\\ntheorem. Let ⇡ and ⇡0 be any pair of deterministic policies such that, for alls 2 S,\\nq⇡(s, ⇡0(s)) \\x00 v⇡(s). (4.7)\\nThen the policy⇡0 must be as good as, or better than,⇡. That is, it must obtain greater\\nor equal expected return from all statess 2 S:\\nv⇡0 (s) \\x00 v⇡(s). (4.8)\\nMoreover, if there is strict inequality of (4.7) at any state, then there must be strict\\ninequality of (4.8) at that state.\\nThe policy improvement theorem applies to the two policies that we considered at the\\nbeginning of this section: an original deterministic policy,⇡, and a changed policy,⇡0,\\nthat is identical to⇡ except that⇡0(s)= a 6= ⇡(s). For states other thans, (4.7) holds\\nbecause the two sides are equal. Thus, ifq⇡(s, a) >v ⇡(s), then the changed policy is\\nindeed better than⇡.\\nThe idea behind the proof of the policy improvement theorem is easy to understand.\\nStarting from (4.7), we keep expanding theq⇡ side with(4.6) and reapplying (4.7) until\\nwe getv⇡0 (s):\\nv⇡(s) \\uf8ff q⇡(s, ⇡0(s))\\n= E[Rt+1 + \\x00v⇡(St+1) | St =s, At =⇡0(s)] (by (4.6))\\n= E⇡0[Rt+1 + \\x00v⇡(St+1) | St =s]\\n\\uf8ff E⇡0[Rt+1 + \\x00q⇡(St+1,⇡ 0(St+1)) | St =s] (by (4.7))\\n= E⇡0[Rt+1 + \\x00E[Rt+2 + \\x00v⇡(St+2)|St+1,A t+1 =⇡0(St+1)] | St =s]\\n= E⇡0\\n⇥\\nRt+1 + \\x00Rt+2 + \\x002v⇡(St+2)\\n\\x00\\x00 St =s\\n⇤\\n\\uf8ff E⇡0\\n⇥\\nRt+1 + \\x00Rt+2 + \\x002Rt+3 + \\x003v⇡(St+3)\\n\\x00\\x00 St =s\\n⇤\\n...\\n\\uf8ff E⇡0\\n⇥\\nRt+1 + \\x00Rt+2 + \\x002Rt+3 + \\x003Rt+4 + ···\\n\\x00\\x00 St =s\\n⇤\\n= v⇡0 (s).\\nSo far we have seen how, given a policy and its value function, we can easily evaluate\\na change in the policy at a single state. It is a natural extension to consider changes at'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 100, 'page_label': '101'}, page_content='4.2. Policy Improvement 79\\nall states, selecting at each state the action that appears best according toq⇡(s, a). In\\nother words, to consider the newgreedy policy, ⇡0, given by\\n⇡0(s) .= argmax\\na\\nq⇡(s, a)\\n= argmax\\na\\nE[Rt+1 + \\x00v⇡(St+1) | St =s, At =a] (4.9)\\n= argmax\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇡(s0)\\ni\\n,\\nwhere argmaxa denotes the value ofa at which the expression that follows is maximized\\n(with ties broken arbitrarily). The greedy policy takes the action that looks best in the\\nshort term—after one step of lookahead—according tov⇡. By construction, the greedy\\npolicy meets the conditions of the policy improvement theorem (4.7), so we know that it\\nis as good as, or better than, the original policy. The process of making a new policy that\\nimproves on an original policy, by making it greedy with respect to the value function of\\nthe original policy, is calledpolicy improvement.\\nSuppose the new greedy policy,⇡0, is as good as, but not better than, the old policy⇡.\\nThen v⇡ = v⇡0 , and from (4.9) it follows that for alls 2 S:\\nv⇡0 (s) = max\\na\\nE[Rt+1 + \\x00v⇡0 (St+1) | St =s, At =a]\\n= max\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇡0 (s0)\\ni\\n.\\nBut this is the same as the Bellman optimality equation (4.1), and therefore,v⇡0 must be\\nv⇤, and both⇡ and ⇡0 must be optimal policies. Policy improvement thus must give us a\\nstrictly better policy except when the original policy is already optimal.\\nSo far in this section we have considered the special case of deterministic policies.\\nIn the general case, a stochastic policy⇡ speciﬁes probabilities, ⇡(a|s), for taking each\\naction, a, in each state,s. We will not go through the details, but in fact all the ideas of\\nthis section extend easily to stochastic policies. In particular, the policy improvement\\ntheorem carries through as stated for the stochastic case. In addition, if there are ties in\\npolicy improvement steps such as (4.9)—that is, if there are several actions at which the\\nmaximum is achieved—then in the stochastic case we need not select a single action from\\namong them. Instead, each maximizing action can be given a portion of the probability\\nof being selected in the new greedy policy. Any apportioning scheme is allowed as long\\nas all submaximal actions are given zero probability.\\nThe last row of Figure 4.1 shows an example of policy improvement for stochastic\\npolicies. Here the original policy,⇡, is the equiprobable random policy, and the new\\npolicy, ⇡0, is greedy with respect tov⇡. The value functionv⇡ is shown in the bottom-left\\ndiagram and the set of possible⇡0 is shown in the bottom-right diagram. The states\\nwith multiple arrows in the⇡0 diagram are those in which several actions achieve the\\nmaximum in (4.9); any apportionment of probability among these actions is permitted.\\nFor any such policy, its state valuesv⇡0 (s) can be seen by inspection to be either\\x001, \\x002,\\nor \\x003, for all statess 2 S, whereas v⇡(s) is at most\\x0014. Thus, v⇡0 (s) \\x00 v⇡(s), for all'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 101, 'page_label': '102'}, page_content='80 Chapter 4: Dynamic Programming\\ns 2 S, illustrating policy improvement. Although in this case the new policy⇡0 happens\\nto be optimal, in general only an improvement is guaranteed.\\n4.3 Policy Iteration\\nOnce a policy,⇡, has been improved usingv⇡ to yield a better policy,⇡0, we can then\\ncompute v⇡0 and improve it again to yield an even better⇡00. We can thus obtain a\\nsequence of monotonically improving policies and value functions:\\n⇡0\\nE\\n\\x00!v⇡0\\nI\\n\\x00!⇡1\\nE\\n\\x00!v⇡1\\nI\\n\\x00!⇡2\\nE\\n\\x00! · · ·\\nI\\n\\x00!⇡⇤\\nE\\n\\x00!v⇤,\\nwhere\\nE\\n\\x00! denotes a policyevaluation and\\nI\\n\\x00! denotes a policyimprovement. Each\\npolicy is guaranteed to be a strict improvement over the previous one (unless it is already\\noptimal). Because a ﬁnite MDP has only a ﬁnite number of deterministic policies, this\\nprocess must converge to an optimal policy and the optimal value function in a ﬁnite\\nnumber of iterations.\\nThis way of ﬁnding an optimal policy is calledpolicy iteration. A complete algorithm is\\ngiven in the box below. Note that each policy evaluation, itself an iterative computation,\\nis started with the value function for the previous policy. This typically results in a great\\nincrease in the speed of convergence of policy evaluation (presumably because the value\\nfunction changes little from one policy to the next).\\nPolicy Iteration (using iterative policy evaluation) for estimating⇡ ⇡ ⇡⇤\\n1. Initialization\\nV (s) 2 R and ⇡(s) 2 A(s) arbitrarily for alls 2 S; V (terminal) .=0\\n2. Policy Evaluation\\nLoop:\\n\\x00  0\\nLoop for eachs 2 S:\\nv  V (s)\\nV (s)  P\\ns0,r p(s0,r |s, ⇡(s))\\n⇥\\nr + \\x00V (s0)\\n⇤\\n\\x00  max(\\x00, |v \\x00 V (s)|)\\nuntil \\x00<✓ (a small positive number determining the accuracy of estimation)\\n3. Policy Improvement\\npolicy-stable  true\\nFor eachs 2 S:\\nold-action  ⇡(s)\\n⇡(s)  argmaxa\\nP\\ns0,r p(s0,r |s, a)\\n⇥\\nr + \\x00V (s0)\\n⇤\\nIf old-action 6= ⇡(s), thenpolicy-stable  false\\nIf policy-stable, then stop and returnV ⇡ v⇤ and ⇡ ⇡ ⇡⇤; else go to 2'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 102, 'page_label': '103'}, page_content='4.3. Policy Iteration 81\\nExample 4.2: Jack’s Car RentalJack manages two locations for a nationwide car\\nrental company. Each day, some number of customers arrive at each location to rent cars.\\nIf Jack has a car available, he rents it out and is credited$10 by the national company.\\nIf he is out of cars at that location, then the business is lost. Cars become available for\\nrenting the day after they are returned. To help ensure that cars are available where\\nthey are needed, Jack can move them between the two locations overnight, at a cost of\\n$2 per car moved. We assume that the number of cars requested and returned at each\\nlocation are Poisson random variables, meaning that the probability that the number is\\nn is \\x00n\\nn! e\\x00\\x00,w h e r e\\x00 is the expected number. Suppose\\x00 is 3 and 4 for rental requests at\\nthe ﬁrst and second locations and 3 and 2 for returns. To simplify the problem slightly,\\nwe assume that there can be no more than 20 cars at each location (any additional cars\\nare returned to the nationwide company, and thus disappear from the problem) and a\\nmaximum of ﬁve cars can be moved from one location to the other in one night. We take\\nthe discount rate to be\\x00 =0 .9 and formulate this as a continuing ﬁnite MDP, where\\nthe time steps are days, the state is the number of cars at each location at the end of\\nthe day, and the actions are the net numbers of cars moved between the two locations\\novernight. Figure 4.2 shows the sequence of policies found by policy iteration starting\\nfrom the policy that never moves any cars.\\n4V\\n612\\n#Cars at second location\\n0420\\n20 0\\n20\\n#Cars at first location\\n1\\n1\\n5\\n\\x021\\x022 -4\\n432\\n432\\n\\x023\\n0\\n0\\n5\\n\\x021\\x022\\x023\\x024\\n1234\\n0\\n\\x011\\x010 \\x012\\n\\x023 \\x024\\x022\\n0\\n1234\\n\\x021\\n\\x013\\n2\\n\\x024\\x023\\x022\\n0\\n1345\\n\\x021\\n\\x014\\n#Cars at second location\\n#Cars at first location 5\\n200 0 20 v⇡4\\n⇡0 ⇡1 ⇡2\\n⇡3 ⇡4\\nFigure 4.2: The sequence of policies found by policy iteration on Jack’s car rental problem,\\nand the ﬁnal state-value function. The ﬁrst ﬁve diagrams show, for each number of cars at\\neach location at the end of the day, the number of cars to be moved from the ﬁrst location to\\nthe second (negative numbers indicate transfers from the second location to the ﬁrst). Each\\nsuccessive policy is a strict improvement over the previous policy, and the last policy is optimal.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 103, 'page_label': '104'}, page_content='82 Chapter 4: Dynamic Programming\\nPolicy iteration often converges in surprisingly few iterations, as illustrated in the\\nexample of Jack’s car rental and in the example in Figure 4.1. The bottom-left diagram of\\nFigure 4.1 shows the value function for the equiprobable random policy, and the bottom-\\nright diagram shows a greedy policy for this value function. The policy improvement\\ntheorem assures us that these policies are better than the original random policy. In this\\ncase, however, these policies are not just better, but optimal, proceeding to the terminal\\nstates in the minimum number of steps. In this example, policy iteration would ﬁnd the\\noptimal policy after just one iteration.\\nExercise 4.4 The policy iteration algorithm on page 80 has a subtle bug in that it may\\nnever terminate if the policy continually switches between two or more policies that are\\nequally good. This is okay for pedagogy, but not for actual use. Modify the pseudocode\\nso that convergence is guaranteed. ⇤\\nExercise 4.5 How would policy iteration be deﬁned for action values? Give a complete\\nalgorithm for computingq⇤, analogous to that on page 80 for computingv⇤. Please pay\\nspecial attention to this exercise, because the ideas involved will be used throughout the\\nrest of the book. ⇤\\nExercise 4.6 Suppose you are restricted to considering only policies that are\"-soft,\\nmeaning that the probability of selecting each action in each state,s, is at least\"/|A(s)|.\\nDescribe qualitatively the changes that would be required in each of the steps 3, 2, and 1,\\nin that order, of the policy iteration algorithm forv⇤ on page 80. ⇤\\nExercise 4.7 (programming) Write a program for policy iteration and re-solve Jack’s car\\nrental problem with the following changes. One of Jack’s employees at the ﬁrst location\\nrides a bus home each night and lives near the second location. She is happy to shuttle\\none car to the second location for free. Each additional car still costs$2, as do all cars\\nmoved in the other direction. In addition, Jack has limited parking space at each location.\\nIf more than 10 cars are kept overnight at a location (after any moving of cars), then an\\nadditional cost of$4 must be incurred to use a second parking lot (independent of how\\nmany cars are kept there). These sorts of nonlinearities and arbitrary dynamics often\\noccur in real problems and cannot easily be handled by optimization methods other than\\ndynamic programming. To check your program, ﬁrst replicate the results given for the\\noriginal problem. ⇤\\n4.4 Value Iteration\\nOne drawback to policy iteration is that each of its iterations involves policy evaluation,\\nwhich may itself be a protracted iterative computation requiring multiple sweeps through\\nthe state set. If policy evaluation is done iteratively, then convergence exactly tov⇡\\noccurs only in the limit. Must we wait for exact convergence, or can we stop short of\\nthat? The example in Figure 4.1 certainly suggests that it may be possible to truncate\\npolicy evaluation. In that example, policy evaluation iterations beyond the ﬁrst three\\nhave no e↵ect on the corresponding greedy policy.\\nIn fact, the policy evaluation step of policy iteration can be truncated in several ways\\nwithout losing the convergence guarantees of policy iteration. One important special'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 104, 'page_label': '105'}, page_content='4.4. Value Iteration 83\\ncase is when policy evaluation is stopped after just one sweep (one update of each state).\\nThis algorithm is calledvalue iteration. It can be written as a particularly simple update\\noperation that combines the policy improvement and truncated policy evaluation steps:\\nvk+1(s) .= max\\na\\nE[Rt+1 + \\x00vk(St+1) | St =s, At =a]\\n= max\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00vk(s0)\\ni\\n, (4.10)\\nfor alls 2 S. For arbitraryv0,t h es e q u e n c e{vk} can be shown to converge tov⇤ under\\nthe same conditions that guarantee the existence ofv⇤.\\nAnother way of understanding value iteration is by reference to the Bellman optimality\\nequation (4.1). Note that value iteration is obtained simply by turning the Bellman\\noptimality equation into an update rule. Also note how the value iteration update is\\nidentical to the policy evaluation update (4.5) except that it requires the maximum to be\\ntaken over all actions. Another way of seeing this close relationship is to compare the\\nbackup diagrams for these algorithms on page 59 (policy evaluation) and on the left of\\nFigure 3.4 (value iteration). These two are the natural backup operations for computing\\nv⇡ and v⇤.\\nFinally, let us consider how value iteration terminates. Like policy evaluation, value\\niteration formally requires an inﬁnite number of iterations to converge exactly tov⇤.I n\\npractice, we stop once the value function changes by only a small amount in a sweep.\\nThe box below shows a complete algorithm with this kind of termination condition.\\nValue Iteration, for estimating⇡ ⇡ ⇡⇤\\nAlgorithm parameter: a small threshold✓> 0 determining accuracy of estimation\\nInitialize V (s), for alls 2 S+, arbitrarily except thatV (terminal)=0\\nLoop:\\n| \\x00  0\\n| Loop for eachs 2 S:\\n| v  V (s)\\n| V (s)  maxa\\nP\\ns0,r p(s0,r |s, a)\\n⇥\\nr + \\x00V (s0)\\n⇤\\n| \\x00  max(\\x00, |v \\x00 V (s)|)\\nuntil \\x00<✓\\nOutput a deterministic policy,⇡ ⇡ ⇡⇤, such that\\n⇡(s) = argmaxa\\nP\\ns0,r p(s0,r |s, a)\\n⇥\\nr + \\x00V (s0)\\n⇤\\nValue iteration e↵ectively combines, in each of its sweeps, one sweep of policy evaluation\\nand one sweep of policy improvement. Faster convergence is often achieved by interposing\\nmultiple policy evaluation sweeps between each policy improvement sweep. In general,\\nthe entire class of truncated policy iteration algorithms can be thought of as sequences\\nof sweeps, some of which use policy evaluation updates and some of which use value\\niteration updates. Because the max operation in (4.10) is the only di↵erence between'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 105, 'page_label': '106'}, page_content='84 Chapter 4: Dynamic Programming\\nthese updates, this just means that the max operation is added to some sweeps of policy\\nevaluation. All of these algorithms converge to an optimal policy for discounted ﬁnite\\nMDPs.\\nExample 4.3: Gambler’s ProblemA gambler has the opportunity to make bets on the\\noutcomes of a sequence of coin ﬂips. If the coin comes up heads, he wins as many dollars as\\nhe has staked on that ﬂip; if it is tails, he loses his stake. The game ends when the gambler\\nwins by reaching his goal of$100, or loses by running out of money. On each ﬂip, the gam-\\nbler must decide what portion of his capital to stake, in integer numbers of dollars. This\\nproblem can be formulated as an undiscounted, episodic, ﬁnite MDP. The state is the gam-\\nbler’s capital,s 2{ 1, 2,..., 99} and the actions are stakes,a 2{ 0, 1,..., min(s, 100 \\x00s)}.\\n99755025111020304050\\n10\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\n25 50 75 99\\nCapital\\nCapital\\nValueestimates\\nFinalpolicy(stake)\\nsweep 1sweep 2sweep 3\\nsweep 32\\nFinal valuefunction\\nFigure 4.3: The solution to the gambler’s problem for\\nph =0 .4. The upper graph shows the value function\\nfound by successive sweeps of value iteration. The\\nlower graph shows the ﬁnal policy.\\nThe reward is zero on all transi-\\ntions except those on which the gam-\\nbler reaches his goal, when it is +1.\\nThe state-value function then gives\\nthe probability of winning from each\\nstate. A policy is a mapping from\\nlevels of capital to stakes. The opti-\\nmal policy maximizes the probability\\nof reaching the goal. Letph denote\\nthe probability of the coin coming\\nup heads. If ph is known, then the\\nentire problem is known and it can\\nbe solved, for instance, by value iter-\\nation. Figure 4.3 shows the change\\nin the value function over successive\\nsweeps of value iteration, and the\\nﬁnal policy found, for the case of\\nph =0 .4. This policy is optimal, but\\nnot unique. In fact, there is a whole\\nfamily of optimal policies, all corre-\\nsponding to ties for the argmax ac-\\ntion selection with respect to the op-\\ntimal value function. Can you guess\\nwhat the entire family looks like?\\nExercise 4.8 Why does the optimal\\npolicy for the gambler’s problem have such a curious form? In particular, for capital of 50\\nit bets it all on one ﬂip, but for capital of 51 it does not. Why is this a good policy?⇤\\nExercise 4.9 (programming)Implement value iteration for the gambler’s problem and\\nsolve it forph =0 .25 and ph =0 .55. In programming, you may ﬁnd it convenient to\\nintroduce two dummy states corresponding to termination with capital of 0 and 100,\\ngiving them values of 0 and 1 respectively. Show your results graphically, as in Figure 4.3.\\nAre your results stable as✓ ! 0? ⇤\\nExercise 4.10 What is the analog of the value iteration update (4.10) for action values,\\nqk+1(s, a)? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 106, 'page_label': '107'}, page_content='4.5. Asynchronous Dynamic Programming 85\\n4.5 Asynchronous Dynamic Programming\\nA major drawback to the DP methods that we have discussed so far is that they involve\\noperations over the entire state set of the MDP, that is, they require sweeps of the state\\nset. If the state set is very large, then even a single sweep can be prohibitively expensive.\\nFor example, the game of backgammon has over 1020 states. Even if we could perform\\nthe value iteration update on a million states per second, it would take over a thousand\\nyears to complete a single sweep.\\nAsynchronous DP algorithms are in-place iterative DP algorithms that are not organized\\nin terms of systematic sweeps of the state set. These algorithms update the values of\\nstates in any order whatsoever, using whatever values of other states happen to be\\navailable. The values of some states may be updated several times before the values of\\nothers are updated once. To converge correctly, however, an asynchronous algorithm\\nmust continue to update the values of all the states: it can’t ignore any state after some\\npoint in the computation. Asynchronous DP algorithms allow great ﬂexibility in selecting\\nstates to update.\\nFor example, one version of asynchronous value iteration updates the value, in place, of\\nonly one state,sk, on each step,k, using the value iteration update (4.10). If 0\\uf8ff \\x00< 1,\\nasymptotic convergence tov⇤ is guaranteed given only that all states occur in the sequence\\n{sk} an inﬁnite number of times (the sequence could even be random).1 Similarly, it is\\npossible to intermix policy evaluation and value iteration updates to produce a kind of\\nasynchronous truncated policy iteration. Although the details of this and other more\\nunusual DP algorithms are beyond the scope of this book, it is clear that a few di↵erent\\nupdates form building blocks that can be used ﬂexibly in a wide variety of sweepless DP\\nalgorithms.\\nOf course, avoiding sweeps does not necessarily mean that we can get away with less\\ncomputation. It just means that an algorithm does not need to get locked into any\\nhopelessly long sweep before it can make progress improving a policy. We can try to\\ntake advantage of this ﬂexibility by selecting the states to which we apply updates so\\nas to improve the algorithm’s rate of progress. We can try to order the updates to let\\nvalue information propagate from state to state in an e\\x00cient way. Some states may not\\nneed their values updated as often as others. We might even try to skip updating some\\nstates entirely if they are not relevant to optimal behavior. Some ideas for doing this are\\ndiscussed in Chapter 8.\\nAsynchronous algorithms also make it easier to intermix computation with real-time\\ninteraction. To solve a given MDP, we can run an iterative DP algorithmat the same\\ntime that an agent is actually experiencing the MDP. The agent’s experience can be used\\nto determine the states to which the DP algorithm applies its updates. At the same time,\\nthe latest value and policy information from the DP algorithm can guide the agent’s\\ndecision making. For example, we can apply updates to states as the agent visits them.\\nThis makes it possible tofocus the DP algorithm’s updates onto parts of the state set\\n1In the undiscounted episodic case, it is possible that there are some orderings of updates that do not\\nresult in convergence, but it is relatively easy to avoid these.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 107, 'page_label': '108'}, page_content='86 Chapter 4: Dynamic Programming\\nthat are most relevant to the agent. This kind of focusing is a repeated theme in\\nreinforcement learning.\\n4.6 Generalized Policy Iteration\\nPolicy iteration consists of two simultaneous, interacting processes, one making the value\\nfunction consistent with the current policy (policy evaluation), and the other making\\nthe policy greedy with respect to the current value function (policy improvement). In\\npolicy iteration, these two processes alternate, each completing before the other begins,\\nbut this is not really necessary. In value iteration, for example, only a single iteration\\nof policy evaluation is performed in between each policy improvement. In asynchronous\\nDP methods, the evaluation and improvement processes are interleaved at an even\\nﬁner grain. In some cases a single state is updated in one process before returning\\nto the other. As long as both processes continue to update all states, the ultimate\\nresult is typically the same—convergence to the optimal value function and an optimal\\npolicy.\\nevaluation\\nimprovement\\n⇡\\x00greedy(V)\\nV⇡\\nV\\x00v⇡\\nv⇤⇡⇤\\nWe use the termgeneralized policy iteration(GPI) to refer\\nto the general idea of letting policy-evaluation and policy-\\nimprovement processes interact, independent of the granularity\\nand other details of the two processes. Almost all reinforcement\\nlearning methods are well described as GPI. That is, all have\\nidentiﬁable policies and value functions, with the policy always\\nbeing improved with respect to the value function and the value\\nfunction always being driven toward the value function for the\\npolicy, as suggested by the diagram to the right. If both the\\nevaluation process and the improvement process stabilize, that\\nis, no longer produce changes, then the value function and policy\\nmust be optimal. The value function stabilizes only when it\\nis consistent with the current policy, and the policy stabilizes\\nonly when it is greedy with respect to the current value function.\\nThus, both processes stabilize only when a policy has been found that is greedy with\\nrespect to its own evaluation function. This implies that the Bellman optimality equation\\n(4.1) holds, and thus that the policy and the value function are optimal.\\nThe evaluation and improvement processes in GPI can be viewed as both competing\\nand cooperating. They compete in the sense that they pull in opposing directions. Making\\nthe policy greedy with respect to the value function typically makes the value function\\nincorrect for the changed policy, and making the value function consistent with the policy\\ntypically causes that policy no longer to be greedy. In the long run, however, these\\ntwo processes interact to ﬁnd a single joint solution: the optimal value function and an\\noptimal policy.\\nOne might also think of the interaction between the evaluation and improvement\\nprocesses in GPI in terms of two constraints or goals—for example, as two lines in'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 108, 'page_label': '109'}, page_content='4.7. E\\x00ciency of Dynamic Programming 87\\nv⇤,⇡⇤\\n⇡= greedy(v)\\nv,⇡\\nv=v⇡\\na two-dimensional space as suggested by the dia-\\ngram to the right. Although the real geometry is\\nmuch more complicated than this, the diagram sug-\\ngests what happens in the real case. Each process\\ndrives the value function or policy toward one of\\nthe lines representing a solution to one of the two\\ngoals. The goals interact because the two lines are\\nnot orthogonal. Driving directly toward one goal\\ncauses some movement away from the other goal.\\nInevitably, however, the joint process is brought closer to the overall goal of optimality.\\nThe arrows in this diagram correspond to the behavior of policy iteration in that each\\ntakes the system all the way to achieving one of the two goals completely. In GPI\\none could also take smaller, incomplete steps toward each goal. In either case, the two\\nprocesses together achieve the overall goal of optimality even though neither is attempting\\nto achieve it directly.\\n4.7 E\\x00ciency of Dynamic Programming\\nDP may not be practical for very large problems, but compared with other methods\\nfor solving MDPs, DP methods are actually quite e\\x00cient. If we ignore a few technical\\ndetails, then, in the worst case, the time that DP methods take to ﬁnd an optimal policy\\nis polynomial in the number of states and actions. Ifn and k denote the number of states\\nand actions, this means that a DP method takes a number of computational operations\\nthat is less than some polynomial function ofn and k. A DP method is guaranteed to\\nﬁnd an optimal policy in polynomial time even though the total number of (deterministic)\\npolicies is kn. In this sense, DP is exponentially faster than any direct search in policy\\nspace could be, because direct search would have to exhaustively examine each policy\\nto provide the same guarantee. Linear programming methods can also be used to solve\\nMDPs, and in some cases their worst-case convergence guarantees are better than those\\nof DP methods. But linear programming methods become impractical at a much smaller\\nnumber of states than do DP methods (by a factor of about 100). For the largest problems,\\nonly DP methods are feasible.\\nDP is sometimes thought to be of limited applicability because of thecurse of dimen-\\nsionality, the fact that the number of states often grows exponentially with the number\\nof state variables. Large state sets do create di\\x00culties, but these are inherent di\\x00culties\\nof the problem, not of DP as a solution method. In fact, DP is comparatively better\\nsuited to handling large state spaces than competing methods such as direct search and\\nlinear programming.\\nIn practice, DP methods can be used with today’s computers to solve MDPs with\\nmillions of states. Both policy iteration and value iteration are widely used, and it is not\\nclear which, if either, is better in general. In practice, these methods usually converge\\nmuch faster than their theoretical worst-case run times, particularly if they are started\\nwith good initial value functions or policies.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 109, 'page_label': '110'}, page_content='88 Chapter 4: Dynamic Programming\\nOn problems with large state spaces,asynchronous DP methods are often preferred. To\\ncomplete even one sweep of a synchronous method requires computation and memory for\\nevery state. For some problems, even this much memory and computation is impractical,\\nyet the problem is still potentially solvable because relatively few states occur along\\noptimal solution trajectories. Asynchronous methods and other variations of GPI can be\\napplied in such cases and may ﬁnd good or optimal policies much faster than synchronous\\nmethods can.\\n4.8 Summary\\nIn this chapter we have become familiar with the basic ideas and algorithms of dynamic\\nprogramming as they relate to solving ﬁnite MDPs.Policy evaluation refers to the (typi-\\ncally) iterative computation of the value function for a given policy.Policy improvement\\nrefers to the computation of an improved policy given the value function for that policy.\\nPutting these two computations together, we obtainpolicy iterationand value iteration,\\nthe two most popular DP methods. Either of these can be used to reliably compute\\noptimal policies and value functions for ﬁnite MDPs given complete knowledge of the\\nMDP.\\nClassical DP methods operate in sweeps through the state set, performing anexpected\\nupdate operation on each state. Each such operation updates the value of one state\\nbased on the values of all possible successor states and their probabilities of occurring.\\nExpected updates are closely related to Bellman equations: they are little more than\\nthese equations turned into assignment statements. When the updates no longer result in\\nany changes in value, convergence has occurred to values that satisfy the corresponding\\nBellman equation. Just as there are four primary value functions (v⇡, v⇤, q⇡, and q⇤),\\nthere are four corresponding Bellman equations and four corresponding expected updates.\\nAn intuitive view of the operation of DP updates is given by theirbackup diagrams.\\nInsight into DP methods and, in fact, into almost all reinforcement learning methods,\\ncan be gained by viewing them asgeneralized policy iteration(GPI). GPI is the general idea\\nof two interacting processes revolving around an approximate policy and an approximate\\nvalue function. One process takes the policy as given and performs some form of policy\\nevaluation, changing the value function to be more like the true value function for the\\npolicy. The other process takes the value function as given and performs some form\\nof policy improvement, changing the policy to make it better, assuming that the value\\nfunction is its value function. Although each process changes the basis for the other,\\noverall they work together to ﬁnd a joint solution: a policy and value function that are\\nunchanged by either process and, consequently, are optimal. In some cases, GPI can be\\nproved to converge, most notably for the classical DP methods that we have presented in\\nthis chapter. In other cases convergence has not been proved, but still the idea of GPI\\nimproves our understanding of the methods.\\nIt is not necessary to perform DP methods in complete sweeps through the state\\nset. Asynchronous DP methods are in-place iterative methods that update states in an\\narbitrary order, perhaps stochastically determined and using out-of-date information.\\nMany of these methods can be viewed as ﬁne-grained forms of GPI.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 110, 'page_label': '111'}, page_content='4.8. Summary 89\\nFinally, we note one last special property of DP methods. All of them update estimates\\nof the values of states based on estimates of the values of successor states. That is, they\\nupdate estimates on the basis of other estimates. We call this general ideabootstrapping.\\nMany reinforcement learning methods perform bootstrapping, even those that do not\\nrequire, as DP requires, a complete and accurate model of the environment. In the next\\nchapter we explore reinforcement learning methods that do not require a model and do\\nnot bootstrap. In the chapter after that we explore methods that do not require a model\\nbut do bootstrap. These key features and properties are separable, yet can be mixed in\\ninteresting combinations.\\nBibliographical and Historical Remarks\\nThe term “dynamic programming” is due to Bellman (1957a), who showed how these\\nmethods could be applied to a wide range of problems. Extensive treatments of DP can\\nbe found in many texts, including Bertsekas (2005, 2012), Bertsekas and Tsitsiklis (1996),\\nDreyfus and Law (1977), Ross (1983), White (1969), and Whittle (1982, 1983). Our\\ninterest in DP is restricted to its use in solving MDPs, but DP also applies to other types\\nof problems. Kumar and Kanal (1988) provide a more general look at DP.\\nTo the best of our knowledge, the ﬁrst connection between DP and reinforcement\\nlearning was made by Minsky (1961) in commenting on Samuel’s checkers player. In\\na footnote, Minsky mentioned that it is possible to apply DP to problems in which\\nSamuel’s backing-up process can be handled in closed analytic form. This remark may\\nhave misled artiﬁcial intelligence researchers into believing that DP was restricted to\\nanalytically tractable problems and therefore largely irrelevant to artiﬁcial intelligence.\\nAndreae (1969) mentioned DP in the context of reinforcement learning. Werbos (1977)\\nsuggested an approach to approximating DP called “heuristic dynamic programming”\\nthat emphasizes gradient-descent methods for continuous-state problems (Werbos, 1982,\\n1987, 1988, 1989, 1992). These methods are closely related to the reinforcement learning\\nalgorithms that we discuss in this book. Watkins (1989) was explicit in connecting\\nreinforcement learning to DP, characterizing a class of reinforcement learning methods as\\n“incremental dynamic programming.”\\n4.1–4 These sections describe well-established DP algorithms that are covered in any of\\nthe general DP references cited above. The policy improvement theorem and the\\npolicy iteration algorithm are due to Bellman (1957a) and Howard (1960). Our\\npresentation was inﬂuenced by the local view of policy improvement taken by\\nWatkins (1989). Our discussion of value iteration as a form of truncated policy\\niteration is based on the approach of Puterman and Shin (1978), who presented a\\nclass of algorithms calledmodiﬁed policy iteration, which includes policy iteration\\nand value iteration as special cases. An analysis showing how value iteration can\\nbe made to ﬁnd an optimal policy in ﬁnite time is given by Bertsekas (1987).\\nIterative policy evaluation is an example of a classical successive approximation\\nalgorithm for solving a system of linear equations. The version of the algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 111, 'page_label': '112'}, page_content='90 Chapter 4: Dynamic Programming\\nthat uses two arrays, one holding the old values while the other is updated, is\\noften called aJacobi-style algorithm, after Jacobi’s classical use of this method.\\nIt is also sometimes called asynchronous algorithm because the e↵ect is as if all\\nthe values are updated at the same time. The second array is needed to simulate\\nthis parallel computation sequentially. The in-place version of the algorithm\\nis often called aGauss–Seidel-style algorithm after the classical Gauss–Seidel\\nalgorithm for solving systems of linear equations. In addition to iterative policy\\nevaluation, other DP algorithms can be implemented in these di↵erent versions.\\nBertsekas and Tsitsiklis (1989) provide excellent coverage of these variations and\\ntheir performance di↵erences.\\n4.5 Asynchronous DP algorithms are due to Bertsekas (1982, 1983), who also called\\nthem distributed DP algorithms. The original motivation for asynchronous\\nDP was its implementation on a multiprocessor system with communication\\ndelays between processors and no global synchronizing clock. These algorithms\\nare extensively discussed by Bertsekas and Tsitsiklis (1989). Jacobi-style and\\nGauss–Seidel-style DP algorithms are special cases of the asynchronous version.\\nWilliams and Baird (1990) presented DP algorithms that are asynchronous at a\\nﬁner grain than the ones we have discussed: the update operations themselves\\nare broken into steps that can be performed asynchronously.\\n4.7 This section, written with the help of Michael Littman, is based on Littman,\\nDean, and Kaelbling (1995). The phrase “curse of dimensionality” is due to\\nBellman (1957a).\\nFoundational work on the linear programming approach to reinforcement learning\\nwas done by Daniela de Farias (de Farias, 2002; de Farias and Van Roy, 2003).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 112, 'page_label': '113'}, page_content='Chapter 5\\nMonte Carlo Methods\\nIn this chapter we consider our ﬁrst learning methods for estimating value functions and\\ndiscovering optimal policies. Unlike the previous chapter, here we do not assume complete\\nknowledge of the environment. Monte Carlo methods require onlyexperience—sample\\nsequences of states, actions, and rewards from actual or simulated interaction with an\\nenvironment. Learning fromactual experience is striking because it requires no prior\\nknowledge of the environment’s dynamics, yet can still attain optimal behavior. Learning\\nfrom simulated experience is also powerful. Although a model is required, the model need\\nonly generate sample transitions, not the complete probability distributions of all possible\\ntransitions that is required for dynamic programming (DP). In surprisingly many cases it\\nis easy to generate experience sampled according to the desired probability distributions,\\nbut infeasible to obtain the distributions in explicit form.\\nMonte Carlo methods are ways of solving the reinforcement learning problem based on\\naveraging sample returns. To ensure that well-deﬁned returns are available, here we deﬁne\\nMonte Carlo methods only for episodic tasks. That is, we assume experience is divided\\ninto episodes, and that all episodes eventually terminate no matter what actions are\\nselected. Only on the completion of an episode are value estimates and policies changed.\\nMonte Carlo methods can thus be incremental in an episode-by-episode sense, but not in\\na step-by-step (online) sense. The term “Monte Carlo” is often used more broadly for\\nany estimation method whose operation involves a signiﬁcant random component. Here\\nwe use it speciﬁcally for methods based on averaging complete returns (as opposed to\\nmethods that learn from partial returns, considered in the next chapter).\\nMonte Carlo methods sample and averagereturns for each state–action pair much like\\nthe bandit methods we explored in Chapter 2 sample and averagerewards for each action.\\nThe main di↵erence is that now there are multiple states, each acting like a di↵erent\\nbandit problem (like an associative-search or contextual bandit) and the di↵erent bandit\\nproblems are interrelated. That is, the return after taking an action in one state depends\\non the actions taken in later states in the same episode. Because all the action selections\\nare undergoing learning, the problem becomes nonstationary from the point of view of\\nthe earlier state.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 113, 'page_label': '114'}, page_content='92 Chapter 5: Monte Carlo Methods\\nTo handle the nonstationarity, we adapt the idea of general policy iteration (GPI)\\ndeveloped in Chapter 4 for DP. Whereas there wecomputed value functions from knowledge\\nof the MDP, here welearn value functions from sample returns with the MDP. The value\\nfunctions and corresponding policies still interact to attain optimality in essentially the\\nsame way (GPI). As in the DP chapter, ﬁrst we consider the prediction problem (the\\ncomputation ofv⇡ and q⇡ for a ﬁxed arbitrary policy⇡) then policy improvement, and,\\nﬁnally, the control problem and its solution by GPI. Each of these ideas taken from DP\\nis extended to the Monte Carlo case in which only sample experience is available.\\n5.1 Monte Carlo Prediction\\nWe begin by considering Monte Carlo methods for learning the state-value function for a\\ngiven policy. Recall that the value of a state is the expected return—expected cumulative\\nfuture discounted reward—starting from that state. An obvious way to estimate it from\\nexperience, then, is simply to average the returns observed after visits to that state. As\\nmore returns are observed, the average should converge to the expected value. This idea\\nunderlies all Monte Carlo methods.\\nIn particular, suppose we wish to estimatev⇡(s), the value of a states under policy ⇡,\\ngiven a set of episodes obtained by following⇡ and passing throughs. Each occurrence\\nof states in an episode is called avisit to s. Of course,s may be visited multiple times\\nin the same episode; let us call the ﬁrst time it is visited in an episode theﬁrst visit\\nto s.T h eﬁrst-visit MC methodestimates v⇡(s) as the average of the returns following\\nﬁrst visits tos, whereas theevery-visit MC methodaverages the returns following all\\nvisits to s. These two Monte Carlo (MC) methods are very similar but have slightly\\ndi↵erent theoretical properties. First-visit MC has been most widely studied, dating back\\nto the 1940s, and is the one we focus on in this chapter. Every-visit MC extends more\\nnaturally to function approximation and eligibility traces, as discussed in Chapters 9 and\\n12. First-visit MC is shown in procedural form in the box. Every-visit MC would be the\\nsame except without the check forSt having occurred earlier in the episode.\\nFirst-visit MC prediction, for estimatingV ⇡ v⇡\\nInput: a policy⇡ to be evaluated\\nInitialize:\\nV (s) 2 R, arbitrarily, for alls 2 S\\nReturns(s)  an empty list, for alls 2 S\\nLoop forever (for each episode):\\nGenerate an episode following⇡: S0,A 0,R 1,S 1,A 1,R 2,...,S T \\x001,A T \\x001,R T\\nG  0\\nLoop for each step of episode,t = T \\x001,T \\x002,..., 0:\\nG  \\x00G + Rt+1\\nUnless St appears in S0,S 1,...,S t\\x001:\\nAppend G to Returns(St)\\nV (St)  average(Returns(St))'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 114, 'page_label': '115'}, page_content='5.1. Monte Carlo Prediction 93\\nBoth ﬁrst-visit MC and every-visit MC converge tov⇡(s) as the number of visits (or\\nﬁrst visits) tos goes to inﬁnity. This is easy to see for the case of ﬁrst-visit MC. In\\nthis case each return is an independent, identically distributed estimate ofv⇡(s)w i t h\\nﬁnite variance. By the law of large numbers the sequence of averages of these estimates\\nconverges to their expected value. Each average is itself an unbiased estimate, and the\\nstandard deviation of its error falls as 1/pn,w h e r en is the number of returns averaged.\\nEvery-visit MC is less straightforward, but its estimates also converge quadratically to\\nv⇡(s) (Singh and Sutton, 1996).\\nThe use of Monte Carlo methods is best illustrated through an example.\\nExample 5.1: BlackjackThe object of the popular casino card game ofblackjack is to\\nobtain cards the sum of whose numerical values is as great as possible without exceeding\\n21. All face cards count as 10, and an ace can count either as 1 or as 11. We consider\\nthe version in which each player competes independently against the dealer. The game\\nbegins with two cards dealt to both dealer and player. One of the dealer’s cards is face\\nup and the other is face down. If the player has 21 immediately (an ace and a 10-card),\\nit is called anatural. He then wins unless the dealer also has a natural, in which case the\\ngame is a draw. If the player does not have a natural, then he can request additional\\ncards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes\\nbust, he loses; if he sticks, then it becomes the dealer’s turn. The dealer hits or sticks\\naccording to a ﬁxed strategy without choice: he sticks on any sum of 17 or greater, and\\nhits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome—win,\\nlose, or draw—is determined by whose ﬁnal sum is closer to 21.\\nPlaying blackjack is naturally formulated as an episodic ﬁnite MDP. Each game of\\nblackjack is an episode. Rewards of +1,\\x001, and 0 are given for winning, losing, and\\ndrawing, respectively. All rewards within a game are zero, and we do not discount (\\x00 = 1);\\ntherefore these terminal rewards are also the returns. The player’s actions are to hit or\\nto stick. The states depend on the player’s cards and the dealer’s showing card. We\\nassume that cards are dealt from an inﬁnite deck (i.e., with replacement) so that there is\\nno advantage to keeping track of the cards already dealt. If the player holds an ace that\\nhe could count as 11 without going bust, then the ace is said to beusable. In this case\\nit is always counted as 11 because counting it as 1 would make the sum 11 or less, in\\nwhich case there is no decision to be made because, obviously, the player should always\\nhit. Thus, the player makes decisions on the basis of three variables: his current sum\\n(12–21), the dealer’s one showing card (ace–10), and whether or not he holds a usable\\nace. This makes for a total of 200 states.\\nConsider the policy that sticks if the player’s sum is 20 or 21, and otherwise hits. To\\nﬁnd the state-value function for this policy by a Monte Carlo approach, one simulates\\nmany blackjack games using the policy and averages the returns following each state.\\nIn this way, we obtained the estimates of the state-value function shown in Figure 5.1.\\nThe estimates for states with a usable ace are less certain and less regular because these\\nstates are less common. In any event, after 500,000 games the value function is very well\\napproximated.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 115, 'page_label': '116'}, page_content='94 Chapter 5: Monte Carlo Methods\\n+1\\n!1\\nA Dealer showing 10 12 Player sum21\\nAfter 500,000 episodesAfter 10,000 episodes\\nUsableace\\nNousableace\\nFigure 5.1: Approximate state-value functions for the blackjack policy that sticks only on 20\\nor 21, computed by Monte Carlo policy evaluation.\\nExercise 5.1 Consider the diagrams on the right in Figure 5.1. Why does the estimated\\nvalue function jump up for the last two rows in the rear? Why does it drop o↵ for the\\nwhole last row on the left? Why are the frontmost values higher in the upper diagrams\\nthan in the lower? ⇤\\nExercise 5.2 Suppose every-visit MC was used instead of ﬁrst-visit MC on the blackjack\\ntask. Would you expect the results to be very di↵erent? Why or why not? ⇤\\nAlthough we have complete knowledge of the environment in the blackjack task, it\\nwould not be easy to apply DP methods to compute the value function. DP methods\\nrequire the distribution of next events—in particular, they require the environments\\ndynamics as given by the four-argument functionp—and it is not easy to determine\\nthis for blackjack. For example, suppose the player’s sum is 14 and he chooses to stick.\\nWhat is his probability of terminating with a reward of +1 as a function of the dealer’s\\nshowing card? All of the probabilities must be computedbefore DP can be applied, and\\nsuch computations are often complex and error-prone. In contrast, generating the sample\\ngames required by Monte Carlo methods is easy. This is the case surprisingly often; the\\nability of Monte Carlo methods to work with sample episodes alone can be a signiﬁcant\\nadvantage even when one has complete knowledge of the environment’s dynamics.\\nCan we generalize the idea of backup diagrams to Monte Carlo algorithms? The\\ngeneral idea of a backup diagram is to show at the top the root node to be updated and\\nto show below all the transitions and leaf nodes whose rewards and estimated values\\ncontribute to the update. For Monte Carlo estimation ofv⇡, the root is a state node, and\\nbelow it is the entire trajectory of transitions along a particular single episode, ending'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 116, 'page_label': '117'}, page_content='5.1. Monte Carlo Prediction 95\\nat the terminal state, as shown to the right. Whereas the DP diagram (page 59)\\nshows all possible transitions, the Monte Carlo diagram shows only those sampled\\non the one episode. Whereas the DP diagram includes only one-step transitions,\\nthe Monte Carlo diagram goes all the way to the end of the episode. These\\ndi↵erences in the diagrams accurately reﬂect the fundamental di↵erences between\\nthe algorithms.\\nAn important fact about Monte Carlo methods is that the estimates for each\\nstate are independent. The estimate for one state does not build upon the estimate\\nof any other state, as is the case in DP. In other words, Monte Carlo methods do\\nnot bootstrap as we deﬁned it in the previous chapter.\\nIn particular, note that the computational expense of estimating the value of\\na single state is independent of the number of states. This can make Monte Carlo\\nmethods particularly attractive when one requires the value of only one or a subset\\nof states. One can generate many sample episodes starting from the states of interest,\\naveraging returns from only these states, ignoring all others. This is a third advantage\\nMonte Carlo methods can have over DP methods (after the ability to learn from actual\\nexperience and from simulated experience).\\nA bubble on a wire loop.\\nFrom Hersh and Griego (1969). Reproduced with\\npermission. ©1969 Scientiﬁc American, a divi-\\nsion of Nature America, Inc. All rights reserved.\\nExample 5.2: Soap BubbleSuppose a wire\\nframe forming a closed loop is dunked in soapy\\nwater to form a soap surface or bubble conform-\\ning at its edges to the wire frame. If the geom-\\netry of the wire frame is irregular but known,\\nhow can you compute the shape of the surface?\\nThe shape has the property that the total force\\non each point exerted by neighboring points is\\nzero (or else the shape would change). This\\nmeans that the surface’s height at any point is\\nthe average of its heights at points in a small\\ncircle around that point. In addition, the sur-\\nface must meet at its boundaries with the wire\\nframe. The usual approach to problems of this\\nkind is to put a grid over the area covered by\\nthe surface and solve for its height at the grid points by an iterative computation. Grid\\npoints at the boundary are forced to the wire frame, and all others are adjusted toward\\nthe average of the heights of their four nearest neighbors. This process then iterates, much\\nlike DP’s iterative policy evaluation, and ultimately converges to a close approximation\\nto the desired surface.\\nThis is similar to the kind of problem for which Monte Carlo methods were originally\\ndesigned. Instead of the iterative computation described above, imagine standing on the\\nsurface and taking a random walk, stepping randomly from grid point to neighboring\\ngrid point, with equal probability, until you reach the boundary. It turns out that the\\nexpected value of the height at the boundary is a close approximation to the height of\\nthe desired surface at the starting point (in fact, it is exactly the value computed by the\\niterative method described above). Thus, one can closely approximate the height of the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 117, 'page_label': '118'}, page_content='96 Chapter 5: Monte Carlo Methods\\nsurface at a point by simply averaging the boundary heights of many walks started at\\nthe point. If one is interested in only the value at one point, or any ﬁxed small set of\\npoints, then this Monte Carlo method can be far more e\\x00cient than the iterative method\\nbased on local consistency.\\n5.2 Monte Carlo Estimation of Action Values\\nIf a model is not available, then it is particularly useful to estimateaction values (the\\nvalues of state–action pairs) rather thanstate values. With a model, state values alone are\\nsu\\x00cient to determine a policy; one simply looks ahead one step and chooses whichever\\naction leads to the best combination of reward and next state, as we did in the chapter on\\nDP. Without a model, however, state values alone are not su\\x00cient. One must explicitly\\nestimate the value of each action in order for the values to be useful in suggesting a policy.\\nThus, one of our primary goals for Monte Carlo methods is to estimateq⇤. To achieve\\nthis, we ﬁrst consider the policy evaluation problem for action values.\\nThe policy evaluation problem for action values is to estimateq⇡(s, a), the expected\\nreturn when starting in states, taking actiona, and thereafter following policy⇡.T h e\\nMonte Carlo methods for this are essentially the same as just presented for state values,\\nexcept now we talk about visits to a state–action pair rather than to a state. A state–\\naction pair s, ais said to be visited in an episode if ever the states is visited and action\\na is taken in it. The every-visit MC method estimates the value of a state–action pair\\nas the average of the returns that have followed all the visits to it. The ﬁrst-visit MC\\nmethod averages the returns following the ﬁrst time in each episode that the state was\\nvisited and the action was selected. These methods converge quadratically, as before, to\\nthe true expected values as the number of visits to each state–action pair approaches\\ninﬁnity.\\nThe only complication is that many state–action pairs may never be visited. If⇡ is\\na deterministic policy, then in following⇡ one will observe returns only for one of the\\nactions from each state. With no returns to average, the Monte Carlo estimates of the\\nother actions will not improve with experience. This is a serious problem because the\\npurpose of learning action values is to help in choosing among the actions available in\\neach state. To compare alternatives we need to estimate the value ofall the actions from\\neach state, not just the one we currently favor.\\nThis is the general problem ofmaintaining exploration, as discussed in the context\\nof thek-armed bandit problem in Chapter 2. For policy evaluation to work for action\\nvalues, we must assure continual exploration. One way to do this is by specifying that\\nthe episodes start in a state–action pair, and that every pair has a nonzero probability of\\nbeing selected as the start. This guarantees that all state–action pairs will be visited an\\ninﬁnite number of times in the limit of an inﬁnite number of episodes. We call this the\\nassumption ofexploring starts.\\nThe assumption of exploring starts is sometimes useful, but of course it cannot be\\nrelied upon in general, particularly when learning directly from actual interaction with an\\nenvironment. In that case the starting conditions are unlikely to be so helpful. The most\\ncommon alternative approach to assuring that all state–action pairs are encountered is'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 118, 'page_label': '119'}, page_content='5.3. Monte Carlo Control 97\\nto consider only policies that are stochastic with a nonzero probability of selecting all\\nactions in each state. We discuss two important variants of this approach in later sections.\\nFor now, we retain the assumption of exploring starts and complete the presentation of a\\nfull Monte Carlo control method.\\nExercise 5.3 What is the backup diagram for Monte Carlo estimation ofq⇡? ⇤\\n5.3 Monte Carlo Control\\nWe are now ready to consider how Monte Carlo estimation can be used in control, that\\nis, to approximate optimal policies. The overall idea is to proceed according to the same\\npattern as in the DP chapter, that is, according to the idea of generalized policy iteration\\nevaluation\\nimprovement\\n⇡ Q⇡\\x00greedy(Q)\\nQ\\x00q⇡\\n(GPI). In GPI one maintains both an approximate policy and\\nan approximate value function. The value function is repeatedly\\naltered to more closely approximate the value function for the\\ncurrent policy, and the policy is repeatedly improved with respect\\nto the current value function, as suggested by the diagram to\\nthe right. These two kinds of changes work against each other to\\nsome extent, as each creates a moving target for the other, but\\ntogether they cause both policy and value function to approach\\noptimality.\\nTo begin, let us consider a Monte Carlo version of classical policy iteration. In\\nthis method, we perform alternating complete steps of policy evaluation and policy\\nimprovement, beginning with an arbitrary policy⇡0 and ending with the optimal policy\\nand optimal action-value function:\\n⇡0\\nE\\n\\x00!q⇡0\\nI\\n\\x00!⇡1\\nE\\n\\x00!q⇡1\\nI\\n\\x00!⇡2\\nE\\n\\x00! · · ·\\nI\\n\\x00!⇡⇤\\nE\\n\\x00!q⇤,\\nwhere\\nE\\n\\x00! denotes a complete policy evaluation and\\nI\\n\\x00! denotes a complete policy\\nimprovement. Policy evaluation is done exactly as described in the preceding section.\\nMany episodes are experienced, with the approximate action-value function approaching\\nthe true function asymptotically. For the moment, let us assume that we do indeed\\nobserve an inﬁnite number of episodes and that, in addition, the episodes are generated\\nwith exploring starts. Under these assumptions, the Monte Carlo methods will compute\\neach q⇡k exactly, for arbitrary⇡k.\\nPolicy improvement is done by making the policy greedy with respect to the current\\nvalue function. In this case we have anaction-value function, and therefore no model is\\nneeded to construct the greedy policy. For any action-value functionq, the corresponding\\ngreedy policy is the one that, for eachs 2 S, deterministically chooses an action with\\nmaximal action-value:\\n⇡(s) .= arg max\\na\\nq(s, a). (5.1)\\nPolicy improvement then can be done by constructing each⇡k+1 as the greedy policy\\nwith respect toq⇡k . The policy improvement theorem (Section 4.2) then applies to⇡k'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 119, 'page_label': '120'}, page_content='98 Chapter 5: Monte Carlo Methods\\nand ⇡k+1 because, for alls 2 S,\\nq⇡k (s, ⇡k+1(s)) = q⇡k (s, argmax\\na\\nq⇡k (s, a))\\n= max\\na\\nq⇡k (s, a)\\n\\x00 q⇡k (s, ⇡k(s))\\n\\x00 v⇡k (s).\\nAs we discussed in the previous chapter, the theorem assures us that each⇡k+1 is uniformly\\nbetter than ⇡k, or just as good as⇡k, in which case they are both optimal policies. This\\nin turn assures us that the overall process converges to the optimal policy and optimal\\nvalue function. In this way Monte Carlo methods can be used to ﬁnd optimal policies\\ngiven only sample episodes and no other knowledge of the environment’s dynamics.\\nWe made two unlikely assumptions above in order to easily obtain this guarantee of\\nconvergence for the Monte Carlo method. One was that the episodes have exploring\\nstarts, and the other was that policy evaluation could be done with an inﬁnite number of\\nepisodes. To obtain a practical algorithm we will have to remove both assumptions. We\\npostpone consideration of the ﬁrst assumption until later in this chapter.\\nFor now we focus on the assumption that policy evaluation operates on an inﬁnite\\nnumber of episodes. This assumption is relatively easy to remove. In fact, the same issue\\narises even in classical DP methods such as iterative policy evaluation, which also converge\\nonly asymptotically to the true value function. In both DP and Monte Carlo cases there\\nare two ways to solve the problem. One is to hold ﬁrm to the idea of approximatingq⇡k\\nin each policy evaluation. Measurements and assumptions are made to obtain bounds\\non the magnitude and probability of error in the estimates, and then su\\x00cient steps are\\ntaken during each policy evaluation to assure that these bounds are su\\x00ciently small.\\nThis approach can probably be made completely satisfactory in the sense of guaranteeing\\ncorrect convergence up to some level of approximation. However, it is also likely to require\\nfar too many episodes to be useful in practice on any but the smallest problems.\\nThere is a second approach to avoiding the inﬁnite number of episodes nominally\\nrequired for policy evaluation, in which we give up trying to complete policy evaluation\\nbefore returning to policy improvement. On each evaluation step we move the value\\nfunction toward q⇡k , but we do not expect to actually get close except over many steps.\\nWe used this idea when we ﬁrst introduced the idea of GPI in Section 4.6. One extreme\\nform of the idea is value iteration, in which only one iteration of iterative policy evaluation\\nis performed between each step of policy improvement. The in-place version of value\\niteration is even more extreme; there we alternate between improvement and evaluation\\nsteps for single states.\\nFor Monte Carlo policy iteration it is natural to alternate between evaluation and\\nimprovement on an episode-by-episode basis. After each episode, the observed returns\\nare used for policy evaluation, and then the policy is improved at all the states visited in\\nthe episode. A complete simple algorithm along these lines, which we callMonte Carlo\\nES, for Monte Carlo with Exploring Starts, is given in pseudocode in the box on the next\\npage.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 120, 'page_label': '121'}, page_content='5.3. Monte Carlo Control 99\\nMonte Carlo ES (Exploring Starts), for estimating⇡ ⇡ ⇡⇤\\nInitialize:\\n⇡(s) 2 A(s) (arbitrarily), for alls 2 S\\nQ(s, a) 2 R (arbitrarily), for alls 2 S, a 2 A(s)\\nReturns(s, a)  empty list, for alls 2 S, a 2 A(s)\\nLoop forever (for each episode):\\nChoose S0 2 S, A0 2 A(S0) randomly such that all pairs have probability> 0\\nGenerate an episode fromS0,A 0, following⇡: S0,A 0,R 1,...,S T\\x001,A T\\x001,R T\\nG  0\\nLoop for each step of episode,t = T \\x001,T \\x002,..., 0:\\nG  \\x00G + Rt+1\\nUnless the pairSt,A t appears in S0,A 0,S 1,A 1 ...,S t\\x001,A t\\x001:\\nAppend G to Returns(St,A t)\\nQ(St,A t)  average(Returns(St,A t))\\n⇡(St)  argmaxa Q(St,a )\\nExercise 5.4 The pseudocode for Monte Carlo ES is ine\\x00cient because, for each state–\\naction pair, it maintains a list of all returns and repeatedly calculates their mean. It would\\nbe more e\\x00cient to use techniques similar to those explained in Section 2.4 to maintain\\njust the mean and a count (for each state–action pair) and update them incrementally.\\nDescribe how the pseudocode would be altered to achieve this. ⇤\\nIn Monte Carlo ES, all the returns for each state–action pair are accumulated and\\naveraged, irrespective of what policy was in force when they were observed. It is easy\\nto see that Monte Carlo ES cannot converge to any suboptimal policy. If it did, then\\nthe value function would eventually converge to the value function for that policy, and\\nthat in turn would cause the policy to change. Stability is achieved only when both\\nthe policy and the value function are optimal. Convergence to this optimal ﬁxed point\\nseems inevitable as the changes to the action-value function decrease over time, but has\\nnot yet been formally proved. In our opinion, this is one of the most fundamental open\\ntheoretical questions in reinforcement learning (for a partial solution, see Tsitsiklis, 2002).\\nExample 5.3: Solving BlackjackIt is straightforward to apply Monte Carlo ES to\\nblackjack. Because the episodes are all simulated games, it is easy to arrange for exploring\\nstarts that include all possibilities. In this case one simply picks the dealer’s cards, the\\nplayer’s sum, and whether or not the player has a usable ace, all at random with equal\\nprobability. As the initial policy we use the policy evaluated in the previous blackjack\\nexample, that which sticks only on 20 or 21. The initial action-value function can be zero\\nfor all state–action pairs. Figure 5.2 shows the optimal policy for blackjack found by\\nMonte Carlo ES. This policy is the same as the “basic” strategy of Thorp (1966) with the\\nsole exception of the leftmost notch in the policy for a usable ace, which is not present\\nin Thorp’s strategy. We are uncertain of the reason for this discrepancy, but conﬁdent\\nthat what is shown here is indeed the optimal policy for the version of blackjack we have\\ndescribed.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 121, 'page_label': '122'}, page_content='100 Chapter 5: Monte Carlo Methods\\nUsableace\\nNousableace\\n20\\n10A23456789\\nDealer showing\\nPlayer sum\\nHIT\\nSTICK 19\\n21\\n1112131415161718\\n!*\\n10A23456789\\nHIT\\nSTICK 2019\\n21\\n1112131415161718\\nV*\\n21\\n10 12\\nA\\nDealer showing Player sum\\n10\\nA\\n12\\n21\\n+1\\n\"1\\nv*\\nUsableace\\nNousableace\\n20\\n10A23456789\\nDealer showing\\nPlayer sum\\nHIT\\nSTICK 19\\n21\\n1112131415161718\\n!*\\n10A23456789\\nHIT\\nSTICK 2019\\n21\\n1112131415161718\\nV*\\n21\\n10 12\\nA\\nDealer showing Player sum\\n10\\nA\\n12\\n21\\n+1\\n\"1\\nv*\\nUsableace\\nNousableace\\n20\\n10A23456789\\nDealer showing\\nPlayer sum\\nHIT\\nSTICK 19\\n21\\n1112131415161718\\n!*\\n10A23456789\\nHIT\\nSTICK 2019\\n21\\n1112131415161718\\nV*\\n21\\n10 12\\nA\\nDealer showing Player sum\\n10\\nA\\n12\\n21\\n+1\\n\"1\\nv*\\nDealer showing\\nPlayer sum\\n* *\\nFigure 5.2: The optimal policy and state-value function for blackjack, found by Monte Carlo\\nES. The state-value function shown was computed from the action-value function found by\\nMonte Carlo ES.\\n5.4 Monte Carlo Control without Exploring Starts\\nHow can we avoid the unlikely assumption of exploring starts? The only general way to\\nensure that all actions are selected inﬁnitely often is for the agent to continue to select\\nthem. There are two approaches to ensuring this, resulting in what we callon-policy\\nmethods and o↵-policy methods. On-policy methods attempt to evaluate or improve the\\npolicy that is used to make decisions, whereas o↵-policy methods evaluate or improve\\na policy di↵erent from that used to generate the data. The Monte Carlo ES method\\ndeveloped above is an example of an on-policy method. In this section we show how an\\non-policy Monte Carlo control method can be designed that does not use the unrealistic\\nassumption of exploring starts. O↵-policy methods are considered in the next section.\\nIn on-policy control methods the policy is generallysoft, meaning that⇡(a|s) > 0\\nfor alls 2 S and alla 2 A(s), but gradually shifted closer and closer to a deterministic\\noptimal policy. Many of the methods discussed in Chapter 2 provide mechanisms for\\nthis. The on-policy method we present in this section uses\"-greedy policies, meaning\\nthat most of the time they choose an action that has maximal estimated action value,\\nbut with probability\" they instead select an action at random. That is, all nongreedy\\nactions are given the minimal probability of selection,\"\\n|A(s)| , and the remaining bulk of\\nthe probability, 1\\x00 \" + \"\\n|A(s)| , is given to the greedy action. The\"-greedy policies are\\nexamples of \"-soft policies, deﬁned as policies for which⇡(a|s) \\x00 \"\\n|A(s)| for all states and\\nactions, for some\"> 0. Among \"-soft policies, \"-greedy policies are in some sense those\\nthat are closest to greedy.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 122, 'page_label': '123'}, page_content='5.4. Monte Carlo Control without Exploring Starts 101\\nThe overall idea of on-policy Monte Carlo control is still that of GPI. As in Monte\\nCarlo ES, we use ﬁrst-visit MC methods to estimate the action-value function for the\\ncurrent policy. Without the assumption of exploring starts, however, we cannot simply\\nimprove the policy by making it greedy with respect to the current value function, because\\nthat would prevent further exploration of nongreedy actions. Fortunately, GPI does not\\nrequire that the policy be taken all the way to a greedy policy, only that it be moved\\ntoward a greedy policy. In our on-policy method we will move it only to an\"-greedy\\npolicy. For any\"-soft policy,⇡, any\"-greedy policy with respect toq⇡ is guaranteed to\\nbe better than or equal to⇡. The complete algorithm is given in the box below.\\nOn-policy ﬁrst-visit MC control (for\"-soft policies), estimates⇡ ⇡ ⇡⇤\\nAlgorithm parameter: small\"> 0\\nInitialize:\\n⇡  an arbitrary\"-soft policy\\nQ(s, a) 2 R (arbitrarily), for alls 2 S, a 2 A(s)\\nReturns(s, a)  empty list, for alls 2 S, a 2 A(s)\\nRepeat forever (for each episode):\\nGenerate an episode following⇡: S0,A 0,R 1,...,S T\\x001,A T\\x001,R T\\nG  0\\nLoop for each step of episode,t = T \\x001,T \\x002,..., 0:\\nG  \\x00G + Rt+1\\nUnless the pairSt,A t appears in S0,A 0,S 1,A 1 ...,S t\\x001,A t\\x001:\\nAppend G to Returns(St,A t)\\nQ(St,A t)  average(Returns(St,A t))\\nA⇤  argmaxa Q(St,a ) (with ties broken arbitrarily)\\nFor alla 2 A(St):\\n⇡(a|St)  \\n⇢\\n1 \\x00 \" + \"/|A(St)| if a = A⇤\\n\"/|A(St)| if a 6= A⇤\\nThat any\"-greedy policy with respect toq⇡ is an improvement over any\"-soft policy\\n⇡ is assured by the policy improvement theorem. Let⇡0 be the \"-greedy policy. The\\nconditions of the policy improvement theorem apply because for anys 2 S:\\nq⇡(s, ⇡0(s)) =\\nX\\na\\n⇡0(a|s)q⇡(s, a)\\n= \"\\n|A(s)|\\nX\\na\\nq⇡(s, a)+( 1 \\x00 \") max\\na\\nq⇡(s, a) (5.2)\\n\\x00 \"\\n|A(s)|\\nX\\na\\nq⇡(s, a)+( 1 \\x00 \")\\nX\\na\\n⇡(a|s) \\x00 \"\\n|A(s)|\\n1 \\x00 \" q⇡(s, a)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 123, 'page_label': '124'}, page_content='102 Chapter 5: Monte Carlo Methods\\n(the sum is a weighted average with nonnegative weights summing to 1, and as such it\\nmust be less than or equal to the largest number averaged)\\n= \"\\n|A(s)|\\nX\\na\\nq⇡(s, a) \\x00 \"\\n|A(s)|\\nX\\na\\nq⇡(s, a)+\\nX\\na\\n⇡(a|s)q⇡(s, a)\\n= v⇡(s).\\nThus, by the policy improvement theorem,⇡0 \\x00 ⇡ (i.e., v⇡0 (s) \\x00 v⇡(s), for alls 2 S). We\\nnow prove that equality can hold only when both⇡0 and ⇡ are optimal among the\"-soft\\npolicies, that is, when they are better than or equal to all other\"-soft policies.\\nConsider a new environment that is just like the original environment, except with the\\nrequirement that policies be\"-soft “moved inside” the environment. The new environment\\nhas the same action and state set as the original and behaves as follows. If in states\\nand taking actiona, then with probability 1\\x00 \" the new environment behaves exactly\\nlike the old environment. With probability\" it repicks the action at random, with equal\\nprobabilities, and then behaves like the old environment with the new, random action.\\nThe best one can do in this new environment with general policies is the same as the\\nbest one could do in the original environment with\"-soft policies. Letev⇤ and eq⇤ denote\\nthe optimal value functions for the new environment. Then a policy⇡ is optimal among\\n\"-soft policies if and only ifv⇡ = ev⇤. We know thatev⇤ is the unique solution to the\\nBellman optimality equation (3.19) with altered transition probabilities:\\nev⇤(s) = max\\na\\nX\\ns0,r\\nh\\n(1 \\x00 \")p(s0,r |s, a)+\\nX\\na0\\n\"\\n|A(s)| p(s0,r |s, a0)\\nih\\nr + \\x00ev⇤(s0)\\ni\\n=( 1\\x00 \") max\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00ev⇤(s0)\\ni\\n+ \"\\n|A(s)|\\nX\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00ev⇤(s0)\\ni\\n.\\nWhen equality holds and the\"-soft policy ⇡ is no longer improved, then we also know,\\nfrom (5.2), that\\nv⇡(s)=( 1 \\x00 \") max\\na\\nq⇡(s, a)+ \"\\n|A(s)|\\nX\\na\\nq⇡(s, a)\\n=( 1 \\x00 \") max\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇡(s0)\\ni\\n+ \"\\n|A(s)|\\nX\\na\\nX\\ns0,r\\np(s0,r |s, a)\\nh\\nr + \\x00v⇡(s0)\\ni\\n.\\nHowever, this equation is the same as the previous one, except for the substitution ofv⇡\\nfor ev⇤. Because ev⇤ is the unique solution, it must be thatv⇡ = ev⇤.\\nIn essence, we have shown in the last few pages that policy iteration works for\"-soft\\npolicies. Using the natural notion of greedy policy for\"-soft policies, one is assured of\\nimprovement on every step, except when the best policy has been found among the\"-soft\\npolicies. This analysis is independent of how the action-value functions are determined'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 124, 'page_label': '125'}, page_content='5.5. O↵-policy Prediction via Importance Sampling 103\\nat each stage, but it does assume that they are computed exactly. This brings us to\\nroughly the same point as in the previous section. Now we only achieve the best policy\\namong the\"-soft policies, but on the other hand, we have eliminated the assumption of\\nexploring starts.\\n5.5 O↵-policy Prediction via Importance Sampling\\nAll learning control methods face a dilemma: They seek to learn action values conditional\\non subsequent optimal behavior, but they need to behave non-optimally in order to\\nexplore all actions (toﬁnd the optimal actions). How can they learn about the optimal\\npolicy while behaving according to an exploratory policy? The on-policy approach in the\\npreceding section is actually a compromise—it learns action values not for the optimal\\npolicy, but for a near-optimal policy that still explores. A more straightforward approach\\nis to use two policies, one that is learned about and that becomes the optimal policy, and\\none that is more exploratory and is used to generate behavior. The policy being learned\\nabout is called thetarget policy, and the policy used to generate behavior is called the\\nbehavior policy. In this case we say that learning is from data “o↵” the target policy, and\\nthe overall process is termedo↵-policy learning.\\nThroughout the rest of this book we consider both on-policy and o↵-policy methods.\\nOn-policy methods are generally simpler and are considered ﬁrst. O↵-policy methods\\nrequire additional concepts and notation, and because the data is due to a di↵erent policy,\\no↵-policy methods are often of greater variance and are slower to converge. On the other\\nhand, o↵-policy methods are more powerful and general. They include on-policy methods\\nas the special case in which the target and behavior policies are the same. O↵-policy\\nmethods also have a variety of additional uses in applications. For example, they can\\noften be applied to learn from data generated by a conventional non-learning controller,\\nor from a human expert. O↵-policy learning is also seen by some as key to learning\\nmulti-step predictive models of the world’s dynamics (see Section 17.2; Sutton, 2009;\\nSutton et al., 2011).\\nIn this section we begin the study of o↵-policy methods by considering theprediction\\nproblem, in which both target and behavior policies are ﬁxed. That is, suppose we wish\\nto estimate v⇡ or q⇡, but all we have are episodes following another policyb,w h e r e\\nb 6= ⇡. In this case,⇡ is the target policy,b is the behavior policy, and both policies are\\nconsidered ﬁxed and given.\\nIn order to use episodes fromb to estimate values for⇡, we require that every action\\ntaken under ⇡ is also taken, at least occasionally, underb. That is, we require that\\n⇡(a|s) > 0i m p l i e sb(a|s) > 0. This is called the assumption ofcoverage. It follows\\nfrom coverage thatb must be stochastic in states where it is not identical to⇡.T h e\\ntarget policy ⇡, on the other hand, may be deterministic, and, in fact, this is a case\\nof particular interest in control applications. In control, the target policy is typically\\nthe deterministic greedy policy with respect to the current estimate of the action-value\\nfunction. This policy becomes a deterministic optimal policy while the behavior policy\\nremains stochastic and more exploratory, for example, an\"-greedy policy. In this section,\\nhowever, we consider the prediction problem, in which⇡ is unchanging and given.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 125, 'page_label': '126'}, page_content='104 Chapter 5: Monte Carlo Methods\\nAlmost all o↵-policy methods utilizeimportance sampling, a general technique for\\nestimating expected values under one distribution given samples from another. We apply\\nimportance sampling to o↵-policy learning by weighting returns according to the relative\\nprobability of their trajectories occurring under the target and behavior policies, called\\nthe importance-sampling ratio. Given a starting stateSt, the probability of the subsequent\\nstate–action trajectory,At,S t+1,A t+1,...,S T , occurring under any policy⇡ is\\nPr{At,S t+1,A t+1,...,S T | St,A t:T\\x001 ⇠ ⇡}\\n= ⇡(At|St)p(St+1 |St,A t)⇡(At+1|St+1) ··· p(ST |ST\\x001,A T\\x001)\\n=\\nT\\x001Y\\nk=t\\n⇡(Ak|Sk)p(Sk+1 |Sk,A k),\\nwhere p here is the state-transition probability function deﬁned by(3.4). Thus, the relative\\nprobability of the trajectory under the target and behavior policies (the importance-\\nsampling ratio) is\\n⇢t:T\\x001\\n.=\\nQT\\x001\\nk=t ⇡(Ak|Sk)p(Sk+1 |Sk,A k)\\nQT\\x001\\nk=t b(Ak|Sk)p(Sk+1 |Sk,A k)\\n=\\nT\\x001Y\\nk=t\\n⇡(Ak|Sk)\\nb(Ak|Sk) . (5.3)\\nAlthough the trajectory probabilities depend on the MDP’s transition probabilities, which\\nare generally unknown, they appear identically in both the numerator and denominator,\\nand thus cancel. The importance sampling ratio ends up depending only on the two\\npolicies and the sequence, not on the MDP.\\nRecall that we wish to estimate the expected returns (values) under the target policy,\\nbut all we have are returnsGt due to the behavior policy. These returns have the wrong\\nexpectation E[Gt|St =s] = vb(s) and so cannot be averaged to obtainv⇡.T h i si sw h e r e\\nimportance sampling comes in. The ratio⇢t:T\\x001 transforms the returns to have the right\\nexpected value:\\nE[⇢t:T\\x001Gt | St =s]= v⇡(s). (5.4)\\nNow we are ready to give a Monte Carlo algorithm that averages returns from a batch\\nof observed episodes following policyb to estimatev⇡(s). It is convenient here to number\\ntime steps in a way that increases across episode boundaries. That is, if the ﬁrst episode\\nof the batch ends in a terminal state at time 100, then the next episode begins at time\\nt = 101. This enables us to use time-step numbers to refer to particular steps in particular\\nepisodes. In particular, we can deﬁne the set of all time steps in which states is visited,\\ndenoted T(s). This is for an every-visit method; for a ﬁrst-visit method,T(s) would only\\ninclude time steps that were ﬁrst visits tos within their episodes. Also, letT(t) denote\\nthe ﬁrst time of termination following timet, andGt denote the return aftert up through\\nT(t). Then {Gt}t2T(s) are the returns that pertain to states, and\\n\\x00\\n⇢t:T(t)\\x001\\n \\nt2T(s) are\\nthe corresponding importance-sampling ratios. To estimatev⇡(s), we simply scale the\\nreturns by the ratios and average the results:\\nV (s) .=\\nP\\nt2T(s) ⇢t:T(t)\\x001Gt\\n|T(s)| . (5.5)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 126, 'page_label': '127'}, page_content='5.5. O↵-policy Prediction via Importance Sampling 105\\nWhen importance sampling is done as a simple average in this way it is calledordinary\\nimportance sampling.\\nAn important alternative isweighted importance sampling,w h i c hu s e saweighted\\naverage, deﬁned as\\nV (s) .=\\nP\\nt2T(s) ⇢t:T(t)\\x001Gt\\nP\\nt2T(s) ⇢t:T(t)\\x001\\n, (5.6)\\nor zero if the denominator is zero. To understand these two varieties of importance\\nsampling, consider the estimates of their ﬁrst-visit methods after observing a single return\\nfrom states. In the weighted-average estimate, the ratio⇢t:T(t)\\x001 for the single return\\ncancels in the numerator and denominator, so that the estimate is equal to the observed\\nreturn independent of the ratio (assuming the ratio is nonzero). Given that this return\\nwas the only one observed, this is a reasonable estimate, but its expectation isvb(s) rather\\nthan v⇡(s), and in this statistical sense it is biased. In contrast, the ﬁrst-visit version\\nof the ordinary importance-sampling estimator(5.5) is alwaysv⇡(s) in expectation (it\\nis unbiased), but it can be extreme. Suppose the ratio were ten, indicating that the\\ntrajectory observed is ten times as likely under the target policy as under the behavior\\npolicy. In this case the ordinary importance-sampling estimate would beten times the\\nobserved return. That is, it would be quite far from the observed return even though the\\nepisode’s trajectory is considered very representative of the target policy.\\nFormally, the di↵erence between the ﬁrst-visit methods of the two kinds of importance\\nsampling is expressed in their biases and variances. Ordinary importance sampling is\\nunbiased whereas weighted importance sampling is biased (though the bias converges\\nasymptotically to zero). On the other hand, the variance of ordinary importance sampling\\nis in general unbounded because the variance of the ratios can be unbounded, whereas in\\nthe weighted estimator the largest weight on any single return is one. In fact, assuming\\nbounded returns, the variance of the weighted importance-sampling estimator converges\\nto zero even if the variance of the ratios themselves is inﬁnite (Precup, Sutton, and\\nDasgupta 2001). In practice, the weighted estimator usually has dramatically lower\\nvariance and is strongly preferred. Nevertheless, we will not totally abandon ordinary\\nimportance sampling as it is easier to extend to the approximate methods using function\\napproximation that we explore in the second part of this book.\\nThe every-visit methods for ordinary and weighed importance sampling are both biased,\\nthough, again, the bias falls asymptotically to zero as the number of samples increases.\\nIn practice, every-visit methods are often preferred because they remove the need to keep\\ntrack of which states have been visited and because they are much easier to extend to\\napproximations. A complete every-visit MC algorithm for o↵-policy policy evaluation\\nusing weighted importance sampling is given in the next section on page 110.\\nExercise 5.5 Consider an MDP with a single nonterminal state and a single action\\nthat transitions back to the nonterminal state with probabilityp and transitions to the\\nterminal state with probability 1\\x00p. Let the reward be +1 on all transitions, and let\\n\\x00 =1. Suppose you observe one episode that lasts 10 steps, with a return of 10. What\\nare the ﬁrst-visit and every-visit estimators of the value of the nonterminal state?⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 127, 'page_label': '128'}, page_content='106 Chapter 5: Monte Carlo Methods\\nExample 5.4: O↵-policy Estimation of a Blackjack State ValueWe applied\\nboth ordinary and weighted importance-sampling methods to estimate the value of a single\\nblackjack state (Example 5.1) from o↵-policy data. Recall that one of the advantages of\\nMonte Carlo methods is that they can be used to evaluate a single state without forming\\nestimates for any other states. In this example, we evaluated the state in which the dealer\\nis showing a deuce, the sum of the player’s cards is 13, and the player has a usable ace (that\\nis, the player holds an ace and a deuce, or equivalently three aces). The data was generated\\nby starting in this state then choosing to hit or stick at random with equal probability\\n(the behavior policy). The target policy was to stick only on a sum of 20 or 21, as in\\nExample 5.1. The value of this state under the target policy is approximately\\x000.27726\\n(this was determined by separately generating one-hundred million episodes using the\\ntarget policy and averaging their returns). Both o↵-policy methods closely approximated\\nthis value after 1000 o↵-policy episodes using the random policy. To make sure they did\\nthis reliably, we performed 100 independent runs, each starting from estimates of zero\\nand learning for 10,000 episodes. Figure 5.3 shows the resultant learning curves—the\\nsquared error of the estimates of each method as a function of number of episodes,\\naveraged over the 100 runs. The error approaches zero for both algorithms, but the\\nweighted importance-sampling method has much lower error at the beginning, as is typical\\nin practice.\\nOrdinary \\nimportance \\nsampling\\nWeighted importance sampling\\nEpisodes (log scale)\\n0 10 100 1000 10,000\\nMean\\nsquare\\nerror\\n(average over\\n100 runs)\\n0\\n5\\nFigure 5.3: Weighted importance sampling produces lower error estimates of the value of a\\nsingle blackjack state from o↵-policy episodes.\\nExample 5.5: Inﬁnite VarianceThe estimates of ordinary importance sampling will\\ntypically have inﬁnite variance, and thus unsatisfactory convergence properties, whenever\\nthe scaled returns have inﬁnite variance—and this can easily happen in o↵-policy learning\\nwhen trajectories contain loops. A simple example is shown inset in Figure 5.4. There is\\nonly one nonterminal states and two actions,right and left.T h eright action causes a\\ndeterministic transition to termination, whereas theleft action transitions, with probability\\n0.9, back tos or, with probability 0.1, on to termination. The rewards are +1 on the\\nlatter transition and otherwise zero. Consider the target policy that always selectsleft.\\nAll episodes under this policy consist of some number (possibly zero) of transitions back'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 128, 'page_label': '129'}, page_content='5.5. O↵-policy Prediction via Importance Sampling 107\\nto s followed by termination with a reward and return of +1. Thus the value ofs under\\nthe target policy is 1 (\\x00 = 1). Suppose we are estimating this value from o↵-policy data\\nusing the behavior policy that selectsright and left with equal probability.\\n1\\n100,000 1,000,00010,000,000100,000,000\\n2\\n0.1\\n0.9\\nR=+ 1\\ns\\n⇡(left|s)=1\\nleft rightR=0\\nR=0\\nb(left|s)= 1\\n2\\nv⇡(s)\\nMonte-Carlo \\nestimate of \\n          with \\nordinary\\nimportance \\nsampling\\n(ten runs)\\nEpisodes (log scale)\\n1 10 100 1000 10,000\\n0\\nFigure 5.4: Ordinary importance sampling produces surprisingly unstable estimates on the\\none-state MDP shown inset (Example 5.5). The correct estimate here is 1 (\\x00 =1 ) ,a n d ,e v e n\\nthough this is the expected value of a sample return (after importance sampling), the variance\\nof the samples is inﬁnite, and the estimates do not converge to this value. These results are for\\no↵-policy ﬁrst-visit MC.\\nThe lower part of Figure 5.4 shows ten independent runs of the ﬁrst-visit MC algorithm\\nusing ordinary importance sampling. Even after millions of episodes, the estimates fail\\nto converge to the correct value of 1. In contrast, the weighted importance-sampling\\nalgorithm would give an estimate of exactly 1 forever after the ﬁrst episode that ended\\nwith the left action. All returns not equal to 1 (that is, ending with theright action)\\nwould be inconsistent with the target policy and thus would have a⇢t:T(t)\\x001 of zero and\\ncontribute neither to the numerator nor denominator of(5.6). The weighted importance-\\nsampling algorithm produces a weighted average of only the returns consistent with the\\ntarget policy, and all of these would be exactly 1.\\nWe can verify that the variance of the importance-sampling-scaled returns is inﬁnite\\nin this example by a simple calculation. The variance of any random variableX is the\\nexpected value of the deviation from its mean¯X, which can be written\\nVar[X] .= E\\nh\\x00\\nX \\x00 ¯X\\n\\x002i\\n= E\\n⇥\\nX2 \\x00 2X ¯X + ¯X2⇤\\n= E\\n⇥\\nX2⇤\\n\\x00 ¯X2.\\nThus, if the mean is ﬁnite, as it is in our case, the variance is inﬁnite if and only if the\\nexpectation of the square of the random variable is inﬁnite. Thus, we need only show'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 129, 'page_label': '130'}, page_content='108 Chapter 5: Monte Carlo Methods\\nthat the expected square of the importance-sampling-scaled return is inﬁnite:\\nE\\n2\\n4\\n T\\x001Y\\nt=0\\n⇡(At|St)\\nb(At|St) G0\\n!23\\n5.\\nTo compute this expectation, we break it down into cases based on episode length and\\ntermination. First note that, for any episode ending with theright action, the importance\\nsampling ratio is zero, because the target policy would never take this action; these\\nepisodes thus contribute nothing to the expectation (the quantity in parenthesis will be\\nzero) and can be ignored. We need only consider episodes that involve some number\\n(possibly zero) ofleft actions that transition back to the nonterminal state, followed by a\\nleft action transitioning to termination. All of these episodes have a return of 1, so the\\nG0 factor can be ignored. To get the expected square we need only consider each length\\nof episode, multiplying the probability of the episode’s occurrence by the square of its\\nimportance-sampling ratio, and add these up:\\n= 1\\n2 · 0.1\\n✓ 1\\n0.5\\n◆2\\n(the length 1 episode)\\n+ 1\\n2 · 0.9 · 1\\n2 · 0.1\\n✓ 1\\n0.5\\n1\\n0.5\\n◆2\\n(the length 2 episode)\\n+ 1\\n2 · 0.9 · 1\\n2 · 0.9 · 1\\n2 · 0.1\\n✓ 1\\n0.5\\n1\\n0.5\\n1\\n0.5\\n◆2\\n(the length 3 episode)\\n+ ···\\n=0 .1\\n1X\\nk=0\\n0.9k · 2k · 2= 0 .2\\n1X\\nk=0\\n1.8k = 1.\\nExercise 5.6 What is the equation analogous to(5.6) for action values Q(s, a) instead of\\nstate valuesV (s), again given returns generated usingb? ⇤\\nExercise 5.7 In learning curves such as those shown in Figure 5.3 error generally decreases\\nwith training, as indeed happened for the ordinary importance-sampling method. But for\\nthe weighted importance-sampling method error ﬁrst increased and then decreased. Why\\ndo you think this happened? ⇤\\nExercise 5.8 The results with Example 5.5 and shown in Figure 5.4 used a ﬁrst-visit MC\\nmethod. Suppose that instead an every-visit MC method was used on the same problem.\\nWould the variance of the estimator still be inﬁnite? Why or why not? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 130, 'page_label': '131'}, page_content='5.7. O↵-policy Monte Carlo Control 109\\n5.6 Incremental Implementation\\nMonte Carlo prediction methods can be implemented incrementally, on an episode-by-\\nepisode basis, using extensions of the techniques described in Chapter 2 (Section 2.4).\\nWhereas in Chapter 2 we averagedrewards, in Monte Carlo methods we averagereturns.\\nIn all other respects exactly the same methods as used in Chapter 2 can be used foron-\\npolicy Monte Carlo methods. Foro↵-policy Monte Carlo methods, we need to separately\\nconsider those that use ordinary importance sampling and those that useweighted\\nimportance sampling.\\nIn ordinary importance sampling, the returns are scaled by the importance sampling\\nratio ⇢t:T(t)\\x001 (5.3), then simply averaged, as in(5.5). For these methods we can again\\nuse the incremental methods of Chapter 2, but using the scaled returns in place of\\nthe rewards of that chapter. This leaves the case of o↵-policy methods usingweighted\\nimportance sampling. Here we have to form a weighted average of the returns, and a\\nslightly di↵erent incremental algorithm is required.\\nSuppose we have a sequence of returnsG1,G 2,...,G n\\x001, all starting in the same state\\nand each with a corresponding random weightWi (e.g., Wi = ⇢ti:T(ti)\\x001). We wish to\\nform the estimate\\nVn\\n.=\\nPn\\x001\\nk=1 WkGk\\nPn\\x001\\nk=1 Wk\\n,n \\x00 2, (5.7)\\nand keep it up-to-date as we obtain a single additional returnGn. In addition to keeping\\ntrack ofVn, we must maintain for each state the cumulative sumCn of the weights given\\nto the ﬁrstn returns. The update rule forVn is\\nVn+1\\n.= Vn + Wn\\nCn\\nh\\nGn \\x00 Vn\\ni\\n,n \\x00 1, (5.8)\\nand\\nCn+1\\n.= Cn + Wn+1,\\nwhere C0\\n.= 0 (and V1 is arbitrary and thus need not be speciﬁed). The box on the\\nnext page contains a complete episode-by-episode incremental algorithm for Monte Carlo\\npolicy evaluation. The algorithm is nominally for the o↵-policy case, using weighted\\nimportance sampling, but applies as well to the on-policy case just by choosing the\\ntarget and behavior policies as the same (in which case (⇡ = b), W is always 1). The\\napproximation Q converges toq⇡ (for all encountered state–action pairs) while actions\\nare selected according to a potentially di↵erent policy,b.\\nExercise 5.9 Modify the algorithm for ﬁrst-visit MC policy evaluation (Section 5.1) to\\nuse the incremental implementation for sample averages described in Section 2.4.⇤\\nExercise 5.10 Derive the weighted-average update rule (5.8) from (5.7). Follow the\\npattern of the derivation of the unweighted rule (2.3). ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 131, 'page_label': '132'}, page_content='110 Chapter 5: Monte Carlo Methods\\nO↵-policy MC prediction (policy evaluation) for estimatingQ ⇡ q⇡\\nInput: an arbitrary target policy⇡\\nInitialize, for alls 2 S, a 2 A(s):\\nQ(s, a) 2 R (arbitrarily)\\nC(s, a)  0\\nLoop forever (for each episode):\\nb  any policy with coverage of⇡\\nGenerate an episode followingb: S0,A 0,R 1,...,S T\\x001,A T\\x001,R T\\nG  0\\nW  1\\nLoop for each step of episode,t = T \\x001,T \\x002,..., 0, whileW 6= 0:\\nG  \\x00G + Rt+1\\nC(St,A t)  C(St,A t)+ W\\nQ(St,A t)  Q(St,A t)+ W\\nC(St,At) [G \\x00 Q(St,A t)]\\nW  W ⇡(At|St)\\nb(At|St)\\n5.7 O↵-policy Monte Carlo Control\\nWe are now ready to present an example of the second class of learning control methods\\nwe consider in this book: o↵-policy methods. Recall that the distinguishing feature of\\non-policy methods is that they estimate the value of a policy while using it for control.\\nIn o↵-policy methods these two functions are separated. The policy used to generate\\nbehavior, called the behavior policy, may in fact be unrelated to the policy that is\\nevaluated and improved, called thetarget policy. An advantage of this separation is\\nthat the target policy may be deterministic (e.g., greedy), while the behavior policy can\\ncontinue to sample all possible actions.\\nO↵-policy Monte Carlo control methods use one of the techniques presented in the\\npreceding two sections. They follow the behavior policy while learning about and\\nimproving the target policy. These techniques require that the behavior policy has a\\nnonzero probability of selecting all actions that might be selected by the target policy\\n(coverage). To explore all possibilities, we require that the behavior policy be soft (i.e.,\\nthat it select all actions in all states with nonzero probability).\\nThe box on the next page shows an o↵-policy Monte Carlo control method, based on\\nGPI and weighted importance sampling, for estimating⇡⇤ and q⇤. The target policy\\n⇡ ⇡ ⇡⇤ is the greedy policy with respect toQ, which is an estimate ofq⇡. The behavior\\npolicy b can be anything, but in order to assure convergence of⇡ to the optimal policy, an\\ninﬁnite number of returns must be obtained for each pair of state and action. This can be\\nassured by choosingb to be \"-soft. The policy⇡ converges to optimal at all encountered\\nstates even though actions are selected according to a di↵erent soft policyb,w h i c hm a y\\nchange between or even within episodes.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 132, 'page_label': '133'}, page_content='5.7. O↵-policy Monte Carlo Control 111\\nO↵-policy MC control, for estimating⇡ ⇡ ⇡⇤\\nInitialize, for alls 2 S, a 2 A(s):\\nQ(s, a) 2 R (arbitrarily)\\nC(s, a)  0\\n⇡(s)  argmaxa Q(s, a) (with ties broken consistently)\\nLoop forever (for each episode):\\nb  any soft policy\\nGenerate an episode usingb: S0,A 0,R 1,...,S T\\x001,A T\\x001,R T\\nG  0\\nW  1\\nLoop for each step of episode,t = T \\x001,T \\x002,..., 0:\\nG  \\x00G + Rt+1\\nC(St,A t)  C(St,A t)+ W\\nQ(St,A t)  Q(St,A t)+ W\\nC(St,At) [G \\x00 Q(St,A t)]\\n⇡(St)  argmaxa Q(St,a ) (with ties broken consistently)\\nIf At 6= ⇡(St) then exit inner Loop (proceed to next episode)\\nW  W 1\\nb(At|St)\\nA potential problem is that this method learns only from the tails of episodes, when\\nall of the remaining actions in the episode are greedy. If nongreedy actions are common,\\nthen learning will be slow, particularly for states appearing in the early portions of\\nlong episodes. Potentially, this could greatly slow learning. There has been insu\\x00cient\\nexperience with o↵-policy Monte Carlo methods to assess how serious this problem is. If\\nit is serious, the most important way to address it is probably by incorporating temporal-\\ndi↵erence learning, the algorithmic idea developed in the next chapter. Alternatively, if\\x00\\nis less than 1, then the idea developed in the next section may also help signiﬁcantly.\\nExercise 5.11 In the boxed algorithm for o↵-policy MC control, you may have been\\nexpecting the W update to have involved the importance-sampling ratio⇡(At|St)\\nb(At|St) ,b u t\\ninstead it involves 1\\nb(At|St) . Why is this nevertheless correct? ⇤\\nExercise 5.12: Racetrack (programming) Consider driving a race car around a turn\\nlike those shown in Figure 5.5. You want to go as fast as possible, but not so fast as\\nto run o↵ the track. In our simpliﬁed racetrack, the car is at one of a discrete set of\\ngrid positions, the cells in the diagram. The velocity is also discrete, a number of grid\\ncells moved horizontally and vertically per time step. The actions are increments to the\\nvelocity components. Each may be changed by +1,\\x001, or 0 in each step, for a total of\\nnine (3⇥ 3) actions. Both velocity components are restricted to be nonnegative and less\\nthan 5, and they cannot both be zero except at the starting line. Each episode begins\\nin one of the randomly selected start states with both velocity components zero and\\nends when the car crosses the ﬁnish line. The rewards are\\x001 for each step until the car\\ncrosses the ﬁnish line. If the car hits the track boundary, it is moved back to a random\\nposition on the starting line, both velocity components are reduced to zero, and the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 133, 'page_label': '134'}, page_content='112 Chapter 5: Monte Carlo Methods\\nStarting line\\nFinishline\\nStarting line\\nFinishline\\nFigure 5.5: A couple of right turns for the racetrack task.\\nepisode continues. Before updating the car’s location at each time step, check to see if\\nthe projected path of the car intersects the track boundary. If it intersects the ﬁnish line,\\nthe episode ends; if it intersects anywhere else, the car is considered to have hit the track\\nboundary and is sent back to the starting line. To make the task more challenging, with\\nprobability 0.1 at each time step the velocity increments are both zero, independently of\\nthe intended increments. Apply a Monte Carlo control method to this task to compute\\nthe optimal policy from each starting state. Exhibit several trajectories following the\\noptimal policy (but turn the noise o↵ for these trajectories). ⇤\\n5.8 *Discounting-aware Importance Sampling\\nThe o↵-policy methods that we have considered so far are based on forming importance-\\nsampling weights for returns considered as unitary wholes, without taking into account\\nthe returns’ internal structures as sums of discounted rewards. We now brieﬂy consider\\ncutting-edge research ideas for using this structure to signiﬁcantly reduce the variance of\\no↵-policy estimators.\\nFor example, consider the case where episodes are long and\\x00 is signiﬁcantly less than\\n1. For concreteness, say that episodes last 100 steps and that\\x00 = 0. The return from\\ntime 0 will then be justG0 = R1, but its importance sampling ratio will be a product of\\n100 factors, ⇡(A0|S0)\\nb(A0|S0)\\n⇡(A1|S1)\\nb(A1|S1) ··· ⇡(A99|S99)\\nb(A99|S99) . In ordinary importance sampling, the return\\nwill be scaled by the entire product, but it is really only necessary to scale by the ﬁrst\\nfactor, by ⇡(A0|S0)\\nb(A0|S0) . The other 99 factors ⇡(A1|S1)\\nb(A1|S1) ··· ⇡(A99|S99)\\nb(A99|S99) are irrelevant because\\nafter the ﬁrst reward the return has already been determined. These later factors are\\nall independent of the return and of expected value 1; they do not change the expected\\nupdate, but they add enormously to its variance. In some cases they could even make the\\nvariance inﬁnite. Let us now consider an idea for avoiding this large extraneous variance.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 134, 'page_label': '135'}, page_content='5.9. Per-decision Importance Sampling 113\\nThe essence of the idea is to think of discounting as determining a probability of\\ntermination or, equivalently, adegree of partial termination. For any\\x00 2 [0, 1), we can\\nthink of the returnG0 as partly terminating in one step, to the degree 1\\x00 \\x00,p r o d u c i n g\\na return of just the ﬁrst reward,R1, and as partly terminating after two steps, to the\\ndegree (1\\x00\\x00)\\x00, producing a return ofR1 + R2, and so on. The latter degree corresponds\\nto terminating on the second step, 1\\x00 \\x00, and not having already terminated on the\\nﬁrst step, \\x00. The degree of termination on the third step is thus (1\\x00 \\x00)\\x002,w i t ht h e\\x002\\nreﬂecting that termination did not occur on either of the ﬁrst two steps. The partial\\nreturns here are calledﬂat partial returns:\\n¯Gt:h\\n.= Rt+1 + Rt+2 + ··· + Rh, 0 \\uf8ff t<h \\uf8ff T,\\nwhere “ﬂat” denotes the absence of discounting, and “partial” denotes that these returns\\ndo not extend all the way to termination but instead stop ath, called thehorizon (and T\\nis the time of termination of the episode). The conventional full returnGt can be viewed\\nas a sum of ﬂat partial returns as suggested above as follows:\\nGt\\n.= Rt+1 + \\x00Rt+2 + \\x002Rt+3 + ··· + \\x00T\\x00t\\x001RT\\n=( 1\\x00 \\x00)Rt+1\\n+( 1\\x00 \\x00)\\x00 (Rt+1 + Rt+2)\\n+( 1\\x00 \\x00)\\x002 (Rt+1 + Rt+2 + Rt+3)\\n...\\n+( 1\\x00 \\x00)\\x00T\\x00t\\x002 (Rt+1 + Rt+2 + ··· + RT\\x001)\\n+ \\x00T\\x00t\\x001 (Rt+1 + Rt+2 + ··· + RT )\\n=( 1\\x00 \\x00)\\nT\\x001X\\nh=t+1\\n\\x00h\\x00t\\x001 ¯Gt:h + \\x00T\\x00t\\x001 ¯Gt:T .\\nNow we need to scale the ﬂat partial returns by an importance sampling ratio that\\nis similarly truncated. As ¯Gt:h only involves rewards up to a horizonh, we only need\\nthe ratio of the probabilities up toh \\x00 1. We deﬁne an ordinary importance-sampling\\nestimator, analogous to (5.5), as\\nV (s) .=\\nP\\nt2T(s)\\n⇣\\n(1 \\x00 \\x00) PT(t)\\x001\\nh=t+1 \\x00h\\x00t\\x001⇢t:h\\x001 ¯Gt:h + \\x00T(t)\\x00t\\x001⇢t:T(t)\\x001 ¯Gt:T(t)\\n⌘\\n|T(s)| , (5.9)\\nand a weighted importance-sampling estimator, analogous to (5.6), as\\nV (s) .=\\nP\\nt2T(s)\\n⇣\\n(1 \\x00 \\x00) PT(t)\\x001\\nh=t+1 \\x00h\\x00t\\x001⇢t:h\\x001 ¯Gt:h + \\x00T(t)\\x00t\\x001⇢t:T(t)\\x001 ¯Gt:T(t)\\n⌘\\nP\\nt2T(s)\\n⇣\\n(1 \\x00 \\x00) PT(t)\\x001\\nh=t+1 \\x00h\\x00t\\x001⇢t:h\\x001 + \\x00T(t)\\x00t\\x001⇢t:T(t)\\x001\\n⌘ . (5.10)\\nWe call these two estimatorsdiscounting-aware importance sampling estimators. They\\ntake into account the discount rate but have no e↵ect (are the same as the o↵-policy\\nestimators from Section 5.5) if\\x00 = 1.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 135, 'page_label': '136'}, page_content='114 Chapter 5: Monte Carlo Methods\\n5.9 *Per-decision Importance Sampling\\nThere is one more way in which the structure of the return as a sum of rewards can be\\ntaken into account in o↵-policy importance sampling, a way that may be able to reduce\\nvariance even in the absence of discounting (that is, even if\\x00 = 1). In the o↵-policy\\nestimators (5.5) and (5.6), each term of the sum in the numerator is itself a sum:\\n⇢t:T\\x001Gt = ⇢t:T\\x001\\n\\x00\\nRt+1 + \\x00Rt+2 + ··· + \\x00T\\x00t\\x001RT\\n\\x00\\n= ⇢t:T\\x001Rt+1 + \\x00⇢t:T\\x001Rt+2 + ··· + \\x00T\\x00t\\x001⇢t:T\\x001RT . (5.11)\\nThe o↵-policy estimators rely on the expected values of these terms, which can be written\\nin a simpler way. Note that each sub-term of(5.11) is a product of a random reward and\\na random importance-sampling ratio. For example, the ﬁrst sub-term can be written,\\nusing (5.3), as\\n⇢t:T\\x001Rt+1 = ⇡(At|St)\\nb(At|St)\\n⇡(At+1|St+1)\\nb(At+1|St+1)\\n⇡(At+2|St+2)\\nb(At+2|St+2) ··· ⇡(AT\\x001|ST\\x001)\\nb(AT\\x001|ST\\x001) Rt+1. (5.12)\\nOf all these factors, one might suspect that only the ﬁrst and the last (the reward)\\nare related; all the others are for events that occurred after the reward. Moreover, the\\nexpected value of all these other factors is one:\\nE\\n\\uf8ff⇡(Ak|Sk)\\nb(Ak|Sk)\\n\\x00\\n.=\\nX\\na\\nb(a|Sk)⇡(a|Sk)\\nb(a|Sk) =\\nX\\na\\n⇡(a|Sk)=1 . (5.13)\\nWith a few more steps, one can show that, as suspected, all of these other factors have\\nno e↵ect in expectation, in other words, that\\nE[⇢t:T\\x001Rt+1]= E[⇢t:tRt+1] . (5.14)\\nIf we repeat this process for thekth sub-term of (5.11), we get\\nE[⇢t:T\\x001Rt+k]= E[⇢t:t+k\\x001Rt+k] .\\nIt follows then that the expectation of our original term (5.11) can be written\\nE[⇢t:T\\x001Gt]= E\\nh\\n˜Gt\\ni\\n,\\nwhere\\n˜Gt = ⇢t:tRt+1 + \\x00⇢t:t+1Rt+2 + \\x002⇢t:t+2Rt+3 + ··· + \\x00T\\x00t\\x001⇢t:T\\x001RT .\\nWe call this ideaper-decision importance sampling.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 136, 'page_label': '137'}, page_content='5.10. Summary 115\\nIt follows immediately that there is an alternate importance-sampling estimator, with\\nthe same unbiased expectation (in the ﬁrst-visit case) as the ordinary-importance-sampling\\nestimator (5.5), using ˜Gt:\\nV (s) .=\\nP\\nt2T(s) ˜Gt\\n|T(s)| , (5.15)\\nwhich we might expect to sometimes be of lower variance.\\nIs there a per-decision version ofweighted importance sampling? This is less clear. So\\nfar, all the estimators that have been proposed for this that we know of are not consistent\\n(that is, they do not converge to the true value with inﬁnite data).\\n⇤Exercise 5.13 Show the steps to derive (5.14) from (5.12). ⇤\\n⇤Exercise 5.14 Modify the algorithm for o↵-policy Monte Carlo control (page 111) to use\\nthe idea of the truncated weighted-average estimator(5.10). Note that you will ﬁrst need\\nto convert this equation to action values. ⇤\\n5.10 Summary\\nThe Monte Carlo methods presented in this chapter learn value functions and optimal\\npolicies from experience in the form ofsample episodes. This gives them at least three\\nkinds of advantages over DP methods. First, they can be used to learn optimal behavior\\ndirectly from interaction with the environment, with no model of the environment’s\\ndynamics. Second, they can be used with simulation orsample models. For surprisingly\\nmany applications it is easy to simulate sample episodes even though it is di\\x00cult to\\nconstruct the kind of explicit model of transition probabilities required by DP methods.\\nThird, it is easy and e\\x00cient tofocus Monte Carlo methods on a small subset of the states.\\nA region of special interest can be accurately evaluated without going to the expense of\\naccurately evaluating the rest of the state set (we explore this further in Chapter 8).\\nA fourth advantage of Monte Carlo methods, which we discuss later in the book, is\\nthat they may be less harmed by violations of the Markov property. This is because they\\ndo not update their value estimates on the basis of the value estimates of successor states.\\nIn other words, it is because they do not bootstrap.\\nIn designing Monte Carlo control methods we have followed the overall schema of\\ngeneralized policy iteration(GPI) introduced in Chapter 4. GPI involves interacting\\nprocesses of policy evaluation and policy improvement. Monte Carlo methods provide an\\nalternative policy evaluation process. Rather than use a model to compute the value of\\neach state, they simply average many returns that start in the state. Because a state’s\\nvalue is the expected return, this average can become a good approximation to the\\nvalue. In control methods we are particularly interested in approximating action-value\\nfunctions, because these can be used to improve the policy without requiring a model of\\nthe environment’s transition dynamics. Monte Carlo methods intermix policy evaluation\\nand policy improvement steps on an episode-by-episode basis, and can be incrementally\\nimplemented on an episode-by-episode basis.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 137, 'page_label': '138'}, page_content='116 Chapter 5: Monte Carlo Methods\\nMaintaining su\\x00cient exploration is an issue in Monte Carlo control methods. It is\\nnot enough just to select the actions currently estimated to be best, because then no\\nreturns will be obtained for alternative actions, and it may never be learned that they\\nare actually better. One approach is to ignore this problem by assuming that episodes\\nbegin with state–action pairs randomly selected to cover all possibilities. Suchexploring\\nstarts can sometimes be arranged in applications with simulated episodes, but are unlikely\\nin learning from real experience. Inon-policy methods, the agent commits to always\\nexploring and tries to ﬁnd the best policy that still explores. Ino↵-policy methods, the\\nagent also explores, but learns a deterministic optimal policy that may be unrelated to\\nthe policy followed.\\nO↵-policy predictionrefers to learning the value function of atarget policy from data\\ngenerated by a di↵erentbehavior policy. Such learning methods are based on some form\\nof importance sampling, that is, on weighting returns by the ratio of the probabilities of\\ntaking the observed actions under the two policies, thereby transforming their expectations\\nfrom the behavior policy to the target policy.Ordinary importance samplinguses a\\nsimple average of the weighted returns, whereasweighted importance samplinguses a\\nweighted average. Ordinary importance sampling produces unbiased estimates, but has\\nlarger, possibly inﬁnite, variance, whereas weighted importance sampling always has\\nﬁnite variance and is preferred in practice. Despite their conceptual simplicity, o↵-policy\\nMonte Carlo methods for both prediction and control remain unsettled and are a subject\\nof ongoing research.\\nThe Monte Carlo methods treated in this chapter di↵er from the DP methods treated\\nin the previous chapter in two major ways. First, they operate on sample experience,\\nand thus can be used for direct learning without a model. Second, they do not bootstrap.\\nThat is, they do not update their value estimates on the basis of other value estimates.\\nThese two di↵erences are not tightly linked, and can be separated. In the next chapter\\nwe consider methods that learn from experience, like Monte Carlo methods, but also\\nbootstrap, like DP methods.\\nBibliographical and Historical Remarks\\nThe term “Monte Carlo” dates from the 1940s, when physicists at Los Alamos devised\\ngames of chance that they could study to help understand complex physical phenomena\\nrelating to the atom bomb. Coverage of Monte Carlo methods in this sense can be found\\nin several textbooks (e.g., Kalos and Whitlock, 1986; Rubinstein, 1981).\\n5.1–2 Singh and Sutton (1996) distinguished between every-visit and ﬁrst-visit MC\\nmethods and proved results relating these methods to reinforcement learning\\nalgorithms. The blackjack example is based on an example used by Widrow,\\nGupta, and Maitra (1973). The soap bubble example is a classical Dirichlet\\nproblem whose Monte Carlo solution was ﬁrst proposed by Kakutani (1945; see\\nHersh and Griego, 1969; Doyle and Snell, 1984).\\nBarto and Du↵ (1994) discussed policy evaluation in the context of classical\\nMonte Carlo algorithms for solving systems of linear equations. They used the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 138, 'page_label': '139'}, page_content='5.10. Summary 117\\nanalysis of Curtiss (1954) to point out the computational advantages of Monte\\nCarlo policy evaluation for large problems.\\n5.3–4 Monte Carlo ES was introduced in the 1998 edition of this book. That may have\\nbeen the ﬁrst explicit connection between Monte Carlo estimation and control\\nmethods based on policy iteration. An early use of Monte Carlo methods to\\nestimate action values in a reinforcement learning context was by Michie and\\nChambers (1968). In pole balancing (page 56), they used averages of episode\\ndurations to assess the worth (expected balancing “life”) of each possible action\\nin each state, and then used these assessments to control action selections. Their\\nmethod is similar in spirit to Monte Carlo ES with every-visit MC estimates.\\nNarendra and Wheeler (1986) studied a Monte Carlo method for ergodic ﬁnite\\nMarkov chains that used the return accumulated between successive visits to the\\nsame state as a reward for adjusting a learning automaton’s action probabilities.\\n5.5 E\\x00cient o↵-policy learning has become recognized as an important challenge\\nthat arises in several ﬁelds. For example, it is closely related to the idea of\\n“interventions” and “counterfactuals” in probabilistic graphical (Bayesian) models\\n(e.g., Pearl, 1995; Balke and Pearl, 1994). O↵-policy methods using importance\\nsampling have a long history and yet still are not well understood. Weighted\\nimportance sampling, which is also sometimes called normalized importance\\nsampling (e.g., Koller and Friedman, 2009), is discussed by Rubinstein (1981),\\nHesterberg (1988), Shelton (2001), and Liu (2001) among others.\\nThe target policy in o↵-policy learning is sometimes referred to in the literature\\nas the “estimation” policy, as it was in the ﬁrst edition of this book.\\n5.7 The racetrack exercise is adapted from Barto, Bradtke, and Singh (1995), and\\nfrom Gardner (1973).\\n5.8 Our treatment of the idea of discounting-aware importance sampling is based on\\nthe analysis of Sutton, Mahmood, Precup, and van Hasselt (2014). It has been\\nworked out most fully to date by Mahmood (2017; Mahmood, van Hasselt, and\\nSutton, 2014).\\n5.9 Per-decision importance sampling was introduced by Precup, Sutton, and Singh\\n(2000). They also combined o↵-policy learning with temporal-di↵erence learning,\\neligibility traces, and approximation methods, introducing subtle issues that we\\nconsider in later chapters.\\nExercise 5.15 Make new equations analogous to the importance-sampling Monte Carlo\\nestimates (5.5) and (5.6), but for action value estimatesQ(s, a). You will need new\\nnotation T(s, a) for the time steps on which the state–action pairs, ais visited on the\\nepisode. Do these estimates involve more or less importance-sampling correction?'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 139, 'page_label': '140'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 140, 'page_label': '141'}, page_content='Chapter 6\\nTemporal-Di↵erence Learning\\nIf one had to identify one idea as central and novel to reinforcement learning, it would\\nundoubtedly be temporal-di↵erence (TD) learning. TD learning is a combination of\\nMonte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods,\\nTD methods can learn directly from raw experience without a model of the environment’s\\ndynamics. Like DP, TD methods update estimates based in part on other learned\\nestimates, without waiting for a ﬁnal outcome (they bootstrap). The relationship between\\nTD, DP, and Monte Carlo methods is a recurring theme in the theory of reinforcement\\nlearning; this chapter is the beginning of our exploration of it. Before we are done, we\\nwill see that these ideas and methods blend into each other and can be combined in many\\nways. In particular, in Chapter 7 we introducen-step algorithms, which provide a bridge\\nfrom TD to Monte Carlo methods, and in Chapter 12 we introduce the TD(\\x00) algorithm,\\nwhich seamlessly uniﬁes them.\\nAs usual, we start by focusing on the policy evaluation orprediction problem, the\\nproblem of estimating the value functionv⇡ for a given policy⇡. For thecontrol problem\\n(ﬁnding an optimal policy), DP, TD, and Monte Carlo methods all use some variation of\\ngeneralized policy iteration (GPI). The di↵erences in the methods are primarily di↵erences\\nin their approaches to the prediction problem.\\n6.1 TD Prediction\\nBoth TD and Monte Carlo methods use experience to solve the prediction problem. Given\\nsome experience following a policy⇡, both methods update their estimateV of v⇡ for\\nthe nonterminal statesSt occurring in that experience. Roughly speaking, Monte Carlo\\nmethods wait until the return following the visit is known, then use that return as a\\ntarget forV (St). A simple every-visit Monte Carlo method suitable for nonstationary\\nenvironments is\\nV (St)  V (St)+ ↵\\nh\\nGt \\x00 V (St)\\ni\\n, (6.1)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 141, 'page_label': '142'}, page_content='120 Chapter 6: Temporal-Di↵erence Learning\\nwhere Gt is the actual return following timet, and↵ is a constant step-size parameter (c.f.,\\nEquation 2.4). Let us call this methodconstant-↵ MC. Whereas Monte Carlo methods\\nmust wait until the end of the episode to determine the increment toV (St) (only then is\\nGt known), TD methods need to wait only until the next time step. At timet +1t h e y\\nimmediately form a target and make a useful update using the observed rewardRt+1 and\\nthe estimateV (St+1). The simplest TD method makes the update\\nV (St)  V (St)+ ↵\\nh\\nRt+1 + \\x00V (St+1) \\x00 V (St)\\ni\\n(6.2)\\nimmediately on transition toSt+1 and receivingRt+1. In e↵ect, the target for the Monte\\nCarlo update isGt, whereas the target for the TD update isRt+1 + \\x00V (St+1). This TD\\nmethod is calledTD(0), orone-step TD, because it is a special case of the TD(\\x00) and\\nn-step TD methods developed in Chapter 12 and Chapter 7. The box below speciﬁes\\nTD(0) completely in procedural form.\\nTabular TD(0) for estimatingv⇡\\nInput: the policy⇡ to be evaluated\\nAlgorithm parameter: step size↵ 2 (0, 1]\\nInitialize V (s), for alls 2 S+, arbitrarily except thatV (terminal)=0\\nLoop for each episode:\\nInitialize S\\nLoop for each step of episode:\\nA  action given by⇡ for S\\nTake actionA, observeR, S0\\nV (S)  V (S)+ ↵\\n⇥\\nR + \\x00V (S0) \\x00 V (S)\\n⇤\\nS  S0\\nuntil S is terminal\\nBecause TD(0) bases its update in part on an existing estimate, we say that it is a\\nbootstrapping method, like DP. We know from Chapter 3 that\\nv⇡(s) .= E⇡[Gt | St =s] (6.3)\\n= E⇡[Rt+1 + \\x00Gt+1 | St =s] (from (3.9))\\n= E⇡[Rt+1 + \\x00v⇡(St+1) | St =s] . (6.4)\\nRoughly speaking, Monte Carlo methods use an estimate of(6.3) as a target, whereas\\nDP methods use an estimate of(6.4) as a target. The Monte Carlo target is an estimate\\nbecause the expected value in(6.3) is not known; a sample return is used in place of the\\nreal expected return. The DP target is an estimate not because of the expected values,\\nwhich are assumed to be completely provided by a model of the environment, but because\\nv⇡(St+1) is not known and the current estimate,V (St+1), is used instead. The TD target\\nis an estimate for both reasons: it samples the expected values in(6.4) and it uses the\\ncurrent estimateV instead of the truev⇡. Thus, TD methods combine the sampling of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 142, 'page_label': '143'}, page_content='6.1. TD Prediction 121\\nMonte Carlo with the bootstrapping of DP. As we shall see, with care and imagination\\nthis can take us a long way toward obtaining the advantages of both Monte Carlo and\\nDP methods.\\nTD(0)\\nShown to the right is the backup diagram for tabular TD(0). The value\\nestimate for the state node at the top of the backup diagram is updated on\\nthe basis of the one sample transition from it to the immediately following\\nstate. We refer to TD and Monte Carlo updates assample updates because\\nthey involve looking ahead to a sample successor state (or state–action pair),\\nusing the value of the successor and the reward along the way to compute a\\nbacked-up value, and then updating the value of the original state (or state–\\naction pair) accordingly. Sample updates di↵er from theexpected updates\\nof DP methods in that they are based on a single sample successor rather than on a\\ncomplete distribution of all possible successors.\\nFinally, note that the quantity in brackets in the TD(0) update is a sort of error,\\nmeasuring the di↵erence between the estimated value ofSt and the better estimate\\nRt+1 + \\x00V (St+1). This quantity, called theTD error, arises in various forms throughout\\nreinforcement learning:\\n\\x00t\\n.= Rt+1 + \\x00V (St+1) \\x00 V (St). (6.5)\\nNotice that the TD error at each time is the error in the estimatemade at that time.\\nBecause the TD error depends on the next state and next reward, it is not actually\\navailable until one time step later. That is,\\x00t is the error inV (St), available at time\\nt + 1. Also note that if the arrayV does not change during the episode (as it does not in\\nMonte Carlo methods), then the Monte Carlo error can be written as a sum of TD errors:\\nGt \\x00 V (St)= Rt+1 + \\x00Gt+1 \\x00 V (St)+ \\x00V (St+1) \\x00 \\x00V (St+1) (from (3.9))\\n= \\x00t + \\x00\\n\\x00\\nGt+1 \\x00 V (St+1)\\n\\x00\\n= \\x00t + \\x00\\x00t+1 + \\x002\\x00\\nGt+2 \\x00 V (St+2)\\n\\x00\\n= \\x00t + \\x00\\x00t+1 + \\x002\\x00t+2 + ··· + \\x00T\\x00t\\x001\\x00T\\x001 + \\x00T\\x00t\\x00\\nGT \\x00 V (ST )\\n\\x00\\n= \\x00t + \\x00\\x00t+1 + \\x002\\x00t+2 + ··· + \\x00T\\x00t\\x001\\x00T\\x001 + \\x00T\\x00t\\x00\\n0 \\x00 0\\n\\x00\\n=\\nT\\x001X\\nk=t\\n\\x00k\\x00t\\x00k. (6.6)\\nThis identity is not exact ifV is updated during the episode (as it is in TD(0)), but if the\\nstep size is small then it may still hold approximately. Generalizations of this identity\\nplay an important role in the theory and algorithms of temporal-di↵erence learning.\\nExercise 6.1 If V changes during the episode, then(6.6) only holds approximately; what\\nwould the di↵erence be between the two sides? LetVt denote the array of state values\\nused at timet in the TD error(6.5) and in the TD update(6.2). Redo the derivation\\nabove to determine the additional amount that must be added to the sum of TD errors\\nin order to equal the Monte Carlo error. ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 143, 'page_label': '144'}, page_content='122 Chapter 6: Temporal-Di↵erence Learning\\nExample 6.1: Driving Home Each day as you drive home from work, you try to\\npredict how long it will take to get home. When you leave your o\\x00ce, you note the time,\\nthe day of week, the weather, and anything else that might be relevant. Say on this\\nFriday you are leaving at exactly 6 o’clock, and you estimate that it will take 30 minutes\\nto get home. As you reach your car it is 6:05, and you notice it is starting to rain. Tra\\x00c\\nis often slower in the rain, so you reestimate that it will take 35 minutes from then, or a\\ntotal of 40 minutes. Fifteen minutes later you have completed the highway portion of\\nyour journey in good time. As you exit onto a secondary road you cut your estimate of\\ntotal travel time to 35 minutes. Unfortunately, at this point you get stuck behind a slow\\ntruck, and the road is too narrow to pass. You end up having to follow the truck until\\nyou turn onto the side street where you live at 6:40. Three minutes later you are home.\\nThe sequence of states, times, and predictions is thus as follows:\\nElapsed Time Predicted Predicted\\nState (minutes) Time to Go Total Time\\nleaving o\\x00ce, friday at 6 0 30 30\\nreach car, raining 5 35 40\\nexiting highway 20 15 35\\n2ndary road, behind truck 30 10 40\\nentering home street 40 3 43\\narrive home 43 0 43\\nThe rewards in this example are the elapsed times on each leg of the journey.1 We are\\nnot discounting (\\x00 = 1), and thus the return for each state is the actual time to go from\\nthat state. The value of each state is theexpected time to go. The second column of\\nnumbers gives the current estimated value for each state encountered.\\nA simple way to view the operation of Monte Carlo methods is to plot the predicted\\ntotal time (the last column) over the sequence, as in Figure 6.1 (left). The red arrows\\nshow the changes in predictions recommended by the constant-↵ MC method (6.1), for\\n↵ = 1. These are exactly the errors between the estimated value (predicted time to go)\\nin each state and the actual return (actual time to go). For example, when you exited\\nthe highway you thought it would take only 15 minutes more to get home, but in fact it\\ntook 23 minutes. Equation 6.1 applies at this point and determines an increment in the\\nestimate of time to go after exiting the highway. The error,Gt \\x00 V (St), at this time is\\neight minutes. Suppose the step-size parameter,↵,i s1/2. Then the predicted time to go\\nafter exiting the highway would be revised upward by four minutes as a result of this\\nexperience. This is probably too large a change in this case; the truck was probably just\\nan unlucky break. In any event, the change can only be made o↵-line, that is, after you\\nhave reached home. Only at this point do you know any of the actual returns.\\nIs it necessary to wait until the ﬁnal outcome is known before learning can begin?\\nSuppose on another day you again estimate when leaving your o\\x00ce that it will take 30\\nminutes to drive home, but then you become stuck in a massive tra\\x00c jam. Twenty-ﬁve\\nminutes after leaving the o\\x00ce you are still bumper-to-bumper on the highway. You now\\n1If this were a control problem with the objective of minimizing travel time, then we would of course\\nmake the rewards thenegative of the elapsed time. But because we are concerned here only with\\nprediction (policy evaluation), we can keep things simple by using positive numbers.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 144, 'page_label': '145'}, page_content='6.1. TD Prediction 123\\nroad\\n30\\n35\\n40\\n45\\nPredictedtotaltraveltime\\nleaving\\noffice\\nexiting\\nhighway\\n2ndary home arrive\\nSituation\\nactual outcome\\nreachcar street home\\nactual\\noutcome\\nSituation\\n30\\n35\\n40\\n45\\nPredictedtotaltraveltime\\nroad\\nleaving\\noffice\\nexiting\\nhighway\\n2ndary home arrivereachcar street home\\nFigure 6.1: Changes recommended in the driving home example by Monte Carlo methods (left)\\nand TD methods (right).\\nestimate that it will take another 25 minutes to get home, for a total of 50 minutes. As\\nyou wait in tra\\x00c, you already know that your initial estimate of 30 minutes was too\\noptimistic. Must you wait until you get home before increasing your estimate for the\\ninitial state? According to the Monte Carlo approach you must, because you don’t yet\\nknow the true return.\\nAccording to a TD approach, on the other hand, you would learn immediately, shifting\\nyour initial estimate from 30 minutes toward 50. In fact, each estimate would be shifted\\ntoward the estimate that immediately follows it. Returning to our ﬁrst day of driving,\\nFigure 6.1 (right) shows the changes in the predictions recommended by the TD rule\\n(6.2) (these are the changes made by the rule if↵ = 1). Each error is proportional to the\\nchange over time of the prediction, that is, to thetemporal di↵erencesin predictions.\\nBesides giving you something to do while waiting in tra\\x00c, there are several computa-\\ntional reasons why it is advantageous to learn based on your current predictions rather\\nthan waiting until termination when you know the actual return. We brieﬂy discuss some\\nof these in the next section.\\nExercise 6.2 This is an exercise to help develop your intuition about why TD methods\\nare often more e\\x00cient than Monte Carlo methods. Consider the driving home example\\nand how it is addressed by TD and Monte Carlo methods. Can you imagine a scenario\\nin which a TD update would be better on average than a Monte Carlo update? Give\\nan example scenario—a description of past experience and a current state—in which\\nyou would expect the TD update to be better. Here’s a hint: Suppose you have lots\\nof experience driving home from work. Then you move to a new building and a new\\nparking lot (but you still enter the highway at the same place). Now you are starting\\nto learn predictions for the new building. Can you see why TD updates are likely to be\\nmuch better, at least initially, in this case? Might the same sort of thing happen in the\\noriginal scenario? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 145, 'page_label': '146'}, page_content='124 Chapter 6: Temporal-Di↵erence Learning\\n6.2 Advantages of TD Prediction Methods\\nTD methods update their estimates based in part on other estimates. They learn a\\nguess from a guess—they bootstrap. Is this a good thing to do? What advantages do\\nTD methods have over Monte Carlo and DP methods? Developing and answering such\\nquestions will take the rest of this book and more. In this section we brieﬂy anticipate\\nsome of the answers.\\nObviously, TD methods have an advantage over DP methods in that they do not\\nrequire a model of the environment, of its reward and next-state probability distributions.\\nThe next most obvious advantage of TD methods over Monte Carlo methods is that\\nthey are naturally implemented in an online, fully incremental fashion. With Monte\\nCarlo methods one must wait until the end of an episode, because only then is the return\\nknown, whereas with TD methods one need wait only one time step. Surprisingly often\\nthis turns out to be a critical consideration. Some applications have very long episodes, so\\nthat delaying all learning until the end of the episode is too slow. Other applications are\\ncontinuing tasks and have no episodes at all. Finally, as we noted in the previous chapter,\\nsome Monte Carlo methods must ignore or discount episodes on which experimental\\nactions are taken, which can greatly slow learning. TD methods are much less susceptible\\nto these problems because they learn from each transition regardless of what subsequent\\nactions are taken.\\nBut are TD methods sound? Certainly it is convenient to learn one guess from the\\nnext, without waiting for an actual outcome, but can we still guarantee convergence\\nto the correct answer? Happily, the answer is yes. For any ﬁxed policy⇡, TD(0) has\\nbeen proved to converge tov⇡, in the mean for a constant step-size parameter if it is\\nsu\\x00ciently small, and with probability 1 if the step-size parameter decreases according to\\nthe usual stochastic approximation conditions (2.7). Most convergence proofs apply only\\nto the table-based case of the algorithm presented above (6.2), but some also apply to\\nthe case of general linear function approximation. These results are discussed in a more\\ngeneral setting in Section 9.4.\\nIf both TD and Monte Carlo methods converge asymptotically to the correct predictions,\\nthen a natural next question is “Which gets there ﬁrst?” In other words, which method\\nlearns faster? Which makes the more e\\x00cient use of limited data? At the current time\\nthis is an open question in the sense that no one has been able to prove mathematically\\nthat one method converges faster than the other. In fact, it is not even clear what is the\\nmost appropriate formal way to phrase this question! In practice, however, TD methods\\nhave usually been found to converge faster than constant-↵ MC methods on stochastic\\ntasks, as illustrated in Example 6.2.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 146, 'page_label': '147'}, page_content='6.2. Advantages of TD Prediction Methods 125\\nExample 6.2 Random Walk\\nIn this example we empirically compare the prediction abilities of TD(0) and\\nconstant-↵ MC when applied to the following Markov reward process:\\nAB CDE 100000\\nstart\\nA Markov reward process, or MRP, is a Markov decision process without actions.\\nWe will often use MRPs when focusing on the prediction problem, in which there is\\nno need to distinguish the dynamics due to the environment from those due to the\\nagent. In this MRP, all episodes start in the center state,C,t h e np r o c e e de i t h e rl e f t\\nor right by one state on each step, with equal probability. Episodes terminate either\\non the extreme left or the extreme right. When an episode terminates on the right,\\na reward of +1 occurs; all other rewards are zero. For example, a typical episode\\nmight consist of the following state-and-reward sequence:C, 0, B, 0, C, 0, D, 0, E, 1.\\nBecause this task is undiscounted, the true value of each state is the probability of\\nterminating on the right if starting from that state. Thus, the true value of the\\ncenter state isv⇡(C)=0 .5. The true values of all the states,A through E, are\\n1\\n6 , 2\\n6 , 3\\n6 , 4\\n6 , and 5\\n6 .\\n0.8\\n0\\n0.2\\n0.4\\n0.6\\nA B C D E\\n0\\n10\\n1\\n100\\nState\\nEstimatedvalue True \\nvalues\\nEstimated\\nvalue\\n0\\n0.05\\n0.1\\n0.15\\n0.2\\n0.25\\n0 25 50 75 100\\nWalks / Episodes\\nTD\\nMCRMS error,\\naveraged\\nover states\\n_\\x1b\\x0c\\x0e\\x0f\\n_\\x1b\\x0c\\x0f\\n_\\x1b\\x0c\\x0e\\x10\\n_\\x1b\\x0c\\x0e\\x11\\n_\\x1b\\x0c\\x0e\\x12\\n_\\x1b\\x0c\\x0f\\x13\\n_\\x1b\\x0c\\x0e\\x13\\nEmpirical RMS error, \\naveraged over states\\nThe left graph above shows the values learned after various numbers of episodes on\\na single run of TD(0). The estimates after 100 episodes are about as close as they\\never come to the true values—with a constant step-size parameter (↵ =0 .1i nt h i s\\nexample), the values ﬂuctuate indeﬁnitely in response to the outcomes of the most\\nrecent episodes. The right graph shows learning curves for the two methods for\\nvarious values of↵. The performance measure shown is the root mean square (RMS)\\nerror between the value function learned and the true value function, averaged over\\nthe ﬁve states, then averaged over 100 runs. In all cases the approximate value\\nfunction was initialized to the intermediate valueV (s)=0 .5, for alls.T h e T D\\nmethod was consistently better than the MC method on this task.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 147, 'page_label': '148'}, page_content='126 Chapter 6: Temporal-Di↵erence Learning\\nExercise 6.3 From the results shown in the left graph of the random walk example it\\nappears that the ﬁrst episode results in a change in onlyV (A). What does this tell you\\nabout what happened on the ﬁrst episode? Why was only the estimate for this one state\\nchanged? By exactly how much was it changed? ⇤\\nExercise 6.4 The speciﬁc results shown in the right graph of the random walk example\\nare dependent on the value of the step-size parameter,↵. Do you think the conclusions\\nabout which algorithm is better would be a↵ected if a wider range of↵ values were used?\\nIs there a di↵erent, ﬁxed value of↵ at which either algorithm would have performed\\nsigniﬁcantly better than shown? Why or why not? ⇤\\n⇤Exercise 6.5 In the right graph of the random walk example, the RMS error of the\\nTD method seems to go down and then up again, particularly at high↵’s. What could\\nhave caused this? Do you think this always occurs, or might it be a function of how the\\napproximate value function was initialized? ⇤\\nExercise 6.6 In Example 6.2 we stated that the true values for the random walk example\\nare 1\\n6 , 2\\n6 , 3\\n6 , 4\\n6 , and 5\\n6 , for statesA through E. Describe at least two di↵erent ways that\\nthese could have been computed. Which would you guess we actually used? Why?⇤\\n6.3 Optimality of TD(0)\\nSuppose there is available only a ﬁnite amount of experience, say 10 episodes or 100\\ntime steps. In this case, a common approach with incremental learning methods is to\\npresent the experience repeatedly until the method converges upon an answer. Given an\\napproximate value function,V , the increments speciﬁed by (6.1) or (6.2) are computed\\nfor every time stept at which a nonterminal state is visited, but the value function is\\nchanged only once, by the sum of all the increments. Then all the available experience is\\nprocessed again with the new value function to produce a new overall increment, and so\\non, until the value function converges. We call thisbatch updatingbecause updates are\\nmade only after processing each completebatch of training data.\\nUnder batch updating, TD(0) converges deterministically to a single answer independent\\nof the step-size parameter,↵, as long as↵ is chosen to be su\\x00ciently small. The constant-\\n↵ MC method also converges deterministically under the same conditions, but to a\\ndi↵erent answer. Understanding these two answers will help us understand the di↵erence\\nbetween the two methods. Under normal updating the methods do not move all the way\\nto their respective batch answers, but in some sense they take steps in these directions.\\nBefore trying to understand the two answers in general, for all possible tasks, we ﬁrst\\nlook at a few examples.\\nExample 6.3: Random walk under batch updatingBatch-updating versions of\\nTD(0) and constant-↵ MC were applied as follows to the random walk prediction example\\n(Example 6.2). After each new episode, all episodes seen so far were treated as a batch.\\nThey were repeatedly presented to the algorithm, either TD(0) or constant-↵ MC, with\\n↵ su\\x00ciently small that the value function converged. The resulting value function was\\nthen compared withv⇡, and the average root mean square error across the ﬁve states\\n(and across 100 independent repetitions of the whole experiment) was plotted to obtain'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 148, 'page_label': '149'}, page_content='6.3. Optimality of TD(0) 127\\nthe learning curves shown in Figure 6.2. Note that the batch TD method was consistently\\nbetter than the batch Monte Carlo method.\\n.0\\n.05\\n.1\\n.15\\n.2\\n.25\\n0 25 50 75 100\\nTD\\nMC\\nBATCH TRAINING\\nWalks / Episodes\\nRMS error,\\naveraged\\nover states\\nFigure 6.2: Performance of TD(0) and constant-↵\\nMC under batch training on the random walk task.\\nUnder batch training, constant-↵\\nMC converges to values,V (s), that\\nare sample averages of the actual re-\\nturns experienced after visiting each\\nstate s. These are optimal estimates\\nin the sense that they minimize the\\nmean square error from the actual\\nreturns in the training set. In this\\nsense it is surprising that the batch\\nTD method was able to perform\\nbetter according to the root mean\\nsquare error measure shown in the\\nﬁgure to the right. How is it that\\nbatch TD was able to perform better\\nthan this optimal method? The an-\\nswer is that the Monte Carlo method\\nis optimal only in a limited way, and\\nthat TD is optimal in a way that is more relevant to predicting returns.\\nExample 6.4: You are the PredictorPlace yourself now in the role of the predictor\\nof returns for an unknown Markov reward process. Suppose you observe the following\\neight episodes:\\nA, 0, B, 0 B, 1\\nB, 1 B, 1\\nB, 1 B, 1\\nB, 1 B, 0\\nThis means that the ﬁrst episode started in stateA, transitioned toB with a reward of\\n0, and then terminated fromB with a reward of 0. The other seven episodes were even\\nshorter, starting fromB and terminating immediately. Given this batch of data, what\\nwould you say are the optimal predictions, the best values for the estimatesV (A) and\\nV (B)? Everyone would probably agree that the optimal value forV (B)i s 3\\n4 , because six\\nout of the eight times in stateB the process terminated immediately with a return of 1,\\nand the other two times inB the process terminated immediately with a return of 0.\\nBut what is the optimal value for the estimateV (A) given this data? Here there are\\nA B\\nr = 1\\n100%\\n75%\\n25%\\nr = 0\\nr = 0\\ntwo reasonable answers. One is to observe that 100% of the\\ntimes the process was in stateA it traversed immediately to\\nB (with a reward of 0); and because we have already decided\\nthat B has value 3\\n4 , thereforeA must have value3\\n4 as well.\\nOne way of viewing this answer is that it is based on ﬁrst\\nmodeling the Markov process, in this case as shown to the\\nright, and then computing the correct estimates given the\\nmodel, which indeed in this case givesV (A)= 3\\n4 .T h i s i s\\nalso the answer that batch TD(0) gives.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 149, 'page_label': '150'}, page_content='128 Chapter 6: Temporal-Di↵erence Learning\\nThe other reasonable answer is simply to observe that we have seenA once and the\\nreturn that followed it was 0; we therefore estimateV (A) as 0. This is the answer that\\nbatch Monte Carlo methods give. Notice that it is also the answer that gives minimum\\nsquared error on the training data. In fact, it gives zero error on the data. But still we\\nexpect the ﬁrst answer to be better. If the process is Markov, we expect that the ﬁrst\\nanswer will produce lower error onfuture data, even though the Monte Carlo answer is\\nbetter on the existing data.\\nExample 6.4 illustrates a general di↵erence between the estimates found by batch\\nTD(0) and batch Monte Carlo methods. Batch Monte Carlo methods always ﬁnd the\\nestimates that minimize mean square error on the training set, whereas batch TD(0)\\nalways ﬁnds the estimates that would be exactly correct for the maximum-likelihood\\nmodel of the Markov process. In general, themaximum-likelihood estimateof a parameter\\nis the parameter value whose probability of generating the data is greatest. In this case,\\nthe maximum-likelihood estimate is the model of the Markov process formed in the\\nobvious way from the observed episodes: the estimated transition probability fromi to j\\nis the fraction of observed transitions fromi that went toj, and the associated expected\\nreward is the average of the rewards observed on those transitions. Given this model,\\nwe can compute the estimate of the value function that would be exactly correct if the\\nmodel were exactly correct. This is called thecertainty-equivalence estimatebecause it\\nis equivalent to assuming that the estimate of the underlying process was known with\\ncertainty rather than being approximated. In general, batch TD(0) converges to the\\ncertainty-equivalence estimate.\\nThis helps explain why TD methods converge more quickly than Monte Carlo methods.\\nIn batch form, TD(0) is faster than Monte Carlo methods because it computes the\\ntrue certainty-equivalence estimate. This explains the advantage of TD(0) shown in the\\nbatch results on the random walk task (Figure 6.2). The relationship to the certainty-\\nequivalence estimate may also explain in part the speed advantage of nonbatch TD(0)\\n(e.g., Example 6.2, page 125, right graph). Although the nonbatch methods do not achieve\\neither the certainty-equivalence or the minimum squared error estimates, they can be\\nunderstood as moving roughly in these directions. Nonbatch TD(0) may be faster than\\nconstant-↵ MC because it is moving toward a better estimate, even though it is not\\ngetting all the way there. At the current time nothing more deﬁnite can be said about\\nthe relative e\\x00ciency of online TD and Monte Carlo methods.\\nFinally, it is worth noting that although the certainty-equivalence estimate is in some\\nsense an optimal solution, it is almost never feasible to compute it directly. Ifn = |S| is\\nthe number of states, then just forming the maximum-likelihood estimate of the process\\nmay require on the order ofn2 memory, and computing the corresponding value function\\nrequires on the order ofn3 computational steps if done conventionally. In these terms it\\nis indeed striking that TD methods can approximate the same solution using memory\\nno more than ordern and repeated computations over the training set. On tasks with\\nlarge state spaces, TD methods may be the only feasible way of approximating the\\ncertainty-equivalence solution.\\n⇤Exercise 6.7 Design an o↵-policy version of the TD(0) update that can be used with\\narbitrary target policy⇡ and covering behavior policyb, using at each stept the importance\\nsampling ratio⇢t:t (5.3). ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 150, 'page_label': '151'}, page_content='6.4. Sarsa: On-policy TD Control 129\\n6.4 Sarsa: On-policy TD Control\\nWe turn now to the use of TD prediction methods for the control problem. As usual, we\\nfollow the pattern of generalized policy iteration (GPI), only this time using TD methods\\nfor the evaluation or prediction part. As with Monte Carlo methods, we face the need to\\ntrade o↵ exploration and exploitation, and again approaches fall into two main classes:\\non-policy and o↵-policy. In this section we present an on-policy TD control method.\\nThe ﬁrst step is to learn an action-value function rather than a state-value function.\\nIn particular, for an on-policy method we must estimateq⇡(s, a) for the current behavior\\npolicy ⇡ and for all statess and actionsa. This can be done using essentially the same TD\\nmethod described above for learningv⇡. Recall that an episode consists of an alternating\\nsequence of states and state–action pairs:\\nAt\\nRt+1St At+1\\nRt+2St+1 At+2\\nRt+3St+2 At+3St+3. . . . . .\\nIn the previous section we considered transitions from state to state and learned the\\nvalues of states. Now we consider transitions from state–action pair to state–action pair,\\nand learn the values of state–action pairs. Formally these cases are identical: they are\\nboth Markov chains with a reward process. The theorems assuring the convergence of\\nstate values under TD(0) also apply to the corresponding algorithm for action values:\\nQ(St,A t)  Q(St,A t)+ ↵\\nh\\nRt+1 + \\x00Q(St+1,A t+1) \\x00 Q(St,A t)\\ni\\n. (6.7)\\nSarsa\\nThis update is done after every transition from a nonterminal stateSt.I f\\nSt+1 is terminal, thenQ(St+1,A t+1) is deﬁned as zero. This rule uses every\\nelement of the quintuple of events, (St,A t,R t+1,S t+1,A t+1), that make up a\\ntransition from one state–action pair to the next. This quintuple gives rise to\\nthe nameSarsa for the algorithm. The backup diagram for Sarsa is as shown\\nto the right.\\nIt is straightforward to design an on-policy control algorithm based on the Sarsa\\nprediction method. As in all on-policy methods, we continually estimateq⇡ for the\\nbehavior policy ⇡, and at the same time change⇡ toward greediness with respect toq⇡.\\nThe general form of the Sarsa control algorithm is given in the box on the next page.\\nThe convergence properties of the Sarsa algorithm depend on the nature of the policy’s\\ndependence onQ. For example, one could use\"-greedy or\"-soft policies. Sarsa converges\\nwith probability 1 to an optimal policy and action-value function, under the usual\\nconditions on the step sizes(2.7), as long as all state–action pairs are visited an inﬁnite\\nnumber of times and the policy converges in the limit to the greedy policy (which can be\\narranged, for example, with\"-greedy policies by setting\" =1 /t).\\nExercise 6.8 Show that an action-value version of(6.6) holds for the action-value form\\nof the TD error\\x00t = Rt+1 + \\x00Q(St+1,A t+1) \\x00 Q(St,A t), again assuming that the values\\ndon’t change from step to step. ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 151, 'page_label': '152'}, page_content='130 Chapter 6: Temporal-Di↵erence Learning\\nSarsa (on-policy TD control) for estimatingQ ⇡ q⇤\\nAlgorithm parameters: step size↵ 2 (0, 1], small\"> 0\\nInitialize Q(s, a), for alls 2 S+,a 2 A(s), arbitrarily except thatQ(terminal, · )=0\\nLoop for each episode:\\nInitialize S\\nChoose A from S using policy derived fromQ (e.g., \"-greedy)\\nLoop for each step of episode:\\nTake actionA, observeR, S0\\nChoose A0 from S0 using policy derived fromQ (e.g., \"-greedy)\\nQ(S, A)  Q(S, A)+ ↵\\n⇥\\nR + \\x00Q(S0,A 0) \\x00 Q(S, A)\\n⇤\\nS  S0; A  A0;\\nuntil S is terminal\\nExample 6.5: Windy GridworldShown inset below is a standard gridworld, with\\nstart and goal states, but with one di↵erence: there is a crosswind running upward\\nthrough the middle of the grid. The actions are the standard four—up, down, right,\\nand left—but in the middle region the resultant next states are shifted upward by a\\n“wind,” the strength of which varies from column to column. The strength of the wind\\n0 1000 2000 3000 4000 5000 6000 70008000\\n0\\n50\\n100\\n150\\n170\\nTime steps\\nS G\\n000 0111 1 22\\nActions\\nEpisodes\\nis given below each column, in num-\\nber of cells shifted upward. For ex-\\nample, if you are one cell to the\\nright of the goal, then the action\\nleft takes you to the cell just above\\nthe goal. This is an undiscounted\\nepisodic task, with constant rewards\\nof \\x001 until the goal state is reached.\\nThe graph to the right shows the\\nresults of applying\"-greedy Sarsa to\\nthis task, with \" =0 .1, ↵ =0 .5,\\nand the initial values Q(s, a)=0\\nfor all s, a. The increasing slope of\\nthe graph shows that the goal was\\nreached more quickly over time. By\\n8000 time steps, the greedy policy was long since optimal (a trajectory from it is shown\\ninset); continued\"-greedy exploration kept the average episode length at about 17 steps,\\ntwo more than the minimum of 15. Note that Monte Carlo methods cannot easily be\\nused here because termination is not guaranteed for all policies. If a policy was ever\\nfound that caused the agent to stay in the same state, then the next episode would\\nnever end. Online learning methods such as Sarsa do not have this problem because they\\nquickly learn during the episodethat such policies are poor, and switch to something\\nelse.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 152, 'page_label': '153'}, page_content='6.5. Q-learning: O↵-policy TD Control 131\\nExercise 6.9: Windy Gridworld with King’s Moves (programming)Re-solve the windy\\ngridworld assuming eight possible actions, including the diagonal moves, rather than four.\\nHow much better can you do with the extra actions? Can you do even better by including\\na ninth action that causes no movement at all other than that caused by the wind?⇤\\nExercise 6.10: Stochastic Wind (programming)Re-solve the windy gridworld task with\\nKing’s moves, assuming that the e↵ect of the wind, if there is any, is stochastic, sometimes\\nvarying by 1 from the mean values given for each column. That is, a third of the time\\nyou move exactly according to these values, as in the previous exercise, but also a third\\nof the time you move one cell above that, and another third of the time you move one\\ncell below that. For example, if you are one cell to the right of the goal and you move\\nleft, then one-third of the time you move one cell above the goal, one-third of the time\\nyou move two cells above the goal, and one-third of the time you move to the goal.⇤\\n6.5 Q-learning: O↵-policy TD Control\\nOne of the early breakthroughs in reinforcement learning was the development of an\\no↵-policy TD control algorithm known asQ-learning (Watkins, 1989), deﬁned by\\nQ(St,A t)  Q(St,A t)+ ↵\\nh\\nRt+1 + \\x00 max\\na\\nQ(St+1,a ) \\x00 Q(St,A t)\\ni\\n. (6.8)\\nIn this case, the learned action-value function,Q, directly approximatesq⇤, the optimal\\naction-value function, independent of the policy being followed. This dramatically\\nsimpliﬁes the analysis of the algorithm and enabled early convergence proofs. The policy\\nstill has an e↵ect in that it determines which state–action pairs are visited and updated.\\nHowever, all that is required for correct convergence is that all pairs continue to be\\nupdated. As we observed in Chapter 5, this is a minimal requirement in the sense that\\nany method guaranteed to ﬁnd optimal behavior in the general case must require it.\\nUnder this assumption and a variant of the usual stochastic approximation conditions on\\nthe sequence of step-size parameters,Q has been shown to converge with probability 1 to\\nq⇤. The Q-learning algorithm is shown below in procedural form.\\nQ-learning (o↵-policy TD control) for estimating⇡ ⇡ ⇡⇤\\nAlgorithm parameters: step size↵ 2 (0, 1], small\"> 0\\nInitialize Q(s, a), for alls 2 S+,a 2 A(s), arbitrarily except thatQ(terminal, · )=0\\nLoop for each episode:\\nInitialize S\\nLoop for each step of episode:\\nChoose A from S using policy derived fromQ (e.g., \"-greedy)\\nTake actionA, observeR, S0\\nQ(S, A)  Q(S, A)+ ↵\\n⇥\\nR + \\x00 maxa Q(S0,a ) \\x00 Q(S, A)\\n⇤\\nS  S0\\nuntil S is terminal'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 153, 'page_label': '154'}, page_content='132 Chapter 6: Temporal-Di↵erence Learning\\nWhat is the backup diagram for Q-learning? The rule (6.8) updates a state–action\\npair, so the top node, the root of the update, must be a small, ﬁlled action node. The\\nupdate is alsofrom action nodes, maximizing over all those actions possible in the next\\nstate. Thus the bottom nodes of the backup diagram should be all these action nodes.\\nFinally, remember that we indicate taking the maximum of these “next action” nodes\\nwith an arc across them (Figure 3.4-right). Can you guess now what the diagram is? If\\nso, please do make a guess before turning to the answer in Figure 6.4 on page 134.\\nExample 6.6: Cli↵ WalkingThis gridworld example compares Sarsa and Q-learning,\\nhighlighting the di↵erence between on-policy (Sarsa) and o↵-policy (Q-learning) methods.\\nRewardperepsiode\\n\\x04\\x04\\x99\\x99\\n\\x04\\x05\\x9a\\n\\x04\\x9a\\x99\\n\\x04\\x06\\x9a\\n\\x99 \\x04\\x99\\x99 \\x06\\x99\\x99 \\x07\\x99\\x99 ~\\x99\\x99 \\x9a\\x99\\x99\\nEpisodes\\nSarsa\\nQ-learning\\nS G\\n  \\nTh e  Cl i f f\\n R\\nSum of rewardsduringepisode\\nR = -1\\nSafer path\\nOptimal path\\nR = -100\\nEpisodes\\nSarsa\\nQ-learning\\nS G\\nr = \\x04\\x04\\x99\\x99\\nTh e  Cl i f f\\nr =\\x08\\x04\\x04 sa\\nop\\nR\\nR\\nSum of rewardsduringepisode\\nR = -1\\nsafe path\\noptimal path\\nR = -100\\nEpisodes\\n-25\\n-50\\n-75\\n-1000 100 200 300 400 500\\nConsider the gridworld shown to the\\nright. This is a standard undis-\\ncounted, episodic task, with start\\nand goal states, and the usual ac-\\ntions causing movement up, down,\\nright, and left. Reward is\\x001 on all\\ntransitions except those into the re-\\ngion marked “The Cli↵.” Stepping\\ninto this region incurs a reward of\\n\\x00100 and sends the agent instantly\\nback to the start.\\nThe graph to the right shows the\\nperformance of the Sarsa and Q-\\nlearning methods with\"-greedy ac-\\ntion selection, \" =0 .1. After an\\ninitial transient, Q-learning learns\\nvalues for the optimal policy, that\\nwhich travels right along the edge\\nof the cli↵. Unfortunately, this re-\\nsults in its occasionally falling o↵\\nthe cli↵ because of the\"-greedy ac-\\ntion selection. Sarsa, on the other\\nhand, takes the action selection into\\naccount and learns the longer but\\nsafer path through the upper part\\nof the grid. Although Q-learning ac-\\ntually learns the values of the opti-\\nmal policy, its online performance\\nis worse than that of Sarsa, which\\nlearns the roundabout policy. Of course, if\" were gradually reduced, then both methods\\nwould asymptotically converge to the optimal policy.\\nExercise 6.11 Why is Q-learning considered ano↵-policy control method? ⇤\\nExercise 6.12 Suppose action selection is greedy. Is Q-learning then exactly the same\\nalgorithm as Sarsa? Will they make exactly the same action selections and weight\\nupdates? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 154, 'page_label': '155'}, page_content='6.6. Expected Sarsa 133\\n6.6 Expected Sarsa\\nConsider the learning algorithm that is just like Q-learning except that instead of the\\nmaximum over next state–action pairs it uses the expected value, taking into account\\nhow likely each action is under the current policy. That is, consider the algorithm with\\nthe update rule\\nQ(St,A t)  Q(St,A t)+ ↵\\nh\\nRt+1 + \\x00 E⇡[Q(St+1,A t+1) | St+1] \\x00 Q(St,A t)\\ni\\n= Q(St,A t)+ ↵\\nh\\nRt+1 + \\x00\\nX\\na\\n⇡(a|St+1)Q(St+1,a ) \\x00 Q(St,A t)\\ni\\n, (6.9)\\nbut that otherwise follows the schema of Q-learning. Given the next state,St+1,t h i s\\nalgorithm movesdeterministically in the same direction as Sarsa movesin expectation,\\nand accordingly it is calledExpected Sarsa. Its backup diagram is shown on the right in\\nFigure 6.4.\\nExpected Sarsa is more complex computationally than Sarsa but, in return, it eliminates\\nthe variance due to the random selection ofAt+1. Given the same amount of experience\\nwe might expect it to perform slightly better than Sarsa, and indeed it generally does.\\nFigure 6.3 shows summary results on the cli↵-walking task with Expected Sarsa compared\\nto Sarsa and Q-learning. Expected Sarsa retains the signiﬁcant advantage of Sarsa over\\nQ-learning on this problem. In addition, Expected Sarsa shows a signiﬁcant improvement\\nWe then present results on two versions of the windy\\ngrid world problem, one with a deterministic environment\\nand one with a stochastic environment. We do so in order\\nto evaluate the inﬂuence of environment stochasticity on\\nthe performance difference between Expected Sarsa and\\nSarsa and conﬁrm the ﬁrst part of Hypothesis 2. We then\\npresent results for different amounts of policy stochasticity\\nto conﬁrm the second part of Hypothesis 2. For completeness,\\nwe also show the performance of Q-learning on this problem.\\nFinally, we present results in other domains verifying the\\nadvantages of Expected Sarsa in a broader setting. All results\\npresented below are averaged over numerous independent\\ntrials such that the standard error becomes negligible.\\nA. Cliff Walking\\nWe begin by testing Hypothesis 1 using the cliff walking\\ntask, an undiscounted, episodic navigation task in which the\\nagent has to ﬁnd its way from start to goal in a deterministic\\ngrid world. Along the edge of the grid world is a cliff (see\\nFigure 1). The agent can take any of four movement actions:\\nup, down, left and right, each of which moves the agent one\\nsquare in the corresponding direction. Each step results in a\\nreward of -1, except when the agent steps into the cliff area,\\nwhich results in a reward of -100 and an immediate return\\nto the start state. The episode ends upon reaching the goal\\nstate.\\nS G\\nFig. 1. The cliff walking task. The agent has to move from the start [S]\\nto the goal [G], while avoiding stepping into the cliff (grey area).\\nWe evaluated the performance over the ﬁrst n episodes as\\na function of the learning rate ↵ using an \\x00-greedy policy\\nwith \\x00 = 0.1. Figure 2 shows the result for n = 100 and\\nn = 100, 000. We averaged the results over 50,000 runs and\\n10 runs, respectively.\\nDiscussion. Expected Sarsa outperforms Q-learning and\\nSarsa for all learning rate values, conﬁrming Hypothesis 1\\nand providing some evidence for Hypothesis 2. The optimal\\n↵ value of Expected Sarsa for n = 100 is 1, while for\\nSarsa it is lower, as expected for a deterministic problem.\\nThat the optimal value of Q-learning is also lower than 1 is\\nsurprising, since Q-learning also has no stochasticity in its\\nupdates in a deterministic environment. Our explanation is\\nthat Q-learning ﬁrst learns policies that are sub-optimal in\\nthe greedy sense, i.e. walking towards the goal with a detour\\nfurther from the cliff. Q-learning iteratively optimizes these\\nearly policies, resulting in a path more closely along the cliff.\\nHowever, although this path is better in the off-line sense, in\\nterms of on-line performance it is worse. A large value of\\n↵ ensures the goal is reached quickly, but a value somewhat\\nlower than 1 ensures that the agent does not try to walk right\\non the edge of the cliff immediately, resulting in a slightly\\nbetter on-line performance.\\nFor n = 100 , 000, the average return is equal for all\\n↵ values in case of Expected Sarsa and Q-learning. This\\nindicates that the algorithms have converged long before the\\nend of the run for all ↵ values, since we do not see any\\neffect of the initial learning phase. For Sarsa the performance\\ncomes close to the performance of Expected Sarsa only for\\n↵ =0 .1, while for large↵, the performance forn = 100, 000\\neven drops below the performance for n = 100. The reason\\nis that for large values of ↵ the Q values of Sarsa diverge.\\nAlthough the policy is still improved over the initial random\\npolicy during the early stages of learning, divergence causes\\nthe policy to get worse in the long run.\\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1−160\\n−140\\n−120\\n−100\\n−80\\n−60\\n−40\\n−20\\n0\\nalpha\\naverage return\\n \\n \\nn = 100, Sarsan = 100, Q−learningn = 100, Expected Sarsan = 1E5, Sarsan = 1E5, Q−learningn = 1E5, Expected Sarsa\\nFig. 2. Average return on the cliff walking task over the ﬁrst n episodes\\nfor n =1 0 0and n =1 0 0, 000 using an \\x00-greedy policy with \\x00 = 0.1. The\\nbig dots indicate the maximal values.\\nB. Windy Grid World\\nWe turn to the windy grid world task to further test Hy-\\npothesis 2. The windy grid world task is another navigation\\ntask, where the agent has to ﬁnd its way from start to goal.\\nThe grid has a height of 7 and a width of 10 squares. There\\nis a wind blowing in the ’up’ direction in the middle part of\\nthe grid, with a strength of 1 or 2 depending on the column.\\nFigure 3 shows the grid world with a number below each\\ncolumn indicating the wind strength. Again, the agent can\\nchoose between four movement actions: up, down, left and\\nright, each resulting in a reward of -1. The result of an action\\nis a movement of 1 square in the corresponding direction plus\\nan additional movement in the ’up’ direction, corresponding\\nwith the wind strength. For example, when the agent is in\\nthe square right of the goal and takes a ’left’ action, it ends\\nup in the square just above the goal.\\n1) Deterministic Environment: We ﬁrst consider a de-\\nterministic environment. As in the cliff walking task, we\\nuse an \\x00-greedy policy with \\x00 =0 .1. Figure 4 shows the\\nperformance as a function of the learning rate ↵ over the\\nﬁrst n episodes for n = 100and n = 100, 000. For n = 100\\nExpected Sarsa\\nSarsaQ-learning\\nAsymptotic Performance\\nInterim Performance\\nQ-learning\\nSum of rewards\\nper episode\\n↵\\n10.1 0.2 0.4 0.6 0.80.3 0.5 0.7 0.9\\n0\\n-40\\n-80\\n-120\\nFigure 6.3: Interim and asymptotic performance of TD control methods on the cli↵-walking\\ntask as a function of↵. All algorithms used an \"-greedy policy with \" =0 .1. Asymptotic\\nperformance is an average over 100,000 episodes whereas interim performance is an average\\nover the ﬁrst 100 episodes. These data are averages of over 50,000 and 10 runs for the interim\\nand asymptotic cases respectively. The solid circles mark the best interim performance of each\\nmethod. Adapted from van Seijen et al. (2009).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 155, 'page_label': '156'}, page_content='134 Chapter 6: Temporal-Di↵erence Learning\\nQ-learning Expected Sarsa\\nFigure 6.4: The backup diagrams for Q-learning and Expected Sarsa.\\nover Sarsa over a wide range of values for the step-size parameter↵. In cli↵ walking\\nthe state transitions are all deterministic and all randomness comes from the policy. In\\nsuch cases, Expected Sarsa can safely set↵ = 1 without su↵ering any degradation of\\nasymptotic performance, whereas Sarsa can only perform well in the long run at a small\\nvalue of↵, at which short-term performance is poor. In this and other examples there is\\na consistent empirical advantage of Expected Sarsa over Sarsa.\\nIn these cli↵ walking results Expected Sarsa was used on-policy, but in general it\\nmight use a policy di↵erent from the target policy⇡ to generate behavior, in which case\\nit becomes an o↵-policy algorithm. For example, suppose⇡ is the greedy policy while\\nbehavior is more exploratory; then Expected Sarsa is exactly Q-learning. In this sense\\nExpected Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa.\\nExcept for the small additional computational cost, Expected Sarsa may completely\\ndominate both of the other more-well-known TD control algorithms.\\n6.7 Maximization Bias and Double Learning\\nAll the control algorithms that we have discussed so far involve maximization in the\\nconstruction of their target policies. For example, in Q-learning the target policy is\\nthe greedy policy given the current action values, which is deﬁned with a max, and in\\nSarsa the policy is often\"-greedy, which also involves a maximization operation. In these\\nalgorithms, a maximum over estimated values is used implicitly as an estimate of the\\nmaximum value, which can lead to a signiﬁcant positive bias. To see why, consider a\\nsingle states where there are many actionsa whose true values,q(s, a), are all zero but\\nwhose estimated values,Q(s, a), are uncertain and thus distributed some above and some\\nbelow zero. The maximum of the true values is zero, but the maximum of the estimates\\nis positive, a positive bias. We call thismaximization bias.\\nExample 6.7: Maximization Bias Example The small MDP shown inset in\\nFigure 6.5 provides a simple example of how maximization bias can harm the performance\\nof TD control algorithms. The MDP has two non-terminal statesA and B.E p i s o d e s\\nalways start inA with a choice between two actions,left and right.T h eright action\\ntransitions immediately to the terminal state with a reward and return of zero. The\\nleft action transitions to B, also with a reward of zero, from which there are many\\npossible actions all of which cause immediate termination with a reward drawn from a\\nnormal distribution with mean\\x000.1 and variance 1.0. Thus, the expected return for\\nany trajectory starting withleft is \\x000.1, and thus takingleft in state A is always a'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 156, 'page_label': '157'}, page_content='6.7. Maximization Bias and Double Learning 135\\nB A rightleft\\n0\\n. . .\\nN(\\x000.1,1)\\n0\\nQ-learning\\nDouble\\nQ-learning\\nEpisodes\\n1001 200 300\\n% left\\nactions\\nfrom A\\n100%\\n75%\\n50%\\n25%\\n5%0 optimal\\nFigure 6.5: Comparison of Q-learning and Double Q-learning on a simple episodic MDP (shown\\ninset). Q-learning initially learns to take theleft action much more often than theright action,\\nand always takes it signiﬁcantly more often than the 5% minimum probability enforced by\\n\"-greedy action selection with\" =0 .1. In contrast, Double Q-learning is essentially una↵ected by\\nmaximization bias. These data are averaged over 10,000 runs. The initial action-value estimates\\nwere zero. Any ties in\"-greedy action selection were broken randomly.\\nmistake. Nevertheless, our control methods may favorleft because of maximization bias\\nmaking B appear to have a positive value. Figure 6.5 shows that Q-learning with\"-greedy\\naction selection initially learns to strongly favor theleft action on this example. Even at\\nasymptote, Q-learning takes theleft action about 5% more often than is optimal at our\\nparameter settings (\" =0 .1, ↵ =0 .1, and\\x00 = 1).\\nAre there algorithms that avoid maximization bias? To start, consider a bandit case in\\nwhich we have noisy estimates of the value of each of many actions, obtained as sample\\naverages of the rewards received on all the plays with each action. As we discussed above,\\nthere will be a positive maximization bias if we use the maximum of the estimates as\\nan estimate of the maximum of the true values. One way to view the problem is that\\nit is due to using the same samples (plays) both to determine the maximizing action\\nand to estimate its value. Suppose we divided the plays in two sets and used them to\\nlearn two independent estimates, call themQ1(a) and Q2(a), each an estimate of the\\ntrue value q(a), for alla 2 A. We could then use one estimate, sayQ1,t od e t e r m i n e\\nthe maximizing actionA⇤ = argmaxa Q1(a), and the other,Q2, to provide the estimate\\nof its value,Q2(A⇤)= Q2(argmaxa Q1(a)). This estimate will then be unbiased in the\\nsense thatE[Q2(A⇤)] = q(A⇤). We can also repeat the process with the role of the two\\nestimates reversed to yield a second unbiased estimateQ1(argmaxa Q2(a)). This is the\\nidea ofdouble learning. Note that although we learn two estimates, only one estimate is\\nupdated on each play; double learning doubles the memory requirements, but does not\\nincrease the amount of computation per step.\\nThe idea of double learning extends naturally to algorithms for full MDPs. For example,\\nthe double learning algorithm analogous to Q-learning, called Double Q-learning, divides\\nthe time steps in two, perhaps by ﬂipping a coin on each step. If the coin comes up heads,\\nthe update is'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 157, 'page_label': '158'}, page_content='136 Chapter 6: Temporal-Di↵erence Learning\\nQ1(St,A t)  Q1(St,A t)+↵\\nh\\nRt+1+\\x00Q2\\n\\x00\\nSt+1, argmax\\na\\nQ1(St+1,a )\\n\\x00\\n\\x00Q1(St,A t)\\ni\\n. (6.10)\\nIf the coin comes up tails, then the same update is done withQ1 and Q2 switched,\\nso that Q2 is updated. The two approximate value functions are treated completely\\nsymmetrically. The behavior policy can use both action-value estimates. For example, an\\n\"-greedy policy for Double Q-learning could be based on the average (or sum) of the two\\naction-value estimates. A complete algorithm for Double Q-learning is given in the box\\nbelow. This is the algorithm used to produce the results in Figure 6.5. In that example,\\ndouble learning seems to eliminate the harm caused by maximization bias. Of course\\nthere are also double versions of Sarsa and Expected Sarsa.\\nDouble Q-learning, for estimatingQ1 ⇡ Q2 ⇡ q⇤\\nAlgorithm parameters: step size↵ 2 (0, 1], small\"> 0\\nInitialize Q1(s, a) andQ2(s, a), for alls 2 S+,a 2 A(s), such thatQ(terminal, · )=0\\nLoop for each episode:\\nInitialize S\\nLoop for each step of episode:\\nChoose A from S using the policy\"-greedy inQ1 + Q2\\nTake actionA, observeR, S0\\nWith 0.5 probabilility:\\nQ1(S, A)  Q1(S, A)+ ↵\\n⇣\\nR + \\x00Q2\\n\\x00\\nS0, argmaxa Q1(S0,a )\\n\\x00\\n\\x00 Q1(S, A)\\n⌘\\nelse:\\nQ2(S, A)  Q2(S, A)+ ↵\\n⇣\\nR + \\x00Q1\\n\\x00\\nS0, argmaxa Q2(S0,a )\\n\\x00\\n\\x00 Q2(S, A)\\n⌘\\nS  S0\\nuntil S is terminal\\n⇤Exercise 6.13 What are the update equations for Double Expected Sarsa with an\\n\"-greedy target policy? ⇤\\n6.8 Games, Afterstates, and Other Special Cases\\nIn this book we try to present a uniform approach to a wide class of tasks, but of\\ncourse there are always exceptional tasks that are better treated in a specialized way. For\\nexample, our general approach involves learning anaction-value function, but in Chapter 1\\nwe presented a TD method for learning to play tic-tac-toe that learned something much\\nmore like astate-value function. If we look closely at that example, it becomes apparent\\nthat the function learned there is neither an action-value function nor a state-value\\nfunction in the usual sense. A conventional state-value function evaluates states in which\\nthe agent has the option of selecting an action, but the state-value function used in'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 158, 'page_label': '159'}, page_content='6.8. Games, Afterstates, and Other Special Cases 137\\ntic-tac-toe evaluates board positionsafter the agent has made its move. Let us call these\\nafterstates, and value functions over these,afterstate value functions. Afterstates are\\nuseful when we have knowledge of an initial part of the environment’s dynamics but not\\nnecessarily of the full dynamics. For example, in games we typically know the immediate\\ne↵ects of our moves. We know for each possible chess move what the resulting position\\nwill be, but not how our opponent will reply. Afterstate value functions are a natural\\nway to take advantage of this kind of knowledge and thereby produce a more e\\x00cient\\nlearning method.\\nThe reason it is more e\\x00cient to design algorithms in terms of afterstates is ap-\\nparent from the tic-tac-toe example. A conventional action-value function would map\\nfrom positions and moves to an estimate of the value. But many position—move\\npairs produce the same resulting position, as in the example below: In such cases the\\nXOX\\nXO + XO +X X\\nposition–move pairs are di↵er-\\nent but produce the same “af-\\nterposition,” and thus must have\\nthe same value. A conventional\\naction-value function would have\\nto separately assess both pairs,\\nwhereas an afterstate value func-\\ntion would immediately assess\\nboth equally. Any learning about\\nthe position–move pair on the left\\nwould immediately transfer to the\\npair on the right.\\nAfterstates arise in many tasks,\\nnot just games. For example, in\\nqueuing tasks there are actions\\nsuch as assigning customers to servers, rejecting customers, or discarding information. In\\nsuch cases the actions are in fact deﬁned in terms of their immediate e↵ects, which are\\ncompletely known.\\nIt is impossible to describe all the possible kinds of specialized problems and corre-\\nsponding specialized learning algorithms. However, the principles developed in this book\\nshould apply widely. For example, afterstate methods are still aptly described in terms\\nof generalized policy iteration, with a policy and (afterstate) value function interacting in\\nessentially the same way. In many cases one will still face the choice between on-policy\\nand o↵-policy methods for managing the need for persistent exploration.\\nExercise 6.14 Describe how the task of Jack’s Car Rental (Example 4.2) could be\\nreformulated in terms of afterstates. Why, in terms of this speciﬁc task, would such a\\nreformulation be likely to speed convergence? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 159, 'page_label': '160'}, page_content='138 Chapter 6: Temporal-Di↵erence Learning\\n6.9 Summary\\nIn this chapter we introduced a new kind of learning method, temporal-di↵erence (TD)\\nlearning, and showed how it can be applied to the reinforcement learning problem. As\\nusual, we divided the overall problem into a prediction problem and a control problem.\\nTD methods are alternatives to Monte Carlo methods for solving the prediction problem.\\nIn both cases, the extension to the control problem is via the idea of generalized policy\\niteration (GPI) that we abstracted from dynamic programming. This is the idea that\\napproximate policy and value functions should interact in such a way that they both\\nmove toward their optimal values.\\nOne of the two processes making up GPI drives the value function to accurately predict\\nreturns for the current policy; this is the prediction problem. The other process drives\\nthe policy to improve locally (e.g., to be\"-greedy) with respect to the current value\\nfunction. When the ﬁrst process is based on experience, a complication arises concerning\\nmaintaining su\\x00cient exploration. We can classify TD control methods according to\\nwhether they deal with this complication by using an on-policy or o↵-policy approach.\\nSarsa is an on-policy method, and Q-learning is an o↵-policy method. Expected Sarsa\\nis also an o↵-policy method as we present it here. There is a third way in which TD\\nmethods can be extended to control which we did not include in this chapter, called\\nactor–critic methods. These methods are covered in full in Chapter 13.\\nThe methods presented in this chapter are today the most widely used reinforcement\\nlearning methods. This is probably due to their great simplicity: they can be applied\\nonline, with a minimal amount of computation, to experience generated from interaction\\nwith an environment; they can be expressed nearly completely by single equations that\\ncan be implemented with small computer programs. In the next few chapters we extend\\nthese algorithms, making them slightly more complicated and signiﬁcantly more powerful.\\nAll the new algorithms will retain the essence of those introduced here: they will be able\\nto process experience online, with relatively little computation, and they will be driven\\nby TD errors. The special cases of TD methods introduced in the present chapter should\\nrightly be calledone-step, tabular, model-freeTD methods. In the next two chapters we\\nextend them ton-step forms (a link to Monte Carlo methods) and forms that include\\na model of the environment (a link to planning and dynamic programming). Then, in\\nthe second part of the book we extend them to various forms of function approximation\\nrather than tables (a link to deep learning and artiﬁcial neural networks).\\nFinally, in this chapter we have discussed TD methods entirely within the context of\\nreinforcement learning problems, but TD methods are actually more general than this.\\nThey are general methods for learning to make long-term predictions about dynamical\\nsystems. For example, TD methods may be relevant to predicting ﬁnancial data, life\\nspans, election outcomes, weather patterns, animal behavior, demands on power stations,\\nor customer purchases. It was only when TD methods were analyzed as pure prediction\\nmethods, independent of their use in reinforcement learning, that their theoretical\\nproperties ﬁrst came to be well understood. Even so, these other potential applications\\nof TD learning methods have not yet been extensively explored.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 160, 'page_label': '161'}, page_content='6.9. Summary 139\\nBibliographical and Historical Remarks\\nAs we outlined in Chapter 1, the idea of TD learning has its early roots in animal learning\\npsychology and artiﬁcial intelligence, most notably the work of Samuel (1959) and Klopf\\n(1972). Samuel’s work is described as a case study in Section 16.2. Also related to TD\\nlearning are Holland’s (1975, 1976) early ideas about consistency among value predictions.\\nThese inﬂuenced one of the authors (Barto), who was a graduate student from 1970 to\\n1975 at the University of Michigan, where Holland was teaching. Holland’s ideas led to\\na number of TD-related systems, including the work of Booker (1982) and the bucket\\nbrigade of Holland (1986), which is related to Sarsa as discussed below.\\n6.1–2 Most of the speciﬁc material from these sections is from Sutton (1988), includ-\\ning the TD(0) algorithm, the random walk example, and the term “temporal-\\ndi↵erence learning.” The characterization of the relationship to dynamic pro-\\ngramming and Monte Carlo methods was inﬂuenced by Watkins (1989), Werbos\\n(1987), and others. The use of backup diagrams was new to the ﬁrst edition of\\nthis book.\\nTabular TD(0) was proved to converge in the mean by Sutton (1988) and with\\nprobability 1 by Dayan (1992), based on the work of Watkins and Dayan (1992).\\nThese results were extended and strengthened by Jaakkola, Jordan, and Singh\\n(1994) and Tsitsiklis (1994) by using extensions of the powerful existing theory\\nof stochastic approximation. Other extensions and generalizations are covered in\\nlater chapters.\\n6.3 The optimality of the TD algorithm under batch training was established by\\nSutton (1988). Illuminating this result is Barnard’s (1993) derivation of the TD\\nalgorithm as a combination of one step of an incremental method for learning a\\nmodel of the Markov chain and one step of a method for computing predictions\\nfrom the model. The termcertainty equivalence is from the adaptive control\\nliterature (e.g., Goodwin and Sin, 1984).\\n6.4 The Sarsa algorithm was introduced by Rummery and Niranjan (1994). They\\nexplored it in conjunction with artiﬁcial neural networks and called it “Modiﬁed\\nConnectionist Q-learning”. The name “Sarsa” was introduced by Sutton (1996).\\nThe convergence of one-step tabular Sarsa (the form treated in this chapter) has\\nbeen proved by Singh, Jaakkola, Littman, and Szepesv´ ari (2000). The “windy\\ngridworld” example was suggested by Tom Kalt.\\nHolland’s (1986) bucket brigade idea evolved into an algorithm closely related to\\nSarsa. The original idea of the bucket brigade involved chains of rules triggering\\neach other; it focused on passing credit back from the current rule to the rules\\nthat triggered it. Over time, the bucket brigade came to be more like TD learning\\nin passing credit back to any temporally preceding rule, not just to the ones\\nthat triggered the current rule. The modern form of the bucket brigade, when\\nsimpliﬁed in various natural ways, is nearly identical to one-step Sarsa, as detailed\\nby Wilson (1994).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 161, 'page_label': '162'}, page_content='140 Chapter 6: Temporal-Di↵erence Learning\\n6.5 Q-learning was introduced by Watkins (1989), whose outline of a convergence\\nproof was made rigorous by Watkins and Dayan (1992). More general convergence\\nresults were proved by Jaakkola, Jordan, and Singh (1994) and Tsitsiklis (1994).\\n6.6 The Expected Sarsa algorithm was introduced by George John (1994), who\\ncalled it “Q-learning” and stressed its advantages over Q-learning as an o↵-policy\\nalgorithm. John’s work was not known to us when we presented Expected\\nSarsa in the ﬁrst edition of this book as an exercise, or to van Seijen, van\\nHasselt, Whiteson, and Weiring (2009) when they established Expected Sarsa’s\\nconvergence properties and conditions under which it will outperform regular\\nSarsa and Q-learning. Our Figure 6.3 is adapted from their results. Van Seijen\\net al. deﬁned “Expected Sarsa” to be an on-policy method exclusively (as we\\ndid in the ﬁrst edition), whereas now we use this name for the general algorithm\\nin which the target and behavior policies may di↵er. The general o↵-policy\\nview of Expected Sarsa was noted by van Hasselt (2011), who called it “General\\nQ-learning.”\\n6.7 Maximization bias and double learning were introduced and extensively investi-\\ngated by van Hasselt (2010, 2011). The example MDP in Figure 6.5 was adapted\\nfrom that in his Figure 4.1 (van Hasselt, 2011).\\n6.8 The notion of an afterstate is the same as that of a “post-decision state” (Van\\nRoy, Bertsekas, Lee, and Tsitsiklis, 1997; Powell, 2011).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 162, 'page_label': '163'}, page_content='Chapter 7\\nn-step Bootstrapping\\nIn this chapter we unify the Monte Carlo (MC) methods and the one-step temporal-\\ndi↵erence (TD) methods presented in the previous two chapters. Neither MC methods nor\\none-step TD methods are always the best. In this chapter we presentn-step TD methods\\nthat generalize both methods so that one can shift from one to the other smoothly as\\nneeded to meet the demands of a particular task.n-step methods span a spectrum with\\nMC methods at one end and one-step TD methods at the other. The best methods are\\noften intermediate between the two extremes.\\nAnother way of looking at the beneﬁts ofn-step methods is that they free you from\\nthe tyranny of the time step. With one-step TD methods the same time step determines\\nhow often the action can be changed and the time interval over which bootstrapping\\nis done. In many applications one wants to be able to update the action very fast to\\ntake into account anything that has changed, but bootstrapping works best if it is over a\\nlength of time in which a signiﬁcant and recognizable state change has occurred. With\\none-step TD methods, these time intervals are the same, and so a compromise must be\\nmade. n-step methods enable bootstrapping to occur over multiple steps, freeing us from\\nthe tyranny of the single time step.\\nThe idea of n-step methods is usually used as an introduction to the algorithmic\\nidea of eligibility traces (Chapter 12), which enable bootstrapping over multiple time\\nintervals simultaneously. Here we instead consider then-step bootstrapping idea on its\\nown, postponing the treatment of eligibility-trace mechanisms until later. This allows us\\nto separate the issues better, dealing with as many of them as possible in the simpler\\nn-step setting.\\nAs usual, we ﬁrst consider the prediction problem and then the control problem. That\\nis, we ﬁrst consider hown-step methods can help in predicting returns as a function of\\nstate for a ﬁxed policy (i.e., in estimatingv⇡). Then we extend the ideas to action values\\nand control methods.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 163, 'page_label': '164'}, page_content='142 Chapter 7: n-step Bootstrapping\\n7.1 n-step TD Prediction\\nWhat is the space of methods lying between Monte Carlo and TD methods? Consider\\nestimating v⇡ from sample episodes generated using⇡. Monte Carlo methods perform\\nan update for each state based on the entire sequence of observed rewards from that\\nstate until the end of the episode. The update of one-step TD methods, on the other\\nhand, is based on just the one next reward, bootstrapping from the value of the state\\none step later as a proxy for the remaining rewards. One kind of intermediate method,\\nthen, would perform an update based on an intermediate number of rewards: more than\\none, but less than all of them until termination. For example, a two-step update would\\nbe based on the ﬁrst two rewards and the estimated value of the state two steps later.\\nSimilarly, we could have three-step updates, four-step updates, and so on. Figure 7.1\\nshows the backup diagrams of the spectrum ofn-step updates for v⇡, with the one-step\\nTD update on the left and the up-until-termination Monte Carlo update on the right.\\n1-step TDand TD(0)2-step TD3-step TD n-step TD∞-step TDand Monte Carlo\\n···\\n···\\n···\\n···\\nFigure 7.1: The backup diagrams ofn-step methods. These methods form a spectrum ranging\\nfrom one-step TD methods to Monte Carlo methods.\\nThe methods that usen-step updates are still TD methods because they still change\\nan earlier estimate based on how it di↵ers from a later estimate. Now the later estimate\\nis not one step later, butn steps later. Methods in which the temporal di↵erence extends\\nover n steps are calledn-step TD methods. The TD methods introduced in the previous\\nchapter all used one-step updates, which is why we called them one-step TD methods.\\nMore formally, consider the update of the estimated value of stateSt as a result of the\\nstate–reward sequence,St,R t+1,S t+1,R t+2,...,R T ,S T (omitting the actions). We know\\nthat in Monte Carlo updates the estimate ofv⇡(St) is updated in the direction of the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 164, 'page_label': '165'}, page_content='7.1. n-step TD Prediction 143\\ncomplete return:\\nGt\\n.= Rt+1 + \\x00Rt+2 + \\x002Rt+3 + ··· + \\x00T\\x00t\\x001RT ,\\nwhere T is the last time step of the episode. Let us call this quantity thetarget of the\\nupdate. Whereas in Monte Carlo updates the target is the return, in one-step updates\\nthe target is the ﬁrst reward plus the discounted estimated value of the next state, which\\nwe call theone-step return:\\nGt:t+1\\n.= Rt+1 + \\x00Vt(St+1),\\nwhere Vt : S ! R here is the estimate at timet of v⇡. The subscripts onGt:t+1 indicate\\nthat it is a truncated return for timet using rewards up until timet+1, with the discounted\\nestimate \\x00Vt(St+1) taking the place of the other terms\\x00Rt+2 + \\x002Rt+3 + ··· + \\x00T\\x00t\\x001RT\\nof the full return, as discussed in the previous chapter. Our point now is that this idea\\nmakes just as much sense after two steps as it does after one. The target for a two-step\\nupdate is thetwo-step return:\\nGt:t+2\\n.= Rt+1 + \\x00Rt+2 + \\x002Vt+1(St+2),\\nwhere now\\x002Vt+1(St+2) corrects for the absence of the terms\\x002Rt+3 + \\x003Rt+4 + ··· +\\n\\x00T\\x00t\\x001RT . Similarly, the target for an arbitraryn-step update is then-step return:\\nGt:t+n\\n.= Rt+1 + \\x00Rt+2 + ··· + \\x00n\\x001Rt+n + \\x00nVt+n\\x001(St+n), (7.1)\\nfor all n, tsuch that n \\x00 1 and 0\\uf8ff t<T \\x00 n. All n-step returns can be considered\\napproximations to the full return, truncated aftern steps and then corrected for the\\nremaining missing terms byVt+n\\x001(St+n). If t + n \\x00 T (if the n-step return extends\\nto or beyond termination), then all the missing terms are taken as zero, and then-step\\nreturn deﬁned to be equal to the ordinary full return (Gt:t+n\\n.= Gt if t + n \\x00 T).\\nNote that n-step returns forn> 1 involve future rewards and states that are not\\navailable at the time of transition fromt to t + 1. No real algorithm can use then-step\\nreturn until after it has seenRt+n and computed Vt+n\\x001. The ﬁrst time these are\\navailable ist + n. The natural state-value learning algorithm for usingn-step returns is\\nthus\\nVt+n(St) .= Vt+n\\x001(St)+ ↵\\n⇥\\nGt:t+n \\x00 Vt+n\\x001(St)\\n⇤\\n, 0 \\uf8ff t<T , (7.2)\\nwhile the values of all other states remain unchanged:Vt+n(s)= Vt+n\\x001(s), for alls6=St.\\nWe call this algorithmn-step TD. Note that no changes at all are made during the ﬁrst\\nn \\x00 1 steps of each episode. To make up for that, an equal number of additional updates\\nare made at the end of the episode, after termination and before starting the next episode.\\nComplete pseudocode is given in the box on the next page.\\nExercise 7.1 In Chapter 6 we noted that the Monte Carlo error can be written as the\\nsum of TD errors(6.6) if the value estimates don’t change from step to step. Show that\\nthe n-step error used in(7.2) can also be written as a sum of TD errors (again if the\\nvalue estimates don’t change) generalizing the earlier result. ⇤\\nExercise 7.2 (programming)With ann-step method, the value estimatesdo change from\\nstep to step, so an algorithm that used the sum of TD errors (see previous exercise) in'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 165, 'page_label': '166'}, page_content='144 Chapter 7: n-step Bootstrapping\\nn-step TD for estimatingV ⇡ v⇡\\nInput: a policy⇡\\nAlgorithm parameters: step size↵ 2 (0, 1], a positive integern\\nInitialize V (s) arbitrarily, for alls 2 S\\nAll store and access operations (forSt and Rt) can take their index modn +1\\nLoop for each episode:\\nInitialize and storeS0 6= terminal\\nT  1\\nLoop for t =0 , 1, 2,... :\\n| If t<T ,t h e n :\\n| Take an action according to⇡(·| St)\\n| Observe and store the next reward asRt+1 and the next state asSt+1\\n| If St+1 is terminal, thenT  t +1\\n| ⌧  t \\x00 n +1 ( ⌧ is the time whose state’s estimate is being updated)\\n| If ⌧ \\x00 0:\\n| G  Pmin(⌧+n,T)\\ni=⌧+1 \\x00i\\x00⌧\\x001Ri\\n| If ⌧ + n<T ,t h e n :G  G + \\x00nV (S⌧+n)( G⌧:⌧+n)\\n| V (S⌧ )  V (S⌧ )+ ↵ [G \\x00 V (S⌧ )]\\nUntil ⌧ = T \\x00 1\\nplace of the error in(7.2) would actually be a slightly di↵erent algorithm. Would it be a\\nbetter algorithm or a worse one? Devise and program a small experiment to answer this\\nquestion empirically. ⇤\\nThe n-step return uses the value functionVt+n\\x001 to correct for the missing rewards\\nbeyond Rt+n. An important property of n-step returns is that their expectation is\\nguaranteed to be a better estimate ofv⇡ than Vt+n\\x001 is, in a worst-state sense. That is,\\nthe worst error of the expectedn-step return is guaranteed to be less than or equal to\\x00n\\ntimes the worst error underVt+n\\x001:\\nmax\\ns\\n\\x00\\x00\\x00E⇡[Gt:t+n|St =s] \\x00 v⇡(s)\\n\\x00\\x00\\x00 \\uf8ff \\x00n max\\ns\\n\\x00\\x00\\x00Vt+n\\x001(s) \\x00 v⇡(s)\\n\\x00\\x00\\x00, (7.3)\\nfor alln \\x00 1. This is called theerror reduction propertyof n-step returns. Because of the\\nerror reduction property, one can show formally that alln-step TD methods converge to\\nthe correct predictions under appropriate technical conditions. Then-step TD methods\\nthus form a family of sound methods, with one-step TD methods and Monte Carlo\\nmethods as extreme members.\\nExample 7.1: n-step TD Methods on the Random WalkConsider usingn-step\\nTD methods on the 5-state random walk task described in Example 6.2 (page 125).\\nSuppose the ﬁrst episode progressed directly from the center state,C, to the right,\\nthrough D and E, and then terminated on the right with a return of 1. Recall that the\\nestimated values of all the states started at an intermediate value,V (s)=0 .5. As a result\\nof this experience, a one-step method would change only the estimate for the last state,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 166, 'page_label': '167'}, page_content='7.2. n-step Sarsa 145\\nV (E), which would be incremented toward 1, the observed return. A two-step method,\\non the other hand, would increment the values of the two states preceding termination:\\nV (D) andV (E) both would be incremented toward 1. A three-step method, or anyn-step\\nmethod for n> 2, would increment the values of all three of the visited states toward 1,\\nall by the same amount.\\nWhich value ofn is better? Figure 7.2 shows the results of a simple empirical test for\\na larger random walk process, with 19 states instead of 5 (and with a\\x001 outcome on the\\nleft, all values initialized to 0), which we use as a running example in this chapter. Results\\nare shown forn-step TD methods with a range of values forn and ↵. The performance\\nmeasure for each parameter setting, shown on the vertical axis, is the square-root of\\nthe average squared error between the predictions at the end of the episode for the 19\\nstates and their true values, then averaged over the ﬁrst 10 episodes and 100 repetitions\\nof the whole experiment (the same sets of walks were used for all parameter settings).\\nNote that methods with an intermediate value ofn worked best. This illustrates how\\nthe generalization of TD and Monte Carlo methods ton-step methods can potentially\\nperform better than either of the two extreme methods.\\n↵\\nAverage\\nRMS error\\nover 19 states\\nand ﬁrst 10 \\nepisodes n=1\\nn=2\\nn=4\\nn=8\\nn=16\\nn=32\\nn=32n=64128512\\n256\\n0.55\\n0.5\\n0.45\\n0.35\\n0.3\\n0.25\\n0.4\\n0.40.20 0.80.6 1\\nFigure 7.2: Performance ofn-step TD methods as a function of↵, for various values ofn,o n\\na 19-state random walk task (Example 7.1).\\nExercise 7.3 Why do you think a larger random walk task (19 states instead of 5) was\\nused in the examples of this chapter? Would a smaller walk have shifted the advantage\\nto a di↵erent value ofn? How about the change in left-side outcome from 0 to\\x001 made\\nin the larger walk? Do you think that made any di↵erence in the best value ofn? ⇤\\n7.2 n-step Sarsa\\nHow cann-step methods be used not just for prediction, but for control? In this section\\nwe show hown-step methods can be combined with Sarsa in a straightforward way to'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 167, 'page_label': '168'}, page_content='146 Chapter 7: n-step Bootstrapping\\nproduce an on-policy TD control method. Then-step version of Sarsa we calln-step\\nSarsa, and the original version presented in the previous chapter we henceforth call\\none-step Sarsa, orSarsa(0).\\nThe main idea is to simply switch states for actions (state–action pairs) and then use\\nan \"-greedy policy. The backup diagrams forn-step Sarsa (shown in Figure 7.3), like\\nthose ofn-step TD (Figure 7.1), are strings of alternating states and actions, except that\\nthe Sarsa ones all start and end with an action rather a state. We redeﬁnen-step returns\\n(update targets) in terms of estimated action values:\\nGt:t+n\\n.= Rt+1+\\x00Rt+2+··· +\\x00n\\x001Rt+n+\\x00nQt+n\\x001(St+n,A t+n),n \\x00 1, 0 \\uf8ff t<T \\x00n,\\n(7.4)\\nwith Gt:t+n\\n.= Gt if t + n \\x00 T. The natural algorithm is then\\nQt+n(St,A t) .= Qt+n\\x001(St,A t)+ ↵ [Gt:t+n \\x00 Qt+n\\x001(St,A t)] , 0 \\uf8ff t<T , (7.5)\\nwhile the values of all other states remain unchanged:Qt+n(s, a)= Qt+n\\x001(s, a), for all\\ns, asuch thats 6= St or a 6= At. This is the algorithm we calln-step Sarsa.P s e u d o c o d e\\nis shown in the box on the next page, and an example of why it can speed up learning\\ncompared to one-step methods is given in Figure 7.4.\\n1-step Sarsaaka Sarsa(0)2-step Sarsa3-step Sarsa n-step Sarsa∞-step Sarsa\\naka Monte Carlo\\nn-step Expected Sarsa\\nFigure 7.3: The backup diagrams for the spectrum ofn-step methods for state–action values.\\nThey range from the one-step update of Sarsa(0) to the up-until-termination update of the\\nMonte Carlo method. In between are then-step updates, based onn steps of real rewards and\\nthe estimated value of thenth next state–action pair, all appropriately discounted. On the far\\nright is the backup diagram forn-step Expected Sarsa.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 168, 'page_label': '169'}, page_content='7.2. n-step Sarsa 147\\nn-step Sarsa for estimatingQ ⇡ q⇤ or q⇡\\nInitialize Q(s, a) arbitrarily, for alls 2 S,a 2 A\\nInitialize ⇡ to be \"-greedy with respect toQ, or to a ﬁxed given policy\\nAlgorithm parameters: step size↵ 2 (0, 1], small\"> 0, a positive integern\\nAll store and access operations (forSt, At, andRt) can take their index modn +1\\nLoop for each episode:\\nInitialize and storeS0 6= terminal\\nSelect and store an actionA0 ⇠ ⇡(·| S0)\\nT  1\\nLoop for t =0 , 1, 2,... :\\n| If t<T ,t h e n :\\n| Take actionAt\\n| Observe and store the next reward asRt+1 and the next state asSt+1\\n| If St+1 is terminal, then:\\n| T  t +1\\n| else:\\n| Select and store an actionAt+1 ⇠ ⇡(·| St+1)\\n| ⌧  t \\x00 n +1 ( ⌧ is the time whose estimate is being updated)\\n| If ⌧ \\x00 0:\\n| G  Pmin(⌧+n,T)\\ni=⌧+1 \\x00i\\x00⌧\\x001Ri\\n| If ⌧ + n<T ,t h e nG  G + \\x00nQ(S⌧+n,A ⌧+n)( G⌧:⌧+n)\\n| Q(S⌧ ,A ⌧ )  Q(S⌧ ,A ⌧ )+ ↵ [G \\x00 Q(S⌧ ,A ⌧ )]\\n| If ⇡ is being learned, then ensure that⇡(·| S⌧ )i s\"-greedy wrtQ\\nUntil ⌧ = T \\x00 1\\nPath taken Action values increased\\nby one-step Sarsa\\nAction values increased\\n by 10-step Sarsa\\nG G G\\nFigure 7.4: Gridworld example of the speedup of policy learning due to the use ofn-step\\nmethods. The ﬁrst panel shows the path taken by an agent in a single episode, ending at a\\nlocation of high reward, marked by theG. In this example the values were all initially 0, and all\\nrewards were zero except for a positive reward atG. The arrows in the other two panels show\\nwhich action values were strengthened as a result of this path by one-step andn-step Sarsa\\nmethods. The one-step method strengthens only the last action of the sequence of actions that\\nled to the high reward, whereas then-step method strengthens the lastn actions of the sequence,\\nso that much more is learned from the one episode.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 169, 'page_label': '170'}, page_content='148 Chapter 7: n-step Bootstrapping\\nExercise 7.4 Prove that then-step return of Sarsa(7.4) can be written exactly in terms\\nof a novel TD error, as\\nGt:t+n = Qt\\x001(St,A t)+\\nmin(t+n,T)\\x001X\\nk=t\\n\\x00k\\x00t [Rk+1 + \\x00Qk(Sk+1,A k+1) \\x00 Qk\\x001(Sk,A k)] .\\n(7.6)\\n⇤\\nWhat about Expected Sarsa? The backup diagram for then-step version of Expected\\nSarsa is shown on the far right in Figure 7.3. It consists of a linear string of sample\\nactions and states, just as inn-step Sarsa, except that its last element is a branch over\\nall action possibilities weighted, as always, by their probability under⇡. This algorithm\\ncan be described by the same equation asn-step Sarsa (above) except with then-step\\nreturn redeﬁned as\\nGt:t+n\\n.= Rt+1 + ··· + \\x00n\\x001Rt+n + \\x00n ¯Vt+n\\x001(St+n),t + n<T , (7.7)\\n(with Gt:t+n\\n.=Gt for t + n \\x00 T)w h e r e¯Vt(s)i st h eexpected approximate valueof state s,\\nusing the estimated action values at timet, under the target policy:\\n¯Vt(s) .=\\nX\\na\\n⇡(a|s)Qt(s, a), for alls 2 S. (7.8)\\nExpected approximate values are used in developing many of the action-value methods\\nin the rest of this book. Ifs is terminal, then its expected approximate value is deﬁned\\nto be 0.\\n7.3 n-step O↵-policy Learning\\nRecall that o↵-policy learning is learning the value function for one policy,⇡,w h i l e\\nfollowing another policy,b.O f t e n ,⇡ is the greedy policy for the current action-value-\\nfunction estimate, and b is a more exploratory policy, perhaps\"-greedy. In order to\\nuse the data fromb we must take into account the di↵erence between the two policies,\\nusing their relative probability of taking the actions that were taken (see Section 5.5). In\\nn-step methods, returns are constructed overn steps, so we are interested in the relative\\nprobability of just thosen actions. For example, to make a simple o↵-policy version of\\nn-step TD, the update for timet (actually made at timet + n) can simply be weighted\\nby ⇢t:t+n\\x001:\\nVt+n(St) .= Vt+n\\x001(St)+ ↵⇢t:t+n\\x001 [Gt:t+n \\x00 Vt+n\\x001(St)] , 0 \\uf8ff t<T , (7.9)\\nwhere ⇢t:t+n\\x001, called theimportance sampling ratio, is the relative probability under\\nthe two policies of taking then actions fromAt to At+n\\x001 (cf. Eq. 5.3):\\n⇢t:h\\n.=\\nmin(h,T\\x001)Y\\nk=t\\n⇡(Ak|Sk)\\nb(Ak|Sk) . (7.10)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 170, 'page_label': '171'}, page_content='7.3. n-step O↵-policy Learning 149\\nFor example, if any one of the actions would never be taken by⇡ (i.e., ⇡(Ak|Sk) = 0) then\\nthe n-step return should be given zero weight and be totally ignored. On the other hand,\\nif by chance an action is taken that⇡ would take with much greater probability thanb\\ndoes, then this will increase the weight that would otherwise be given to the return. This\\nmakes sense because that action is characteristic of⇡ (and therefore we want to learn\\nabout it) but is selected only rarely byb and thus rarely appears in the data. To make\\nup for this we have to over-weight it when it does occur. Note that if the two policies\\nare actually the same (the on-policy case) then the importance sampling ratio is always\\n1. Thus our new update(7.9) generalizes and can completely replace our earliern-step\\nTD update. Similarly, our previousn-step Sarsa update can be completely replaced by a\\nsimple o↵-policy form:\\nQt+n(St,A t) .= Qt+n\\x001(St,A t)+ ↵⇢t+1:t+n [Gt:t+n \\x00 Qt+n\\x001(St,A t)] , (7.11)\\nfor 0\\uf8ff t<T . Note that the importance sampling ratio here starts and ends one step\\nlater than forn-step TD (7.9). This is because here we are updating a state–action\\npair. We do not have to care how likely we were to select the action; now that we have\\nselected it we want to learn fully from what happens, with importance sampling only for\\nsubsequent actions. Pseudocode for the full algorithm is shown in the box below.\\nO↵-policy n-step Sarsa for estimatingQ ⇡ q⇤ or q⇡\\nInput: an arbitrary behavior policyb such thatb(a|s) > 0, for alls 2 S,a 2 A\\nInitialize Q(s, a) arbitrarily, for alls 2 S,a 2 A\\nInitialize ⇡ to be greedy with respect toQ,o ra saﬁ x e dg i v e np o l i c y\\nAlgorithm parameters: step size↵ 2 (0, 1], a positive integern\\nAll store and access operations (forSt, At,a n dRt)c a nt a k et h e i ri n d e xm o dn +1\\nLoop for each episode:\\nInitialize and storeS0 6= terminal\\nSelect and store an actionA0 ⇠ b(·| S0)\\nT  1\\nLoop for t =0 , 1, 2,... :\\n| If t<T ,t h e n :\\n| Take actionAt\\n| Observe and store the next reward asRt+1 and the next state asSt+1\\n| If St+1 is terminal, then:\\n| T  t +1\\n| else:\\n| Select and store an actionAt+1 ⇠ b(·| St+1)\\n| ⌧  t \\x00 n +1 ( ⌧ is the time whose estimate is being updated)\\n| If ⌧ \\x00 0:\\n| ⇢  Qmin(⌧+n,T \\x001)\\ni=⌧+1\\n⇡(Ai|Si)\\nb(Ai|Si) (⇢⌧+1:⌧+n)\\n| G  Pmin(⌧+n,T )\\ni=⌧+1 \\x00i\\x00⌧\\x001Ri\\n| If ⌧ + n<T ,t h e n :G  G + \\x00nQ(S⌧+n,A ⌧+n)( G⌧:⌧+n)\\n| Q(S⌧ ,A ⌧ )  Q(S⌧ ,A ⌧ )+ ↵⇢ [G \\x00 Q(S⌧ ,A ⌧ )]\\n| If ⇡ is being learned, then ensure that⇡(·| S⌧ ) is greedy wrtQ\\nUntil ⌧ = T \\x00 1'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 171, 'page_label': '172'}, page_content='150 Chapter 7: n-step Bootstrapping\\nThe o↵-policy version ofn-step Expected Sarsa would use the same update as above\\nfor n-step Sarsa except that the importance sampling ratio would have one less factor in\\nit. That is, the above equation would use⇢t+1:t+n\\x001 instead of⇢t+1:t+n, and of course\\nit would use the Expected Sarsa version of then-step return (7.7). This is because in\\nExpected Sarsa all possible actions are taken into account in the last state; the one\\nactually taken has no e↵ect and does not have to be corrected for.\\n7.4 *Per-decision Methods with Control Variates\\nThe multi-step o↵-policy methods presented in the previous section are simple and\\nconceptually clear, but are probably not the most e\\x00cient. A more sophisticated approach\\nwould use per-decision importance sampling ideas such as were introduced in Section 5.9.\\nTo understand this approach, ﬁrst note that the ordinaryn-step return (7.1), like all\\nreturns, can be written recursively. For then steps ending at horizonh,t h en-step return\\ncan be written\\nGt:h = Rt+1 + \\x00Gt+1:h,t < h < T , (7.12)\\nwhere Gh:h\\n.= Vh\\x001(Sh). (Recall that this return is used at timeh, previously denoted\\nt + n.) Now consider the e↵ect of following a behavior policyb that is not the same\\nas the target policy⇡. All of the resulting experience, including the ﬁrst rewardRt+1\\nand the next stateSt+1, must be weighted by the importance sampling ratio for timet,\\n⇢t = ⇡(At|St)\\nb(At|St) . One might be tempted to simply weight the righthand side of the above\\nequation, but one can do better. Suppose the action at timet would never be selected by\\n⇡, so that⇢t is zero. Then a simple weighting would result in then-step return being\\nzero, which could result in high variance when it was used as a target. Instead, in this\\nmore sophisticated approach, one uses an alternate,o↵-policy deﬁnition of then-step\\nreturn ending at horizonh, as\\nGt:h\\n.= ⇢t (Rt+1 + \\x00Gt+1:h)+( 1\\x00 ⇢t)Vh\\x001(St),t < h < T , (7.13)\\nwhere againGh:h\\n.= Vh\\x001(Sh). In this approach, if⇢t is zero, then instead of the target\\nbeing zero and causing the estimate to shrink, the target is the same as the estimate and\\ncauses no change. The importance sampling ratio being zero means we should ignore the\\nsample, so leaving the estimate unchanged seems appropriate. The second, additional\\nterm in(7.13) is called acontrol variate(for obscure reasons). Notice that the control\\nvariate does not change the expected update; the importance sampling ratio has expected\\nvalue one (Section 5.9) and is uncorrelated with the estimate, so the expected value\\nof the control variate is zero. Also note that the o↵-policy deﬁnition(7.13) is a strict\\ngeneralization of the earlier on-policy deﬁnition of then-step return (7.1), as the two are\\nidentical in the on-policy case, in which⇢t is always 1.\\nFor a conventionaln-step method, the learning rule to use in conjunction with(7.13)\\nis then-step TD update(7.2), which has no explicit importance sampling ratios other\\nthan those embedded in the return.\\nExercise 7.5 Write the pseudocode for the o↵-policy state-value prediction algorithm\\ndescribed above. ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 172, 'page_label': '173'}, page_content='7.4. Per-decision Methods with Control Variates 151\\nFor action values, the o↵-policy deﬁnition of then-step return is a little di↵erent\\nbecause the ﬁrst action does not play a role in the importance sampling. That ﬁrst action\\nis the one being learned; it does not matter if it was unlikely or even impossible under the\\ntarget policy—it has been taken and now full unit weight must be given to the reward\\nand state that follows it. Importance sampling will apply only to the actions that follow\\nit.\\nFirst note that for action values then-step on-policy return ending at horizonh,\\nexpectation form (7.7), can be written recursively just as in(7.12), except that for action\\nvalues the recursion ends withGh:h\\n.= ¯Vh\\x001(Sh) as in(7.8). An o↵-policy form with\\ncontrol variates is\\nGt:h\\n.= Rt+1 + \\x00\\n⇣\\n⇢t+1Gt+1:h + ¯Vh\\x001(St+1) \\x00 ⇢t+1Qh\\x001(St+1,A t+1)\\n⌘\\n,\\n= Rt+1 + \\x00⇢t+1\\n⇣\\nGt+1:h \\x00 Qh\\x001(St+1,A t+1)\\n⌘\\n+ \\x00 ¯Vh\\x001(St+1),t < h \\uf8ff T.\\n(7.14)\\nIf h<T , then the recursion ends with Gh:h\\n.= Qh\\x001(Sh,A h), whereas, if h \\x00 T,\\nthe recursion ends with andGT\\x001:h\\n.= RT . The resultant prediction algorithm (after\\ncombining with (7.5)) is analogous to Expected Sarsa.\\nExercise 7.6 Prove that the control variate in the above equations does not change the\\nexpected value of the return. ⇤\\n⇤Exercise 7.7 Write the pseudocode for the o↵-policy action-value prediction algorithm\\ndescribed immediately above. Pay particular attention to the termination conditions for\\nthe recursion upon hitting the horizon or the end of episode. ⇤\\nExercise 7.8 Show that the general (o↵-policy) version of then-step return(7.13) can\\nstill be written exactly and compactly as the sum of state-based TD errors(6.5) if the\\napproximate state value function does not change. ⇤\\nExercise 7.9 Repeat the above exercise for the action version of the o↵-policyn-step\\nreturn (7.14) and the Expected Sarsa TD error (the quantity in brackets in Equation 6.9).\\n⇤\\nExercise 7.10 (programming) Devise a small o↵-policy prediction problem and use it to\\nshow that the o↵-policy learning algorithm using(7.13) and (7.2) is more data e\\x00cient\\nthan the simpler algorithm using (7.1) and (7.9). ⇤\\nThe importance sampling that we have used in this section, the previous section, and\\nin Chapter 5, enables sound o↵-policy learning, but also results in high variance updates,\\nforcing the use of a small step-size parameter and thereby causing learning to be slow. It\\nis probably inevitable that o↵-policy training is slower than on-policy training—after all,\\nthe data is less relevant to what is being learned. However, it is probably also true that\\nthese methods can be improved on. The control variates are one way of reducing the\\nvariance. Another is to rapidly adapt the step sizes to the observed variance, as in the\\nAutostep method (Mahmood, Sutton, Degris and Pilarski, 2012). Yet another promising\\napproach is the invariant updates of Karampatziakis and Langford (2010) as extended\\nto TD by Tian (in preparation). The usage technique of Mahmood (2017; Mahmood'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 173, 'page_label': '174'}, page_content='152 Chapter 7: n-step Bootstrapping\\nand Sutton, 2015) may also be part of the solution. In the next section we consider an\\no↵-policy learning method that does not use importance sampling.\\n7.5 O↵-policy Learning Without Importance Sampling:\\nThe n-step Tree Backup Algorithm\\nIs o↵-policy learning possible without importance sampling? Q-learning and Expected\\nSarsa from Chapter 6 do this for the one-step case, but is there a corresponding multi-step\\nalgorithm? In this section we present just such ann-step method, called thetree-backup\\nalgorithm.\\nSt,At\\nAt+1\\nRt+1\\nSt+1\\nSt+2\\nRt+2\\nAt+2Rt+3\\nSt+3\\nthe 3-step\\ntree-backup\\nupdate\\nThe idea of the algorithm is suggested by the 3-step tree-backup backup\\ndiagram shown to the right. Down the central spine and labeled in the\\ndiagram are three sample states and rewards, and two sample actions.\\nThese are the random variables representing the events occurring after the\\ninitial state–action pairSt,A t. Hanging o↵ to the sides of each state are\\nthe actions that werenot selected. (For the last state, all the actions are\\nconsidered to have not (yet) been selected.) Because we have no sample\\ndata for the unselected actions, we bootstrap and use the estimates of\\ntheir values in forming the target for the update. This slightly extends the\\nidea of a backup diagram. So far we have always updated the estimated\\nvalue of the node at the top of the diagram toward a target combining\\nthe rewards along the way (appropriately discounted) and the estimated\\nvalues of the nodes at the bottom. In the tree-backup update, the target\\nincludes all these thingsplus the estimated values of the dangling action\\nnodes hanging o↵ the sides, at all levels. This is why it is called atree-\\nbackup update; it is an update from the entire tree of estimated action\\nvalues.\\nMore precisely, the update is from the estimated action values of the\\nleaf nodes of the tree. The action nodes in the interior, corresponding to\\nthe actual actions taken, do not participate. Each leaf node contributes to the target\\nwith a weight proportional to its probability of occurring under the target policy⇡.T h u s\\neach ﬁrst-level actiona contributes with a weight of⇡(a|St+1), except that the action\\nactually taken,At+1, does not contribute at all. Its probability,⇡(At+1|St+1), is used\\nto weight all the second-level action values. Thus, each non-selected second-level action\\na0 contributes with weight⇡(At+1|St+1)⇡(a0|St+2). Each third-level action contributes\\nwith weight⇡(At+1|St+1)⇡(At+2|St+2)⇡(a00|St+3), and so on. It is as if each arrow to an\\naction node in the diagram is weighted by the action’s probability of being selected under\\nthe target policy and, if there is a tree below the action, then that weight applies to all\\nthe leaf nodes in the tree.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 174, 'page_label': '175'}, page_content='7.5. O↵-policy Learning Without Importance Sampling: n-step Tree Backup 153\\nWe can think of the 3-step tree-backup update as consisting of 6 half-steps, alternating\\nbetween sample half-steps from an action to a subsequent state, and expected half-steps\\nconsidering from that state all possible actions with their probabilities of occurring under\\nthe policy.\\nNow let us develop the detailed equations for then-step tree-backup algorithm. The\\none-step return (target) is the same as that of Expected Sarsa,\\nGt:t+1\\n.= Rt+1 + \\x00\\nX\\na\\n⇡(a|St+1)Qt(St+1,a ), (7.15)\\nfor t<T \\x00 1, and the two-step tree-backup return is\\nGt:t+2\\n.= Rt+1 + \\x00\\nX\\na6=At+1\\n⇡(a|St+1)Qt+1(St+1,a )\\n+ \\x00⇡(At+1|St+1)\\n⇣\\nRt+2 + \\x00\\nX\\na\\n⇡(a|St+2)Qt+1(St+2,a )\\n⌘\\n= Rt+1 + \\x00\\nX\\na6=At+1\\n⇡(a|St+1)Qt+1(St+1,a )+ \\x00⇡(At+1|St+1)Gt+1:t+2,\\nfor t<T \\x002. The latter form suggests the general recursive deﬁnition of the tree-backup\\nn-step return:\\nGt:t+n\\n.= Rt+1 + \\x00\\nX\\na6=At+1\\n⇡(a|St+1)Qt+n\\x001(St+1,a )+ \\x00⇡ (At+1|St+1)Gt+1:t+n, (7.16)\\nfor t<T \\x00 1,n \\x00 2, with then = 1 case handled by(7.15) except for GT\\x001:t+n\\n.= RT .\\nThis target is then used with the usual action-value update rule fromn-step Sarsa:\\nQt+n(St,A t) .= Qt+n\\x001(St,A t)+ ↵ [Gt:t+n \\x00 Qt+n\\x001(St,A t)] ,\\nfor 0 \\uf8ff t<T , while the values of all other state–action pairs remain unchanged:\\nQt+n(s, a)= Qt+n\\x001(s, a), for alls, asuch that s 6= St or a 6= At. Pseudocode for this\\nalgorithm is shown in the box on the next page.\\nExercise 7.11 Show that if the approximate action values are unchanging, then the\\ntree-backup return (7.16) can be written as a sum of expectation-based TD errors:\\nGt:t+n = Q(St,A t)+\\nmin(t+n\\x001,T\\x001)X\\nk=t\\n\\x00k\\nkY\\ni=t+1\\n\\x00⇡(Ai|Si),\\nwhere \\x00t\\n.= Rt+1 + \\x00 ¯Vt(St+1) \\x00 Q(St,A t) and ¯Vt is given by (7.8). ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 175, 'page_label': '176'}, page_content='154 Chapter 7: n-step Bootstrapping\\nn-step Tree Backup for estimatingQ ⇡ q⇤ or q⇡\\nInitialize Q(s, a) arbitrarily, for alls 2 S,a 2 A\\nInitialize ⇡ to be greedy with respect toQ, or as a ﬁxed given policy\\nAlgorithm parameters: step size↵ 2 (0, 1], a positive integern\\nAll store and access operations can take their index modn +1\\nLoop for each episode:\\nInitialize and storeS0 6= terminal\\nChoose an actionA0 arbitrarily as a function ofS0; StoreA0\\nT  1\\nLoop for t =0 , 1, 2,... :\\n| If t<T :\\n| Take actionAt; observe and store the next reward and state asRt+1,S t+1\\n| If St+1 is terminal:\\n| T  t +1\\n| else:\\n| Choose an actionAt+1 arbitrarily as a function ofSt+1; StoreAt+1\\n| ⌧  t +1 \\x00 n (⌧ is the time whose estimate is being updated)\\n| If ⌧ \\x00 0:\\n| If t +1 \\x00 T:\\n| G  RT\\n| else\\n| G  Rt+1 + \\x00 P\\na ⇡(a|St+1)Q(St+1,a )\\n| Loop for k =m i n (t, T\\x00 1) down through⌧ + 1:\\n| G  Rk + \\x00 P\\na6=Ak\\n⇡(a|Sk)Q(Sk,a )+ \\x00⇡(Ak|Sk)G\\n| Q(S⌧ ,A ⌧ )  Q(S⌧ ,A ⌧ )+ ↵ [G \\x00 Q(S⌧ ,A ⌧ )]\\n| If ⇡ is being learned, then ensure that⇡(·| S⌧ ) is greedy wrtQ\\nUntil ⌧ = T \\x00 1\\n7.6 *A Unifying Algorithm: n-step Q(\\x00)\\nSo far in this chapter we have considered three di↵erent kinds of action-value algorithms,\\ncorresponding to the ﬁrst three backup diagrams shown in Figure 7.5.n-step Sarsa has\\nall sample transitions, the tree-backup algorithm has all state-to-action transitions fully\\nbranched without sampling, andn-step Expected Sarsa has all sample transitions except\\nfor the last state-to-action one, which is fully branched with an expected value. To what\\nextent can these algorithms be uniﬁed?\\nOne idea for uniﬁcation is suggested by the fourth backup diagram in Figure 7.5. This\\nis the idea that one might decide on a step-by-step basis whether one wanted to take the\\naction as a sample, as in Sarsa, or consider the expectation over all actions instead, as in\\nthe tree-backup update. Then, if one chose always to sample, one would obtain Sarsa,\\nwhereas if one chose never to sample, one would get the tree-backup algorithm. Expected\\nSarsa would be the case where one chose to sample for all steps except for the last one.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 176, 'page_label': '177'}, page_content='7.6. A Unifying Algorithm: n-stepQ(\\x00) 155\\n⇢\\n⇢\\n⇢\\n⇢\\n⇢\\n⇢\\n⇢\\n⇢\\n⇢\\n\\x00=1\\n\\x00=0\\n\\x00=1\\n\\x00=0\\n4-stepSarsa 4-stepTree backup 4-stepExpected Sarsa4-step Q(\\x00)\\nFigure 7.5: The backup diagrams of the three kinds ofn-step action-value updates considered\\nso far in this chapter (4-step case) plus the backup diagram of a fourth kind of update that uniﬁes\\nthem all. The label ‘⇢’ indicates half transitions on which importance sampling is required in the\\no↵-policy case. The fourth kind of update uniﬁes all the others by choosing on a state-by-state\\nbasis whether to sample (\\x00t =1 )o rn o t(\\x00t =0 ) .\\nAnd of course there would be many other possibilities, as suggested by the last diagram\\nin the ﬁgure. To increase the possibilities even further we can consider a continuous\\nvariation between sampling and expectation. Let\\x00t 2 [0, 1] denote the degree of sampling\\non step t,w i t h\\x00 = 1 denoting full sampling and\\x00 = 0 denoting a pure expectation with\\nno sampling. The random variable\\x00t might be set as a function of the state, action, or\\nstate–action pair at timet. We call this proposed new algorithmn-step Q(\\x00).\\nNow let us develop the equations ofn-step Q(\\x00). First we write the tree-backup\\nn-step return (7.16) in terms of the horizonh = t + n and then in terms of the expected\\napproximate value ¯V (7.8):\\nGt:h = Rt+1 + \\x00\\nX\\na6=At+1\\n⇡(a|St+1)Qh\\x001(St+1,a )+ \\x00⇡ (At+1|St+1)Gt+1:h\\n= Rt+1 + \\x00 ¯Vh\\x001(St+1) \\x00 \\x00⇡(At+1|St+1)Qh\\x001(St+1,A t+1)+ \\x00⇡(At+1|St+1)Gt+1:h\\n= Rt+1 + \\x00⇡(At+1|St+1)\\n⇣\\nGt+1:h \\x00 Qh\\x001(St+1,A t+1)\\n⌘\\n+ \\x00 ¯Vh\\x001(St+1),\\nafter which it is exactly like then-step return for Sarsa with control variates(7.14) except\\nwith the action probability⇡(At+1|St+1) substituted for the importance-sampling ratio\\n⇢t+1. For Q(\\x00), we slide linearly between these two cases:\\nGt:h\\n.= Rt+1 + \\x00\\n⇣\\n\\x00t+1⇢t+1 +( 1\\x00 \\x00t+1)⇡(At+1|St+1)\\n⌘⇣\\nGt+1:h \\x00 Qh\\x001(St+1,A t+1)\\n⌘\\n+ \\x00 ¯Vh\\x001(St+1), (7.17)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 177, 'page_label': '178'}, page_content='156 Chapter 7: n-step Bootstrapping\\nfor t<h \\uf8ff T. The recursion ends with Gh:h\\n.= Qh\\x001(Sh,A h)i f h<T , or with\\nGT\\x001:T\\n.= RT if h = T. Then we use the earlier update for n-step Sarsa without\\nimportance-sampling ratios(7.5) instead of(7.11), because now the ratios are incorporated\\nin then-step return. A complete algorithm is given in the box.\\nO↵-policy n-step Q(\\x00) for estimatingQ ⇡ q⇤ or q⇡\\nInput: an arbitrary behavior policyb such thatb(a|s) > 0, for alls 2 S,a 2 A\\nInitialize Q(s, a) arbitrarily, for alls 2 S,a 2 A\\nInitialize ⇡ to be greedy with respect toQ, or else it is a ﬁxed given policy\\nAlgorithm parameters: step size↵ 2 (0, 1], a positive integern\\nAll store and access operations can take their index modn +1\\nLoop for each episode:\\nInitialize and storeS0 6= terminal\\nChoose and store an actionA0 ⇠ b(·| S0)\\nT  1\\nLoop for t =0 , 1, 2,... :\\n| If t<T :\\n| Take actionAt; observe and store the next reward and state asRt+1,S t+1\\n| If St+1 is terminal:\\n| T  t +1\\n| else:\\n| Choose and store an actionAt+1 ⇠ b(·| St+1)\\n| Select and store\\x00t+1\\n| Store ⇡(At+1|St+1)\\nb(At+1|St+1) as ⇢t+1\\n| ⌧  t \\x00 n +1 ( ⌧ is the time whose estimate is being updated)\\n| If ⌧ \\x00 0:\\n| If t +1 <T :\\n| G  Q(St+1,A t+1)\\n| Loop for k =m i n (t +1 ,T ) down through⌧ + 1:\\n| if k = T:\\n| G  RT\\n| else:\\n| ¯V  P\\na ⇡(a|Sk)Q(Sk,a )\\n| G  Rk + \\x00\\n\\x00\\n\\x00k⇢k +( 1\\x00 \\x00k)⇡(Ak|Sk)\\n\\x00\\x00\\nG \\x00 Q(Sk,A k)\\n\\x00\\n+ \\x00 ¯V\\n| Q(S⌧ ,A ⌧ )  Q(S⌧ ,A ⌧ )+ ↵ [G \\x00 Q(S⌧ ,A ⌧ )]\\n| If ⇡ is being learned, then ensure that⇡(·| S⌧ ) is greedy wrtQ\\nUntil ⌧ = T \\x00 1'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 178, 'page_label': '179'}, page_content='7.7. Summary 157\\n7.7 Summary\\nIn this chapter we have developed a range of temporal-di↵erence learning methods that lie\\nin between the one-step TD methods of the previous chapter and the Monte Carlo methods\\nof the chapter before. Methods that involve an intermediate amount of bootstrapping\\nare important because they will typically perform better than either extreme.\\n⇢\\n⇢\\n\\x00=1\\n\\x00=0\\n\\x00=1\\n\\x00=0\\n4-step\\n Q(\\x00)\\n⇢\\n⇢\\n⇢\\n⇢\\n4-stepTD\\nOur focus in this chapter has been onn-step methods, which\\nlook ahead to the nextn rewards, states, and actions. The two\\n4-step backup diagrams to the right together summarize most of the\\nmethods introduced. The state-value update shown is forn-step\\nTD with importance sampling, and the action-value update is for\\nn-step Q(\\x00), which generalizes Expected Sarsa and Q-learning. All\\nn-step methods involve a delay ofn time steps before updating,\\nas only then are all the required future events known. A further\\ndrawback is that they involve more computation per time step\\nthan previous methods. Compared to one-step methods,n-step\\nmethods also require more memory to record the states, actions,\\nrewards, and sometimes other variables over the lastn time steps.\\nEventually, in Chapter 12, we will see how multi-step TD methods\\ncan be implemented with minimal memory and computational\\ncomplexity using eligibility traces, but there will always be some\\nadditional computation beyond one-step methods. Such costs can\\nbe well worth paying to escape the tyranny of the single time step.\\nAlthough n-step methods are more complex than those using\\neligibility traces, they have the great beneﬁt of being conceptually\\nclear. We have sought to take advantage of this by developing two\\napproaches to o↵-policy learning in then-step case. One, based on\\nimportance sampling is conceptually simple but can be of high variance. If the target and\\nbehavior policies are very di↵erent it probably needs some new algorithmic ideas before\\nit can be e\\x00cient and practical. The other, based on tree-backup updates, is the natural\\nextension of Q-learning to the multi-step case with stochastic target policies. It involves\\nno importance sampling but, again if the target and behavior policies are substantially\\ndi↵erent, the bootstrapping may span only a few steps even ifn is large.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 179, 'page_label': '180'}, page_content='158 Chapter 7: n-step Bootstrapping\\nBibliographical and Historical Remarks\\nThe notion ofn-step returns is due to Watkins (1989), who also ﬁrst discussed their error\\nreduction property. n-step algorithms were explored in the ﬁrst edition of this book,\\nin which they were treated as of conceptual interest, but not feasible in practice. The\\nwork of Cichosz (1995) and particularly van Seijen (2016) showed that they are actually\\ncompletely practical algorithms. Given this, and their conceptual clarity and simplicity,\\nwe have chosen to highlight them here in the second edition. In particular, we now\\npostpone all discussion of the backward view and of eligibility traces until Chapter 12.\\n7.1–2 The results in the random walk examples were made for this text based on work\\nof Sutton (1988) and Singh and Sutton (1996). The use of backup diagrams to\\ndescribe these and other algorithms in this chapter is new.\\n7.3–5 The developments in these sections are based on the work of Precup, Sutton,\\nand Singh (2000), Precup, Sutton, and Dasgupta (2001), and Sutton, Mahmood,\\nPrecup, and van Hasselt (2014).\\nThe tree-backup algorithm is due to Precup, Sutton, and Singh (2000), but the\\npresentation of it here is new.\\n7.6 The Q(\\x00) algorithm is new to this text, but closely related algorithms have been\\nexplored further by De Asis, Hernandez-Garcia, Holland, and Sutton (2017).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 180, 'page_label': '181'}, page_content='Chapter 8\\nPlanning and Learning with\\nTabular Methods\\nIn this chapter we develop a uniﬁed view of reinforcement learning methods that require\\na model of the environment, such as dynamic programming and heuristic search, and\\nmethods that can be used without a model, such as Monte Carlo and temporal-di↵erence\\nmethods. These are respectively calledmodel-based and model-free reinforcement learning\\nmethods. Model-based methods rely onplanning as their primary component, while\\nmodel-free methods primarily rely onlearning. Although there are real di↵erences between\\nthese two kinds of methods, there are also great similarities. In particular, the heart of\\nboth kinds of methods is the computation of value functions. Moreover, all the methods\\nare based on looking ahead to future events, computing a backed-up value, and then\\nusing it as an update target for an approximate value function. Earlier in this book we\\npresented Monte Carlo and temporal-di↵erence methods as distinct alternatives, then\\nshowed how they can be uniﬁed byn-step methods. Our goal in this chapter is a similar\\nintegration of model-based and model-free methods. Having established these as distinct\\nin earlier chapters, we now explore the extent to which they can be intermixed.\\n8.1 Models and Planning\\nBy amodel of the environment we mean anything that an agent can use to predict how the\\nenvironment will respond to its actions. Given a state and an action, a model produces a\\nprediction of the resultant next state and next reward. If the model is stochastic, then\\nthere are several possible next states and next rewards, each with some probability of\\noccurring. Some models produce a description of all possibilities and their probabilities;\\nthese we calldistribution models. Other models produce just one of the possibilities,\\nsampled according to the probabilities; these we callsample models. For example, consider\\nmodeling the sum of a dozen dice. A distribution model would produce all possible sums\\nand their probabilities of occurring, whereas a sample model would produce an individual'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 181, 'page_label': '182'}, page_content='160 Chapter 8: Planning and Learning with Tabular Methods\\nsum drawn according to this probability distribution. The kind of model assumed in\\ndynamic programming—estimates of the MDP’s dynamics,p(s0,r |s, a)—is a distribution\\nmodel. The kind of model used in the blackjack example in Chapter 5 is a sample model.\\nDistribution models are stronger than sample models in that they can always be used\\nto produce samples. However, in many applications it is much easier to obtain sample\\nmodels than distribution models. The dozen dice are a simple example of this. It would\\nbe easy to write a computer program to simulate the dice rolls and return the sum, but\\nharder and more error-prone to ﬁgure out all the possible sums and their probabilities.\\nModels can be used to mimic or simulate experience. Given a starting state and action,\\na sample model produces a possible transition, and a distribution model generates all\\npossible transitions weighted by their probabilities of occurring. Given a starting state\\nand a policy, a sample model could produce an entire episode, and a distribution model\\ncould generate all possible episodes and their probabilities. In either case, we say the\\nmodel is used tosimulate the environment and producesimulated experience.\\nThe word planning is used in several di↵erent ways in di↵erent ﬁelds. We use the\\nterm to refer to any computational process that takes a model as input and produces or\\nimproves a policy for interacting with the modeled environment:\\nplanningmodel policy\\nIn artiﬁcial intelligence, there are two distinct approaches to planning according to our\\ndeﬁnition. State-space planning, which includes the approach we take in this book,\\nis viewed primarily as a search through the state space for an optimal policy or an\\noptimal path to a goal. Actions cause transitions from state to state, and value functions\\nare computed over states. In what we callplan-space planning, planning is instead a\\nsearch through the space of plans. Operators transform one plan into another, and\\nvalue functions, if any, are deﬁned over the space of plans. Plan-space planning includes\\nevolutionary methods and “partial-order planning,” a common kind of planning in artiﬁcial\\nintelligence in which the ordering of steps is not completely determined at all stages of\\nplanning. Plan-space methods are di\\x00cult to apply e\\x00ciently to the stochastic sequential\\ndecision problems that are the focus in reinforcement learning, and we do not consider\\nthem further (but see, e.g., Russell and Norvig, 2010).\\nThe uniﬁed view we present in this chapter is that all state-space planning methods\\nshare a common structure, a structure that is also present in the learning methods\\npresented in this book. It takes the rest of the chapter to develop this view, but there are\\ntwo basic ideas: (1) all state-space planning methods involve computing value functions\\nas a key intermediate step toward improving the policy, and (2) they compute value\\nfunctions by updates or backup operations applied to simulated experience. This common\\nstructure can be diagrammed as follows:\\nvaluesbackupsmodel simulated\\nexperience policyupdatesbackups\\nDynamic programming methods clearly ﬁt this structure: they make sweeps through the\\nspace of states, generating for each state the distribution of possible transitions. Each\\ndistribution is then used to compute a backed-up value (update target) and update the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 182, 'page_label': '183'}, page_content='8.2. Dyna: Integrated Planning, Acting, and Learning 161\\nstate’s estimated value. In this chapter we argue that various other state-space planning\\nmethods also ﬁt this structure, with individual methods di↵ering only in the kinds of\\nupdates they do, the order in which they do them, and in how long the backed-up\\ninformation is retained.\\nViewing planning methods in this way emphasizes their relationship to the learning\\nmethods that we have described in this book. The heart of both learning and planning\\nmethods is the estimation of value functions by backing-up update operations. The\\ndi↵erence is that whereas planning uses simulated experience generated by a model,\\nlearning methods use real experience generated by the environment. Of course this\\ndi↵erence leads to a number of other di↵erences, for example, in how performance is\\nassessed and in how ﬂexibly experience can be generated. But the common structure\\nmeans that many ideas and algorithms can be transferred between planning and learning.\\nIn particular, in many cases a learning algorithm can be substituted for the key update\\nstep of a planning method. Learning methods require only experience as input, and in\\nmany cases they can be applied to simulated experience just as well as to real experience.\\nThe box below shows a simple example of a planning method based on one-step tabular\\nQ-learning and on random samples from a sample model. This method, which we call\\nrandom-sample one-step tabular Q-planning, converges to the optimal policy for the model\\nunder the same conditions that one-step tabular Q-learning converges to the optimal\\npolicy for the real environment (each state–action pair must be selected an inﬁnite number\\nof times in Step 1, and↵ must decrease appropriately over time).\\nRandom-sample one-step tabular Q-planning\\nLoop forever:\\n1. Select a state,S 2 S, and an action,A 2 A(S), at random\\n2. Send S, Ato a sample model, and obtain\\na sample next reward,R, and a sample next state,S0\\n3. Apply one-step tabular Q-learning toS, A, R, S0:\\nQ(S, A)  Q(S, A)+ ↵\\n⇥\\nR + \\x00 maxa Q(S0,a ) \\x00 Q(S, A)\\n⇤\\nIn addition to the uniﬁed view of planning and learning methods, a second theme in\\nthis chapter is the beneﬁts of planning in small, incremental steps. This enables planning\\nto be interrupted or redirected at any time with little wasted computation, which appears\\nto be a key requirement for e\\x00ciently intermixing planning with acting and with learning\\nof the model. Planning in very small steps may be the most e\\x00cient approach even on\\npure planning problems if the problem is too large to be solved exactly.\\n8.2 Dyna: Integrated Planning, Acting, and Learning\\nWhen planning is done online, while interacting with the environment, a number of\\ninteresting issues arise. New information gained from the interaction may change the\\nmodel and thereby interact with planning. It may be desirable to customize the planning\\nprocess in some way to the states or decisions currently under consideration, or expected'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 183, 'page_label': '184'}, page_content='162 Chapter 8: Planning and Learning with Tabular Methods\\nin the near future. If decision making and model learning are both computation-intensive\\nprocesses, then the available computational resources may need to be divided between\\nthem. To begin exploring these issues, in this section we present Dyna-Q, a simple\\narchitecture integrating the major functions needed in an online planning agent. Each\\nfunction appears in Dyna-Q in a simple, almost trivial, form. In subsequent sections we\\nelaborate some of the alternate ways of achieving each function and the trade-o↵s between\\nthem. For now, we seek merely to illustrate the ideas and stimulate your intuition.\\nWithin a planning agent, there are at least two roles for real experience: it can be\\nused to improve the model (to make it more accurately match the real environment)\\nand it can be used to directly improve the value function and policy using the kinds of\\nplanning\\nvalue/policy\\nexperiencemodel\\nmodellearning\\nacting\\ndirectRL\\nreinforcement learning methods we have discussed\\nin previous chapters. The former we callmodel-\\nlearning, and the latter we calldirect reinforcement\\nlearning (direct RL). The possible relationships\\nbetween experience, model, values, and policy are\\nsummarized in the diagram to the right. Each ar-\\nrow shows a relationship of inﬂuence and presumed\\nimprovement. Note how experience can improve\\nvalue functions and policies either directly or in-\\ndirectly via the model. It is the latter, which is\\nsometimes calledindirect reinforcement learning,\\nthat is involved in planning.\\nBoth direct and indirect methods have advantages and disadvantages. Indirect methods\\noften make fuller use of a limited amount of experience and thus achieve a better policy\\nwith fewer environmental interactions. On the other hand, direct methods are much\\nsimpler and are not a↵ected by biases in the design of the model. Some have argued\\nthat indirect methods are always superior to direct ones, while others have argued that\\ndirect methods are responsible for most human and animal learning. Related debates\\nin psychology and artiﬁcial intelligence concern the relative importance of cognition as\\nopposed to trial-and-error learning, and of deliberative planning as opposed to reactive\\ndecision making (see Chapter 14 for discussion of some of these issues from the perspective\\nof psychology). Our view is that the contrast between the alternatives in all these debates\\nhas been exaggerated, that more insight can be gained by recognizing the similarities\\nbetween these two sides than by opposing them. For example, in this book we have\\nemphasized the deep similarities between dynamic programming and temporal-di↵erence\\nmethods, even though one was designed for planning and the other for model-free learning.\\nDyna-Q includes all of the processes shown in the diagram above—planning, acting,\\nmodel-learning, and direct RL—all occurring continually. The planning method is the\\nrandom-sample one-step tabular Q-planning method on page 161. The direct RL method\\nis one-step tabular Q-learning. The model-learning method is also table-based and assumes\\nthe environment is deterministic. After each transitionSt,A t ! Rt+1,S t+1,t h em o d e l\\nrecords in its table entry forSt,A t the prediction thatRt+1,S t+1 will deterministically\\nfollow. Thus, if the model is queried with a state–action pair that has been experienced\\nbefore, it simply returns the last-observed next state and next reward as its prediction.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 184, 'page_label': '185'}, page_content='8.2. Dyna: Integrated Planning, Acting, and Learning 163\\nDuring planning, the Q-planning algorithm randomly samples only from state–action\\npairs that have previously been experienced (in Step 1), so the model is never queried\\nwith a pair about which it has no information.\\nThe overall architecture of Dyna agents, of which the Dyna-Q algorithm is one example,\\nis shown in Figure 8.1. The central column represents the basic interaction between\\nagent and environment, giving rise to a trajectory of real experience. The arrow on the\\nleft of the ﬁgure represents direct reinforcement learning operating on real experience to\\nimprove the value function and the policy. On the right are model-based processes. The\\nmodel is learned from real experience and gives rise to simulated experience. We use the\\nterm search controlto refer to the process that selects the starting states and actions\\nfor the simulated experiences generated by the model. Finally, planning is achieved by\\napplying reinforcement learning methods to the simulated experiences just as if they had\\nreally happened. Typically, as in Dyna-Q, the same reinforcement learning method is\\nused both for learning from real experience and for planning from simulated experience.\\nThe reinforcement learning method is thus the “ﬁnal common path” for both learning\\nand planning. Learning and planning are deeply integrated in the sense that they share\\nalmost all the same machinery, di↵ering only in the source of their experience.\\nreal\\ndirect RLupdate\\nModel\\nplanning update\\nsearchcontrol\\nPolicy/value functions\\nexperience modellearning\\nEnvironment\\nsimulatedexperience\\nFigure 8.1: The general Dyna Architecture. Real experience, passing back and forth between\\nthe environment and the policy, a↵ects policy and value functions in much the same way as does\\nsimulated experience generated by the model of the environment.\\nConceptually, planning, acting, model-learning, and direct RL occur simultaneously\\nand in parallel in Dyna agents. For concreteness and implementation on a serial computer,\\nhowever, we fully specify the order in which they occur within a time step. In Dyna-Q,\\nthe acting, model-learning, and direct RL processes require little computation, and we\\nassume they consume just a fraction of the time. The remaining time in each step can be\\ndevoted to the planning process, which is inherently computation-intensive. Let us assume\\nthat there is time in each step, after acting, model-learning, and direct RL, to complete'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 185, 'page_label': '186'}, page_content='164 Chapter 8: Planning and Learning with Tabular Methods\\nn iterations (Steps 1–3) of the Q-planning algorithm. In the pseudocode algorithm for\\nDyna-Q in the box below,Model(s, a) denotes the contents of the model (predicted\\nnext state and reward) for state–action pair (s, a). Direct reinforcement learning, model-\\nlearning, and planning are implemented by steps (d), (e), and (f), respectively. If (e) and\\n(f) were omitted, the remaining algorithm would be one-step tabular Q-learning.\\nTabular Dyna-Q\\nInitialize Q(s, a) andModel(s, a) for alls 2 S and a 2 A(s)\\nLoop forever:\\n(a) S  current (nonterminal) state\\n(b) A  \"-greedy(S, Q)\\n(c) Take actionA; observe resultant reward,R, and state,S0\\n(d) Q(S, A)  Q(S, A)+ ↵\\n⇥\\nR + \\x00 maxa Q(S0,a ) \\x00 Q(S, A)\\n⇤\\n(e) Model(S, A)  R, S0 (assuming deterministic environment)\\n(f) Loop repeatn times:\\nS  random previously observed state\\nA  random action previously taken inS\\nR, S0  Model(S, A)\\nQ(S, A)  Q(S, A)+ ↵\\n⇥\\nR + \\x00 maxa Q(S0,a ) \\x00 Q(S, A)\\n⇤\\nExample 8.1: Dyna MazeConsider the simple maze shown inset in Figure 8.2. In\\neach of the 47 states there are four actions,up, down, right, andleft, which take the\\nagent deterministically to the corresponding neighboring states, except when movement\\nis blocked by an obstacle or the edge of the maze, in which case the agent remains where\\nit is. Reward is zero on all transitions, except those into the goal state, on which it is +1.\\nAfter reaching the goal state (G), the agent returns to the start state (S) to begin a new\\nepisode. This is a discounted, episodic task with\\x00 =0 .95.\\nThe main part of Figure 8.2 shows average learning curves from an experiment in\\nwhich Dyna-Q agents were applied to the maze task. The initial action values were zero,\\nthe step-size parameter was↵ =0 .1, and the exploration parameter was\" =0 .1. When\\nselecting greedily among actions, ties were broken randomly. The agents varied in the\\nnumber of planning steps,n, they performed per real step. For eachn,t h ec u r v e ss h o w\\nthe number of steps taken by the agent to reach the goal in each episode, averaged over 30\\nrepetitions of the experiment. In each repetition, the initial seed for the random number\\ngenerator was held constant across algorithms. Because of this, the ﬁrst episode was\\nexactly the same (about 1700 steps) for all values ofn, and its data are not shown in\\nthe ﬁgure. After the ﬁrst episode, performance improved for all values ofn,b u tm u c h\\nmore rapidly for larger values. Recall that then = 0 agent is a nonplanning agent, using\\nonly direct reinforcement learning (one-step tabular Q-learning). This was by far the\\nslowest agent on this problem, despite the fact that the parameter values (↵ and \")w e r e\\noptimized for it. The nonplanning agent took about 25 episodes to reach (\"-)optimal\\nperformance, whereas then = 5 agent took about ﬁve episodes, and then = 50 agent\\ntook only three episodes.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 186, 'page_label': '187'}, page_content='8.2. Dyna: Integrated Planning, Acting, and Learning 165\\n2\\n800\\n600\\n400\\n200\\n14\\n2010 30 40 50\\n  0 planning steps\\n(direct RL only)\\nEpisodes\\nSteps\\nper\\nepisode 5 planning steps\\n  50 planning steps\\nS\\nG\\nactions\\nFigure 8.2: A simple maze (inset) and the average learning curves for Dyna-Q agents varying\\nin their number of planning steps (n) per real step. The task is to travel fromS to G as quickly\\nas possible.\\nFigure 8.3 shows why the planning agents found the solution so much faster than\\nthe nonplanning agent. Shown are the policies found by then = 0 andn = 50 agents\\nhalfway through the second episode. Without planning (n = 0), each episode adds only\\none additional step to the policy, and so only one step (the last) has been learned so far.\\nWith planning, again only one step is learned during the ﬁrst episode, but here during\\nthe second episode an extensive policy has been developed that by the end of the episode\\nwill reach almost back to the start state. This policy is built by the planning process\\nwhile the agent is still wandering near the start state. By the end of the third episode a\\ncomplete optimal policy will have been found and perfect performance attained.\\nS\\nG\\nS\\nG\\nWITHOUT PLANNING ( =0) WITH PLANNING ( =50)n n\\nFigure 8.3: Policies found by planning and nonplanning Dyna-Q agents halfway through the\\nsecond episode. The arrows indicate the greedy action in each state; if no arrow is shown for a\\nstate, then all of its action values were equal. The black square indicates the location of the\\nagent.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 187, 'page_label': '188'}, page_content='166 Chapter 8: Planning and Learning with Tabular Methods\\nIn Dyna-Q, learning and planning are accomplished by exactly the same algorithm,\\noperating on real experience for learning and on simulated experience for planning.\\nBecause planning proceeds incrementally, it is trivial to intermix planning and acting.\\nBoth proceed as fast as they can. The agent is always reactive and always deliberative,\\nresponding instantly to the latest sensory information and yet always planning in the\\nbackground. Also ongoing in the background is the model-learning process. As new\\ninformation is gained, the model is updated to better match reality. As the model changes,\\nthe ongoing planning process will gradually compute a di↵erent way of behaving to match\\nthe new model.\\nExercise 8.1 The nonplanning method looks particularly poor in Figure 8.3 because it is\\na one-step method; a method using multi-step bootstrapping would do better. Do you\\nthink one of the multi-step bootstrapping methods from Chapter 7 could do as well as\\nthe Dyna method? Explain why or why not. ⇤\\n8.3 When the Model Is Wrong\\nIn the maze example presented in the previous section, the changes in the model were\\nrelatively modest. The model started out empty, and was then ﬁlled only with exactly\\ncorrect information. In general, we cannot expect to be so fortunate. Models may be\\nincorrect because the environment is stochastic and only a limited number of samples\\nhave been observed, or because the model was learned using function approximation that\\nhas generalized imperfectly, or simply because the environment has changed and its new\\nbehavior has not yet been observed. When the model is incorrect, the planning process is\\nlikely to compute a suboptimal policy.\\nIn some cases, the suboptimal policy computed by planning quickly leads to the\\ndiscovery and correction of the modeling error. This tends to happen when the model\\nis optimistic in the sense of predicting greater reward or better state transitions than\\nare actually possible. The planned policy attempts to exploit these opportunities and in\\ndoing so discovers that they do not exist.\\nExample 8.2: Blocking Maze A maze example illustrating this relatively minor\\nkind of modeling error and recovery from it is shown in Figure 8.4. Initially, there is a\\nshort path from start to goal, to the right of the barrier, as shown in the upper left of the\\nﬁgure. After 1000 time steps, the short path is “blocked,” and a longer path is opened up\\nalong the left-hand side of the barrier, as shown in upper right of the ﬁgure. The graph\\nshows average cumulative reward for a Dyna-Q agent and an enhanced Dyna-Q+ agent\\nto be described shortly. The ﬁrst part of the graph shows that both Dyna agents found\\nthe short path within 1000 steps. When the environment changed, the graphs become\\nﬂat, indicating a period during which the agents obtained no reward because they were\\nwandering around behind the barrier. After a while, however, they were able to ﬁnd the\\nnew opening and the new optimal behavior.\\nGreater di\\x00culties arise when the environment changes to becomebetter than it was\\nbefore, and yet the formerly correct policy does not reveal the improvement. In these\\ncases the modeling error may not be detected for a long time, if ever.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 188, 'page_label': '189'}, page_content='8.3. When the Model Is Wrong 167\\nCumulative\\nreward\\n0 1000 2000 3000\\nTime steps\\n150\\n0\\nDyna-Q+\\nS\\nG G\\nS\\nDyna-Q\\nFigure 8.4: Average performance of Dyna agents on a blocking task. The left environment\\nwas used for the ﬁrst 1000 steps, the right environment for the rest. Dyna-Q+ is Dyna-Q with\\nan exploration bonus that encourages exploration.\\nCumulative\\nreward\\nS\\nG G\\nS\\n0 3000 6000\\nTime steps\\n400\\n0\\nDyna-Q+\\nDyna-Q\\nFigure 8.5: Average performance of Dyna agents on\\na shortcut task. The left environment was used for the\\nﬁrst 3000 steps, the right environment for the rest.\\nExample 8.3: Shortcut Maze\\nThe problem caused by this kind of\\nenvironmental change is illustrated\\nby the maze example shown in Fig-\\nure 8.5. Initially, the optimal path is\\nto go around the left side of the bar-\\nrier (upper left). After 3000 steps,\\nhowever, a shorter path is opened up\\nalong the right side, without disturb-\\ning the longer path (upper right).\\nThe graph shows that the regular\\nDyna-Q agent never switched to the\\nshortcut. In fact, it never realized\\nthat it existed. Its model said that\\nthere was no shortcut, so the more it\\nplanned, the less likely it was to step\\nto the right and discover it. Even\\nwith an \"-greedy policy, it is very\\nunlikely that an agent will take so\\nmany exploratory actions as to dis-\\ncover the shortcut.\\nThe general problem here is another version of the conﬂict between exploration and\\nexploitation. In a planning context, exploration means trying actions that improve the\\nmodel, whereas exploitation means behaving in the optimal way given the current model.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 189, 'page_label': '190'}, page_content='168 Chapter 8: Planning and Learning with Tabular Methods\\nWe want the agent to explore to ﬁnd changes in the environment, but not so much that\\nperformance is greatly degraded. As in the earlier exploration/exploitation conﬂict, there\\nprobably is no solution that is both perfect and practical, but simple heuristics are often\\ne↵ective.\\nThe Dyna-Q+ agent that did solve the shortcut maze uses one such heuristic. This\\nagent keeps track for each state–action pair of how many time steps have elapsed since\\nthe pair was last tried in a real interaction with the environment. The more time that\\nhas elapsed, the greater (we might presume) the chance that the dynamics of this pair\\nhas changed and that the model of it is incorrect. To encourage behavior that tests\\nlong-untried actions, a special “bonus reward” is given on simulated experiences involving\\nthese actions. In particular, if the modeled reward for a transition isr, and the transition\\nhas not been tried in⌧ time steps, then planning updates are done as if that transition\\nproduced a reward ofr + \\uf8ffp⌧, for some small\\uf8ff. This encourages the agent to keep\\ntesting all accessible state transitions and even to ﬁnd long sequences of actions in order\\nto carry out such tests.1 Of course all this testing has its cost, but in many cases, as in the\\nshortcut maze, this kind of computational curiosity is well worth the extra exploration.\\nExercise 8.2 Why did the Dyna agent with exploration bonus, Dyna-Q+, perform\\nbetter in the ﬁrst phase as well as in the second phase of the blocking and shortcut\\nexperiments? ⇤\\nExercise 8.3 Careful inspection of Figure 8.5 reveals that the di↵erence between Dyna-Q+\\nand Dyna-Q narrowed slightly over the ﬁrst part of the experiment. What is the reason\\nfor this? ⇤\\nExercise 8.4 (programming) The exploration bonus described above actually changes\\nthe estimated values of states and actions. Is this necessary? Suppose the bonus\\uf8ffp⌧\\nwas used not in updates, but solely in action selection. That is, suppose the action\\nselected was always that for whichQ(St,a )+ \\uf8ff\\np\\n⌧(St,a ) was maximal. Carry out a\\ngridworld experiment that tests and illustrates the strengths and weaknesses of this\\nalternate approach. ⇤\\nExercise 8.5 How might the tabular Dyna-Q algorithm shown on page 164 be modiﬁed\\nto handle stochastic environments? How might this modiﬁcation perform poorly on\\nchanging environments such as considered in this section? How could the algorithm be\\nmodiﬁed to handle stochastic environmentsand changing environments? ⇤\\n8.4 Prioritized Sweeping\\nIn the Dyna agents presented in the preceding sections, simulated transitions are started in\\nstate–action pairs selected uniformly at random from all previously experienced pairs. But\\na uniform selection is usually not the best; planning can be much more e\\x00cient if simulated\\ntransitions and updates are focused on particular state–action pairs. For example, consider\\n1The Dyna-Q+ agent was changed in two other ways as well. First, actions that had never been\\ntried before from a state were allowed to be considered in the planning step (f) of the Tabular Dyna-Q\\nalgorithm in the box above. Second, the initial model for such actions was that they would lead back to\\nthe same state with a reward of zero.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 190, 'page_label': '191'}, page_content='8.4. Prioritized Sweeping 169\\nwhat happens during the second episode of the ﬁrst maze task (Figure 8.3). At the\\nbeginning of the second episode, only the state–action pair leading directly into the goal\\nhas a positive value; the values of all other pairs are still zero. This means that it is\\npointless to perform updates along almost all transitions, because they take the agent\\nfrom one zero-valued state to another, and thus the updates would have no e↵ect. Only\\nan update along a transition into the state just prior to the goal, or from it, will change\\nany values. If simulated transitions are generated uniformly, then many wasteful updates\\nwill be made before stumbling onto one of these useful ones. As planning progresses, the\\nregion of useful updates grows, but planning is still far less e\\x00cient than it would be if\\nfocused where it would do the most good. In the much larger problems that are our real\\nobjective, the number of states is so large that an unfocused search would be extremely\\nine\\x00cient.\\nThis example suggests that search might be usefully focused by workingbackward from\\ngoal states. Of course, we do not really want to use any methods speciﬁc to the idea of\\n“goal state.” We want methods that work for general reward functions. Goal states are\\njust a special case, convenient for stimulating intuition. In general, we want to work back\\nnot just from goal states but from any state whose value has changed. Suppose that the\\nvalues are initially correct given the model, as they were in the maze example prior to\\ndiscovering the goal. Suppose now that the agent discovers a change in the environment\\nand changes its estimated value of one state, either up or down. Typically, this will imply\\nthat the values of many other states should also be changed, but the only useful one-step\\nupdates are those of actions that lead directly into the one state whose value has been\\nchanged. If the values of these actions are updated, then the values of the predecessor\\nstates may change in turn. If so, then actions leading into them need to be updated, and\\nthen their predecessor states may have changed. In this way one can work backward\\nfrom arbitrary states that have changed in value, either performing useful updates or\\nterminating the propagation. This general idea might be termedbackward focusingof\\nplanning computations.\\nAs the frontier of useful updates propagates backward, it often grows rapidly, producing\\nmany state–action pairs that could usefully be updated. But not all of these will be\\nequally useful. The values of some states may have changed a lot, whereas others may\\nhave changed little. The predecessor pairs of those that have changed a lot are more\\nlikely to also change a lot. In a stochastic environment, variations in estimated transition\\nprobabilities also contribute to variations in the sizes of changes and in the urgency with\\nwhich pairs need to be updated. It is natural to prioritize the updates according to a\\nmeasure of their urgency, and perform them in order of priority. This is the idea behind\\nprioritized sweeping. A queue is maintained of every state–action pair whose estimated\\nvalue would change nontrivially if updated , prioritized by the size of the change. When\\nthe top pair in the queue is updated, the e↵ect on each of its predecessor pairs is computed.\\nIf the e↵ect is greater than some small threshold, then the pair is inserted in the queue\\nwith the new priority (if there is a previous entry of the pair in the queue, then insertion\\nresults in only the higher priority entry remaining in the queue). In this way the e↵ects\\nof changes are e\\x00ciently propagated backward until quiescence. The full algorithm for\\nthe case of deterministic environments is given in the box on the next page.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 191, 'page_label': '192'}, page_content='170 Chapter 8: Planning and Learning with Tabular Methods\\nPrioritized sweeping for a deterministic environment\\nInitialize Q(s, a), Model(s, a), for alls, a, andPQueue to empty\\nLoop forever:\\n(a) S  current (nonterminal) state\\n(b) A  policy(S, Q)\\n(c) Take actionA; observe resultant reward,R, and state,S0\\n(d) Model(S, A)  R, S0\\n(e) P  | R + \\x00 maxa Q(S0,a ) \\x00 Q(S, A)|.\\n(f) if P>✓ ,t h e ni n s e r tS, Ainto PQueue with priorityP\\n(g) Loop repeatn times, whilePQueue is not empty:\\nS, A first(PQueue)\\nR, S0  Model(S, A)\\nQ(S, A)  Q(S, A)+ ↵\\n⇥\\nR + \\x00 maxa Q(S0,a ) \\x00 Q(S, A)\\n⇤\\nLoop for all¯S, ¯A predicted to lead toS:\\n¯R  predicted reward for¯S, ¯A, S\\nP  | ¯R + \\x00 maxa Q(S, a) \\x00 Q( ¯S, ¯A)|.\\nif P>✓ then insert ¯S, ¯A into PQueue with priorityP\\nBackupsuntiloptimalsolution\\n10\\n103\\n104\\n105\\n106\\n107\\n102\\n0 47 94 186 376 752 1504 30086016\\nGridworld size (#states)\\nDyna-Q\\nPrioritizedsweeping\\nUpdatesUpdatesuntiloptimalsolution\\nExample 8.4: Prioritized Sweeping\\non Mazes Prioritized sweeping has been\\nfound to dramatically increase the speed\\nat which optimal solutions are found in\\nmaze tasks, often by a factor of 5 to 10.\\nA typical example is shown to the right.\\nThese data are for a sequence of maze\\ntasks of exactly the same structure as the\\none shown in Figure 8.2, except that they\\nvary in the grid resolution. Prioritized\\nsweeping maintained a decisive advantage\\nover unprioritized Dyna-Q. Both systems\\nmade at mostn = 5 updates per environ-\\nmental interaction. Adapted from Peng\\nand Williams (1993).\\nExtensions of prioritized sweeping to stochastic environments are straightforward. The\\nmodel is maintained by keeping counts of the number of times each state–action pair has\\nbeen experienced and of what the next states were. It is natural then to update each pair\\nnot with a sample update, as we have been using so far, but with an expected update,\\ntaking into account all possible next states and their probabilities of occurring.\\nPrioritized sweeping is just one way of distributing computations to improve planning\\ne\\x00ciency, and probably not the best way. One of prioritized sweeping’s limitations is that\\nit usesexpected updates, which in stochastic environments may waste lots of computation\\non low-probability transitions. As we show in the following section, sample updates'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 192, 'page_label': '193'}, page_content='8.4. Prioritized Sweeping 171\\nExample 8.5 Prioritized Sweeping for Rod Maneuvering\\nStart\\nGoal\\nThe objective in this task is to\\nmaneuver a rod around some awk-\\nwardly placed obstacles within a\\nlimited rectangular work space to a\\ngoal position in the fewest number\\nof steps. The rod can be translated\\nalong its long axis or perpendicu-\\nlar to that axis, or it can be ro-\\ntated in either direction around its\\ncenter. The distance of each move-\\nment is approximately 1/20 of the\\nwork space, and the rotation incre-\\nment is 10 degrees. Translations\\nare deterministic and quantized to\\none of 20⇥ 20 positions. To the\\nright is shown the obstacles and the\\nshortest solution from start to goal,\\nfound by prioritized sweeping. This problem is deterministic, but has four actions\\nand 14,400 potential states (some of these are unreachable because of the obstacles).\\nThis problem is probably too large to be solved with unprioritized methods. Figure\\nreprinted from Moore and Atkeson (1993).\\ncan in many cases get closer to the true value function with less computation despite\\nthe variance introduced by sampling. Sample updates can win because they break the\\noverall backing-up computation into smaller pieces—those corresponding to individual\\ntransitions—which then enables it to be focused more narrowly on the pieces that will\\nhave the largest impact. This idea was taken to what may be its logical limit in the “small\\nbackups” introduced by van Seijen and Sutton (2013). These are updates along a single\\ntransition, like a sample update, but based on the probability of the transition without\\nsampling, as in an expected update. By selecting the order in which small updates\\nare done it is possible to greatly improve planning e\\x00ciency beyond that possible with\\nprioritized sweeping.\\nWe have suggested in this chapter that all kinds of state-space planning can be viewed\\nas sequences of value updates, varying only in the type of update, expected or sample,\\nlarge or small, and in the order in which the updates are done. In this section we have\\nemphasized backward focusing, but this is just one strategy. For example, another would\\nbe to focus on states according to how easily they can be reached from the states that\\nare visited frequently under the current policy, which might be calledforward focusing.\\nPeng and Williams (1993) and Barto, Bradtke and Singh (1995) have explored versions\\nof forward focusing, and the methods introduced in the next few sections take it to an\\nextreme form.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 193, 'page_label': '194'}, page_content='172 Chapter 8: Planning and Learning with Tabular Methods\\n8.5 Expected vs. Sample Updates\\nThe examples in the previous sections give some idea of the range of possibilities for\\ncombining methods of learning and planning. In the rest of this chapter, we analyze some\\nof the component ideas involved, starting with the relative advantages of expected and\\nsample updates.\\nMuch of this book has been about di↵erent kinds of value-function updates, and we\\nhave considered a great many varieties. Focusing for the moment on one-step updates,\\nthey vary primarily along three binary dimensions. The ﬁrst two dimensions are whether\\nthey update state values or action values and whether they estimate the value for the\\noptimal policy or for an arbitrary given policy. These two dimensions give rise to four\\nclasses of updates for approximating the four value functions,q⇤, v⇤, q⇡, and v⇡.T h e\\nValueestimatedExpected updates(DP) Sample updates (one-step TD)\\n⇡\\ns\\ns0\\n⇡\\nrp\\na\\nq⇡(s, a)\\nq⇤(s, a)\\nv⇡(s)\\nv⇤(s)\\ns\\ns0r\\nmax ap\\npolicy evaluation\\nvalue iteration\\nrs0\\ns, a\\na0⇡\\np\\nq-policy evaluation\\nrs0\\ns, a\\na0max\\np\\nq-value iteration\\ns\\nA\\nS0R\\nRS0\\ns, a\\nA0\\nRS0\\ns, a\\nmax\\nTD(0)\\nSarsa\\nQ-learninga0\\nFigure 8.6: Backup diagrams for all the one-step\\nupdates considered in this book.\\nother binary dimension is whether the\\nupdates are expected updates, consider-\\ning all possible events that might hap-\\npen, or sample updates, considering a\\nsingle sample of what might happen.\\nThese three binary dimensions give rise\\nto eight cases, seven of which corre-\\nspond to speciﬁc algorithms, as shown\\nin the ﬁgure to the right. (The eighth\\ncase does not seem to correspond to\\nany useful update.) Any of these one-\\nstep updates can be used in planning\\nmethods. The Dyna-Q agents discussed\\nearlier useq⇤ sample updates, but they\\ncould just as well useq⇤ expected up-\\ndates, or either expected or sampleq⇡\\nupdates. The Dyna-AC system usesv⇡\\nsample updates together with a learning\\npolicy structure (as in Chapter 13). For\\nstochastic problems, prioritized sweep-\\ning is always done using one of the ex-\\npected updates.\\nWhen we introduced one-step sam-\\nple updates in Chapter 6, we presented\\nthem as substitutes for expected up-\\ndates. In the absence of a distribution\\nmodel, expected updates are not pos-\\nsible, but sample updates can be done\\nusing sample transitions from the envi-\\nronment or a sample model. Implicit in\\nthat point of view is that expected up-\\ndates, if possible, are preferable to sam-\\nple updates. But are they? Expected'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 194, 'page_label': '195'}, page_content='8.5. Expected vs. Sample Updates 173\\nupdates certainly yield a better estimate because they are uncorrupted by sampling error,\\nbut they also require more computation, and computation is often the limiting resource\\nin planning. To properly assess the relative merits of expected and sample updates for\\nplanning we must control for their di↵erent computational requirements.\\nFor concreteness, consider the expected and sample updates for approximatingq⇤,\\nand the special case of discrete states and actions, a table-lookup representation of\\nthe approximate value function,Q, and a model in the form of estimated dynamics,\\nˆp(s0,r |s, a). The expected update for a state–action pair,s, a,i s :\\nQ(s, a)  \\nX\\ns0,r\\nˆp(s0,r |s, a)\\nh\\nr + \\x00 max\\na0\\nQ(s0,a 0)\\ni\\n. (8.1)\\nThe corresponding sample update fors, a, given a sample next state and reward,S0 and\\nR (from the model), is the Q-learning-like update:\\nQ(s, a)  Q(s, a)+ ↵\\nh\\nR + \\x00 max\\na0\\nQ(S0,a 0) \\x00 Q(s, a)\\ni\\n, (8.2)\\nwhere ↵ is the usual positive step-size parameter.\\nThe di↵erence between these expected and sample updates is signiﬁcant to the extent\\nthat the environment is stochastic, speciﬁcally, to the extent that, given a state and\\naction, many possible next states may occur with various probabilities. If only one next\\nstate is possible, then the expected and sample updates given above are identical (taking\\n↵ = 1). If there are many possible next states, then there may be signiﬁcant di↵erences.\\nIn favor of the expected update is that it is an exact computation, resulting in a new\\nQ(s, a) whose correctness is limited only by the correctness of theQ(s0,a 0) at successor\\nstates. The sample update is in addition a↵ected by sampling error. On the other hand,\\nthe sample update is cheaper computationally because it considers only one next state,\\nnot all possible next states. In practice, the computation required by update operations\\nis usually dominated by the number of state–action pairs at whichQ is evaluated. For a\\nparticular starting pair,s, a,l e tb be the branching factor(i.e., the number of possible\\nnext states,s0, for whichˆp(s0|s, a) > 0). Then an expected update of this pair requires\\nroughly b times as much computation as a sample update.\\nIf there is enough time to complete an expected update, then the resulting estimate is\\ngenerally better than that ofb sample updates because of the absence of sampling error.\\nBut if there is insu\\x00cient time to complete an expected update, then sample updates are\\nalways preferable because they at least make some improvement in the value estimate\\nwith fewer thanb updates. In a large problem with many state–action pairs, we are often\\nin the latter situation. With so many state–action pairs, expected updates of all of them\\nwould take a very long time. Before that we may be much better o↵ with a few sample\\nupdates at many state–action pairs than with expected updates at a few pairs. Given a\\nunit of computational e↵ort, is it better devoted to a few expected updates or tob times\\nas many sample updates?\\nFigure 8.7 shows the results of an analysis that suggests an answer to this question. It\\nshows the estimation error as a function of computation time for expected and sample\\nupdates for a variety of branching factors,b. The case considered is that in which all'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 195, 'page_label': '196'}, page_content='174 Chapter 8: Planning and Learning with Tabular Methods\\nb = 2 (branching factor)\\nb =10\\nb =100\\nb =1000b =10,000\\nsampleupdates expectedupdates\\n1\\n00 1b 2b\\nRMS errorin valueestimate\\nNumber of                         computationsmaxa0\\nQ(s0,a 0)\\nFigure 8.7: Comparison of e\\x00ciency of expected and sample updates.\\nb successor states are equally likely and in which the error in the initial estimate is\\n1. The values at the next states are assumed correct, so the expected update reduces\\nthe error to zero upon its completion. In this case, sample updates reduce the error\\naccording to\\nq\\nb\\x001\\nbt where t is the number of sample updates that have been performed\\n(assuming sample averages, i.e.,↵ =1 /t). The key observation is that for moderately\\nlarge b the error falls dramatically with a tiny fraction ofb updates. For these cases,\\nmany state–action pairs could have their values improved dramatically, to within a few\\npercent of the e↵ect of an expected update, in the same time that a single state–action\\npair could undergo an expected update.\\nThe advantage of sample updates shown in Figure 8.7 is probably an underestimate of\\nthe real e↵ect. In a real problem, the values of the successor states would be estimates\\nthat are themselves updated. By causing estimates to be more accurate sooner, sample\\nupdates will have a second advantage in that the values backed up from the successor\\nstates will be more accurate. These results suggest that sample updates are likely to be\\nsuperior to expected updates on problems with large stochastic branching factors and\\ntoo many states to be solved exactly.\\nExercise 8.6 The analysis above assumed that all of theb possible next states were\\nequally likely to occur. Suppose instead that the distribution was highly skewed, that\\nsome of theb states were much more likely to occur than most. Would this strengthen or\\nweaken the case for sample updates over expected updates? Support your answer.⇤\\n8.6 Trajectory Sampling\\nIn this section we compare two ways of distributing updates. The classical approach, from\\ndynamic programming, is to perform sweeps through the entire state (or state–action)\\nspace, updating each state (or state–action pair) once per sweep. This is problematic'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 196, 'page_label': '197'}, page_content='8.6. Trajectory Sampling 175\\non large tasks because there may not be time to complete even one sweep. In many\\ntasks the vast majority of the states are irrelevant because they are visited only under\\nvery poor policies or with very low probability. Exhaustive sweeps implicitly devote\\nequal time to all parts of the state space rather than focusing where it is needed. As we\\ndiscussed in Chapter 4, exhaustive sweeps and the equal treatment of all states that they\\nimply are not necessary properties of dynamic programming. In principle, updates can\\nbe distributed any way one likes (to assure convergence, all states or state–action pairs\\nmust be visited in the limit an inﬁnite number of times; although an exception to this is\\ndiscussed in Section 8.7 below), but in practice exhaustive sweeps are often used.\\nThe second approach is to sample from the state or state–action space according\\nto some distribution. One could sample uniformly, as in the Dyna-Q agent, but this\\nwould su↵er from some of the same problems as exhaustive sweeps. More appealing\\nis to distribute updates according to the on-policy distribution, that is, according to\\nthe distribution observed when following the current policy. One advantage of this\\ndistribution is that it is easily generated; one simply interacts with the model, following\\nthe current policy. In an episodic task, one starts in a start state (or according to the\\nstarting-state distribution) and simulates until the terminal state. In a continuing task,\\none starts anywhere and just keeps simulating. In either case, sample state transitions\\nand rewards are given by the model, and sample actions are given by the current policy.\\nIn other words, one simulates explicit individual trajectories and performs updates at the\\nstate or state–action pairs encountered along the way. We call this way of generating\\nexperience and updatestrajectory sampling.\\nIt is hard to imagine any e\\x00cient way of distributing updates according to the on-policy\\ndistribution other than by trajectory sampling. If one had an explicit representation\\nof the on-policy distribution, then one could sweep through all states, weighting the\\nupdate of each according to the on-policy distribution, but this leaves us again with all\\nthe computational costs of exhaustive sweeps. Possibly one could sample and update\\nindividual state–action pairs from the distribution, but even if this could be done e\\x00ciently,\\nwhat beneﬁt would this provide over simulating trajectories? Even knowing the on-policy\\ndistribution in an explicit form is unlikely. The distribution changes whenever the policy\\nchanges, and computing the distribution requires computation comparable to a complete\\npolicy evaluation. Consideration of such other possibilities makes trajectory sampling\\nseem both e\\x00cient and elegant.\\nIs the on-policy distribution of updates a good one? Intuitively it seems like a good\\nchoice, at least better than the uniform distribution. For example, if you are learning to\\nplay chess, you study positions that might arise in real games, not random positions of\\nchess pieces. The latter may be valid states, but to be able to accurately value them is a\\ndi↵erent skill from evaluating positions in real games. We will also see in Part II that the\\non-policy distribution has signiﬁcant advantages when function approximation is used.\\nWhether or not function approximation is used, one might expect on-policy focusing to\\nsigniﬁcantly improve the speed of planning.\\nFocusing on the on-policy distribution could be beneﬁcial because it causes vast,\\nuninteresting parts of the space to be ignored, or it could be detrimental because it causes\\nthe same old parts of the space to be updated over and over. We conducted a small'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 197, 'page_label': '198'}, page_content='176 Chapter 8: Planning and Learning with Tabular Methods\\nexperiment to assess the e↵ect empirically. To isolate the e↵ect of the update distribution,\\nwe used entirely one-step expected tabular updates, as deﬁned by (8.1). In theuniform\\ncase, we cycled through all state–action pairs, updating each in place, and in theon-policy\\ncase we simulated episodes, all starting in the same state, updating each state–action pair\\nthat occurred under the current\"-greedy policy (\"=0.1). The tasks were undiscounted\\nepisodic tasks, generated randomly as follows. From each of the|S| states, two actions\\nwere possible, each of which resulted in one ofb next states, all equally likely, with a\\ndi↵erent random selection ofb states for each state–action pair. The branching factor,b,\\nwas the same for all state–action pairs. In addition, on all transitions there was a 0.1\\nprobability of transition to the terminal state, ending the episode. The expected reward\\non each transition was selected from a Gaussian distribution with mean 0 and variance 1.\\nb=10\\nb=3\\nb=1\\nb=1\\nio n - p o l c y\\nio n - p o l c y\\nuniform\\nuniform\\n \\n0\\n1\\n2\\n3\\nValue ofstart stateundergreedypolicy\\n0 5,000 10,000 15,000 20,000\\nComputation time, in full backups\\n0\\n1\\n2\\n3\\nValue ofstart stateundergreedypolicy\\n0 50,000 100,000 150,000 200,000\\nComputation time, in full backups\\nuniform\\nuniform\\non-policy\\non-policy\\nexpected updates\\nexpected updates\\n1,000 STATES\\n10,000 STATES\\nFigure 8.8: Relative e\\x00ciency of updates dis-\\ntributed uniformly across the state space versus\\nfocused on simulated on-policy trajectories, each\\nstarting in the same state. Results are for randomly\\ngenerated tasks of two sizes and various branching\\nfactors, b.\\nAt any point in the planning process\\none can stop and exhaustively compute\\nv˜⇡(s0), the true value of the start state\\nunder the greedy policy,˜⇡, given the cur-\\nrent action-value functionQ, as an indi-\\ncation of how well the agent would do on\\na new episode on which it acted greed-\\nily (all the while assuming the model is\\ncorrect).\\nThe upper part of the ﬁgure to\\nthe right shows results averaged over\\n200 sample tasks with 1000 states and\\nbranching factors of 1, 3, and 10. The\\nquality of the policies found is plotted as\\na function of the number of expected up-\\ndates completed. In all cases, sampling\\naccording to the on-policy distribution\\nresulted in faster planning initially and\\nretarded planning in the long run. The\\ne↵ect was stronger, and the initial pe-\\nriod of faster planning was longer, at\\nsmaller branching factors. In other ex-\\nperiments, we found that these e↵ects\\nalso became stronger as the number of\\nstates increased. For example, the lower\\npart of the ﬁgure shows results for a\\nbranching factor of 1 for tasks with\\n10,000 states. In this case the advan-\\ntage of on-policy focusing is large and\\nlong-lasting.\\nAll of these results make sense. In the\\nshort term, sampling according to the\\non-policy distribution helps by focusing\\non states that are near descendants of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 198, 'page_label': '199'}, page_content='8.7. Real-time Dynamic Programming 177\\nthe start state. If there are many states and a small branching factor, this e↵ect will be\\nlarge and long-lasting. In the long run, focusing on the on-policy distribution may hurt\\nbecause the commonly occurring states all already have their correct values. Sampling\\nthem is useless, whereas sampling other states may actually perform some useful work.\\nThis presumably is why the exhaustive, unfocused approach does better in the long run,\\nat least for small problems. These results are not conclusive because they are only for\\nproblems generated in a particular, random way, but they do suggest that sampling\\naccording to the on-policy distribution can be a great advantage for large problems, in\\nparticular for problems in which a small subset of the state–action space is visited under\\nthe on-policy distribution.\\nExercise 8.7 Some of the graphs in Figure 8.8 seem to be scalloped in their early portions,\\nparticularly the upper graph forb = 1 and the uniform distribution. Why do you think\\nthis is? What aspects of the data shown support your hypothesis? ⇤\\nExercise 8.8 (programming) Replicate the experiment whose results are shown in the\\nlower part of Figure 8.8, then try the same experiment but withb = 3. Discuss the\\nmeaning of your results. ⇤\\n8.7 Real-time Dynamic Programming\\nReal-time dynamic programming, or RTDP, is an on-policy trajectory-sampling version of\\nthe value-iteration algorithm of dynamic programming (DP). Because it is closely related\\nto conventional sweep-based policy iteration, RTDP illustrates in a particularly clear way\\nsome of the advantages that on-policy trajectory sampling can provide. RTDP updates\\nthe values of states visited in actual or simulated trajectories by means of expected\\ntabular value-iteration updates as deﬁned by (4.10). It is basically the algorithm that\\nproduced the on-policy results shown in Figure 8.8.\\nThe close connection between RTDP and conventional DP makes it possible to derive\\nsome theoretical results by adapting existing theory. RTDP is an example of anasyn-\\nchronous DP algorithm as described in Section 4.5. Asynchronous DP algorithms are\\nnot organized in terms of systematic sweeps of the state set; they update state values in\\nany order whatsoever, using whatever values of other states happen to be available. In\\nRTDP, the update order is dictated by the order states are visited in real or simulated\\ntrajectories.\\nStart StatesIrrelevant States: unreachable  from any start stateunder any optimal policy\\nRelevant Statesreachable from some start state under some optimal policy\\nIf trajectories can start only from a designated\\nset of start states, and if you are interested in\\nthe prediction problem for a given policy, then on-\\npolicy trajectory sampling allows the algorithm to\\ncompletely skip states that cannot be reached by\\nthe given policy from any of the start states: such\\nstates are irrelevant to the prediction problem.\\nFor a control problem, where the goal is to ﬁnd\\nan optimal policy instead of evaluating a given\\npolicy, there might well be states that cannot be'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 199, 'page_label': '200'}, page_content='178 Chapter 8: Planning and Learning with Tabular Methods\\nreached by any optimal policy from any of the start states, and there is no need to specify\\noptimal actions for these irrelevant states. What is needed is anoptimal partial policy,\\nmeaning a policy that is optimal for the relevant states but can specify arbitrary actions,\\nor even be undeﬁned, for the irrelevant states.\\nBut ﬁnding such an optimal partial policy with an on-policy trajectory-sampling\\ncontrol method, such as Sarsa (Section 6.4), in general requires visiting all state–action\\npairs—even those that will turn out to be irrelevant—an inﬁnite number of times. This\\ncan be done, for example, by using exploring starts (Section 5.3). This is true for RTDP\\nas well: for episodic tasks with exploring starts, RTDP is an asynchronous value-iteration\\nalgorithm that converges to optimal policies for discounted ﬁnite MDPs (and for the\\nundiscounted case under certain conditions). Unlike the situation for a prediction problem,\\nit is generally not possible to stop updating any state or state–action pair if convergence\\nto an optimal policy is important.\\nThe most interesting result for RTDP is that for certain types of problems satisfying\\nreasonable conditions, RTDP is guaranteed to ﬁnd a policy that is optimal on the relevant\\nstates without visiting every state inﬁnitely often, or even without visiting some states at\\nall. Indeed, in some problems, only a small fraction of the states need to be visited. This\\ncan be a great advantage for problems with very large state sets, where even a single\\nsweep may not be feasible.\\nThe tasks for which this result holds are undiscounted episodic tasks for MDPs with\\nabsorbing goal states that generate zero rewards, as described in Section 3.4. At every step\\nof a real or simulated trajectory, RTDP selects a greedy action (breaking ties randomly)\\nand applies the expected value-iteration update operation to the current state. It can\\nalso update the values of an arbitrary collection of other states at each step; for example,\\nit can update the values of states visited in a limited-horizon look-ahead search from the\\ncurrent state.\\nFor these problems, with each episode beginning in a state randomly chosen from the\\nset of start states and ending at a goal state, RTDP converges with probability one to a\\npolicy that is optimal for all the relevant states provided: (1) the initial value of every\\ngoal state is zero, (2) there exists at least one policy that guarantees that a goal state\\nwill be reached with probability one from any start state, (3) all rewards for transitions\\nfrom non-goal states are strictly negative, and (4) all the initial values are equal to, or\\ngreater than, their optimal values (which can be satisﬁed by simply setting the initial\\nvalues of all states to zero). This result was proved by Barto, Bradtke, and Singh (1995)\\nby combining results for asynchronous DP with results about a heuristic search algorithm\\nknown aslearning real-time A*due to Korf (1990).\\nTasks having these properties are examples ofstochastic optimal path problems,w h i c h\\nare usually stated in terms of cost minimization instead of as reward maximization as\\nwe do here. Maximizing the negative returns in our version is equivalent to minimizing\\nthe costs of paths from a start state to a goal state. Examples of this kind of task are\\nminimum-time control tasks, where each time step required to reach a goal produces a\\nreward of\\x001, or problems like the Golf example in Section 3.5, whose objective is to hit\\nthe hole with the fewest strokes.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 200, 'page_label': '201'}, page_content='8.7. Real-time Dynamic Programming 179\\nExample 8.6: RTDP on the RacetrackThe racetrack problem of Exercise 5.12\\n(page 111) is a stochastic optimal path problem. Comparing RTDP and the conventional\\nDP value iteration algorithm on an example racetrack problem illustrates some of the\\nadvantages of on-policy trajectory sampling.\\nRecall from the exercise that an agent has to learn how to drive a car around a turn\\nlike those shown in Figure 5.5 and cross the ﬁnish line as quickly as possible while staying\\non the track. Start states are all the zero-speed states on the starting line; the goal states\\nare all the states that can be reached in one time step by crossing the ﬁnish line from\\ninside the track. Unlike Exercise 5.12, here there is no limit on the car’s speed, so the\\nstate set is potentially inﬁnite. However, the set of states that can be reached from the\\nset of start states via any policy is ﬁnite and can be considered to be the state set of the\\nproblem. Each episode begins in a randomly selected start state and ends when the car\\ncrosses the ﬁnish line. The rewards are\\x001 for each step until the car crosses the ﬁnish\\nline. If the car hits the track boundary, it is moved back to a random start state, and the\\nepisode continues.\\nA racetrack similar to the small racetrack on the left of Figure 5.5 has 9,115 states\\nreachable from start states by any policy, only 599 of which are relevant, meaning that\\nthey are reachable from some start state via some optimal policy. (The number of relevant\\nstates was estimated by counting the states visited while executing optimal actions for\\n107 episodes.)\\nThe table below compares solving this task by conventional DP and by RTDP. These\\nresults are averages over 25 runs, each begun with a di↵erent random number seed.\\nConventional DP in this case is value iteration using exhaustive sweeps of the state set,\\nwith values updated one state at a time in place, meaning that the update for each state\\nuses the most recent values of the other states (This is the Gauss-Seidel version of value\\niteration, which was found to be approximately twice as fast as the Jacobi version on\\nthis problem. See Section 4.8.) No special attention was paid to the ordering of the\\nupdates; other orderings could have produced faster convergence. Initial values were all\\nzero for each run of both methods. DP was judged to have converged when the maximum\\nchange in a state value over a sweep was less than 10\\x004, and RTDP was judged to have\\nconverged when the average time to cross the ﬁnish line over 20 episodes appeared to\\nstabilize at an asymptotic number of steps. This version of RTDP updated only the value\\nof the current state on each step.\\nDP RTDP\\nAverage computation to convergence 28 sweeps 4000 episodes\\nAverage number of updates to convergence 252,784 127,600\\nAverage number of updates per episode — 31.9\\n% of states updated\\uf8ff 100 times — 98.45\\n% of states updated\\uf8ff 10 times — 80.51\\n% of states updated 0 times — 3.18\\nBoth methods produced policies averaging between 14 and 15 steps to cross the ﬁnish\\nline, but RTDP required only roughly half of the updates that DP did. This is the result\\nof RTDP’s on-policy trajectory sampling. Whereas the value of every state was updated'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 201, 'page_label': '202'}, page_content='180 Chapter 8: Planning and Learning with Tabular Methods\\nin each sweep of DP, RTDP focused updates on fewer states. In an average run, RTDP\\nupdated the values of 98.45% of the states no more than 100 times and 80.51% of the\\nstates no more than 10 times; the values of about 290 states were not updated at all in\\nan average run.\\nAnother advantage of RTDP is that as the value function approaches the optimal\\nvalue functionv⇤, the policy used by the agent to generate trajectories approaches an\\noptimal policy because it is always greedy with respect to the current value function.\\nThis is in contrast to the situation in conventional value iteration. In practice, value\\niteration terminates when the value function changes by only a small amount in a sweep,\\nwhich is how we terminated it to obtain the results in the table above. At this point,\\nthe value function closely approximatesv⇤, and a greedy policy is close to an optimal\\npolicy. However, it is possible that policies that are greedy with respect to the latest\\nvalue function were optimal, or nearly so, long before value iteration terminates. (Recall\\nfrom Chapter 4 that optimal policies can be greedy with respect to many di↵erent\\nvalue functions, not justv⇤.) Checking for the emergence of an optimal policy before\\nvalue iteration converges is not a part of the conventional DP algorithm and requires\\nconsiderable additional computation.\\nIn the racetrack example, by running many test episodes after each DP sweep, with\\nactions selected greedily according to the result of that sweep, it was possible to estimate\\nthe earliest point in the DP computation at which the approximated optimal evaluation\\nfunction was good enough so that the corresponding greedy policy was nearly optimal.\\nFor this racetrack, a close-to-optimal policy emerged after 15 sweeps of value iteration, or\\nafter 136,725 value-iteration updates. This is considerably less than the 252,784 updates\\nDP needed to converge tov⇤, but still more than the 127,600 updates RTDP required.\\nAlthough these simulations are certainly not deﬁnitive comparisons of the RTDP with\\nconventional sweep-based value iteration, they illustrate some of advantages of on-policy\\ntrajectory sampling. Whereas conventional value iteration continued to update the value\\nof all the states, RTDP strongly focused on subsets of the states that were relevant to\\nthe problem’s objective. This focus became increasingly narrow as learning continued.\\nBecause the convergence theorem for RTDP applies to the simulations, we know that\\nRTDP eventually would have focused only on relevant states, i.e., on states making up\\noptimal paths. RTDP achieved nearly optimal control with about 50% of the computation\\nrequired by sweep-based value iteration.\\n8.8 Planning at Decision Time\\nPlanning can be used in at least two ways. The one we have considered so far in this\\nchapter, typiﬁed by dynamic programming and Dyna, is to use planning to gradually\\nimprove a policy or value function on the basis of simulated experience obtained from a\\nmodel (either a sample or a distribution model). Selecting actions is then a matter of\\ncomparing the current state’s action values obtained from a table in the tabular case we\\nhave thus far considered, or by evaluating a mathematical expression in the approximate\\nmethods we consider in Part II below. Well before an action is selected for any current\\nstate St, planning has played a part in improving the table entries, or the function'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 202, 'page_label': '203'}, page_content='8.9. Heuristic Search 181\\napproximation parameters, needed to select actions for many states, includingSt. Used\\nthis way, planning is not focused on the current state. We call planning used in this way\\nbackground planning.\\nThe other way to use planning is to begin and complete itafter encountering each\\nnew state St, as a computation whose output is the selection of a single actionAt; on\\nthe next step planning begins anew withSt+1 to produce At+1, and so on. The simplest,\\nand almost degenerate, example of this use of planning is when only state values are\\navailable, and an action is selected by comparing the values of model-predicted next states\\nfor each action (or by comparing the values of afterstates as in the tic-tac-toe example\\nin Chapter 1). More generally, planning used in this way can look much deeper than\\none-step-ahead and evaluate action choices leading to many di↵erent predicted state and\\nreward trajectories. Unlike the ﬁrst use of planning, here planning focuses on a particular\\nstate. We call thisdecision-time planning.\\nThese two ways of thinking about planning—using simulated experience to gradually\\nimprove a policy or value function, or using simulated experience to select an action for\\nthe current state—can blend together in natural and interesting ways, but they have\\ntended to be studied separately, and that is a good way to ﬁrst understand them. Let us\\nnow take a closer look at decision-time planning.\\nEven when planning is only done at decision time, we can still view it, as we did\\nin Section 8.1, as proceeding from simulated experience to updates and values, and\\nultimately to a policy. It is just that now the values and policy are speciﬁc to the current\\nstate and the action choices available there, so much so that the values and policy created\\nby the planning process are typically discarded after being used to select the current\\naction. In many applications this is not a great loss because there are very many states\\nand we are unlikely to return to the same state for a long time. In general, one may\\nwant to do a mix of both: focus planning on the current stateand store the results\\nof planning so as to be that much farther along should one return to the same state\\nlater. Decision-time planning is most useful in applications in which fast responses are\\nnot required. In chess playing programs, for example, one may be permitted seconds or\\nminutes of computation for each move, and strong programs may plan dozens of moves\\nahead within this time. On the other hand, if low latency action selection is the priority,\\nthen one is generally better o↵ doing planning in the background to compute a policy\\nthat can then be rapidly applied to each newly encountered state.\\n8.9 Heuristic Search\\nThe classical state-space planning methods in artiﬁcial intelligence are decision-time\\nplanning methods collectively known asheuristic search. In heuristic search, for each\\nstate encountered, a large tree of possible continuations is considered. The approximate\\nvalue function is applied to the leaf nodes and then backed up toward the current state\\nat the root. The backing up within the search tree is just the same as in the expected\\nupdates with maxes (those forv⇤ and q⇤) discussed throughout this book. The backing\\nup stops at the state–action nodes for the current state. Once the backed-up values of\\nthese nodes are computed, the best of them is chosen as the current action, and then all\\nbacked-up values are discarded.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 203, 'page_label': '204'}, page_content='182 Chapter 8: Planning and Learning with Tabular Methods\\nIn conventional heuristic search no e↵ort is made to save the backed-up values by\\nchanging the approximate value function. In fact, the value function is generally designed\\nby people and never changed as a result of search. However, it is natural to consider\\nallowing the value function to be improved over time, using either the backed-up values\\ncomputed during heuristic search or any of the other methods presented throughout\\nthis book. In a sense we have taken this approach all along. Our greedy,\"-greedy, and\\nUCB (Section 2.7) action-selection methods are not unlike heuristic search, albeit on a\\nsmaller scale. For example, to compute the greedy action given a model and a state-value\\nfunction, we must look ahead from each possible action to each possible next state, take\\ninto account the rewards and estimated values, and then pick the best action. Just as\\nin conventional heuristic search, this process computes backed-up values of the possible\\nactions, but does not attempt to save them. Thus, heuristic search can be viewed as an\\nextension of the idea of a greedy policy beyond a single step.\\nThe point of searching deeper than one step is to obtain better action selections. If one\\nhas a perfect model and an imperfect action-value function, then in fact deeper search\\nwill usually yield better policies.2 Certainly, if the search is all the way to the end of\\nthe episode, then the e↵ect of the imperfect value function is eliminated, and the action\\ndetermined in this way must be optimal. If the search is of su\\x00cient depthk such that\\x00k\\nis very small, then the actions will be correspondingly near optimal. On the other hand,\\nthe deeper the search, the more computation is required, usually resulting in a slower\\nresponse time. A good example is provided by Tesauro’s grandmaster-level backgammon\\nplayer, TD-Gammon (Section 16.1). This system used TD learning to learn an afterstate\\nvalue function through many games of self-play, using a form of heuristic search to make\\nits moves. As a model, TD-Gammon used a priori knowledge of the probabilities of dice\\nrolls and the assumption that the opponent always selected the actions that TD-Gammon\\nrated as best for it. Tesauro found that the deeper the heuristic search, the better the\\nmoves made by TD-Gammon, but the longer it took to make each move. Backgammon\\nhas a large branching factor, yet moves must be made within a few seconds. It was\\nonly feasible to search ahead selectively a few steps, but even so the search resulted in\\nsigniﬁcantly better action selections.\\nWe should not overlook the most obvious way in which heuristic search focuses updates:\\non the current state. Much of the e↵ectiveness of heuristic search is due to its search tree\\nbeing tightly focused on the states and actions that might immediately follow the current\\nstate. You may spend more of your life playing chess than checkers, but when you play\\ncheckers, it pays to think about checkers and about your particular checkers position,\\nyour likely next moves, and successor positions. No matter how you select actions, it\\nis these states and actions that are of highest priority for updates and where you most\\nurgently want your approximate value function to be accurate. Not only should your\\ncomputation be preferentially devoted to imminent events, but so should your limited\\nmemory resources. In chess, for example, there are far too many possible positions to\\nstore distinct value estimates for each of them, but chess programs based on heuristic\\nsearch can easily store distinct estimates for the millions of positions they encounter\\n2There are interesting exceptions to this (see Pearl, 1984).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 204, 'page_label': '205'}, page_content='8.10. Rollout Algorithms 183\\nlooking ahead from a single position. This great focusing of memory and computational\\nresources on the current decision is presumably the reason why heuristic search can be so\\ne↵ective.\\nThe distribution of updates can be altered in similar ways to focus on the current\\nstate and its likely successors. As a limiting case we might use exactly the methods of\\nheuristic search to construct a search tree, and then perform the individual, one-step\\nupdates from bottom up, as suggested by Figure 8.9. If the updates are ordered in this\\nway and a tabular representation is used, then exactly the same overall update would\\nbe achieved as in depth-ﬁrst heuristic search. Any state-space search can be viewed in\\nthis way as the piecing together of a large number of individual one-step updates. Thus,\\nthe performance improvement observed with deeper searches is not due to the use of\\nmultistep updates as such. Instead, it is due to the focus and concentration of updates\\non states and actions immediately downstream from the current state. By devoting a\\nlarge amount of computation speciﬁcally relevant to the candidate actions, decision-time\\nplanning can produce better decisions than can be produced by relying on unfocused\\nupdates.\\n1 2\\n3\\n4 5\\n6\\n7\\n8 9\\n10\\nFigure 8.9: Heuristic search can be implemented as a sequence of one-step updates (shown\\nhere outlined in blue) backing up values from the leaf nodes toward the root. The ordering\\nshown here is for a selective depth-ﬁrst search.\\n8.10 Rollout Algorithms\\nRollout algorithms are decision-time planning algorithms based on Monte Carlo control\\napplied to simulated trajectories that all begin at the current environment state. They\\nestimate action values for a given policy by averaging the returns of many simulated\\ntrajectories that start with each possible action and then follow the given policy. When\\nthe action-value estimates are considered to be accurate enough, the action (or one of the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 205, 'page_label': '206'}, page_content='184 Chapter 8: Planning and Learning with Tabular Methods\\nactions) having the highest estimated value is executed, after which the process is carried\\nout anew from the resulting next state. As explained by Tesauro and Galperin (1997),\\nwho experimented with rollout algorithms for playing backgammon, the term “rollout”\\ncomes from estimating the value of a backgammon position by playing out, i.e., “rolling\\nout,” the position many times to the game’s end with randomly generated sequences of\\ndice rolls, where the moves of both players are made by some ﬁxed policy.\\nUnlike the Monte Carlo control algorithms described in Chapter 5, the goal of a\\nrollout algorithm is not to estimate a complete optimal action-value function,q⇤, or a\\ncomplete action-value function,q⇡, for a given policy⇡. Instead, they produce Monte\\nCarlo estimates of action values only for each current state and for a given policy usually\\ncalled therollout policy. As decision-time planning algorithms, rollout algorithms make\\nimmediate use of these action-value estimates, then discard them. This makes rollout\\nalgorithms relatively simple to implement because there is no need to sample outcomes\\nfor every state-action pair, and there is no need to approximate a function over either\\nthe state space or the state-action space.\\nWhat then do rollout algorithms accomplish? The policy improvement theorem\\ndescribed in Section 4.2 tells us that given any two policies⇡ and ⇡0 that are identical\\nexcept that⇡0(s)= a 6= ⇡(s) for some states,i fq⇡(s, a) \\x00 v⇡(s), then policy⇡0 is as good\\nas, or better, than⇡. Moreover, if the inequality is strict, then⇡0 is in fact better than⇡.\\nThis applies to rollout algorithms wheres is the current state and⇡ is the rollout policy.\\nAveraging the returns of the simulated trajectories produces estimates ofq⇡(s, a0) for\\neach actiona0 2 A(s). Then the policy that selects an action ins that maximizes these\\nestimates and thereafter follows⇡ is a good candidate for a policy that improves over\\n⇡. The result is like one step of the policy-iteration algorithm of dynamic programming\\ndiscussed in Section 4.3 (though it is more like one step ofasynchronous value iteration\\ndescribed in Section 4.5 because it changes the action for just the current state).\\nIn other words, the aim of a rollout algorithm is to improve upon the rollout policy;\\nnot to ﬁnd an optimal policy. Experience has shown that rollout algorithms can be\\nsurprisingly e↵ective. For example, Tesauro and Galperin (1997) were surprised by the\\ndramatic improvements in backgammon playing ability produced by the rollout method.\\nIn some applications, a rollout algorithm can produce good performance even if the\\nrollout policy is completely random. But the performance of the improved policy depends\\non properties of the rollout policy and the ranking of actions produced by the Monte\\nCarlo value estimates. Intuition suggests that the better the rollout policy and the more\\naccurate the value estimates, the better the policy produced by a rollout algorithm is\\nlikely be (but see Gelly and Silver, 2007).\\nThis involves important tradeo↵s because better rollout policies typically mean that\\nmore time is needed to simulate enough trajectories to obtain good value estimates.\\nAs decision-time planning methods, rollout algorithms usually have to meet strict time\\nconstraints. The computation time needed by a rollout algorithm depends on the number\\nof actions that have to be evaluated for each decision, the number of time steps in the\\nsimulated trajectories needed to obtain useful sample returns, the time it takes the rollout\\npolicy to make decisions, and the number of simulated trajectories needed to obtain good\\nMonte Carlo action-value estimates.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 206, 'page_label': '207'}, page_content='8.11. Monte Carlo Tree Search 185\\nBalancing these factors is important in any application of rollout methods, though there\\nare several ways to ease the challenge. Because the Monte Carlo trials are independent of\\none another, it is possible to run many trials in parallel on separate processors. Another\\napproach is to truncate the simulated trajectories short of complete episodes, correcting\\nthe truncated returns by means of a stored evaluation function (which brings into play\\nall that we have said about truncated returns and updates in the preceding chapters).\\nIt is also possible, as Tesauro and Galperin (1997) suggest, to monitor the Monte Carlo\\nsimulations and prune away candidate actions that are unlikely to turn out to be the\\nbest, or whose values are close enough to that of the current best that choosing them\\ninstead would make no real di↵erence (though Tesauro and Galperin point out that this\\nwould complicate a parallel implementation).\\nWe do not ordinarily think of rollout algorithms aslearning algorithms because they\\ndo not maintain long-term memories of values or policies. However, these algorithms take\\nadvantage of some of the features of reinforcement learning that we have emphasized\\nin this book. As instances of Monte Carlo control, they estimate action values by\\naveraging the returns of a collection of sample trajectories, in this case trajectories of\\nsimulated interactions with a sample model of the environment. In this way they are\\nlike reinforcement learning algorithms in avoiding the exhaustive sweeps of dynamic\\nprogramming by trajectory sampling, and in avoiding the need for distribution models\\nby relying on sample, instead of expected, updates. Finally, rollout algorithms take\\nadvantage of the policy improvement property by acting greedily with respect to the\\nestimated action values.\\n8.11 Monte Carlo Tree Search\\nMonte Carlo Tree Search(MCTS) is a recent and strikingly successful example of decision-\\ntime planning. At its base, MCTS is a rollout algorithm as described above, but enhanced\\nby the addition of a means for accumulating value estimates obtained from the Monte\\nCarlo simulations in order to successively direct simulations toward more highly-rewarding\\ntrajectories. MCTS is largely responsible for the improvement in computer Go from\\na weak amateur level in 2005 to a grandmaster level (6 dan or more) in 2015. Many\\nvariations of the basic algorithm have been developed, including a variant that we discuss\\nin Section 16.6 that was critical for the stunning 2016 victories of the program AlphaGo\\nover an 18-time world champion Go player. MCTS has proved to be e↵ective in a wide\\nvariety of competitive settings, including general game playing (e.g., see Finnsson and\\nBj¨ ornsson, 2008; Genesereth and Thielscher, 2014), but it is not limited to games; it can\\nbe e↵ective for single-agent sequential decision problems if there is an environment model\\nsimple enough for fast multistep simulation.\\nMCTS is executed after encountering each new state to select the agent’s action for\\nthat state; it is executed again to select the action for the next state, and so on. As in a\\nrollout algorithm, each execution is an iterative process that simulates many trajectories\\nstarting from the current state and running to a terminal state (or until discounting\\nmakes any further reward negligible as a contribution to the return). The core idea\\nof MCTS is to successively focus multiple simulations starting at the current state by'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 207, 'page_label': '208'}, page_content='186 Chapter 8: Planning and Learning with Tabular Methods\\nextending the initial portions of trajectories that have received high evaluations from\\nearlier simulations. MCTS does not have to retain approximate value functions or policies\\nfrom one action selection to the next, though in many implementations it retains selected\\naction values likely to be useful for its next execution.\\nFor the most part, the actions in the simulated trajectories are generated using a simple\\npolicy, usually called a rollout policy as it is for simpler rollout algorithms. When both\\nthe rollout policy and the model do not require a lot of computation, many simulated\\ntrajectories can be generated in a short period of time. As in any tabular Monte Carlo\\nmethod, the value of a state–action pair is estimated as the average of the (simulated)\\nreturns from that pair. Monte Carlo value estimates are maintained only for the subset\\nof state–action pairs that are most likely to be reached in a few steps, which form a tree\\nrooted at the current state, as illustrated in Figure 8.10. MCTS incrementally extends\\nthe tree by adding nodes representing states that look promising based on the results\\nof the simulated trajectories. Any simulated trajectory will pass through the tree and\\nthen exit it at some leaf node. Outside the tree and at the leaf nodes the rollout policy is\\nused for action selections, but at the states inside the tree something better is possible.\\nFor these states we have value estimates for at least some of the actions, so we can pick\\namong them using an informed policy, called thetree policy, that balances exploration\\nSelection SimulationExpansion BackupRepeat while time remains \\nTree Policy RolloutPolicy\\nFigure 8.10: Monte Carlo Tree Search. When the environment changes to a new state, MCTS\\nexecutes as many iterations as possible before an action needs to be selected, incrementally\\nbuilding a tree whose root node represents the current state. Each iteration consists of the four\\noperations Selection, Expansion (though possibly skipped on some iterations),Simulation,\\nand Backup, as explained in the text and illustrated by the bold arrows in the trees. Adapted\\nfrom Chaslot, Bakkes, Szita, and Spronck (2008).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 208, 'page_label': '209'}, page_content='8.11. Monte Carlo Tree Search 187\\nand exploitation. For example, the tree policy could select actions using an\"-greedy or\\nUCB selection rule (Chapter 2).\\nIn more detail, each iteration of a basic version of MCTS consists of the following four\\nsteps as illustrated in Figure 8.10:\\n1. Selection. Starting at the root node, atree policy based on the action values\\nattached to the edges of the tree traverses the tree to select a leaf node.\\n2. Expansion. On some iterations (depending on details of the application), the tree\\nis expanded from the selected leaf node by adding one or more child nodes reached\\nfrom the selected node via unexplored actions.\\n3. Simulation. From the selected node, or from one of its newly-added child nodes\\n(if any), simulation of a complete episode is run with actions selected by the rollout\\npolicy. The result is a Monte Carlo trial with actions selected ﬁrst by the tree\\npolicy and beyond the tree by the rollout policy.\\n4. Backup. The return generated by the simulated episode is backed up to update,\\nor to initialize, the action values attached to the edges of the tree traversed by\\nthe tree policy in this iteration of MCTS. No values are saved for the states and\\nactions visited by the rollout policy beyond the tree. Figure 8.10 illustrates this by\\nshowing a backup from the terminal state of the simulated trajectory directly to the\\nstate–action node in the tree where the rollout policy began (though in general, the\\nentire return over the simulated trajectory is backed up to this state–action node).\\nMCTS continues executing these four steps, starting each time at the tree’s root node,\\nuntil no more time is left, or some other computational resource is exhausted. Then,\\nﬁnally, an action from the root node (which still represents the current state of the\\nenvironment) is selected according to some mechanism that depends on the accumulated\\nstatistics in the tree; for example, it may be an action having the largest action value\\nof all the actions available from the root state, or perhaps the action with the largest\\nvisit count to avoid selecting outliers. This is the action MCTS actually selects. After\\nthe environment transitions to a new state, MCTS is run again, sometimes starting\\nwith a tree of a single root node representing the new state, but often starting with a\\ntree containing any descendants of this node left over from the tree constructed by the\\nprevious execution of MCTS; all the remaining nodes are discarded, along with the action\\nvalues associated with them.\\nMCTS was ﬁrst proposed to select moves in programs playing two-person competitive\\ngames, such as Go. For game playing, each simulated episode is one complete play of the\\ngame in which both players select actions by the tree and rollout policies. Section 16.6\\ndescribes an extension of MCTS used in the AlphaGo program that combines the Monte\\nCarlo evaluations of MCTS with action values learned by a deep artiﬁcial neural network\\nvia self-play reinforcement learning.\\nRelating MCTS to the reinforcement learning principles we describe in this book\\nprovides some insight into how it achieves such impressive results. At its base, MCTS is\\na decision-time planning algorithm based on Monte Carlo control applied to simulations'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 209, 'page_label': '210'}, page_content='188 Chapter 8: Planning and Learning with Tabular Methods\\nthat start from the root state; that is, it is a kind of rollout algorithm as described in\\nthe previous section. It therefore beneﬁts from online, incremental, sample-based value\\nestimation and policy improvement. Beyond this, it saves action-value estimates attached\\nto the tree edges and updates them using reinforcement learning’s sample updates. This\\nhas the e↵ect of focusing the Monte Carlo trials on trajectories whose initial segments\\nare common to high-return trajectories previously simulated. Further, by incrementally\\nexpanding the tree, MCTS e↵ectively grows a lookup table to store a partial action-value\\nfunction, with memory allocated to the estimated values of state–action pairs visited in\\nthe initial segments of high-yielding sample trajectories. MCTS thus avoids the problem\\nof globally approximating an action-value function while it retains the beneﬁt of using\\npast experience to guide exploration.\\nThe striking success of decision-time planning by MCTS has deeply inﬂuenced artiﬁcial\\nintelligence, and many researchers are studying modiﬁcations and extensions of the basic\\nprocedure for use in both games and single-agent applications.\\n8.12 Summary of the Chapter\\nPlanning requires a model of the environment. Adistribution model consists of the\\nprobabilities of next states and rewards for possible actions; a sample model produces\\nsingle transitions and rewards generated according to these probabilities. Dynamic\\nprogramming requires a distribution model because it usesexpected updates, which involve\\ncomputing expectations over all the possible next states and rewards. Asample model,\\non the other hand, is what is needed to simulate interacting with the environment during\\nwhich sample updates, like those used by many reinforcement learning algorithms, can be\\nused. Sample models are generally much easier to obtain than distribution models.\\nWe have presented a perspective emphasizing the surprisingly close relationships be-\\ntween planning optimal behavior and learning optimal behavior. Both involve estimating\\nthe same value functions, and in both cases it is natural to update the estimates incre-\\nmentally, in a long series of small backing-up operations. This makes it straightforward\\nto integrate learning and planning processes simply by allowing both to update the same\\nestimated value function. In addition, any of the learning methods can be converted into\\nplanning methods simply by applying them to simulated (model-generated) experience\\nrather than to real experience. In this case learning and planning become even more\\nsimilar; they are possibly identical algorithms operating on two di↵erent sources of\\nexperience.\\nIt is straightforward to integrate incremental planning methods with acting and model-\\nlearning. Planning, acting, and model-learning interact in a circular fashion (as in\\nthe diagram on page 162), each producing what the other needs to improve; no other\\ninteraction among them is either required or prohibited. The most natural approach\\nis for all processes to proceed asynchronously and in parallel. If the processes must\\nshare computational resources, then the division can be handled almost arbitrarily—by\\nwhatever organization is most convenient and e\\x00cient for the task at hand.\\nIn this chapter we have touched upon a number of dimensions of variation among\\nstate-space planning methods. One dimension is the variation in the size of updates. The'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 210, 'page_label': '211'}, page_content='8.13. Summary of Part I: Dimensions 189\\nsmaller the updates, the more incremental the planning methods can be. Among the\\nsmallest updates are one-step sample updates, as in Dyna. Another important dimension\\nis the distribution of updates, that is, of the focus of search. Prioritized sweeping focuses\\nbackward on the predecessors of states whose values have recently changed. On-policy\\ntrajectory sampling focuses on states or state–action pairs that the agent is likely to\\nencounter when controlling its environment. This can allow computation to skip over\\nparts of the state space that are irrelevant to the prediction or control problem. Real-\\ntime dynamic programming, an on-policy trajectory sampling version of value iteration,\\nillustrates some of the advantages this strategy has over conventional sweep-based policy\\niteration.\\nPlanning can also focus forward from pertinent states, such as states actually encoun-\\ntered during an agent-environment interaction. The most important form of this is when\\nplanning is done at decision time, that is, as part of the action-selection process. Classical\\nheuristic search as studied in artiﬁcial intelligence is an example of this. Other examples\\nare rollout algorithms and Monte Carlo Tree Search that beneﬁt from online, incremental,\\nsample-based value estimation and policy improvement.\\n8.13 Summary of Part I: Dimensions\\nThis chapter concludes Part I of this book. In it we have tried to present reinforcement\\nlearning not as a collection of individual methods, but as a coherent set of ideas cutting\\nacross methods. Each idea can be viewed as a dimension along which methods vary. The\\nset of such dimensions spans a large space of possible methods. By exploring this space\\nat the level of dimensions we hope to obtain the broadest and most lasting understanding.\\nIn this section we use the concept of dimensions in method space to recapitulate the view\\nof reinforcement learning developed so far in this book.\\nAll of the methods we have explored so far in this book have three key ideas in common:\\nﬁrst, they all seek to estimate value functions; second, they all operate by backing up\\nvalues along actual or possible state trajectories; and third, they all follow the general\\nstrategy of generalized policy iteration (GPI), meaning that they maintain an approximate\\nvalue function and an approximate policy, and they continually try to improve each on the\\nbasis of the other. These three ideas are central to the subjects covered in this book. We\\nsuggest that value functions, backing up value updates, and GPI are powerful organizing\\nprinciples potentially relevant to any model of intelligence, whether artiﬁcial or natural.\\nTwo of the most important dimensions along which the methods vary are shown in\\nFigure 8.11. These dimensions have to do with the kind of update used to improve the\\nvalue function. The horizontal dimension is whether they are sample updates (based on a\\nsample trajectory) or expected updates (based on a distribution of possible trajectories).\\nExpected updates require a distribution model, whereas sample updates need only a\\nsample model, or can be done from actual experience with no model at all (another\\ndimension of variation). The vertical dimension of Figure 8.11 corresponds to the depth\\nof updates, that is, to the degree of bootstrapping. At three of the four corners of the\\nspace are the three primary methods for estimating values: dynamic programming, TD,\\nand Monte Carlo. Along the left edge of the space are the sample-update methods,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 211, 'page_label': '212'}, page_content='190 Chapter 8: Planning and Learning with Tabular Methods\\nwidth\\nof update\\ndepth\\n(length)\\nof update\\nTemporal-\\ndifference\\nlearning\\nDynamic\\nprogramming\\nMonte\\nCarlo\\n...\\nExhaustive\\nsearch\\nFigure 8.11: A slice through the space of reinforcement learning methods, highlighting the\\ntwo of the most important dimensions explored in Part I of this book: the depth and width of\\nthe updates.\\nranging from one-step TD updates to full-return Monte Carlo updates. Between these\\nis a spectrum including methods based onn-step updates (and in Chapter 12 we will\\nextend this to mixtures ofn-step updates such as the\\x00-updates implemented by eligibility\\ntraces).\\nDynamic programming methods are shown in the extreme upper-right corner of the\\nspace because they involve one-step expected updates. The lower-right corner is the\\nextreme case of expected updates so deep that they run all the way to terminal states\\n(or, in a continuing task, until discounting has reduced the contribution of any further\\nrewards to a negligible level). This is the case of exhaustive search. Intermediate methods\\nalong this dimension include heuristic search and related methods that search and update\\nup to a limited depth, perhaps selectively. There are also methods that are intermediate\\nalong the horizontal dimension. These include methods that mix expected and sample\\nupdates, as well as the possibility of methods that mix samples and distributions within\\na single update. The interior of the square is ﬁlled in to represent the space of all such\\nintermediate methods.\\nA third dimension that we have emphasized in this book is the binary distinction\\nbetween on-policy and o↵-policy methods. In the former case, the agent learns the value\\nfunction for the policy it is currently following, whereas in the latter case it learns the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 212, 'page_label': '213'}, page_content='8.13. Summary of Part I: Dimensions 191\\nvalue function for the policy for a di↵erent policy, often the one that the agent currently\\nthinks is best. The policy generating behavior is typically di↵erent from what is currently\\nthought best because of the need to explore. This third dimension might be visualized as\\nperpendicular to the plane of the page in Figure 8.11.\\nIn addition to the three dimensions just discussed, we have identiﬁed a number of\\nothers throughout the book:\\nDeﬁnition of returnIs the task episodic or continuing, discounted or undiscounted?\\nAction values vs. state values vs. afterstate valuesWhat kind of values should\\nbe estimated? If only state values are estimated, then either a model or a separate\\npolicy (as in actor–critic methods) is required for action selection.\\nAction selection/exploration How are actions selected to ensure a suitable trade-o↵\\nbetween exploration and exploitation? We have considered only the simplest ways to\\ndo this: \"-greedy, optimistic initialization of values, soft-max, and upper conﬁdence\\nbound.\\nSynchronous vs. asynchronousAre the updates for all states performed simultane-\\nously or one by one in some order?\\nReal vs. simulatedShould one update based on real experience or simulated experi-\\nence? If both, how much of each?\\nLocation of updatesWhat states or state–action pairs should be updated? Model-\\nfree methods can choose only among the states and state–action pairs actually\\nencountered, but model-based methods can choose arbitrarily. There are many\\npossibilities here.\\nTiming of updatesShould updates be done as part of selecting actions, or only after-\\nward?\\nMemory for updatesHow long should updated values be retained? Should they be\\nretained permanently, or only while computing an action selection, as in heuristic\\nsearch?\\nOf course, these dimensions are neither exhaustive nor mutually exclusive. Individual\\nalgorithms di↵er in many other ways as well, and many algorithms lie in several places\\nalong several dimensions. For example, Dyna methods use both real and simulated\\nexperience to a↵ect the same value function. It is also perfectly sensible to maintain\\nmultiple value functions computed in di↵erent ways or over di↵erent state and action\\nrepresentations. These dimensions do, however, constitute a coherent set of ideas for\\ndescribing and exploring a wide space of possible methods.\\nThe most important dimension not mentioned here, and not covered in Part I of\\nthis book, is that of function approximation. Function approximation can be viewed as\\nan orthogonal spectrum of possibilities ranging from tabular methods at one extreme\\nthrough state aggregation, a variety of linear methods, and then a diverse set of nonlinear\\nmethods. This dimension is explored in Part II.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 213, 'page_label': '214'}, page_content='192 Chapter 8: Planning and Learning with Tabular Methods\\nBibliographical and Historical Remarks\\n8.1 The overall view of planning and learning presented here has developed gradually\\nover a number of years, in part by the authors (Sutton, 1990, 1991a, 1991b;\\nBarto, Bradtke, and Singh, 1991, 1995; Sutton and Pinette, 1985; Sutton and\\nBarto, 1981b); it has been strongly inﬂuenced by Agre and Chapman (1990; Agre\\n1988), Bertsekas and Tsitsiklis (1989), Singh (1993), and others. The authors\\nwere also strongly inﬂuenced by psychological studies of latent learning (Tolman,\\n1932) and by psychological views of the nature of thought (e.g., Galanter and\\nGerstenhaber, 1956; Craik, 1943; Campbell, 1960; Dennett, 1978). In Part\\nIII of the book, Section 14.6 relates model-based and model-free methods to\\npsychological theories of learning and behavior, and Section 15.11 discusses ideas\\nabout how the brain might implement these types of methods.\\n8.2 The terms direct and indirect, which we use to describe di↵erent kinds of\\nreinforcement learning, are from the adaptive control literature (e.g., Goodwin\\nand Sin, 1984), where they are used to make the same kind of distinction. The\\nterm system identiﬁcation is used in adaptive control for what we callmodel-\\nlearning (e.g., Goodwin and Sin, 1984; Ljung and S¨ oderstrom, 1983; Young,\\n1984). The Dyna architecture is due to Sutton (1990), and the results in this\\nand the next section are based on results reported there. Barto and Singh\\n(1990) consider some of the issues in comparing direct and indirect reinforcement\\nlearning methods. Early work extending Dyna to linear function approximation\\nwas done by Sutton, Szepesv´ ari, Geramifard, and Bowling (2008) and by Parr,\\nLi, Taylor, Painter-Wakeﬁeld, and Littman (2008).\\n8.3 There have been several works with model-based reinforcement learning that take\\nthe idea of exploration bonuses and optimistic initialization to its logical extreme,\\nin which all incompletely explored choices are assumed maximally rewarding\\nand optimal paths are computed to test them. The E3 algorithm of Kearns and\\nSingh (2002) and the R-max algorithm of Brafman and Tennenholtz (2003) are\\nguaranteed to ﬁnd a near-optimal solution in time polynomial in the number\\nof states and actions. This is usually too slow for practical algorithms but is\\nprobably the best that can be done in the worst case.\\n8.4 Prioritized sweeping was developed simultaneously and independently by Moore\\nand Atkeson (1993) and Peng and Williams (1993). The results in the box on\\npage 170 are due to Peng and Williams (1993). The results in the box on page 171\\nare due to Moore and Atkeson. Key subsequent work in this area includes that\\nby McMahan and Gordon (2005) and by van Seijen and Sutton (2013).\\n8.5 This section was strongly inﬂuenced by the experiments of Singh (1993).\\n8.6–7 Trajectory sampling has implicitly been a part of reinforcement learning from\\nthe outset, but it was most explicitly emphasized by Barto, Bradtke, and Singh\\n(1995) in their introduction of RTDP. They recognized that Korf’s (1990)learning'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 214, 'page_label': '215'}, page_content='8.13. Summary of Part I: Dimensions 193\\nreal-time A* (LRTA*) algorithm is an asynchronous DP algorithm that applies\\nto stochastic problems as well as the deterministic problems on which Korf\\nfocused. Beyond LRTA*, RTDP includes the option of updating the values of\\nmany states in the time intervals between the execution of actions. Barto et\\nal. (1995) proved the convergence result described here by combining Korf’s (1990)\\nconvergence proof for LRTA* with the result of Bertsekas (1982) (also Bertsekas\\nand Tsitsiklis, 1989) ensuring convergence of asynchronous DP for stochastic\\nshortest path problems in the undiscounted case. Combining model-learning\\nwith RTDP is calledAdaptive RTDP, also presented by Barto et al. (1995) and\\ndiscussed by Barto (2011).\\n8.9 For further reading on heuristic search, the reader is encouraged to consult texts\\nand surveys such as those by Russell and Norvig (2009) and Korf (1988). Peng\\nand Williams (1993) explored a forward focusing of updates much as is suggested\\nin this section.\\n8.10 Abramson’s (1990) expected-outcome model is a rollout algorithm applied to two-\\nperson games in which the play of both simulated players is random. He argued\\nthat even with random play, it is a “powerful heuristic” that is “precise, accurate,\\neasily estimable, e\\x00ciently calculable, and domain-independent.” Tesauro and\\nGalperin (1997) demonstrated the e↵ectiveness of rollout algorithms for improving\\nthe play of backgammon programs, adopting the term “rollout” from its use\\nin evaluating backgammon positions by playing out positions with di↵erent\\nrandomly generating sequences of dice rolls. Bertsekas, Tsitsiklis, and Wu (1997)\\nexamine rollout algorithms applied to combinatorial optimization problems, and\\nBertsekas (2013) surveys their use in discrete deterministic optimization problems,\\nremarking that they are “often surprisingly e↵ective.”\\n8.11 The central ideas of MCTS were introduced by Coulom (2006) and by Kocsis\\nand Szepesv´ ari (2006). They built upon previous research with Monte Carlo\\nplanning algorithms as reviewed by these authors. Browne, Powley, Whitehouse,\\nLucas, Cowling, Rohlfshagen, Tavener, Perez, Samothrakis, and Colton (2012)\\nis an excellent survey of MCTS methods and their applications. David Silver\\ncontributed to the ideas and presentation in this section.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 215, 'page_label': '216'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 216, 'page_label': '217'}, page_content='Part II:\\nApproximate Solution Methods\\nIn the second part of the book we extend the tabular methods presented in the ﬁrst part\\nto apply to problems with arbitrarily large state spaces. In many of the tasks to which we\\nwould like to apply reinforcement learning the state space is combinatorial and enormous;\\nthe number of possible camera images, for example, is much larger than the number of\\natoms in the universe. In such cases we cannot expect to ﬁnd an optimal policy or the\\noptimal value function even in the limit of inﬁnite time and data; our goal instead is to\\nﬁnd a good approximate solution using limited computational resources. In this part of\\nthe book we explore such approximate solution methods.\\nThe problem with large state spaces is not just the memory needed for large tables,\\nbut the time and data needed to ﬁll them accurately. In many of our target tasks, almost\\nevery state encountered will never have been seen before. To make sensible decisions in\\nsuch states it is necessary to generalize from previous encounters with di↵erent states\\nthat are in some sense similar to the current one. In other words, the key issue is that of\\ngeneralization. How can experience with a limited subset of the state space be usefully\\ngeneralized to produce a good approximation over a much larger subset?\\nFortunately, generalization from examples has already been extensively studied, and\\nwe do not need to invent totally new methods for use in reinforcement learning. To some\\nextent we need only combine reinforcement learning methods with existing generalization\\nmethods. The kind of generalization we require is often calledfunction approximation\\nbecause it takes examples from a desired function (e.g., a value function) and attempts\\nto generalize from them to construct an approximation of the entire function. Function\\napproximation is an instance ofsupervised learning, the primary topic studied in machine\\nlearning, artiﬁcial neural networks, pattern recognition, and statistical curve ﬁtting. In\\ntheory, any of the methods studied in these ﬁelds can be used in the role of function\\napproximator within reinforcement learning algorithms, although in practice some ﬁt\\nmore easily into this role than others.\\nReinforcement learning with function approximation involves a number of new issues\\nthat do not normally arise in conventional supervised learning, such as nonstationarity,\\nbootstrapping, and delayed targets. We introduce these and other issues successively over\\nthe ﬁve chapters of this part. Initially we restrict attention to on-policy training, treating\\nin Chapter 9 the prediction case, in which the policy is given and only its value function\\nis approximated, and then in Chapter 10 the control case, in which an approximation to\\nthe optimal policy is found. The challenging problem of o↵-policy learning with function\\napproximation is treated in Chapter 11. In each of these three chapters we will have'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 217, 'page_label': '218'}, page_content='196 Part II: Approximate Solution Methods\\nto return to ﬁrst principles and re-examine the objectives of the learning to take into\\naccount function approximation. Chapter 12 introduces and analyzes the algorithmic\\nmechanism ofeligibility traces, which dramatically improves the computational properties\\nof multi-step reinforcement learning methods in many cases. The ﬁnal chapter of this\\npart explores a di↵erent approach to control,policy-gradient methods, which approximate\\nthe optimal policy directly and need never form an approximate value function (although\\nthey may be much more e\\x00cient if they do approximate a value function as well the\\npolicy).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 218, 'page_label': '219'}, page_content='Chapter 9\\nOn-policy Prediction with\\nApproximation\\nIn this chapter, we begin our study of function approximation in reinforcement learning\\nby considering its use in estimating the state-value function from on-policy data, that is,\\nin approximatingv⇡ from experience generated using a known policy⇡.T h en o v e l t yi n\\nthis chapter is that the approximate value function is represented not as a table but as a\\nparameterized functional form with weight vectorw 2 Rd.W ew i l lw r i t eˆv(s,w) ⇡ v⇡(s)\\nfor the approximate value of states given weight vectorw. For example, ˆv might be\\na linear function in features of the state, withw the vector of feature weights. More\\ngenerally, ˆv might be the function computed by a multi-layer artiﬁcial neural network,\\nwith w the vector of connection weights in all the layers. By adjusting the weights, any\\nof a wide range of di↵erent functions can be implemented by the network. Orˆv might be\\nthe function computed by a decision tree, wherew is all the numbers deﬁning the split\\npoints and leaf values of the tree. Typically, the number of weights (the dimensionality of\\nw) is much less than the number of states (d ⌧| S|), and changing one weight changes the\\nestimated value of many states. Consequently, when a single state is updated, the change\\ngeneralizes from that state to a↵ect the values of many other states. Suchgeneralization\\nmakes the learning potentially more powerful but also potentially more di\\x00cult to manage\\nand understand.\\nPerhaps surprisingly, extending reinforcement learning to function approximation also\\nmakes it applicable to partially observable problems, in which the full state is not available\\nto the agent. If the parameterized function form forˆv does not allow the estimated\\nvalue to depend on certain aspects of the state, then it is just as if those aspects are\\nunobservable. In fact, all the theoretical results for methods using function approximation\\npresented in this part of the book apply equally well to cases of partial observability.\\nWhat function approximation can’t do, however, is augment the state representation\\nwith memories of past observations. Some such possible further extensions are discussed\\nbrieﬂy in Section 17.3.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 219, 'page_label': '220'}, page_content='198 Chapter 9: On-policy Prediction with Approximation\\n9.1 Value-function Approximation\\nAll of the prediction methods covered in this book have been described as updates to an\\nestimated value function that shift its value at particular states toward a “backed-up value,”\\nor update target, for that state. Let us refer to an individual update by the notations 7! u,\\nwhere s is the state updated andu is the update target thats’s estimated value is shifted\\ntoward. For example, the Monte Carlo update for value prediction isSt 7! Gt, the TD(0)\\nupdate isSt 7! Rt+1 +\\x00ˆv(St+1,wt), and then-step TD update isSt 7! Gt:t+n.I n t h e D P\\n(dynamic programming) policy-evaluation update,s 7! E⇡[Rt+1 + \\x00ˆv(St+1,wt) | St =s],\\nan arbitrary states is updated, whereas in the other cases the state encountered in actual\\nexperience, St, is updated.\\nIt is natural to interpret each update as specifying an example of the desired input–\\noutput behavior of the value function. In a sense, the updates 7! u means that the\\nestimated value for states should be more like the update targetu. Up to now, the\\nactual update has been trivial: the table entry fors’s estimated value has simply been\\nshifted a fraction of the way towardu, and the estimated values of all other states\\nwere left unchanged. Now we permit arbitrarily complex and sophisticated methods to\\nimplement the update, and updating ats generalizes so that the estimated values of\\nmany other states are changed as well. Machine learning methods that learn to mimic\\ninput–output examples in this way are calledsupervised learningmethods, and when the\\noutputs are numbers, likeu, the process is often calledfunction approximation. Function\\napproximation methods expect to receive examples of the desired input–output behavior\\nof the function they are trying to approximate. We use these methods for value prediction\\nsimply by passing to them thes 7! u of each update as a training example. We then\\ninterpret the approximate function they produce as an estimated value function.\\nViewing each update as a conventional training example in this way enables us to use\\nany of a wide range of existing function approximation methods for value prediction. In\\nprinciple, we can use any method for supervised learning from examples, including artiﬁcial\\nneural networks, decision trees, and various kinds of multivariate regression. However,\\nnot all function approximation methods are equally well suited for use in reinforcement\\nlearning. The most sophisticated artiﬁcial neural network and statistical methods all\\nassume a static training set over which multiple passes are made. In reinforcement\\nlearning, however, it is important that learning be able to occur online, while the agent\\ninteracts with its environment or with a model of its environment. To do this requires\\nmethods that are able to learn e\\x00ciently from incrementally acquired data. In addition,\\nreinforcement learning generally requires function approximation methods able to handle\\nnonstationary target functions (target functions that change over time). For example,\\nin control methods based on GPI (generalized policy iteration) we often seek to learn\\nq⇡ while ⇡ changes. Even if the policy remains the same, the target values of training\\nexamples are nonstationary if they are generated by bootstrapping methods (DP and TD\\nlearning). Methods that cannot easily handle such nonstationarity are less suitable for\\nreinforcement learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 220, 'page_label': '221'}, page_content='9.2. The Prediction Objective (VE) 199\\n9.2 The Prediction Objective (VE)\\nUp to now we have not speciﬁed an explicit objective for prediction. In the tabular case\\na continuous measure of prediction quality was not necessary because the learned value\\nfunction could come to equal the true value function exactly. Moreover, the learned\\nvalues at each state were decoupled—an update at one state a↵ected no other. But with\\ngenuine approximation, an update at one state a↵ects many others, and it is not possible\\nto get the values of all states exactly correct. By assumption we have far more states\\nthan weights, so making one state’s estimate more accurate invariably means making\\nothers’ less accurate. We are obligated then to say which states we care most about. We\\nmust specify a state distributionµ(s) \\x00 0, P\\ns µ(s) = 1, representing how much we care\\nabout the error in each states. By the error in a states we mean the square of the\\ndi↵erence between the approximate valueˆv(s,w) and the true valuev⇡(s). Weighting\\nthis over the state space byµ, we obtain a natural objective function, themean square\\nvalue error, denotedVE:\\nVE(w) .=\\nX\\ns2S\\nµ(s)\\nh\\nv⇡(s) \\x00 ˆv(s,w)\\ni2\\n. (9.1)\\nThe square root of this measure, the rootVE, gives a rough measure of how much the\\napproximate values di↵er from the true values and is often used in plots. Oftenµ(s)i s\\nchosen to be the fraction of time spent ins. Under on-policy training this is called the\\non-policy distribution; we focus entirely on this case in this chapter. In continuing tasks,\\nthe on-policy distribution is the stationary distribution under⇡.\\nThe on-policy distribution in episodic tasks\\nIn an episodic task, the on-policy distribution is a little di↵erent in that it depends\\non how the initial states of episodes are chosen. Leth(s) denote the probability\\nthat an episode begins in each states, and let⌘(s) denote the number of time\\nsteps spent, on average, in states in a single episode. Time is spent in a states\\nif episodes start ins, or if transitions are made intos from a preceding state¯s in\\nwhich time is spent:\\n⌘(s)= h(s)+\\nX\\n¯s\\n⌘(¯s)\\nX\\na\\n⇡(a|¯s)p(s| ¯s, a), for alls 2 S. (9.2)\\nThis system of equations can be solved for the expected number of visits⌘(s). The\\non-policy distribution is then the fraction of time spent in each state normalized to\\nsum to one:\\nµ(s)= ⌘(s)P\\ns0 ⌘(s0), for alls 2 S. (9.3)\\nThis is the natural choice without discounting. If there is discounting (\\x00< 1) it\\nshould be treated as a form of termination, which can be done simply by including\\na factor of\\x00 in the second term of (9.2).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 221, 'page_label': '222'}, page_content='200 Chapter 9: On-policy Prediction with Approximation\\nThe two cases, continuing and episodic, behave similarly, but with approximation they\\nmust be treated separately in formal analyses, as we will see repeatedly in this part of\\nthe book. This completes the speciﬁcation of the learning objective.\\nIt is not completely clear that theVE is the right performance objective for rein-\\nforcement learning. Remember that our ultimate purpose—the reason we are learning\\na value function—is to ﬁnd a better policy. The best value function for this purpose is\\nnot necessarily the best for minimizingVE. Nevertheless, it is not yet clear what a more\\nuseful alternative goal for value prediction might be. For now, we will focus onVE.\\nAn ideal goal in terms ofVE would be to ﬁnd aglobal optimum, a weight vectorw⇤\\nfor whichVE(w⇤) \\uf8ff VE(w) for all possiblew. Reaching this goal is sometimes possible\\nfor simple function approximators such as linear ones, but is rarely possible for complex\\nfunction approximators such as artiﬁcial neural networks and decision trees. Short of\\nthis, complex function approximators may seek to converge instead to alocal optimum,\\na weight vectorw⇤ for whichVE(w⇤) \\uf8ff VE(w) for allw in some neighborhood ofw⇤.\\nAlthough this guarantee is only slightly reassuring, it is typically the best that can be\\nsaid for nonlinear function approximators, and often it is enough. Still, for many cases of\\ninterest in reinforcement learning there is no guarantee of convergence to an optimum, or\\neven to within a bounded distance of an optimum. Some methods may in fact diverge,\\nwith theirVE approaching inﬁnity in the limit.\\nIn the last two sections we outlined a framework for combining a wide range of\\nreinforcement learning methods for value prediction with a wide range of function\\napproximation methods, using the updates of the former to generate training examples\\nfor the latter. We also described aVE performance measure which these methods may\\naspire to minimize. The range of possible function approximation methods is far too\\nlarge to cover all, and anyway too little is known about most of them to make a reliable\\nevaluation or recommendation. Of necessity, we consider only a few possibilities. In\\nthe rest of this chapter we focus on function approximation methods based on gradient\\nprinciples, and on linear gradient-descent methods in particular. We focus on these\\nmethods in part because we consider them to be particularly promising and because they\\nreveal key theoretical issues, but also because they are simple and our space is limited.\\n9.3 Stochastic-gradient and Semi-gradient Methods\\nWe now develop in detail one class of learning methods for function approximation in\\nvalue prediction, those based on stochastic gradient descent (SGD). SGD methods are\\namong the most widely used of all function approximation methods and are particularly\\nwell suited to online reinforcement learning.\\nIn gradient-descent methods, the weight vector is a column vector with a ﬁxed number\\nof real valued components,w .= (w1,w 2,...,w d)>,1 and the approximate value function\\nˆv(s,w) is a di↵erentiable function ofw for alls 2 S. We will be updatingw at each of\\na series of discrete time steps,t =0 , 1, 2, 3,... , so we will need a notationwt for the\\n1The > denotes transpose, needed here to turn the horizontal row vector in the text into a vertical\\ncolumn vector; in this book vectors are generally taken to be column vectors unless explicitly written out\\nhorizontally or transposed.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 222, 'page_label': '223'}, page_content='9.3. Stochastic-gradient and Semi-gradient Methods 201\\nweight vector at each step. For now, let us assume that, on each step, we observe a new\\nexample St 7! v⇡(St) consisting of a (possibly randomly selected) stateSt and its true\\nvalue under the policy. These states might be successive states from an interaction with\\nthe environment, but for now we do not assume so. Even though we are given the exact,\\ncorrect values,v⇡(St) for eachSt, there is still a di\\x00cult problem because our function\\napproximator has limited resources and thus limited resolution. In particular, there is\\ngenerally no w that gets all the states, or even all the examples, exactly correct. In\\naddition, we must generalize to all the other states that have not appeared in examples.\\nWe assume that states appear in examples with the same distribution,µ,o v e rw h i c h\\nwe are trying to minimize theVE as given by (9.1). A good strategy in this case is\\nto try to minimize error on the observed examples.Stochastic gradient-descent(SGD)\\nmethods do this by adjusting the weight vector after each example by a small amount in\\nthe direction that would most reduce the error on that example:\\nwt+1\\n.= wt \\x00 1\\n2↵r\\nh\\nv⇡(St) \\x00 ˆv(St,wt)\\ni2\\n(9.4)\\n= wt + ↵\\nh\\nv⇡(St) \\x00 ˆv(St,wt)\\ni\\nrˆv(St,wt), (9.5)\\nwhere ↵ is a positive step-size parameter, andrf(w), for any scalar expressionf(w)\\nthat is a function of a vector (herew), denotes the column vector of partial derivatives\\nof the expression with respect to the components of the vector:\\nrf(w) .=\\n✓@f (w)\\n@w1\\n, @f (w)\\n@w2\\n,..., @f (w)\\n@wd\\n◆>\\n. (9.6)\\nThis derivative vector is thegradient of f with respect tow. SGD methods are “gradient\\ndescent” methods because the overall step inwt is proportional to the negative gradient\\nof the example’s squared error(9.4). This is the direction in which the error falls most\\nrapidly. Gradient descent methods are called “stochastic” when the update is done, as\\nhere, on only a single example, which might have been selected stochastically. Over many\\nexamples, making small steps, the overall e↵ect is to minimize an average performance\\nmeasure such as theVE.\\nIt may not be immediately apparent why SGD takes only a small step in the direction\\nof the gradient. Could we not move all the way in this direction and completely eliminate\\nthe error on the example? In many cases this could be done, but usually it is not desirable.\\nRemember that we do not seek or expect to ﬁnd a value function that has zero error for\\nall states, but only an approximation that balances the errors in di↵erent states. If we\\ncompletely corrected each example in one step, then we would not ﬁnd such a balance.\\nIn fact, the convergence results for SGD methods assume that↵ decreases over time. If\\nit decreases in such a way as to satisfy the standard stochastic approximation conditions\\n(2.7), then the SGD method (9.5) is guaranteed to converge to a local optimum.\\nWe turn now to the case in which the target output, here denotedUt 2 R, of thetth\\ntraining example, St 7! Ut, is not the true value,v⇡(St), but some, possibly random,\\napproximation to it. For example,Ut might be a noise-corrupted version ofv⇡(St), or it\\nmight be one of the bootstrapping targets usingˆv mentioned in the previous section. In'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 223, 'page_label': '224'}, page_content='202 Chapter 9: On-policy Prediction with Approximation\\nthese cases we cannot perform the exact update (9.5) becausev⇡(St)i su n k n o w n ,b u t\\nwe can approximate it by substitutingUt in place ofv⇡(St). This yields the following\\ngeneral SGD method for state-value prediction:\\nwt+1\\n.= wt + ↵\\nh\\nUt \\x00 ˆv(St,wt)\\ni\\nrˆv(St,wt). (9.7)\\nIf Ut is an unbiased estimate, that is, ifE[Ut|St =s] = v⇡(s), for each t,t h e nwt is\\nguaranteed to converge to a local optimum under the usual stochastic approximation\\nconditions (2.7) for decreasing↵.\\nFor example, suppose the states in the examples are the states generated by interaction\\n(or simulated interaction) with the environment using policy⇡. Because the true value of\\na state is the expected value of the return following it, the Monte Carlo targetUt\\n.= Gt is\\nby deﬁnition an unbiased estimate ofv⇡(St). With this choice, the general SGD method\\n(9.7) converges to a locally optimal approximation tov⇡(St). Thus, the gradient-descent\\nversion of Monte Carlo state-value prediction is guaranteed to ﬁnd a locally optimal\\nsolution. Pseudocode for a complete algorithm is shown in the box below.\\nGradient Monte Carlo Algorithm for Estimatingˆv ⇡ v⇡\\nInput: the policy⇡ to be evaluated\\nInput: a di↵erentiable function ˆv : S ⇥ Rd ! R\\nAlgorithm parameter: step size↵> 0\\nInitialize value-function weightsw 2 Rd arbitrarily (e.g.,w = 0)\\nLoop forever (for each episode):\\nGenerate an episodeS0,A 0,R 1,S 1,A 1,...,R T ,S T using ⇡\\nLoop for each step of episode,t =0 , 1,...,T \\x00 1:\\nw  w + ↵\\n⇥\\nGt \\x00 ˆv(St,w)\\n⇤\\nrˆv(St,w)\\nOne does not obtain the same guarantees if a bootstrapping estimate ofv⇡(St)i su s e d\\nas the targetUt in (9.7). Bootstrapping targets such asn-step returns Gt:t+n or the DP\\ntarget P\\na,s0,r ⇡(a|St)p(s0,r |St,a )[r + \\x00ˆv(s0,wt)] all depend on the current value of the\\nweight vectorwt, which implies that they will be biased and that they will not produce a\\ntrue gradient-descent method. One way to look at this is that the key step from(9.4)\\nto (9.5) relies on the target being independent ofwt. This step would not be valid if\\na bootstrapping estimate were used in place ofv⇡(St). Bootstrapping methods are not\\nin fact instances of true gradient descent (Barnard, 1993). They take into account the\\ne↵ect of changing the weight vectorwt on the estimate, but ignore its e↵ect on the target.\\nThey include only a part of the gradient and, accordingly, we call themsemi-gradient\\nmethods.\\nAlthough semi-gradient (bootstrapping) methods do not converge as robustly as\\ngradient methods, they do converge reliably in important cases such as the linear case\\ndiscussed in the next section. Moreover, they o↵er important advantages that make them\\noften clearly preferred. One reason for this is that they typically enable signiﬁcantly faster\\nlearning, as we have seen in Chapters 6 and 7. Another is that they enable learning to'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 224, 'page_label': '225'}, page_content='9.3. Stochastic-gradient and Semi-gradient Methods 203\\nbe continual and online, without waiting for the end of an episode. This enables them to\\nbe used on continuing problems and provides computational advantages. A prototypical\\nsemi-gradient method is semi-gradient TD(0), which usesUt\\n.= Rt+1 + \\x00ˆv(St+1,w) as its\\ntarget. Complete pseudocode for this method is given in the box below.\\nSemi-gradient TD(0) for estimatingˆv ⇡ v⇡\\nInput: the policy⇡ to be evaluated\\nInput: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,· )=0\\nAlgorithm parameter: step size↵> 0\\nInitialize value-function weightsw 2 Rd arbitrarily (e.g.,w = 0)\\nLoop for each episode:\\nInitialize S\\nLoop for each step of episode:\\nChoose A ⇠ ⇡(·| S)\\nTake actionA, observeR, S0\\nw  w + ↵\\n⇥\\nR + \\x00ˆv(S0,w) \\x00 ˆv(S,w)\\n⇤\\nrˆv(S,w)\\nS  S0\\nuntil S is terminal\\nState aggregation is a simple form of generalizing function approximation in which\\nstates are grouped together, with one estimated value (one component of the weight\\nvector w) for each group. The value of a state is estimated as its group’s component,\\nand when the state is updated, that component alone is updated. State aggregation\\nis a special case of SGD(9.7) in which the gradient,rˆv(St,wt), is 1 forSt’s group’s\\ncomponent and 0 for the other components.\\nExample 9.1: State Aggregation on the 1000-state Random WalkConsider a\\n1000-state version of the random walk task (Examples 6.2 and 7.1 on pages 125 and\\n144). The states are numbered from 1 to 1000, left to right, and all episodes begin near\\nthe center, in state 500. State transitions are from the current state to one of the 100\\nneighboring states to its left, or to one of the 100 neighboring states to its right, all with\\nequal probability. Of course, if the current state is near an edge, then there may be fewer\\nthan 100 neighbors on that side of it. In this case, all the probability that would have\\ngone into those missing neighbors goes into the probability of terminating on that side\\n(thus, state 1 has a 0.5 chance of terminating on the left, and state 950 has a 0.25 chance\\nof terminating on the right). As usual, termination on the left produces a reward of\\n\\x001, and termination on the right produces a reward of +1. All other transitions have a\\nreward of zero. We use this task as a running example throughout this section.\\nFigure 9.1 shows the true value functionv⇡ for this task. It is nearly a straight line,\\ncurving very slightly toward the horizontal for the last 100 states at each end. Also shown\\nis the ﬁnal approximate value function learned by the gradient Monte-Carlo algorithm\\nwith state aggregation after 100,000 episodes with a step size of↵ =2 ⇥ 10\\x005. For the\\nstate aggregation, the 1000 states were partitioned into 10 groups of 100 states each (i.e.,\\nstates 1–100 were one group, states 101–200 were another, and so on). The staircase e↵ect'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 225, 'page_label': '226'}, page_content='204 Chapter 9: On-policy Prediction with Approximation\\n0\\nState\\nValue\\nscale\\n    True \\nvalue v⇡\\n    Approximate \\nMC value ˆv\\n    State distribution         0.0017\\n0.0137\\nDistribution\\nscale\\n10001\\n0\\n-1\\n1\\nµ\\nFigure 9.1: Function approximation by state aggregation on the 1000-state random walk task,\\nusing the gradient Monte Carlo algorithm (page 202).\\nshown in the ﬁgure is typical of state aggregation; within each group, the approximate\\nvalue is constant, and it changes abruptly from one group to the next. These approximate\\nvalues are close to the global minimum of theVE (9.1).\\nSome of the details of the approximate values are best appreciated by reference to\\nthe state distributionµ for this task, shown in the lower portion of the ﬁgure with a\\nright-side scale. State 500, in the center, is the ﬁrst state of every episode, but is rarely\\nvisited again. On average, about 1.37% of the time steps are spent in the start state.\\nThe states reachable in one step from the start state are the second most visited, with\\nabout 0.17% of the time steps being spent in each of them. From thereµ falls o↵ almost\\nlinearly, reaching about 0.0147% at the extreme states 1 and 1000. The most visible\\ne↵ect of the distribution is on the leftmost groups, whose values are clearly shifted higher\\nthan the unweighted average of the true values of states within the group, and on the\\nrightmost groups, whose values are clearly shifted lower. This is due to the states in\\nthese areas having the greatest asymmetry in their weightings byµ. For example, in the\\nleftmost group, state 100 is weighted more than 3 times more strongly than state 1. Thus\\nthe estimate for the group is biased toward the true value of state 100, which is higher\\nthan the true value of state 1.\\n9.4 Linear Methods\\nOne of the most important special cases of function approximation is that in which the\\napproximate function, ˆv(· ,w), is a linear function of the weight vector,w. Corresponding\\nto every states, there is a real-valued vectorx(s) .= (x1(s),x 2(s),...,x d(s))>,w i t ht h e\\nsame number of components asw. Linear methods approximate the state-value function'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 226, 'page_label': '227'}, page_content='9.4. Linear Methods 205\\nby the inner product betweenw and x(s):\\nˆv(s,w) .= w>x(s) .=\\ndX\\ni=1\\nwixi(s). (9.8)\\nIn this case the approximate value function is said to belinear in the weights, or simply\\nlinear.\\nThe vectorx(s) is called afeature vectorrepresenting states. Each componentxi(s)\\nof x(s) is the value of a functionxi : S ! R. We think of afeature as the entirety of one\\nof these functions, and we call its value for a states a feature ofs. For linear methods,\\nfeatures arebasis functions because they form a linear basis for the set of approximate\\nfunctions. Constructing d-dimensional feature vectors to represent states is the same as\\nselecting a set ofd basis functions. Features may be deﬁned in many di↵erent ways; we\\ncover a few possibilities in the next sections.\\nIt is natural to use SGD updates with linear function approximation. The gradient of\\nthe approximate value function with respect tow in this case is\\nrˆv(s,w)= x(s).\\nThus, in the linear case the general SGD update (9.7) reduces to a particularly simple\\nform:\\nwt+1\\n.= wt + ↵\\nh\\nUt \\x00 ˆv(St,wt)\\ni\\nx(St).\\nBecause it is so simple, the linear SGD case is one of the most favorable for mathematical\\nanalysis. Almost all useful convergence results for learning systems of all kinds are for\\nlinear (or simpler) function approximation methods.\\nIn particular, in the linear case there is only one optimum (or, in degenerate cases,\\none set of equally good optima), and thus any method that is guaranteed to converge to\\nor near a local optimum is automatically guaranteed to converge to or near the global\\noptimum. For example, the gradient Monte Carlo algorithm presented in the previous\\nsection converges to the global optimum of theVE under linear function approximation\\nif ↵ is reduced over time according to the usual conditions.\\nThe semi-gradient TD(0) algorithm presented in the previous section also converges\\nunder linear function approximation, but this does not follow from general results on\\nSGD; a separate theorem is necessary. The weight vector converged to is also not the\\nglobal optimum, but rather a point near the local optimum. It is useful to consider this\\nimportant case in more detail, speciﬁcally for the continuing case. The update at each\\ntime t is\\nwt+1\\n.= wt + ↵\\n⇣\\nRt+1 + \\x00w>\\nt xt+1 \\x00 w>\\nt xt\\n⌘\\nxt (9.9)\\n= wt + ↵\\n⇣\\nRt+1xt \\x00 xt\\n\\x00\\nxt \\x00 \\x00xt+1\\n\\x00>\\nwt\\n⌘\\n,\\nwhere here we have used the notational shorthandxt = x(St). Once the system has\\nreached steady state, for any givenwt, the expected next weight vector can be written\\nE[wt+1|wt]= wt + ↵(b \\x00 Awt), (9.10)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 227, 'page_label': '228'}, page_content='206 Chapter 9: On-policy Prediction with Approximation\\nwhere\\nb .= E[Rt+1xt] 2 Rd and A .= E\\nh\\nxt\\n\\x00\\nxt \\x00 \\x00xt+1\\n\\x00>i\\n2 Rd⇥d (9.11)\\nFrom (9.10) it is clear that, if the system converges, it must converge to the weight vector\\nwTD at which\\nb \\x00 AwTD = 0\\n) b = AwTD\\n) wTD\\n.= A\\x001b. (9.12)\\nThis quantity is called theTD ﬁxed point. In fact linear semi-gradient TD(0) converges\\nto this point. Some of the theory proving its convergence, and the existence of the inverse\\nabove, is given in the box.\\nProof of Convergence of Linear TD(0)\\nWhat properties assure convergence of the linear TD(0) algorithm(9.9)? Some\\ninsight can be gained by rewriting (9.10) as\\nE[wt+1|wt]=( I \\x00 ↵A)wt + ↵b. (9.13)\\nNote that the matrixA multiplies the weight vectorwt and not b; only A is\\nimportant to convergence. To develop intuition, consider the special case in which\\nA is a diagonal matrix. If any of the diagonal elements are negative, then the\\ncorresponding diagonal element of I \\x00 ↵A will be greater than one, and the\\ncorresponding component ofwt will be ampliﬁed, which will lead to divergence if\\ncontinued. On the other hand, if the diagonal elements ofA are all positive, then\\n↵ can be chosen smaller than one over the largest of them, such thatI \\x00 ↵A is\\ndiagonal with all diagonal elements between 0 and 1. In this case the ﬁrst term\\nof the update tends to shrinkwt, and stability is assured. In general,wt will be\\nreduced toward zero wheneverA is positive deﬁnite, meaning y>Ay> 0 for any\\nreal vectory 6= 0. Positive deﬁniteness also ensures that the inverseA\\x001 exists.\\nFor linear TD(0), in the continuing case with\\x00< 1, the A matrix (9.11) can be\\nwritten\\nA =\\nX\\ns\\nµ(s)\\nX\\na\\n⇡(a|s)\\nX\\nr,s0\\np(r, s0|s, a)x(s)\\n\\x00\\nx(s) \\x00 \\x00x(s0)\\n\\x00>\\n=\\nX\\ns\\nµ(s)\\nX\\ns0\\np(s0|s)x(s)\\n\\x00\\nx(s) \\x00 \\x00x(s0)\\n\\x00>\\n=\\nX\\ns\\nµ(s)x(s)\\n✓\\nx(s) \\x00 \\x00\\nX\\ns0\\np(s0|s)x(s0)\\n◆>\\n= X>D(I \\x00 \\x00P)X,\\nwhere µ(s) is the stationary distribution under⇡, p(s0|s) is the probability of\\ntransition froms to s0 under policy⇡, P is the|S|⇥| S| matrix of these probabilities,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 228, 'page_label': '229'}, page_content='9.4. Linear Methods 207\\nD is the|S|⇥| S| diagonal matrix with theµ(s) on its diagonal, andX is the|S|⇥ d\\nmatrix withx(s) as its rows. From here it is clear that the inner matrixD(I \\x00 \\x00P)\\nis key to determining the positive deﬁniteness ofA.\\nFor a key matrix of this form, positive deﬁniteness is assured if all of its columns\\nsum to a nonnegative number. This was shown by Sutton (1988, p. 27) based\\non two previously established theorems. One theorem says that any matrixM\\nis positive deﬁnite if and only if the symmetric matrixS = M + M> is positive\\ndeﬁnite (Sutton 1988, appendix). The second theorem says that any symmetric\\nreal matrixS is positive deﬁnite if all of its diagonal entries are positive and greater\\nthan the sum of the absolute values of the corresponding o↵-diagonal entries (Varga\\n1962, p. 23). For our key matrix,D(I \\x00 \\x00P), the diagonal entries are positive\\nand the o↵-diagonal entries are negative, so all we have to show is that each row\\nsum plus the corresponding column sum is positive. The row sums are all positive\\nbecause P is a stochastic matrix and\\x00< 1. Thus it only remains to show that\\nthe column sums are nonnegative. Note that the row vector of the column sums\\nof any matrixM can be written as1>M,w h e r e1 is the column vector with all\\ncomponents equal to 1. Letµ denote the|S|-vector of theµ(s), whereµ = P>µ by\\nvirtue ofµ being the stationary distribution. The column sums of our key matrix,\\nthen, are:\\n1>D(I \\x00 \\x00P)= µ>(I \\x00 \\x00P)\\n= µ> \\x00 \\x00µ>P\\n= µ> \\x00 \\x00µ> (because µ is the stationary distribution)\\n=( 1\\x00 \\x00)µ>,\\nall components of which are positive. Thus, the key matrix and itsA matrix\\nare positive deﬁnite, and on-policy TD(0) is stable. (Additional conditions and a\\nschedule for reducing↵ over time are needed to prove convergence with probability\\none.)\\nAt the TD ﬁxed point, it has also been proven (in the continuing case) that theVE is\\nwithin a bounded expansion of the lowest possible error:\\nVE(wTD) \\uf8ff 1\\n1 \\x00 \\x00 min\\nw\\nVE(w). (9.14)\\nThat is, the asymptotic error of the TD method is no more than1\\n1\\x00\\x00 times the smallest\\npossible error, that attained in the limit by the Monte Carlo method. Because\\x00 is often\\nnear one, this expansion factor can be quite large, so there is substantial potential loss in\\nasymptotic performance with the TD method. On the other hand, recall that the TD\\nmethods are often of vastly reduced variance compared to Monte Carlo methods, and\\nthus faster, as we saw in Chapters 6 and 7. Which method will be best depends on the\\nnature of the approximation and problem, and on how long learning continues.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 229, 'page_label': '230'}, page_content='208 Chapter 9: On-policy Prediction with Approximation\\nA bound analogous to(9.14) applies to other on-policy bootstrapping methods as well.\\nFor example, linear semi-gradient DP (Eq. 9.7 withUt\\n.= P\\na ⇡(a|St) P\\ns0,r p(s0,r |St,a )[r+\\n\\x00ˆv(s0,wt)]) with updates according to the on-policy distribution will also converge to\\nthe TD ﬁxed point. One-step semi-gradientaction-value methods, such as semi-gradient\\nSarsa(0) covered in the next chapter converge to an analogous ﬁxed point and an analogous\\nbound. For episodic tasks, there is a slightly di↵erent but related bound (see Bertsekas\\nand Tsitsiklis, 1996). There are also a few technical conditions on the rewards, features,\\nand decrease in the step-size parameter, which we have omitted here. The full details\\ncan be found in the original paper (Tsitsiklis and Van Roy, 1997).\\nCritical to these convergence results is that states are updated according to the on-\\npolicy distribution. For other update distributions, bootstrapping methods using function\\napproximation may actually diverge to inﬁnity. Examples of this and a discussion of\\npossible solution methods are given in Chapter 11.\\nExample 9.2: Bootstrapping on the 1000-state Random WalkState aggregation\\nis a special case of linear function approximation, so let’s return to the 1000-state random\\nwalk to illustrate some of the observations made in this chapter. The left panel of\\nFigure 9.2 shows the ﬁnal value function learned by the semi-gradient TD(0) algorithm\\n(page 203) using the same state aggregation as in Example 9.1. We see that the near-\\nasymptotic TD approximation is indeed farther from the true values than the Monte\\nCarlo approximation shown in Figure 9.1.\\nNevertheless, TD methods retain large potential advantages in learning rate, and\\ngeneralize Monte Carlo methods, as we investigated fully withn-step TD methods in\\nChapter 7. The right panel of Figure 9.2 shows results with ann-step semi-gradient\\nTD method using state aggregation on the 1000-state random walk that are strikingly\\nsimilar to those we obtained earlier with tabular methods and the 19-state random\\nwalk (Figure 7.2). To obtain such quantitatively similar results we switched the state\\naggregation to 20 groups of 50 states each. The 20 groups were then quantitatively close\\n0.55\\n0.5\\n0.45\\n0.35\\n0.3\\n0.25\\n0.4\\n0.40.20 0.80.6 1\\n↵\\nAverage\\nRMS error\\nover 1000 states\\nand ﬁrst 10 \\nepisodes\\nn=1\\nn=2\\nn=4n=8\\nn=16\\nn=32\\nn=64\\n128512\\n256\\nState\\n    True \\nvaluev⇡\\n    Approximate \\nTD value\\n1\\n0\\n-1\\n1\\n1000\\nˆv\\nFigure 9.2: Bootstrapping with state aggregation on the 1000-state random walk task.Left:\\nAsymptotic values of semi-gradient TD are worse than the asymptotic Monte Carlo values in\\nFigure 9.1. Right: Performance ofn-step methods with state-aggregation are strikingly similar\\nto those with tabular representations (cf. Figure 7.2). These data are averages over 100 runs.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 230, 'page_label': '231'}, page_content='9.4. Linear Methods 209\\nto the 19 states of the tabular problem. In particular, recall that state transitions were\\nup to 100 states to the left or right. A typical transition would then be of 50 states to\\nthe right or left, which is quantitatively analogous to the single-state state transitions of\\nthe 19-state tabular system. To complete the match, we use here the same performance\\nmeasure—an unweighted average of the RMS error over all states and over the ﬁrst\\n10 episodes—rather than a VE objective as is otherwise more appropriate when using\\nfunction approximation.\\nThe semi-gradient n-step TD algorithm used in the example above is the natural\\nextension of the tabularn-step TD algorithm presented in Chapter 7 to semi-gradient\\nfunction approximation. Pseudocode is given in the box below.\\nn-step semi-gradient TD for estimatingˆv ⇡ v⇡\\nInput: the policy⇡ to be evaluated\\nInput: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,· )=0\\nAlgorithm parameters: step size↵> 0, a positive integern\\nInitialize value-function weightsw arbitrarily (e.g.,w = 0)\\nAll store and access operations (St and Rt) can take their index modn +1\\nLoop for each episode:\\nInitialize and storeS0 6= terminal\\nT  1\\nLoop for t =0 , 1, 2,... :\\n| If t<T ,t h e n :\\n| Take an action according to⇡(·| St)\\n| Observe and store the next reward asRt+1 and the next state asSt+1\\n| If St+1 is terminal, thenT  t +1\\n| ⌧  t \\x00 n +1 ( ⌧ is the time whose state’s estimate is being updated)\\n| If ⌧ \\x00 0:\\n| G  Pmin(⌧+n,T)\\ni=⌧+1 \\x00i\\x00⌧\\x001Ri\\n| If ⌧ + n<T ,t h e n :G  G + \\x00nˆv(S⌧+n,w)( G⌧:⌧+n)\\n| w  w + ↵ [G \\x00 ˆv(S⌧ ,w)] rˆv(S⌧ ,w)\\nUntil ⌧ = T \\x00 1\\nThe key equation of this algorithm, analogous to (7.2), is\\nwt+n\\n.= wt+n\\x001 + ↵ [Gt:t+n \\x00 ˆv(St,wt+n\\x001)] rˆv(St,wt+n\\x001), 0 \\uf8ff t<T , (9.15)\\nwhere then-step return is generalized from (7.1) to\\nGt:t+n\\n.= Rt+1 +\\x00Rt+2 +··· +\\x00n\\x001Rt+n +\\x00nˆv(St+n,wt+n\\x001), 0 \\uf8ff t \\uf8ff T \\x00n. (9.16)\\nExercise 9.1 Show that tabular methods such as presented in Part I of this book are a\\nspecial case of linear function approximation. What would the feature vectors be?⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 231, 'page_label': '232'}, page_content='210 Chapter 9: On-policy Prediction with Approximation\\n9.5 Feature Construction for Linear Methods\\nLinear methods are interesting because of their convergence guarantees, but also because\\nin practice they can be very e\\x00cient in terms of both data and computation. Whether or\\nnot this is so depends critically on how the states are represented in terms of features,\\nwhich we investigate in this large section. Choosing features appropriate to the task is\\nan important way of adding prior domain knowledge to reinforcement learning systems.\\nIntuitively, the features should correspond to the aspects of the state space along which\\ngeneralization may be appropriate. If we are valuing geometric objects, for example,\\nwe might want to have features for each possible shape, color, size, or function. If we\\nare valuing states of a mobile robot, then we might want to have features for locations,\\ndegrees of remaining battery power, recent sonar readings, and so on.\\nA limitation of the linear form is that it cannot take into account any interactions\\nbetween features, such as the presence of featurei being good only in the absence of\\nfeature j. For example, in the pole-balancing task (Example 3.4), high angular velocity\\ncan be either good or bad depending on the angle. If the angle is high, then high angular\\nvelocity means an imminent danger of falling—a bad state—whereas if the angle is low,\\nthen high angular velocity means the pole is righting itself—a good state. A linear value\\nfunction could not represent this if its features coded separately for the angle and the\\nangular velocity. It needs instead, or in addition, features for combinations of these two\\nunderlying state dimensions. In the following subsections we consider a variety of general\\nways of doing this.\\n9.5.1 Polynomials\\nThe states of many problems are initially expressed as numbers, such as positions and\\nvelocities in the pole-balancing task (Example 3.4), the number of cars in each lot in the\\nJack’s car rental problem (Example 4.2), or the gambler’s capital in the gambler problem\\n(Example 4.3). In these types of problems, function approximation for reinforcement\\nlearning has much in common with the familiar tasks of interpolation and regression.\\nVarious families of features commonly used for interpolation and regression can also be\\nused in reinforcement learning. Polynomials make up one of the simplest families of\\nfeatures used for interpolation and regression. While the basic polynomial features we\\ndiscuss here do not work as well as other types of features in reinforcement learning, they\\nserve as a good introduction because they are simple and familiar.\\nAs an example, suppose a reinforcement learning problem has states with two numerical\\ndimensions. For a single representative states,l e ti t st w on u m b e r sb es1 2 R and s2 2 R.\\nYou might choose to represents simply by its two state dimensions, so thatx(s)=\\n(s1,s 2)>, but then you would not be able to take into account any interactions between\\nthese dimensions. In addition, if boths1 and s2 were zero, then the approximate value\\nwould have to also be zero. Both limitations can be overcome by instead representings by\\nthe four-dimensional feature vectorx(s)=( 1,s 1,s 2,s 1s2)>. The initial 1 feature allows\\nthe representation of a\\x00ne functions in the original state numbers, and the ﬁnal product\\nfeature, s1s2, enables interactions to be taken into account. Or you might choose to use\\nhigher-dimensional feature vectors likex(s)=( 1,s 1,s 2,s 1s2,s 2\\n1,s 2\\n2,s 1s2\\n2,s 2\\n1s2,s 2\\n1s2\\n2)> to'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 232, 'page_label': '233'}, page_content='9.5. Feature Construction for Linear Methods 211\\ntake more complex interactions into account. Such feature vectors enable approximations\\nas arbitrary quadratic functions of the state numbers—even though the approximation is\\nstill linear in the weights that have to be learned. Generalizing this example from two\\nto k numbers, we can represent highly-complex interactions among a problem’s state\\ndimensions:\\nSuppose each states corresponds to k numbers, s1, s2,. . . ,sk, with eachsi 2 R.\\nFor thisk-dimensional state space, each order-n polynomial-basis feature xi can be\\nwritten as\\nxi(s)=⇧ k\\nj=1sci,j\\nj , (9.17)\\nwhere each ci,j is an integer in the set{0, 1,...,n } for an integern \\x00 0. These\\nfeatures make up the order-n polynomial basis for dimensionk, which contains\\n(n + 1)k di↵erent features.\\nHigher-order polynomial bases allow for more accurate approximations of more compli-\\ncated functions. But because the number of features in an order-n polynomial basis grows\\nexponentially with the dimensionk of the natural state space (ifn> 0), it is generally\\nnecessary to select a subset of them for function approximation. This can be done using\\nprior beliefs about the nature of the function to be approximated, and some automated\\nselection methods developed for polynomial regression can be adapted to deal with the\\nincremental and nonstationary nature of reinforcement learning.\\nExercise 9.2 Why does (9.17) deﬁne (n + 1)k distinct features for dimensionk? ⇤\\nExercise 9.3 What n and ci,j produce the feature vectorsx(s)=( 1,s 1,s 2,s 1s2,s 2\\n1,s 2\\n2,\\ns1s2\\n2,s 2\\n1s2,s 2\\n1s2\\n2)>? ⇤\\n9.5.2 Fourier Basis\\nAnother linear function approximation method is based on the time-honored Fourier\\nseries, which expresses periodic functions as weighted sums of sine and cosine basis\\nfunctions (features) of di↵erent frequencies. (A functionf is periodic iff(x)= f(x + ⌧)\\nfor allx and some period⌧.) The Fourier series and the more general Fourier transform\\nare widely used in applied sciences in part because if a function to be approximated is\\nknown, then the basis function weights are given by simple formulae and, further, with\\nenough basis functions essentially any function can be approximated as accurately as\\ndesired. In reinforcement learning, where the functions to be approximated are unknown,\\nFourier basis functions are of interest because they are easy to use and can perform well\\nin a range of reinforcement learning problems.\\nFirst consider the one-dimensional case. The usual Fourier series representation of a\\nfunction of one dimension having period⌧ represents the function as a linear combination\\nof sine and cosine functions that are each periodic with periods that evenly divide⌧ (in\\nother words, whose frequencies are integer multiples of a fundamental frequency 1/⌧).\\nBut if you are interested in approximating an aperiodic function deﬁned over a bounded'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 233, 'page_label': '234'}, page_content='212 Chapter 9: On-policy Prediction with Approximation\\ninterval, then you can use these Fourier basis features with⌧ set to the length of the\\ninterval. The function of interest is then just one period of the periodic linear combination\\nof the sine and cosine features.\\nFurthermore, if you set⌧ to twice the length of the interval of interest and restrict\\nattention to the approximation over the half interval [0,⌧/ 2], then you can use just the\\ncosine features. This is possible because you can represent anyeven function, that is,\\nany function that is symmetric about the origin, with just the cosine basis. So any\\nfunction over the half-period [0,⌧/ 2] can be approximated as closely as desired with\\nenough cosine features. (Saying “any function” is not exactly correct because the function\\nhas to be mathematically well-behaved, but we skip this technicality here.) Alternatively,\\nit is possible to use just sine features, linear combinations of which are alwaysodd\\nfunctions, that is functions that are anti-symmetric about the origin. But it is generally\\nbetter to keep just the cosine features because “half-even” functions tend to be easier to\\napproximate than “half-odd” functions because the latter are often discontinuous at the\\norigin. Of course, this does not rule out using both sine and cosine features to approximate\\nover the interval [0,⌧/ 2], which might be advantageous in some circumstances.\\nFollowing this logic and letting⌧ = 2 so that the features are deﬁned over the half-⌧\\ninterval [0, 1], the one-dimensional order-n Fourier cosine basis consists of then +1\\nfeatures\\nxi(s) = cos(i⇡s),s 2 [0, 1],\\nfor i =0 ,...,n . Figure 9.3 shows one-dimensional Fourier cosine featuresxi, for i =\\n1, 2, 3, 4; x0 is a constant function.\\n00.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81 Univariate Fourier Basis Function k=11\\n-1 10 00.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81 Univariate Fourier Basis Function k=21\\n-1 10 00.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81 Univariate Fourier Basis Function k=31\\n-1 10 00.10.20.30.40.50.60.70.80.91−1−0.8−0.6−0.4−0.200.20.40.60.81 Univariate Fourier Basis Function k=41\\n-1 10\\nFigure 9.3: One-dimensional Fourier cosine-basis featuresxi, i =1 , 2, 3, 4, for approximating\\nfunctions over the interval [0, 1]. After Konidaris et al. (2011).\\nThis same reasoning applies to the Fourier cosine series approximation in the multi-\\ndimensional case as described in the box below.\\nSuppose each states corresponds to a vector ofk numbers, s =( s1,s 2,. . . ,sk)>,\\nwith eachsi 2 [0, 1]. The ith feature in the order-n Fourier cosine basis can then\\nbe written\\nxi(s) = cos\\n\\x00\\n⇡s>ci\\x00\\n, (9.18)\\nwhere ci =( ci\\n1,...,c i\\nk)>,w i t hci\\nj 2{ 0,...,n } for j =1 ,...,k and i =1 ,..., (n+1)k.\\nThis deﬁnes a feature for each of the (n + 1)k possible integer vectorsci.T h ei n n e r'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 234, 'page_label': '235'}, page_content='9.5. Feature Construction for Linear Methods 213\\nproduct s>ci has the e↵ect of assigning an integer in{0,...,n } to each dimension\\nof s. As in the one-dimensional case, this integer determines the feature’s frequency\\nalong that dimension. The features can of course be shifted and scaled to suit the\\nbounded state space of a particular application.\\nAs an example, consider thek = 2 case in whichs =( s1,s 2)>, where eachci =( ci\\n1,c i\\n2)>.\\nFigure 9.4 shows a selection of six Fourier cosine features, each labeled by the vectorci\\nthat deﬁnes it (s1 is the horizontal axis andci is shown as a row vector with the indexi\\nomitted). Any zero inc means the feature is constant along that state dimension. So if\\nc =( 0, 0)>, the feature is constant over both dimensions; ifc =( c1, 0)> the feature is\\nconstant over the second dimension and varies over the ﬁrst with frequency depending\\non c1; and similarly, forc =( 0,c 2)>.W h e nc =( c1,c 2)> with neither cj = 0, the\\nfeature varies along both dimensions and represents an interaction between the two state\\nvariables. The values ofc1 and c2 determine the frequency along each dimension, and\\ntheir ratio gives the direction of the interaction.\\nc = (0, 1)\\n1\\n1\\n00\\n1\\n1\\n00\\nc=( 0,1)>\\nc = (1, 0)\\n1\\n1\\n00\\n1\\n1\\n00\\nc=( 1,0)>\\nc = (1, 1)\\n1\\n1\\n00\\n1\\n1\\n00\\nc=( 1,1)>\\nc = (1, 5)\\n1\\n1\\n00\\n1\\n1\\n00\\nc=( 0,5)>\\nc = (2, 5)\\n1\\n1\\n00\\n1\\n1\\n00\\nc=( 2,5)>\\n1\\n1\\n00\\nc=( 5,2)>\\nFigure 9.4: A selection of six two-dimensional Fourier cosine features, each labeled by the\\nvector ci that deﬁnes it (s1 is the horizontal axis, andci is shown with the indexi omitted).\\nAfter Konidaris et al. (2011).\\nWhen using Fourier cosine features with a learning algorithm such as (9.7), semi-\\ngradient TD(0), or semi-gradient Sarsa, it may be helpful to use a di↵erent step-size\\nparameter for each feature. If↵ is the basic step-size parameter, then Konidaris, Osentoski,\\nand Thomas (2011) suggest setting the step-size parameter for featurexi to ↵i =\\n↵/\\np\\n(ci\\n1)2 + ··· +( ci\\nk)2 (except when eachci\\nj = 0, in which case↵i = ↵).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 235, 'page_label': '236'}, page_content='214 Chapter 9: On-policy Prediction with Approximation\\nFourier cosine features with Sarsa can produce good performance compared to several\\nother collections of basis functions, including polynomial and radial basis functions. Not\\nsurprisingly, however, Fourier features have trouble with discontinuities because it is\\ndi\\x00cult to avoid “ringing” around points of discontinuity unless very high frequency basis\\nfunctions are included.\\nThe number of features in the order-n Fourier basis grows exponentially with the\\ndimension of the state space, but if that dimension is small enough (e.g.,k \\uf8ff 5), then\\none can selectn so that all of the order-n Fourier features can be used. This makes the\\nselection of features more-or-less automatic. For high dimension state spaces, however, it\\nis necessary to select a subset of these features. This can be done using prior beliefs about\\nthe nature of the function to be approximated, and some automated selection methods\\ncan be adapted to deal with the incremental and nonstationary nature of reinforcement\\nlearning. An advantage of Fourier basis features in this regard is that it is easy to select\\nfeatures by setting theci vectors to account for suspected interactions among the state\\nvariables and by limiting the values in thecj vectors so that the approximation can\\nﬁlter out high frequency components considered to be noise. On the other hand, because\\nFourier features are non-zero over the entire state space (with the few zeros excepted),\\nthey represent global properties of states, which can make it di\\x00cult to ﬁnd good ways\\nto represent local properties.\\nFigure 9.5 shows learning curves comparing the Fourier and polynomial bases on the\\n1000-state random walk example. In general, we do not recommend using polynomials\\nfor online learning.2\\n.4\\n.3\\n.2\\n.1\\n00 5000Episodes\\nPolynomial basis\\nFourier basis\\npVE averagedover 30 runs\\nFigure 9.5: Fourier basis vs polynomials on the 1000-state random walk. Shown are learning\\ncurves for the gradient Monte Carlo method with Fourier and polynomial bases of order 5, 10,\\nand 20. The step-size parameters were roughly optimized for each case:↵ =0 .0001 for the\\npolynomial basis and↵ =0 .00005 for the Fourier basis. The performance measure (y-axis) is\\nthe root mean square value error (9.1).\\n2There are families of polynomials more complicated than those we have discussed, for example,\\ndi↵erent families of orthogonal polynomials, and these might work better, but at present there is little\\nexperience with them in reinforcement learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 236, 'page_label': '237'}, page_content='9.5. Feature Construction for Linear Methods 215\\n9.5.3 Coarse Coding\\ns0\\ns\\nFigure 9.6: Coarse coding. Generaliza-\\ntion from states to states0 depends on\\nthe number of their features whose recep-\\ntive ﬁelds (in this case, circles) overlap.\\nThese states have one feature in common,\\nso there will be slight generalization be-\\ntween them.\\nConsider a task in which the natural repre-\\nsentation of the state set is a continuous two-\\ndimensional space. One kind of representation for\\nthis case is made up of features corresponding to\\ncircles in state space, as shown to the right. If\\nthe state is inside a circle, then the corresponding\\nfeature has the value 1 and is said to bepresent;\\notherwise the feature is 0 and is said to beabsent.\\nThis kind of 1–0-valued feature is called abinary\\nfeature. Given a state, which binary features are\\npresent indicate within which circles the state lies,\\nand thus coarsely code for its location. Represent-\\ning a state with features that overlap in this way\\n(although they need not be circles or binary) is\\nknown ascoarse coding.\\nAssuming linear gradient-descent function ap-\\nproximation, consider the e↵ect of the size and\\ndensity of the circles. Corresponding to each cir-\\ncle is a single weight (a component ofw) that is\\na↵ected by learning. If we train at one state, a\\npoint in the space, then the weights of all circles\\nintersecting that state will be a↵ected. Thus, by (9.8), the approximate value function\\nwill be a↵ected at all states within the union of the circles, with a greater e↵ect the more\\ncircles a point has “in common” with the state, as shown in Figure 9.6. If the circles are\\nsmall, then the generalization will be over a short distance, as in Figure 9.7 (left), whereas\\nif they are large, it will be over a large distance, as in Figure 9.7 (middle). Moreover,\\na) Narrow generalization b) Broad generalization c) Asymmetric generalization\\nFigure 9.7: Generalization in linear function approximation methods is determined by the\\nsizes and shapes of the features’ receptive ﬁelds. All three of these cases have roughly the same\\nnumber and density of features.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 237, 'page_label': '238'}, page_content='216 Chapter 9: On-policy Prediction with Approximation\\nthe shape of the features will determine the nature of the generalization. For example, if\\nthey are not strictly circular, but are elongated in one direction, then generalization will\\nbe similarly a↵ected, as in Figure 9.7 (right).\\nFeatures with large receptive ﬁelds give broad generalization, but might also seem to\\nlimit the learned function to a coarse approximation, unable to make discriminations\\nmuch ﬁner than the width of the receptive ﬁelds. Happily, this is not the case. Initial\\ngeneralization from one point to another is indeed controlled by the size and shape of\\nthe receptive ﬁelds, but acuity, the ﬁnest discrimination ultimately possible, is controlled\\nmore by the total number of features.\\nExample 9.3: Coarseness of Coarse CodingThis example illustrates the e↵ect on\\nlearning of the size of the receptive ﬁelds in coarse coding. Linear function approximation\\nbased on coarse coding and (9.7) was used to learn a one-dimensional square-wave function\\n(shown at the top of Figure 9.8). The values of this function were used as the targets,Ut.\\nWith just one dimension, the receptive ﬁelds were intervals rather than circles. Learning\\nwas repeated with three di↵erent sizes of the intervals: narrow, medium, and broad, as\\nshown at the bottom of the ﬁgure. All three cases had the same density of features,\\nabout 50 over the extent of the function being learned. Training examples were generated\\nuniformly at random over this extent. The step-size parameter was↵ = 0.2\\nn ,w h e r en is\\nthe number of features that were present at one time. Figure 9.8 shows the functions\\nlearned in all three cases over the course of learning. Note that the width of the features\\nhad a strong e↵ect early in learning. With broad features, the generalization tended to be\\nbroad; with narrow features, only the close neighbors of each trained point were changed,\\ncausing the function learned to be more bumpy. However, the ﬁnal function learned was\\na↵ected only slightly by the width of the features. Receptive ﬁeld shape tends to have a\\nstrong e↵ect on generalization but little e↵ect on asymptotic solution quality.\\n10\\n40\\n160\\n640\\n2560\\n10240\\nNarrowfeatures\\ndesiredfunction\\nMediumfeatures Broadfeatures\\n#Examples approx-imation\\nfeaturewidth\\nFigure 9.8: Example of feature width’s strong e↵ect on initial generalization (ﬁrst row) and\\nweak e↵ect on asymptotic accuracy (last row).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 238, 'page_label': '239'}, page_content='9.5. Feature Construction for Linear Methods 217\\n9.5.4 Tile Coding\\nTile coding is a form of coarse coding for multi-dimensional continuous spaces that is\\nﬂexible and computationally e\\x00cient. It may be the most practical feature representation\\nfor modern sequential digital computers.\\nIn tile coding the receptive ﬁelds of the features are grouped into partitions of the state\\nspace. Each such partition is called atiling, and each element of the partition is called a\\ntile. For example, the simplest tiling of a two-dimensional state space is a uniform grid\\nsuch as that shown on the left side of Figure 9.9. The tiles or receptive ﬁeld here are\\nsquares rather than the circles in Figure 9.6. If just this single tiling were used, then the\\nstate indicated by the white spot would be represented by the single feature whose tile\\nit falls within; generalization would be complete to all states within the same tile and\\nnonexistent to states outside it. With just one tiling, we would not have coarse coding\\nbut just a case of state aggregation.\\nPoint in \\nstate space\\nto be\\nrepresented\\nTiling 1\\nTiling 2\\nTiling 3\\nTiling 4Continuous \\n2D state \\nspace\\nFour active\\ntiles/features \\noverlap the point\\nand are used to \\nrepresent it\\nFigure 9.9: Multiple, overlapping grid-tilings on a limited two-dimensional space. These tilings\\nare o↵set from one another by a uniform amount in each dimension.\\nTo get the strengths of coarse coding requires overlapping receptive ﬁelds, and by\\ndeﬁnition the tiles of a partition do not overlap. To get true coarse coding with tile coding,\\nmultiple tilings are used, each o↵set by a fraction of a tile width. A simple case with\\nfour tilings is shown on the right side of Figure 9.9. Every state, such as that indicated\\nby the white spot, falls in exactly one tile in each of the four tilings. These four tiles\\ncorrespond to four features that become active when the state occurs. Speciﬁcally, the\\nfeature vectorx(s) has one component for each tile in each tiling. In this example there\\nare 4⇥4 ⇥4 = 64 components, all of which will be 0 except for the four corresponding to\\nthe tiles thats falls within. Figure 9.10 shows the advantage of multiple o↵set tilings\\n(coarse coding) over a single tiling on the 1000-state random walk example.\\nAn immediate practical advantage of tile coding is that, because it works with partitions,\\nthe overall number of features that are active at one time is the same for any state.\\nExactly one feature is present in each tiling, so the total number of features present is\\nalways the same as the number of tilings. This allows the step-size parameter,↵,t o'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 239, 'page_label': '240'}, page_content='218 Chapter 9: On-policy Prediction with Approximation\\n.4\\n.3\\n.2\\n.1\\n0\\n averagedover 30 runs\\n0 5000Episodes\\nState aggregation(one tiling)Tile coding (50 tilings)\\npVE\\nFigure 9.10: Why we use coarse coding. Shown are learning curves on the 1000-state random\\nwalk example for the gradient Monte Carlo algorithm with a single tiling and with multiple\\ntilings. The space of 1000 states was treated as a single continuous dimension, covered with tiles\\neach 200 states wide. The multiple tilings were o↵set from each other by 4 states. The step-size\\nparameter was set so that the initial learning rate in the two cases was the same,↵ =0 .0001 for\\nthe single tiling and↵ =0 .0001/50 for the 50 tilings.\\nbe set in an easy, intuitive way. For example, choosing↵ = 1\\nn ,w h e r en is the number\\nof tilings, results in exact one-trial learning. If the examples 7! v is trained on, then\\nwhatever the prior estimate,ˆv(s,wt), the new estimate will beˆv(s,wt+1)= v. Usually\\none wishes to change more slowly than this, to allow for generalization and stochastic\\nvariation in target outputs. For example, one might choose↵ = 1\\n10n , in which case the\\nestimate for the trained state would move one-tenth of the way to the target in one\\nupdate, and neighboring states will be moved less, proportional to the number of tiles\\nthey have in common.\\nTile coding also gains computational advantages from its use of binary feature vectors.\\nBecause each component is either 0 or 1, the weighted sum making up the approximate\\nvalue function (9.8) is almost trivial to compute. Rather than performingd multiplications\\nand additions, one simply computes the indices of then ⌧ d active features and then\\nadds up then corresponding components of the weight vector.\\nGeneralization occurs to states other than the one trained if those states fall within\\nany of the same tiles, proportional to the number of tiles in common. Even the choice of\\nhow to o↵set the tilings from each other a↵ects generalization. If they are o↵set uniformly\\nin each dimension, as they were in Figure 9.9, then di↵erent states can generalize in\\nqualitatively di↵erent ways, as shown in the upper half of Figure 9.11. Each of the eight\\nsubﬁgures show the pattern of generalization from a trained state to nearby points. In this\\nexample there are eight tilings, thus 64 subregions within a tile that generalize distinctly,\\nbut all according to one of these eight patterns. Note how uniform o↵sets result in a\\nstrong e↵ect along the diagonal in many patterns. These artifacts can be avoided if the\\ntilings are o↵set asymmetrically, as shown in the lower half of the ﬁgure. These lower\\ngeneralization patterns are better because they are all well centered on the trained state\\nwith no obvious asymmetries.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 240, 'page_label': '241'}, page_content='9.5. Feature Construction for Linear Methods 219\\nPossible generalizations for uniformly oﬀset tilings\\nPossible generalizationsfor asymmetrically oﬀset tilings\\nFigure 9.11: Why tile asymmetrical o↵sets are preferred in tile coding. Shown is the strength\\nof generalization from a trained state, indicated by the small black plus, to nearby states, for the\\ncase of eight tilings. If the tilings are uniformly o↵set (above), then there are diagonal artifacts\\nand substantial variations in the generalization, whereas with asymmetrically o↵set tilings the\\ngeneralization is more spherical and homogeneous.\\nTilings in all cases are o↵set from each other by a fraction of a tile width in each\\ndimension. Ifw denotes the tile width andn the number of tilings, thenw\\nn is a fundamental\\nunit. Within small squaresw\\nn on a side, all states activate the same tiles, have the same\\nfeature representation, and the same approximated value. If a state is moved byw\\nn\\nin any cartesian direction, the feature representation changes by one component/tile.\\nUniformly o↵set tilings are o↵set from each other by exactly this unit distance. For a\\ntwo-dimensional space, we say that each tiling is o↵set by the displacement vector (1, 1),\\nmeaning that it is o↵set from the previous tiling byw\\nn times this vector. In these terms,\\nthe asymmetrically o↵set tilings shown in the lower part of Figure 9.11 are o↵set by a\\ndisplacement vector of (1, 3).\\nExtensive studies have been made of the e↵ect of di↵erent displacement vectors on the\\ngeneralization of tile coding (Parks and Militzer, 1991; An, 1991; An, Miller and Parks,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 241, 'page_label': '242'}, page_content='220 Chapter 9: On-policy Prediction with Approximation\\n1991; Miller, An, Glanz and Carter, 1990), assessing their homegeneity and tendency\\ntoward diagonal artifacts like those seen for the (1, 1) displacement vectors. Based on this\\nwork, Miller and Glanz (1996) recommend using displacement vectors consisting of the\\nﬁrst odd integers. In particular, for a continuous space of dimensionk, a good choice is\\nto use the ﬁrst odd integers (1, 3, 5, 7,..., 2k \\x001), withn (the number of tilings) set to an\\ninteger power of 2 greater than or equal to 4k. This is what we have done to produce the\\ntilings in the lower half of Figure 9.11, in whichk = 2,n =2 3 \\x00 4k, and the displacement\\nvector is (1, 3). In a three-dimensional case, the ﬁrst four tilings would be o↵set in total\\nfrom a base position by (0, 0, 0), (1, 3, 5), (2, 6, 10), and (3, 9, 15). Open-source software\\nthat can e\\x00ciently make tilings like this for anyk is readily available.\\nIn choosing a tiling strategy, one has to pick the number of the tilings and the shape of\\nthe tiles. The number of tilings, along with the size of the tiles, determines the resolution\\nor ﬁneness of the asymptotic approximation, as in general coarse coding and illustrated\\nin Figure 9.8. The shape of the tiles will determine the nature of generalization as in\\nFigure 9.7. Square tiles will generalize roughly equally in each dimension as indicated in\\nFigure 9.11 (lower). Tiles that are elongated along one dimension, such as the stripe tilings\\nin Figure 9.12 (middle), will promote generalization along that dimension. The tilings in\\nFigure 9.12 (middle) are also denser and thinner on the left, promoting discrimination\\nalong the horizontal dimension at lower values along that dimension. The diagonal stripe\\ntiling in Figure 9.12 (right) will promote generalization along one diagonal. In higher\\ndimensions, axis-aligned stripes correspond to ignoring some of the dimensions in some\\nof the tilings, that is, to hyperplanar slices. Irregular tilings such as shown in Figure 9.12\\n(left) are also possible, though rare in practice and beyond the standard software.\\na) Irregular b) Log stripes c) Diagonal stripes\\nFigure 9.12: Tilings need not be grids. They can be arbitrarily shaped and non-uniform, while\\nstill in many cases being computationally e\\x00cient to compute.\\nIn practice, it is often desirable to use di↵erent shaped tiles in di↵erent tilings. For\\nexample, one might use some vertical stripe tilings and some horizontal stripe tilings.\\nThis would encourage generalization along either dimension. However, with stripe tilings\\nalone it is not possible to learn that a particular conjunction of horizontal and vertical\\ncoordinates has a distinctive value (whatever is learned for it will bleed into states with the\\nsame horizontal and vertical coordinates). For this one needs the conjunctive rectangular\\ntiles such as originally shown in Figure 9.9. With multiple tilings—some horizontal, some\\nvertical, and some conjunctive—one can get everything: a preference for generalizing\\nalong each dimension, yet the ability to learn speciﬁc values for conjunctions (see Sutton,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 242, 'page_label': '243'}, page_content='9.5. Feature Construction for Linear Methods 221\\n1996 for examples). The choice of tilings determines generalization, and until this choice\\ncan be e↵ectively automated, it is important that tile coding enables the choice to be\\nmade ﬂexibly and in a way that makes sense to people.\\nAnother useful trick for reducing memory requirements ishashing—a consistent pseudo-\\nrandom collapsing of a large tiling into a much smaller set of tiles. Hashing produces\\ntiles consisting of noncontiguous, disjoint regions randomly spread throughout the state\\nonetile\\nspace, but that still form an exhaustive partition. For example,\\none tile might consist of the four subtiles shown to the right.\\nThrough hashing, memory requirements are often reduced by\\nlarge factors with little loss of performance. This is possible\\nbecause high resolution is needed in only a small fraction of the\\nstate space. Hashing frees us from the curse of dimensionality\\nin the sense that memory requirements need not be exponential\\nin the number of dimensions, but need merely match the real\\ndemands of the task. Open-source implementations of tile coding\\ncommonly include e\\x00cient hashing.\\nExercise 9.4 Suppose we believe that one of two state dimensions is more likely to have\\nan e↵ect on the value function than is the other, that generalization should be primarily\\nacross this dimension rather than along it. What kind of tilings could be used to take\\nadvantage of this prior knowledge? ⇤\\n9.5.5 Radial Basis Functions\\nRadial basis functions (RBFs) are the natural generalization of coarse coding to continuous-\\nvalued features. Rather than each feature being either 0 or 1, it can be anything in the\\ninterval [0, 1], reﬂecting variousdegrees to which the feature is present. A typical RBF\\nfeature, xi, has a Gaussian (bell-shaped) responsexi(s) dependent only on the distance\\nbetween the state,s, and the feature’s prototypical or center state,ci, and relative to the\\nfeature’s width,\\x00i:\\nxi(s) .=e x p\\n✓\\n\\x00||s \\x00 ci||2\\n2\\x002\\ni\\n◆\\n.\\nThe norm or distance metric of course can be chosen in whatever way seems most\\nappropriate to the states and task at hand. The ﬁgure below shows a one-dimensional\\nexample with a Euclidean distance metric.\\nci\\n!i\\nci+1ci-1\\nFigure 9.13: One-dimensional radial basis functions.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 243, 'page_label': '244'}, page_content='222 Chapter 9: On-policy Prediction with Approximation\\nThe primary advantage of RBFs over binary features is that they produce approximate\\nfunctions that vary smoothly and are di↵erentiable. Although this is appealing, in most\\ncases it has no practical signiﬁcance. Nevertheless, extensive studies have been made of\\ngraded response functions such as RBFs in the context of tile coding (An, 1991; Miller et\\nal., 1991; An et al., 1991; Lane, Handelman and Gelfand, 1992). All of these methods\\nrequire substantial additional computational complexity (over tile coding) and often\\nreduce performance when there are more than two state dimensions. In high dimensions\\nthe edges of tiles are much more important, and it has proven di\\x00cult to obtain well\\ncontrolled graded tile activations near the edges.\\nAn RBF networkis a linear function approximator using RBFs for its features. Learning\\nis deﬁned by equations (9.7) and (9.8), exactly as in other linear function approximators.\\nIn addition, some learning methods for RBF networks change the centers and widths of\\nthe features as well, bringing them into the realm of nonlinear function approximators.\\nNonlinear methods may be able to ﬁt target functions much more precisely. The downside\\nto RBF networks, and to nonlinear RBF networks especially, is greater computational\\ncomplexity and, often, more manual tuning before learning is robust and e\\x00cient.\\n9.6 Selecting Step-Size Parameters Manually\\nMost SGD methods require the designer to select an appropriate step-size parameter↵.\\nIdeally this selection would be automated, and in some cases it has been, but for most\\ncases it is still common practice to set it manually. To do this, and to better understand\\nthe algorithms, it is useful to develop some intuitive sense of the role of the step-size\\nparameter. Can we say in general how it should be set?\\nTheoretical considerations are unfortunately of little help. The theory of stochastic\\napproximation gives us conditions(2.7) on a slowly decreasing step-size sequence that are\\nsu\\x00cient to guarantee convergence, but these tend to result in learning that is too slow.\\nThe classical choice↵t =1 /t, which produces sample averages in tabular MC methods, is\\nnot appropriate for TD methods, for nonstationary problems, or for any method using\\nfunction approximation. For linear methods, there are recursive least-squares methods\\nthat set an optimalmatrix step size, and these methods can be extended to temporal-\\ndi↵erence learning as in the LSTD method described in Section 9.8, but these require\\nO(d2) step-size parameters, ord times more parameters than we are learning. For this\\nreason we rule them out for use on large problems where function approximation is most\\nneeded.\\nTo get some intuitive feel for how to set the step-size parameter manually, it is best\\nto go back momentarily to the tabular case. There we can understand that a step size\\nof ↵ = 1 will result in a complete elimination of the sample error after one target (see\\n(2.4) with a step size of one). As discussed on page 201, we usually want to learn slower\\nthan this. In the tabular case, a step size of↵ = 1\\n10 would take about 10 experiences to\\nconverge approximately to their mean target, and if we wanted to learn in 100 experiences\\nwe would use↵ = 1\\n100 . In general, if↵ = 1\\n⌧ , then the tabular estimate for a state will\\napproach the mean of its targets, with the most recent targets having the greatest e↵ect,\\nafter about ⌧ experiences with the state.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 244, 'page_label': '245'}, page_content='9.7. Nonlinear Function Approximation: Artiﬁcial Neural Networks 223\\nWith general function approximation there is not such a clear notion ofnumber of\\nexperiences with a state, as each state may be similar to and dissimilar from all the others\\nto various degrees. However, there is a similar rule that gives similar behavior in the case\\nof linear function approximation. Suppose you wanted to learn in about⌧ experiences\\nwith substantially the same feature vector. A good rule of thumb for setting the step-size\\nparameter of linear SGD methods is then\\n↵ .=\\n\\x00\\n⌧E\\n⇥\\nx>x\\n⇤\\x00\\x001\\n, (9.19)\\nwhere x is a random feature vector chosen from the same distribution as input vectors\\nwill be in the SGD. This method works best if the feature vectors do not vary greatly in\\nlength; ideallyx>x is a constant.\\nExercise 9.5Suppose you are using tile coding to transform a seven-dimensional continuous\\nstate space into binary feature vectors to estimate a state value functionˆv(s,w) ⇡ v⇡(s).\\nYou believe that the dimensions do not interact strongly, so you decide to use eight tilings\\nof each dimension separately (stripe tilings), for 7⇥ 8 = 56 tilings. In addition, in case\\nthere are some pairwise interactions between the dimensions, you also take all\\n\\x007\\n2\\n\\x00\\n= 21\\npairs of dimensions and tile each pair conjunctively with rectangular tiles. You make\\ntwo tilings for each pair of dimensions, making a grand total of 21⇥ 2 + 56 = 98 tilings.\\nGiven these feature vectors, you suspect that you still have to average out some noise,\\nso you decide that you want learning to be gradual, taking about 10 presentations with\\nthe same feature vector before learning nears its asymptote. What step-size parameter↵\\nshould you use? Why? ⇤\\nExercise 9.6 If ⌧ = 1 andx(St)>x(St)= E\\n⇥\\nx>x\\n⇤\\n, prove that(9.19) together with(9.7)\\nand linear function approximation results in the error being reduced to zero in one update.\\n9.7 Nonlinear Function Approximation:\\nArtiﬁcial Neural Networks\\nArtiﬁcial neural networks (ANNs) are widely used for nonlinear function approximation.\\nAn ANN is a network of interconnected units that have some of the properties of neurons,\\nthe main components of nervous systems. ANNs have a long history, with the latest\\nadvances in training deeply-layered ANNs (deep learning) being responsible for some\\nof the most impressive abilities of machine learning systems, including reinforcement\\nlearning systems. In Chapter 16 we describe several impressive examples of reinforcement\\nlearning systems that use ANN function approximation.\\nFigure 9.14 shows a generic feedforward ANN, meaning that there are no loops in the\\nnetwork, that is, there are no paths within the network by which a unit’s output can\\ninﬂuence its input. The network in the ﬁgure has an output layer consisting of two output\\nunits, an input layer with four input units, and two “hidden layers”: layers that are neither\\ninput nor output layers. A real-valued weight is associated with each link. A weight\\nroughly corresponds to the e\\x00cacy of a synaptic connection in a real neural network (see\\nSection 15.1). If an ANN has at least one loop in its connections, it is a recurrent rather'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 245, 'page_label': '246'}, page_content='224 Chapter 9: On-policy Prediction with Approximation\\nFigure 9.14: A generic feedforward ANN with four input units, two output units, and two\\nhidden layers.\\nthan a feedforward ANN. Although both feedforward and recurrent ANNs have been\\nused in reinforcement learning, here we look only at the simpler feedforward case.\\nThe units (the circles in Figure 9.14) are typically semi-linear units, meaning that they\\ncompute a weighted sum of their input signals and then apply to the result a nonlinear\\nfunction, called the activation function, to produce the unit’s output, or activation.\\nDi↵erent activation functions are used, but they are typically S-shaped, or sigmoid,\\nfunctions such as the logistic functionf(x)=1 /(1 +e\\x00x), though sometimes the rectiﬁer\\nnonlinearity f(x)= max(0,x ) is used. A step function likef(x)=1i f x \\x00 ✓, and 0\\notherwise, results in a binary unit with threshold✓. The units in a network’s input layer\\nare somewhat di↵erent in having their activations set to externally-supplied values that\\nare the inputs to the function the network is approximating.\\nThe activation of each output unit of a feedforward ANN is a nonlinear function of the\\nactivation patterns over the network’s input units. The functions are parameterized by\\nthe network’s connection weights. An ANN with no hidden layers can represent only a\\nvery small fraction of the possible input-output functions. However an ANN with a single\\nhidden layer containing a large enough ﬁnite number of sigmoid units can approximate\\nany continuous function on a compact region of the network’s input space to any degree\\nof accuracy (Cybenko, 1989). This is also true for other nonlinear activation functions\\nthat satisfy mild conditions, but nonlinearity is essential: if all the units in a multi-layer\\nfeedforward ANN have linear activation functions, the entire network is equivalent to a\\nnetwork with no hidden layers (because linear functions of linear functions are themselves\\nlinear).\\nDespite this “universal approximation” property of one-hidden-layer ANNs, both\\nexperience and theory show that approximating the complex functions needed for many\\nartiﬁcial intelligence tasks is made easier—indeed may require—abstractions that are\\nhierarchical compositions of many layers of lower-level abstractions, that is, abstractions'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 246, 'page_label': '247'}, page_content='9.7. Nonlinear Function Approximation: Artiﬁcial Neural Networks 225\\nproduced by deep architectures such as ANNs with many hidden layers. (See Bengio,\\n2009, for a thorough review.) The successive layers of a deep ANN compute increasingly\\nabstract representations of the network’s “raw” input, with each unit providing a feature\\ncontributing to a hierarchical representation of the overall input-output function of the\\nnetwork.\\nTraining the hidden layers of an ANN is therefore a way to automatically create\\nfeatures appropriate for a given problem so that hierarchical representations can be\\nproduced without relying exclusively on hand-crafted features. This has been an enduring\\nchallenge for artiﬁcial intelligence and explains why learning algorithms for ANNs with\\nhidden layers have received so much attention over the years. ANNs typically learn by a\\nstochastic gradient method (Section 9.3). Each weight is adjusted in a direction aimed at\\nimproving the network’s overall performance as measured by an objective function to\\nbe either minimized or maximized. In the most common supervised learning case, the\\nobjective function is the expected error, or loss, over a set of labeled training examples. In\\nreinforcement learning, ANNs can use TD errors to learn value functions, or they can aim\\nto maximize expected reward as in a gradient bandit (Section 2.8) or a policy-gradient\\nalgorithm (Chapter 13). In all of these cases it is necessary to estimate how a change\\nin each connection weight would inﬂuence the network’s overall performance, in other\\nwords, to estimate the partial derivative of an objective function with respect to each\\nweight, given the current values of all the network’s weights. The gradient is the vector\\nof these partial derivatives.\\nThe most successful way to do this for ANNs with hidden layers (provided the units\\nhave di↵erentiable activation functions) is the backpropagation algorithm, which consists\\nof alternating forward and backward passes through the network. Each forward pass\\ncomputes the activation of each unit given the current activations of the network’s input\\nunits. After each forward pass, a backward pass e\\x00ciently computes a partial derivative\\nfor each weight. (As in other stochastic gradient learning algorithms, the vector of these\\npartial derivatives is an estimate of the true gradient.) In Section 15.10 we discuss\\nmethods for training ANNs with hidden layers that use reinforcement learning principles\\ninstead of backpropagation. These methods are less e\\x00cient than the backpropagation\\nalgorithm, but they may be closer to how real neural networks learn.\\nThe backpropagation algorithm can produce good results for shallow networks having\\n1 or 2 hidden layers, but it may not work well for deeper ANNs. In fact, training a\\nnetwork withk + 1 hidden layers can actually result in poorer performance than training\\na network withk hidden layers, even though the deeper network can represent all the\\nfunctions that the shallower network can (Bengio, 2009). Explaining results like these\\nis not easy, but several factors are important. First, the large number of weights in\\na typical deep ANN makes it di\\x00cult to avoid the problem of overﬁtting, that is, the\\nproblem of failing to generalize correctly to cases on which the network has not been\\ntrained. Second, backpropagation does not work well for deep ANNs because the partial\\nderivatives computed by its backward passes either decay rapidly toward the input side\\nof the network, making learning by deep layers extremely slow, or the partial derivatives\\ngrow rapidly toward the input side of the network, making learning unstable. Methods\\nfor dealing with these problems are largely responsible for many impressive recent results'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 247, 'page_label': '248'}, page_content='226 Chapter 9: On-policy Prediction with Approximation\\nachieved by systems that use deep ANNs.\\nOverﬁtting is a problem for any function approximation method that adjusts functions\\nwith many degrees of freedom on the basis of limited training data. It is less of a\\nproblem for online reinforcement learning that does not rely on limited training sets, but\\ngeneralizing e↵ectively is still an important issue. Overﬁtting is a problem for ANNs in\\ngeneral, but especially so for deep ANNs because they tend to have very large numbers\\nof weights. Many methods have been developed for reducing overﬁtting. These include\\nstopping training when performance begins to decrease on validation data di↵erent\\nfrom the training data (cross validation), modifying the objective function to discourage\\ncomplexity of the approximation (regularization), and introducing dependencies among\\nthe weights to reduce the number of degrees of freedom (e.g., weight sharing).\\nA particularly e↵ective method for reducing overﬁtting by deep ANNs is the dropout\\nmethod introduced by Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov\\n(2014). During training, units are randomly removed from the network (dropped out)\\nalong with their connections. This can be thought of as training a large number of\\n“thinned” networks. Combining the results of these thinned networks at test time is a way\\nto improve generalization performance. The dropout method e\\x00ciently approximates this\\ncombination by multiplying each outgoing weight of a unit by the probability that that\\nunit was retained during training. Srivastava et al. found that this method signiﬁcantly\\nimproves generalization performance. It encourages individual hidden units to learn\\nfeatures that work well with random collections of other features. This increases the\\nversatility of the features formed by the hidden units so that the network does not overly\\nspecialize to rarely-occurring cases.\\nHinton, Osindero, and Teh (2006) took a major step toward solving the problem of\\ntraining the deep layers of a deep ANN in their work with deep belief networks, layered\\nnetworks closely related to the deep ANNs discussed here. In their method, the deepest\\nlayers are trained one at a time using an unsupervised learning algorithm. Without\\nrelying on the overall objective function, unsupervised learning can extract features that\\ncapture statistical regularities of the input stream. The deepest layer is trained ﬁrst, then\\nwith input provided by this trained layer, the next deepest layer is trained, and so on,\\nuntil the weights in all, or many, of the network’s layers are set to values that now act as\\ninitial values for supervised learning. The network is then ﬁne-tuned by backpropagation\\nwith respect to the overall objective function. Studies show that this approach generally\\nworks much better than backpropagation with weights initialized with random values.\\nThe better performance of networks trained with weights initialized this way could be\\ndue to many factors, but one idea is that this method places the network in a region of\\nweight space from which a gradient-based algorithm can make good progress.\\nBatch normalization (Io↵e and Szegedy, 2015) is another technique that makes it easier\\nto train deep ANNs. It has long been known that ANN learning is easier if the network\\ninput is normalized, for example, by adjusting each input variable to have zero mean and\\nunit variance. Batch normalization for training deep ANNs normalizes the output of deep\\nlayers before they feed into the following layer. Io↵e and Szegedy (2015) used statistics\\nfrom subsets, or “mini-batches,” of training examples to normalize these between-layer\\nsignals to improve the learning rate of deep ANNs.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 248, 'page_label': '249'}, page_content='9.7. Nonlinear Function Approximation: Artiﬁcial Neural Networks 227\\nAnother technique useful for training deep ANNs isdeep residual learning(He, Zhang,\\nRen, and Sun, 2016). Sometimes it is easier to learn how a function di↵ers from the\\nidentity function than to learn the function itself. Then adding this di↵erence, or residual\\nfunction, to the input produces the desired function. In deep ANNs, a block of layers\\ncan be made to learn a residual function simply by adding shortcut, or skip, connections\\naround the block. These connections add the input to the block to its output, and\\nno additional weights are needed. He et al. (2016) evaluated this method using deep\\nconvolutional networks with skip connections around every pair of adjacent layers, ﬁnding\\nsubstantial improvement over networks without the skip connections on benchmark image\\nclassiﬁcation tasks. Both batch normalization and deep residual learning were used in\\nthe reinforcement learning application to the game of Go that we describe in Chapter 16.\\nA type of deep ANN that has proven to be very successful in applications, including\\nimpressive reinforcement learning applications (Chapter 16), is thedeep convolutional\\nnetwork. This type of network is specialized for processing high-dimensional data arranged\\nin spatial arrays, such as images. It was inspired by how early visual processing works in\\nthe brain (LeCun, Bottou, Bengio and Ha↵ner, 1998). Because of its special architecture,\\na deep convolutional network can be trained by backpropagation without resorting to\\nmethods like those described above to train the deep layers.\\nFigure 9.15 illustrates the architecture of a deep convolutional network. This instance,\\nfrom LeCun et al. (1998), was designed to recognize hand-written characters. It consists\\nof alternating convolutional and subsampling layers, followed by several fully connected\\nﬁnal layers. Each convolutional layer produces a number of feature maps. A feature\\nmap is a pattern of activity over an array of units, where each unit performs the same\\noperation on data in its receptive ﬁeld, which is the part of the data it “sees” from the\\npreceding layer (or from the external input in the case of the ﬁrst convolutional layer).\\nThe units of a feature map are identical to one another except that their receptive ﬁelds,\\nwhich are all the same size and shape, are shifted to di↵erent locations on the arrays\\nof incoming data. Units in the same feature map share the same weights. This means\\nthat a feature map detects the same feature no matter where it is located in the input\\nFigure 9.15: Deep Convolutional Network. Republished with permission of Proceedings of the\\nIEEE, from Gradient-based learning applied to document recognition, LeCun, Bottou, Bengio,\\nand Ha↵ner, volume 86, 1998; permission conveyed through Copyright Clearance Center, Inc.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 249, 'page_label': '250'}, page_content='228 Chapter 9: On-policy Prediction with Approximation\\narray. In the network in Figure 9.15, for example, the ﬁrst convolutional layer produces\\n6 feature maps, each consisting of 28⇥ 28 units. Each unit in each feature map has a\\n5 ⇥ 5 receptive ﬁeld, and these receptive ﬁelds overlap (in this case by four columns and\\nfour rows). Consequently, each of the 6 feature maps is speciﬁed by just 25 adjustable\\nweights.\\nThe subsampling layers of a deep convolutional network reduce the spatial resolution of\\nthe feature maps. Each feature map in a subsampling layer consists of units that average\\nover a receptive ﬁeld of units in the feature maps of the preceding convolutional layer.\\nFor example, each unit in each of the 6 feature maps in the ﬁrst subsampling layer of the\\nnetwork of Figure 9.15 averages over a 2⇥ 2 non-overlapping receptive ﬁeld over one of\\nthe feature maps produced by the ﬁrst convolutional layer, resulting in six 14⇥ 14 feature\\nmaps. Subsampling layers reduce the network’s sensitivity to the spatial locations of the\\nfeatures detected, that is, they help make the network’s responses spatially invariant.\\nThis is useful because a feature detected at one place in an image is likely to be useful at\\nother places as well.\\nAdvances in the design and training of ANNs—of which we have only mentioned a\\nfew—all contribute to reinforcement learning. Although current reinforcement learning\\ntheory is mostly limited to methods using tabular or linear function approximation\\nmethods, the impressive performances of notable reinforcement learning applications owe\\nmuch of their success to nonlinear function approximation by multi-layer ANNs. We\\ndiscuss several of these applications in Chapter 16.\\n9.8 Least-Squares TD\\nAll the methods we have discussed so far in this chapter have required computation per\\ntime step proportional to the number of parameters. With more computation, however,\\none can do better. In this section we present a method for linear function approximation\\nthat is arguably the best that can be done for this case.\\nAs we established in Section 9.4 TD(0) with linear function approximation converges\\nasymptotically (for appropriately decreasing step sizes) to the TD ﬁxed point:\\nwTD = A\\x001b,\\nwhere\\nA .= E\\n⇥\\nxt(xt \\x00 \\x00xt+1)>⇤\\nand b .= E[Rt+1xt] .\\nWhy, one might ask, must we compute this solution iteratively? This is wasteful of data!\\nCould one not do better by computing estimates ofA and b, and then directly computing\\nthe TD ﬁxed point? TheLeast-Squares TDalgorithm, commonly known asLSTD,d o e s\\nexactly this. It forms the natural estimates\\nbAt\\n.=\\nt\\x001X\\nk=0\\nxk(xk \\x00 \\x00xk+1)> + \"I and bbt\\n.=\\nt\\x001X\\nk=0\\nRk+1xk, (9.20)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 250, 'page_label': '251'}, page_content='9.8. Least-Squares TD 229\\nwhere I is the identity matrix, and\"I, for some small\"> 0, ensures thatbAt is always\\ninvertible. It might seem that these estimates should both be divided byt, and indeed\\nthey should; as deﬁned here, these are really estimates oft times A and t times b.\\nHowever, the extrat factors cancel out when LSTD uses these estimates to estimate the\\nTD ﬁxed point as\\nwt\\n.= bA\\x001\\nt bbt. (9.21)\\nThis algorithm is the most data e\\x00cient form of linear TD(0), but it is also more\\nexpensive computationally. Recall that semi-gradient TD(0) requires memory and per-\\nstep computation that is onlyO(d).\\nHow complex is LSTD? As it is written above the complexity seems to increase with\\nt, but the two approximations in(9.20) could be implemented incrementally using the\\ntechniques we have covered earlier (e.g., in Chapter 2) so that they can be done in\\nconstant time per step. Even so, the update forbAt would involve an outer product (a\\ncolumn vector times a row vector) and thus would be a matrix update; its computational\\ncomplexity would beO(d2), and of course the memory required to hold thebAt matrix\\nwould beO(d2).\\nA potentially greater problem is that our ﬁnal computation(9.21) uses the inverse\\nof bAt, and the computational complexity of a general inverse computation isO(d3).\\nFortunately, an inverse of a matrix of our special form—a sum of outer products—can\\nalso be updated incrementally with onlyO(d2) computations, as\\nbA\\x001\\nt =\\n⇣\\nbAt\\x001 + xt\\x001(xt\\x001 \\x00 \\x00xt)>\\n⌘\\x001\\n(from (9.20))\\n= bA\\x001\\nt\\x001 \\x00\\nbA\\x001\\nt\\x001xt\\x001(xt\\x001 \\x00 \\x00xt)> bA\\x001\\nt\\x001\\n1+( xt\\x001 \\x00 \\x00xt)> bA\\x001\\nt\\x001xt\\x001\\n, (9.22)\\nfor t> 0, with bA0\\n.= \"I. Although the identity(9.22), known asthe Sherman-Morrison\\nformula, is superﬁcially complicated, it involves only vector-matrix and vector-vector\\nmultiplications and thus is onlyO(d2). Thus we can store the inverse matrixbA\\x001\\nt ,\\nmaintain it with (9.22), and then use it in(9.21), all with only O(d2) memory and\\nper-step computation. The complete algorithm is given in the box on the next page.\\nOf course,O(d2) is still signiﬁcantly more expensive than theO(d) of semi-gradient\\nTD. Whether the greater data e\\x00ciency of LSTD is worth this computational expense\\ndepends on how larged is, how important it is to learn quickly, and the expense of other\\nparts of the system. The fact that LSTD requires no step-size parameter is sometimes\\nalso touted, but the advantage of this is probably overstated. LSTD does not require a\\nstep size, but it does requires\";i f \" is chosen too small the sequence of inverses can vary\\nwildly, and if\" is chosen too large then learning is slowed. In addition, LSTD’s lack of a\\nstep-size parameter means that it never forgets. This is sometimes desirable, but it is\\nproblematic if the target policy⇡ changes as it does in reinforcement learning and GPI.\\nIn control applications, LSTD typically has to be combined with some other mechanism\\nto induce forgetting, mooting any initial advantage of not requiring a step-size parameter.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 251, 'page_label': '252'}, page_content='230 Chapter 9: On-policy Prediction with Approximation\\nLSTD for estimatingˆv = w>x(· ) ⇡ v⇡ (O(d2) version)\\nInput: feature representationx : S+ ! Rd such thatx(terminal)= 0\\nAlgorithm parameter: small\"> 0\\ndA\\x001  \"\\x001I A d ⇥ d matrix\\nbb  0 A d-dimensional vector\\nLoop for each episode:\\nInitialize S; x  x(S)\\nLoop for each step of episode:\\nChoose and take actionA ⇠ ⇡(·| S), observeR, S0; x0  x(S0)\\nv  dA\\x001>\\n(x \\x00 \\x00x0)\\ndA\\x001  dA\\x001 \\x00\\n\\x00 dA\\x001x\\n\\x00\\nv>/\\n\\x00\\n1+ v>x\\n\\x00\\nbb  bb + Rx\\nw  dA\\x001bb\\nS  S0; x  x0\\nuntil S0 is terminal\\n9.9 Memory-based Function Approximation\\nSo far we have discussed theparametric approach to approximating value functions. In\\nthis approach, a learning algorithm adjusts the parameters of a functional form intended\\nto approximate the value function over a problem’s entire state space. Each update,\\ns 7! g, is a training example used by the learning algorithm to change the parameters\\nwith the aim of reducing the approximation error. After the update, the training example\\ncan be discarded (although it might be saved to be used again). When an approximate\\nvalue of a state (which we will call thequery state) is needed, the function is simply\\nevaluated at that state using the latest parameters produced by the learning algorithm.\\nMemory-based function approximation methods are very di↵erent. They simply save\\ntraining examples in memory as they arrive (or at least save a subset of the examples)\\nwithout updating any parameters. Then, whenever a query state’s value estimate is\\nneeded, a set of examples is retrieved from memory and used to compute a value estimate\\nfor the query state. This approach is sometimes calledlazy learning because processing\\ntraining examples is postponed until the system is queried to provide an output.\\nMemory-based function approximation methods are prime examples ofnonparametric\\nmethods. Unlike parametric methods, the approximating function’s form is not limited\\nto a ﬁxed parameterized class of functions, such as linear functions or polynomials, but is\\ninstead determined by the training examples themselves, together with some means for\\ncombining them to output estimated values for query states. As more training examples\\naccumulate in memory, one expects nonparametric methods to produce increasingly\\naccurate approximations of any target function.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 252, 'page_label': '253'}, page_content='9.9. Memory-based Function Approximation 231\\nThere are many di↵erent memory-based methods depending on how the stored training\\nexamples are selected and how they are used to respond to a query. Here, we focus on\\nlocal-learning methods that approximate a value function only locally in the neighborhood\\nof the current query state. These methods retrieve a set of training examples from memory\\nwhose states are judged to be the most relevant to the query state, where relevance\\nusually depends on the distance between states: the closer a training example’s state is\\nto the query state, the more relevant it is considered to be, where distance can be deﬁned\\nin many di↵erent ways. After the query state is given a value, the local approximation is\\ndiscarded.\\nThe simplest example of the memory-based approach is thenearest neighbormethod,\\nwhich simply ﬁnds the example in memory whose state is closest to the query state and\\nreturns that example’s value as the approximate value of the query state. In other words,\\nif the query state iss, and s0 7! g is the example in memory in whichs0 is the closest\\nstate tos,t h e ng is returned as the approximate value ofs. Slightly more complicated\\nare weighted averagemethods that retrieve a set of nearest neighbor examples and return\\na weighted average of their target values, where the weights generally decrease with\\nincreasing distance between their states and the query state.Locally weighted regressionis\\nsimilar, but it ﬁts a surface to the values of a set of nearest states by means of a parametric\\napproximation method that minimizes a weighted error measure like (9.1), where the\\nweights depend on distances from the query state. The value returned is the evaluation of\\nthe locally-ﬁtted surface at the query state, after which the local approximation surface\\nis discarded.\\nBeing nonparametric, memory-based methods have the advantage over parametric\\nmethods of not limiting approximations to pre-speciﬁed functional forms. This allows\\naccuracy to improve as more data accumulates. Memory-basedlocal approximation\\nmethods have other properties that make them well suited for reinforcement learning.\\nBecause trajectory sampling is of such importance in reinforcement learning, as discussed\\nin Section 8.6, memory-based local methods can focus function approximation on local\\nneighborhoods of states (or state–action pairs) visited in real or simulated trajectories.\\nThere may be no need for global approximation because many areas of the state space will\\nnever (or almost never) be reached. In addition, memory-based methods allow an agent’s\\nexperience to have a relatively immediate a↵ect on value estimates in the neighborhood\\nof the current state, in contrast with a parametric method’s need to incrementally adjust\\nparameters of a global approximation.\\nAvoiding global approximation is also a way to address the curse of dimensionality.\\nFor example, for a state space withk dimensions, a tabular method storing a global\\napproximation requires memory exponential ink. On the other hand, in storing examples\\nfor a memory-based method, each example requires memory proportional tok, and the\\nmemory required to store, say,n examples is linear inn. Nothing is exponential ink or\\nn. Of course, the critical remaining issue is whether a memory-based method can answer\\nqueries quickly enough to be useful to an agent. A related concern is how speed degrades\\nas the size of the memory grows. Finding nearest neighbors in a large database can take\\ntoo long to be practical in many applications.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 253, 'page_label': '254'}, page_content='232 Chapter 9: On-policy Prediction with Approximation\\nProponents of memory-based methods have developed ways to accelerate the nearest\\nneighbor search. Using parallel computers or special purpose hardware is one approach;\\nanother is the use of special multi-dimensional data structures to store the training data.\\nOne data structure studied for this application is thek-d tree (short fork-dimensional\\ntree), which recursively splits ak-dimensional space into regions arranged as nodes of a\\nbinary tree. Depending on the amount of data and how it is distributed over the state\\nspace, nearest-neighbor search usingk-d trees can quickly eliminate large regions of the\\nspace in the search for neighbors, making the searches feasible in some problems where\\nnaive searches would take too long.\\nLocally weighted regression additionally requires fast ways to do the local regression\\ncomputations which have to be repeated to answer each query. Researchers have developed\\nmany ways to address these problems, including methods for forgetting entries in order to\\nkeep the size of the database within bounds. The Bibliographic and Historical Comments\\nsection at the end of this chapter points to some of the relevant literature, including a\\nselection of papers describing applications of memory-based learning to reinforcement\\nlearning.\\n9.10 Kernel-based Function Approximation\\nMemory-based methods such as the weighted average and locally weighted regression\\nmethods described above depend on assigning weights to exampless0 7! g in the database\\ndepending on the distance betweens0 and a query statess. The function that assigns\\nthese weights is called akernel function, or simply akernel. In the weighted average and\\nlocally weighted regressions methods, for example, a kernel functionk : R ! R assigns\\nweights to distances between states. More generally, weights do not have to depend on\\ndistances; they can depend on some other measure of similarity between states. In this\\ncase, k : S ⇥ S ! R, so thatk(s, s0) is the weight given to data abouts0 in its inﬂuence\\non answering queries abouts.\\nViewed slightly di↵erently,k(s, s0) is a measure of the strength of generalization from\\ns0 to s. Kernel functions numerically express howrelevant knowledge about any state\\nis to any other state. As an example, the strengths of generalization for tile coding\\nshown in Figure 9.11 correspond to di↵erent kernel functions resulting from uniform and\\nasymmetrical tile o↵sets. Although tile coding does not explicitly use a kernel function\\nin its operation, it generalizes according to one. In fact, as we discuss more below, the\\nstrength of generalization resulting from linear parametric function approximation can\\nalways be described by a kernel function.\\nKernel regressionis the memory-based method that computes a kernel weighted average\\nof the targets ofall examples stored in memory, assigning the result to the query state.\\nIf D is the set of stored examples, andg(s0) denotes the target for states0 in a stored\\nexample, then kernel regression approximates the target function, in this case a value\\nfunction depending onD, as\\nˆv(s,D)=\\nX\\ns02D\\nk(s, s0)g(s0). (9.23)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 254, 'page_label': '255'}, page_content='9.10. Kernel-based Function Approximation 233\\nThe weighted average method described above is a special case in whichk(s, s0) is non-zero\\nonly whens and s0 are close to one another so that the sum need not be computed over\\nall ofD.\\nA common kernel is the Gaussian radial basis function (RBF) used in RBF function\\napproximation as described in Section 9.5.5. In the method described there, RBFs are\\nfeatures whose centers and widths are either ﬁxed from the start, with centers presumably\\nconcentrated in areas where many examples are expected to fall, or are adjusted in some\\nway during learning. Barring methods that adjust centers and widths, this is a linear\\nparametric method whose parameters are the weights of each RBF, which are typically\\nlearned by stochastic gradient, or semi-gradient, descent. The form of the approximation\\nis a linear combination of the pre-determined RBFs. Kernel regression with an RBF\\nkernel di↵ers from this in two ways. First, it is memory-based: the RBFs are centered on\\nthe states of the stored examples. Second, it is nonparametric: there are no parameters\\nto learn; the response to a query is given by (9.23).\\nOf course, many issues have to be addressed for practical implementation of kernel\\nregression, issues that are beyond the scope of our brief discussion. However, it turns out\\nthat any linear parametric regression method like those we described in Section 9.4, with\\nstates represented by feature vectorsx(s)=( x1(s),x 2(s),...,x d(s))>, can be recast as\\nkernel regression wherek(s, s0) is the inner product of the feature vector representations\\nof s and s0; that is\\nk(s, s0)= x(s)>x(s0). (9.24)\\nKernel regression with this kernel function produces the same approximation that a linear\\nparametric method would if it used these feature vectors and learned with the same\\ntraining data.\\nWe skip the mathematical justiﬁcation for this, which can be found in any modern\\nmachine learning text, such as Bishop (2006), and simply point out an important\\nimplication. Instead of constructing features for linear parametric function approximators,\\none can instead construct kernel functions directly without referring at all to feature\\nvectors. Not all kernel functions can be expressed as inner products of feature vectors\\nas in (9.24), but a kernel function that can be expressed like this can o↵er signiﬁcant\\nadvantages over the equivalent parametric method. For many sets of feature vectors,\\n(9.24) has a compact functional form that can be evaluated without any computation\\ntaking place in thed-dimensional feature space. In these cases, kernel regression is much\\nless complex than directly using a linear parametric method with states represented by\\nthese feature vectors. This is the so-called “kernel trick” that allows e↵ectively working\\nin the high-dimension of an expansive feature space while actually working only with the\\nset of stored training examples. The kernel trick is the basis of many machine learning\\nmethods, and researchers have shown how it can sometimes beneﬁt reinforcement learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 255, 'page_label': '256'}, page_content='234 Chapter 9: On-policy Prediction with Approximation\\n9.11 Looking Deeper at On-policy Learning:\\nInterest and Emphasis\\nThe algorithms we have considered so far in this chapter have treated all the states\\nencountered equally, as if they were all equally important. In some cases, however, we\\nare more interested in some states than others. In discounted episodic problems, for\\nexample, we may be more interested in accurately valuing early states in the episode\\nthan in later states where discounting may have made the rewards much less important\\nto the value of the start state. Or, if an action-value function is being learned, it may be\\nless important to accurately value poor actions whose value is much less than the greedy\\naction. Function approximation resources are always limited, and if they were used in a\\nmore targeted way, then performance could be improved.\\nOne reason we have treated all states encountered equally is that then we are updating\\naccording to the on-policy distribution, for which stronger theoretical results are available\\nfor semi-gradient methods. Recall that the on-policy distribution was deﬁned as the\\ndistribution of states encountered in an MDP while following the target policy. Now we\\nwill generalize this concept signiﬁcantly. Rather than having one on-policy distribution\\nfor the MDP, we will have many. All of them will have in common that they are a\\ndistribution of states encountered in trajectories while following the target policy, but\\nthey will vary in how the trajectories are, in a sense, initiated.\\nWe now introduce some new concepts. First we introduce a non-negative scalar measure,\\na random variableIt called interest, indicating the degree to which we are interested in\\naccurately valuing the state (or state–action pair) at timet. If we don’t care at all about\\nthe state, then the interest should be zero; if we fully care, it might be one, though it is\\nformally allowed to take any non-negative value. The interest can be set in any causal\\nway; for example, it may depend on the trajectory up to timet or the learned parameters\\nat time t. The distribution µ in the VE (9.1) is then deﬁned as the distribution of\\nstates encountered while following the target policy, weighted by the interest. Second, we\\nintroduce another non-negative scalar random variable, theemphasis Mt. This scalar\\nmultiplies the learning update and thus emphasizes or de-emphasizes the learning done\\nat timet. The generaln-step learning rule, replacing (9.15), is\\nwt+n\\n.= wt+n\\x001 +↵Mt [Gt:t+n \\x00 ˆv(St,wt+n\\x001)] rˆv(St,wt+n\\x001), 0 \\uf8ff t<T , (9.25)\\nwith the n-step return given by(9.16) and the emphasis determined recursively from the\\ninterest by:\\nMt = It + \\x00nMt\\x00n, 0 \\uf8ff t<T , (9.26)\\nwith Mt\\n.= 0, for allt< 0. These equations are taken to include the Monte Carlo case,\\nfor whichGt:t+n = Gt, all the updates are made at end of the episode,n = T \\x00 t, and\\nMt = It.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 256, 'page_label': '257'}, page_content='9.11. Looking Deeper at On-policy Learning: Interest and Emphasis 235\\nExample 9.4 illustrates how interest and emphasis can result in more accurate value\\nestimates.\\nExample 9.4: Interest and Emphasis\\nTo see the potential beneﬁts of using interest and emphasis, consider the four-state\\nMarkov reward process shown below:\\n+1 +1 +1+1\\nv⇡=4 v⇡=3 v⇡=2 v⇡=1\\ni=1 i=0 i=0 i=0\\nw1 w1 w2 w2\\nEpisodes start in the leftmost state, then transition one state to the right, with a\\nreward of +1, on each step until the terminal state is reached. The true value of\\nthe ﬁrst state is thus 4, of the second state 3, and so on as shown below each state.\\nThese are the true values; the estimated values can only approximate these because\\nthey are constrained by the parameterization. There are two components to the\\nparameter vector w =( w1,w 2)>, and the parameterization is as written inside\\neach state. The estimated values of the ﬁrst two states are given byw1 alone and\\nthus must be the same even though their true values are di↵erent. Similarly, the\\nestimated values of the third and fourth states are given byw2 alone and must be\\nthe same even though their true values are di↵erent. Suppose that we are interested\\nin accurately valuing only the leftmost state; we assign it an interest of 1 while all\\nthe other states are assigned an interest of 0, as indicated above the states.\\nFirst consider applying gradient Monte Carlo algorithms to this problem. The\\nalgorithms presented earlier in this chapter that do not take into account interest\\nand emphasis (in(9.7) and the box on page 202) will converge (for decreasing step\\nsizes) to the parameter vectorw1 =( 3.5, 1.5), which gives the ﬁrst state—the only\\none we are interested in—a value of 3.5 (i.e., intermediate between the true values\\nof the ﬁrst and second states). The methods presented in this section that do use\\ninterest and emphasis, on the other hand, will learn the value of the ﬁrst state\\nexactly correctly;w1 will converge to 4 whilew2 will never be updated because the\\nemphasis is zero in all states save the leftmost.\\nNow consider applying two-step semi-gradient TD methods. The methods from\\nearlier in this chapter without interest and emphasis (in(9.15) and (9.16) and\\nthe box on page 209) will again converge tow1 =( 3.5, 1.5), while the methods\\nwith interest and emphasis converge tow1 =( 4, 2). The latter produces the\\nexactly correct values for the ﬁrst state and for the third state (which the ﬁrst state\\nbootstraps from) while never making any updates corresponding to the second or\\nfourth states.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 257, 'page_label': '258'}, page_content='236 Chapter 9: On-policy Prediction with Approximation\\n9.12 Summary\\nReinforcement learning systems must be capable ofgeneralization if they are to be\\napplicable to artiﬁcial intelligence or to large engineering applications. To achieve this,\\nany of a broad range of existing methods forsupervised-learning function approximation\\ncan be used simply by treating each update as a training example.\\nPerhaps the most suitable supervised learning methods are those usingparameterized\\nfunction approximation, in which the policy is parameterized by a weight vectorw.\\nAlthough the weight vector has many components, the state space is much larger still,\\nand we must settle for an approximate solution. We deﬁned themean square value error,\\nVE(w), as a measure of the error in the valuesv⇡w(s) for a weight vectorw under the\\non-policy distribution, µ.T h eVE gives us a clear way to rank di↵erent value-function\\napproximations in the on-policy case.\\nTo ﬁnd a good weight vector, the most popular methods are variations ofstochastic\\ngradient descent(SGD). In this chapter we have focused on theon-policy case with aﬁxed\\npolicy, also known as policy evaluation or prediction; a natural learning algorithm for this\\ncase isn-step semi-gradient TD, which includes gradient Monte Carlo and semi-gradient\\nTD(0) algorithms as the special cases whenn=1 and n=1 respectively. Semi-gradient\\nTD methods are not true gradient methods. In such bootstrapping methods (including\\nDP), the weight vector appears in the update target, yet this is not taken into account in\\ncomputing the gradient—thus they aresemi-gradient methods. As such, they cannot\\nrely on classical SGD results.\\nNevertheless, good results can be obtained for semi-gradient methods in the special case\\nof linear function approximation, in which the value estimates are sums of features times\\ncorresponding weights. The linear case is the most well understood theoretically and\\nworks well in practice when provided with appropriate features. Choosing the features\\nis one of the most important ways of adding prior domain knowledge to reinforcement\\nlearning systems. They can be chosen as polynomials, but this case generalizes poorly in\\nthe online learning setting typically considered in reinforcement learning. Better is to\\nchoose features according the Fourier basis, or according to some form of coarse coding\\nwith sparse overlapping receptive ﬁelds. Tile coding is a form of coarse coding that\\nis particularly computationally e\\x00cient and ﬂexible. Radial basis functions are useful\\nfor one- or two-dimensional tasks in which a smoothly varying response is important.\\nLSTD is the most data-e\\x00cient linear TD prediction method, but requires computation\\nproportional to the square of the number of weights, whereas all the other methods are of\\ncomplexity linear in the number of weights. Nonlinear methods include artiﬁcial neural\\nnetworks trained by backpropagation and variations of SGD; these methods have become\\nvery popular in recent years under the namedeep reinforcement learning.\\nLinear semi-gradientn-step TD is guaranteed to converge under standard conditions,\\nfor alln,t oa VE that is within a bound of the optimal error (achieved asymptotically\\nby Monte Carlo methods). This bound is always tighter for highern and approaches\\nzero as n !1 . However, in practice very highn results in very slow learning, and some\\ndegree of bootstrapping (n< 1) is usually preferable, just as we saw in comparisons of\\ntabular n-step methods in Chapter 7 and in comparisons of tabular TD and Monte Carlo\\nmethods in Chapter 6.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 258, 'page_label': '259'}, page_content='9.12. Summary 237\\nExercise 9.7 One of the simplest artiﬁcial neural networks consists of a single semi-linear\\nunit with a logistic nonlinearity. The need to handle approximate value functions of this\\nform is common in games that end with either a win or a loss, in which case the value of\\na state can be interpreted as the probability of winning. Derive the learning algorithm\\nfor this case, from (9.7), such that no gradient notation appears.\\n⇤Exercise 9.8 Arguably, the squared error used to derive(9.7) is inappropriate for the\\ncase treated in the preceding exercise, and the right error measure is thecross-entropy\\nloss (which you can ﬁnd on Wikipedia). Repeat the derivation in Section 9.3, using the\\ncross-entropy loss instead of the squared error in(9.4), all the way to an explicit form\\nwith no gradient or logarithm notation in it. Is your ﬁnal form more complex, or simpler,\\nthan that you obtained in the preceding exercise?\\nBibliographical and Historical Remarks\\nGeneralization and function approximation have always been an integral part of rein-\\nforcement learning. Bertsekas and Tsitsiklis (1996), Bertsekas (2012), and Sugiyama et\\nal. (2013) present the state of the art in function approximation in reinforcement learning.\\nSome of the early work with function approximation in reinforcement learning is discussed\\nat the end of this section.\\n9.3 Gradient-descent methods for minimizing mean square error in supervised learning\\nare well known. Widrow and Ho↵ (1960) introduced the least-mean-square (LMS)\\nalgorithm, which is the prototypical incremental gradient-descent algorithm.\\nDetails of this and related algorithms are provided in many texts (e.g., Widrow\\nand Stearns, 1985; Bishop, 1995; Duda and Hart, 1973).\\nSemi-gradient TD(0) was ﬁrst explored by Sutton (1984, 1988), as part of the\\nlinear TD(\\x00) algorithm that we will treat in Chapter 12. The term “semi-gradient”\\nto describe these bootstrapping methods is new to the second edition of this\\nbook.\\nThe earliest use of state aggregation in reinforcement learning may have been\\nMichie and Chambers’s BOXES system (1968). The theory of state aggregation\\nin reinforcement learning has been developed by Singh, Jaakkola, and Jordan\\n(1995) and Tsitsiklis and Van Roy (1996). State aggregation has been used in\\ndynamic programming from its earliest days (e.g., Bellman, 1957a).\\n9.4 Sutton (1988) proved convergence of linear TD(0) in the mean to the minimal\\nVE solution for the case in which the feature vectors,{x(s): s 2 S}, are linearly\\nindependent. Convergence with probability 1 was proved by several researchers\\nat about the same time (Peng, 1993; Dayan and Sejnowski, 1994; Tsitsiklis,\\n1994; Gurvits, Lin, and Hanson, 1994). In addition, Jaakkola, Jordan, and Singh\\n(1994) proved convergence under online updating. All of these results assumed\\nlinearly independent feature vectors, which implies at least as many components\\nto wt as there are states. Convergence for the more important case of general\\n(dependent) feature vectors was ﬁrst shown by Dayan (1992). A signiﬁcant'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 259, 'page_label': '260'}, page_content='238 Chapter 9: On-policy Prediction with Approximation\\ngeneralization and strengthening of Dayan’s result was proved by Tsitsiklis and\\nVan Roy (1997). They proved the main result presented in this section, the\\nbound on the asymptotic error of linear bootstrapping methods.\\n9.5 Our presentation of the range of possibilities for linear function approximation is\\nbased on that by Barto (1990).\\n9.5.2 Konidaris, Osentoski, and Thomas (2011) introduced the Fourier basis in a\\nsimple form suitable for reinforcement learning problems with multi-dimensional\\ncontinuous state spaces and functions that do not have to be periodic.\\n9.5.3 The termcoarse codingis due to Hinton (1984), and our Figure 9.6 is based on\\none of his ﬁgures. Waltz and Fu (1965) provide an early example of this type of\\nfunction approximation in a reinforcement learning system.\\n9.5.4 Tile coding, including hashing, was introduced by Albus (1971, 1981). He de-\\nscribed it in terms of his “cerebellar model articulator controller,” or CMAC, as\\ntile coding is sometimes known in the literature. The term “tile coding” was new\\nto the ﬁrst edition of this book, though the idea of describing CMAC in these\\nterms is taken from Watkins (1989). Tile coding has been used in many rein-\\nforcement learning systems (e.g., Shewchuk and Dean, 1990; Lin and Kim, 1991;\\nMiller, Scalera, and Kim, 1994; Sofge and White, 1992; Tham, 1994; Sutton, 1996;\\nWatkins, 1989) as well as in other types of learning control systems (e.g., Kraft and\\nCampagna, 1990; Kraft, Miller, and Dietz, 1992). This section draws heavily on\\nthe work of Miller and Glanz (1996). General software for tile coding is available in\\nseveral languages (e.g., seehttp://incompleteideas.net/tiles/tiles3.html).\\n9.5.5 Function approximation using radial basis functions has received wide attention\\never since being related to ANNs by Broomhead and Lowe (1988). Powell (1987)\\nreviewed earlier uses of RBFs, and Poggio and Girosi (1989, 1990) extensively\\ndeveloped and applied this approach.\\n9.6 Automatic methods for adapting the step-size parameter include RMSprop (Tiele-\\nman and Hinton, 2012), Adam (Kingma and Ba, 2015), stochastic meta-descent\\nmethods such as Delta-Bar-Delta (Jacobs, 1988), its incremental generaliza-\\ntion (Sutton, 1992b, c; Mahmood et al., 2012), and nonlinear generalizations\\n(Schraudolph, 1999, 2002). Methods explicitly designed for reinforcement learn-\\ning include AlphaBound (Dabney and Barto, 2012), SID and NOSID (Dabney,\\n2014), TIDBD (Kearney et al., in preparation) and the application of stochastic\\nmeta-descent to policy gradient learning (Schraudolph, Yu, and Aberdeen, 2006).\\n9.7 The introduction of the threshold logic unit as an abstract model neuron by\\nMcCulloch and Pitts (1943) was the beginning of ANNs. The history of ANNs as\\nlearning methods for classiﬁcation or regression has passed through several stages:\\nroughly, the Perceptron (Rosenblatt, 1962) and ADALINE (ADAptive LINear\\nElement) (Widrow and Ho↵, 1960) stage of learning by single-layer ANNs, the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 260, 'page_label': '261'}, page_content='9.12. Summary 239\\nerror-backpropagation stage (LeCun, 1985; Rumelhart, Hinton, and Williams,\\n1986) of learning by multi-layer ANNs, and the current deep-learning stage with\\nits emphasis on representation learning (e.g., Bengio, Courville, and Vincent,\\n2012; Goodfellow, Bengio, and Courville, 2016). Examples of the many books on\\nANNs are Haykin (1994), Bishop (1995), and Ripley (2007).\\nANNs as function approximation for reinforcement learning goes back to the early\\nwork of Farley and Clark (1954), who used reinforcement-like learning to modify\\nthe weights of linear threshold functions representing policies. Widrow, Gupta,\\nand Maitra (1973) presented a neuron-like linear threshold unit implementing a\\nlearning process they calledlearning with a criticor selective bootstrap adaptation,\\na reinforcement-learning variant of the ADALINE algorithm. Werbos (1987,\\n1994) developed an approach to prediction and control that uses ANNs trained by\\nerror backpropation to learn policies and value functions using TD-like algorithms.\\nBarto, Sutton, and Brouwer (1981) and Barto and Sutton (1981b) extended the\\nidea of an associative memory network (e.g., Kohonen, 1977; Anderson, Silverstein,\\nRitz, and Jones, 1977) to reinforcement learning. Barto, Anderson, and Sutton\\n(1982) used a two-layer ANN to learn a nonlinear control policy, and emphasized\\nthe ﬁrst layer’s role of learning a suitable representation. Hampson (1983, 1989)\\nwas an early proponent of multilayer ANNs for learning value functions. Barto,\\nSutton, and Anderson (1983) presented an actor–critic algorithm in the form of an\\nANN learning to balance a simulated pole (see Sections 15.7 and 15.8). Barto and\\nAnandan (1985) introduced a stochastic version of Widrow et al.’s (1973) selective\\nbootstrap algorithm called theassociative reward-penalty (AR\\x00P ) algorithm.\\nBarto (1985, 1986) and Barto and Jordan (1987) described multi-layer ANNs\\nconsisting of AR\\x00P units trained with a globally-broadcast reinforcement signal\\nto learn classiﬁcation rules that are not linearly separable. Barto (1985) discussed\\nthis approach to ANNs and how this type of learning rule is related to others in\\nthe literature at that time. (See Section 15.10 for additional discussion of this\\napproach to training multi-layer ANNs.) Anderson (1986, 1987, 1989) evaluated\\nnumerous methods for training multilayer ANNs and showed that an actor–critic\\nalgorithm in which both the actor and critic were implemented by two-layer\\nANNs trained by error backpropagation outperformed single-layer ANNs in the\\npole-balancing and tower of Hanoi tasks. Williams (1988) described several ways\\nthat backpropagation and reinforcement learning can be combined for training\\nANNs. Gullapalli (1990) and Williams (1992) devised reinforcement learning\\nalgorithms for neuron-like units having continuous, rather than binary, outputs.\\nBarto, Sutton, and Watkins (1990) argued that ANNs can play signiﬁcant roles\\nfor approximating functions required for solving sequential decision problems.\\nWilliams (1992) related REINFORCE learning rules (Section 13.3) to the error\\nbackpropagation method for training multi-layer ANNs. Tesauro’s TD-Gammon\\n(Tesauro 1992, 1994; Section 16.1) inﬂuentially demonstrated the learning abilities\\nof TD(\\x00) algorithm with function approximation by multi-layer ANNs in learning\\nto play backgammon. TheAlphaGo, AlphaGo Zero, and AlphaZero programs\\nof Silver et al. (2016, 2017a, b; Section 16.6) used reinforcement learning with'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 261, 'page_label': '262'}, page_content='240 Chapter 9: On-policy Prediction with Approximation\\ndeep convolutional ANNs in achieving impressive results with the game of Go.\\nSchmidhuber (2015) reviews applications of ANNs in reinforcement learning,\\nincluding applications of recurrent ANNs.\\n9.8 LSTD is due to Bradtke and Barto (see Bradtke, 1993, 1994; Bradtke and Barto,\\n1996; Bradtke, Ydstie, and Barto, 1994), and was further developed by Boyan\\n(1999, 2002), Nedi´ c and Bertsekas (2003), and Yu (2010). The incremental update\\nof the inverse matrix has been known at least since 1949 (Sherman and Morrison,\\n1949). An extension of least-squares methods to control was introduced by\\nLagoudakis and Parr (2003; Bu¸ soniu, Lazaric, Ghavamzadeh, Munos, Babu˘ ska,\\nand De Schutter, 2012).\\n9.9 Our discussion of memory-based function approximation is largely based on\\nthe review of locally weighted learning by Atkeson, Moore, and Schaal (1997).\\nAtkeson (1992) discussed the use of locally weighted regression in memory-based\\nrobot learning and supplied an extensive bibliography covering the history of\\nthe idea. Stanﬁll and Waltz (1986) inﬂuentially argued for the importance of\\nmemory based methods in artiﬁcial intelligence, especially in light of parallel\\narchitectures then becoming available, such as the Connection Machine. Baird\\nand Klopf (1993) introduced a novel memory-based approach and used it as the\\nfunction approximation method for Q-learning applied to the pole-balancing task.\\nSchaal and Atkeson (1994) applied locally weighted regression to a robot juggling\\ncontrol problem, where it was used to learn a system model. Peng (1995) used\\nthe pole-balancing task to experiment with several nearest-neighbor methods\\nfor approximating value functions, policies, and environment models. Tadepalli\\nand Ok (1996) obtained promising results with locally-weighted linear regression\\nto learn a value function for a simulated automatic guided vehicle task. Bottou\\nand Vapnik (1992) demonstrated surprising e\\x00ciency of several local learning\\nalgorithms compared to non-local algorithms in some pattern recognition tasks,\\ndiscussing the impact of local learning on generalization.\\nBentley (1975) introducedk-d trees and reported observing average running\\ntime of O(logn) for nearest neighbor search overn records. Friedman, Bentley,\\nand Finkel (1977) clariﬁed the algorithm for nearest neighbor search withk-d\\ntrees. Omohundro (1987) discussed e\\x00ciency gains possible with hierarchical\\ndata structures such ask-d-trees. Moore, Schneider, and Deng (1997) introduced\\nthe use ofk-d trees for e\\x00cient locally weighted regression.\\n9.10 The origin of kernel regression is themethod of potential functionsof Aizerman,\\nBraverman, and Rozonoer (1964). They likened the data to point electric charges\\nof various signs and magnitudes distributed over space. The resulting electric\\npotential over space produced by summing the potentials of the point charges\\ncorresponded to the interpolated surface. In this analogy, the kernel function is\\nthe potential of a point charge, which falls o↵ as the reciprocal of the distance\\nfrom the charge. Connell and Utgo↵ (1987) applied an actor–critic method\\nto the pole-balancing task in which the critic approximated the value function'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 262, 'page_label': '263'}, page_content='9.12. Summary 241\\nusing kernel regression with an inverse-distance weighting. Predating widespread\\ninterest in kernel regression in machine learning, these authors did not use the\\nterm kernel, but referred to “Shepard’s method” (Shepard, 1968). Other kernel-\\nbased approaches to reinforcement learning include those of Ormoneit and Sen\\n(2002), Dietterich and Wang (2002), Xu, Xie, Hu, and Lu (2005), Taylor and Parr\\n(2009), Barreto, Precup, and Pineau (2011), and Bhat, Farias, and Moallemi\\n(2012).\\n9.11 For Emphatic-TD methods, see the bibliographical notes to Section 11.8.\\nThe earliest example we know of in which function approximation methods were\\nused for learning value functions was Samuel’s checkers player (1959, 1967). Samuel\\nfollowed Shannon’s (1950) suggestion that a value function did not have to be exact to\\nbe a useful guide to selecting moves in a game and that it might be approximated by\\na linear combination of features. In addition to linear function approximation, Samuel\\nexperimented with lookup tables and hierarchical lookup tables called signature tables\\n(Gri\\x00th, 1966, 1974; Page, 1977; Biermann, Fairﬁeld, and Beres, 1982).\\nAt about the same time as Samuel’s work, Bellman and Dreyfus (1959) proposed using\\nfunction approximation methods with DP. (It is tempting to think that Bellman and\\nSamuel had some inﬂuence on one another, but we know of no reference to the other in\\nthe work of either.) There is now a fairly extensive literature on function approximation\\nmethods and DP, such as multigrid methods and methods using splines and orthogonal\\npolynomials (e.g., Bellman and Dreyfus, 1959; Bellman, Kalaba, and Kotkin, 1963; Daniel,\\n1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis,\\n1991; Kushner and Dupuis, 1992; Rust, 1996).\\nHolland’s (1986) classiﬁer system used a selective feature-match technique to generalize\\nevaluation information across state–action pairs. Each classiﬁer matched a subset of states\\nhaving speciﬁed values for a subset of features, with the remaining features having arbitrary\\nvalues (“wild cards”). These subsets were then used in a conventional state-aggregation\\napproach to function approximation. Holland’s idea was to use a genetic algorithm\\nto evolve a set of classiﬁers that collectively would implement a useful action-value\\nfunction. Holland’s ideas inﬂuenced the early research of the authors on reinforcement\\nlearning, but we focused on di↵erent approaches to function approximation. As function\\napproximators, classiﬁers are limited in several ways. First, they are state-aggregation\\nmethods, with concomitant limitations in scaling and in representing smooth functions\\ne\\x00ciently. In addition, the matching rules of classiﬁers can implement only aggregation\\nboundaries that are parallel to the feature axes. Perhaps the most important limitation of\\nconventional classiﬁer systems is that the classiﬁers are learned via the genetic algorithm,\\nan evolutionary method. As we discussed in Chapter 1, there is available during learning\\nmuch more detailed information about how to learn than can be used by evolutionary\\nmethods. This perspective led us to instead adapt supervised learning methods for\\nuse in reinforcement learning, speciﬁcally gradient-descent and ANN methods. These\\ndi↵erences between Holland’s approach and ours are not surprising because Holland’s\\nideas were developed during a period when ANNs were generally regarded as being too\\nweak in computational power to be useful, whereas our work was at the beginning of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 263, 'page_label': '264'}, page_content='242 Chapter 9: On-policy Prediction with Approximation\\nthe period that saw widespread questioning of that conventional wisdom. There remain\\nmany opportunities for combining aspects of these di↵erent approaches.\\nChristensen and Korf (1986) experimented with regression methods for modifying\\ncoe\\x00cients of linear value function approximations in the game of chess. Chapman\\nand Kaelbling (1991) and Tan (1991) adapted decision-tree methods for learning value\\nfunctions. Explanation-based learning methods have also been adapted for learning\\nvalue functions, yielding compact representations (Yee, Saxena, Utgo↵, and Barto, 1990;\\nDietterich and Flann, 1995).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 264, 'page_label': '265'}, page_content='Chapter 10\\nOn-policy Control with\\nApproximation\\nIn this chapter we return to the control problem, now with parametric approximation of\\nthe action-value functionˆq(s, a,w) ⇡ q⇤(s, a), wherew 2 Rd is a ﬁnite-dimensional weight\\nvector. We continue to restrict attention to the on-policy case, leaving o↵-policy methods\\nto Chapter 11. The present chapter features the semi-gradient Sarsa algorithm, the\\nnatural extension of semi-gradient TD(0) (last chapter) to action values and to on-policy\\ncontrol. In the episodic case, the extension is straightforward, but in the continuing case\\nwe have to take a few steps backward and re-examine how we have used discounting to\\ndeﬁne an optimal policy. Surprisingly, once we have genuine function approximation we\\nhave to give up discounting and switch to a new “average-reward” formulation of the\\ncontrol problem, with new “di↵erential” value functions.\\nStarting ﬁrst in the episodic case, we extend the function approximation ideas presented\\nin the last chapter from state values to action values. Then we extend them to control\\nfollowing the general pattern of on-policy GPI, using\"-greedy for action selection. We\\nshow results forn-step linear Sarsa on the Mountain Car problem. Then we turn to the\\ncontinuing case and repeat the development of these ideas for the average-reward case\\nwith di↵erential values.\\n10.1 Episodic Semi-gradient Control\\nThe extension of the semi-gradient prediction methods of Chapter 9 to action values is\\nstraightforward. In this case it is the approximate action-value function, ˆq ⇡ q⇡, that is\\nrepresented as a parameterized functional form with weight vectorw. Whereas before we\\nconsidered random training examples of the formSt 7! Ut, now we consider examples\\nof the formSt,A t 7! Ut. The update targetUt can be any approximation ofq⇡(St,A t),\\nincluding the usual backed-up values such as the full Monte Carlo return (Gt) or any\\nof then-step Sarsa returns(7.4). The general gradient-descent update for action-value'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 265, 'page_label': '266'}, page_content='244 Chapter 10: On-policy Control with Approximation\\nprediction is\\nwt+1\\n.= wt + ↵\\nh\\nUt \\x00 ˆq(St,A t, wt)\\ni\\nrˆq(St,A t, wt). (10.1)\\nFor example, the update for the one-step Sarsa method is\\nwt+1\\n.= wt + ↵\\nh\\nRt+1 + \\x00ˆq(St+1,A t+1, wt) \\x00 ˆq(St,A t, wt)\\ni\\nrˆq(St,A t, wt). (10.2)\\nWe call this methodepisodic semi-gradient one-step Sarsa. For a constant policy, this\\nmethod converges in the same way that TD(0) does, with the same kind of error bound\\n(9.14).\\nTo form control methods, we need to couple such action-value prediction methods with\\ntechniques for policy improvement and action selection. Suitable techniques applicable to\\ncontinuous actions, or to actions from large discrete sets, are a topic of ongoing research\\nwith as yet no clear resolution. On the other hand, if the action set is discrete and not too\\nlarge, then we can use the techniques already developed in previous chapters. That is, for\\neach possible actiona available in the next stateSt+1, we can computeˆq(St+1,a ,wt) and\\nthen ﬁnd the greedy actionA⇤\\nt+1 = argmaxa ˆq(St+1,a ,wt). Policy improvement is then\\ndone (in the on-policy case treated in this chapter) by changing the estimation policy to a\\nsoft approximation of the greedy policy such as the\"-greedy policy. Actions are selected\\naccording to this same policy. Pseudocode for the complete algorithm is given in the box.\\nEpisodic Semi-gradient Sarsa for Estimatingˆq ⇡ q⇤\\nInput: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\\nAlgorithm parameters: step size↵> 0, small\"> 0\\nInitialize value-function weightsw 2 Rd arbitrarily (e.g.,w = 0)\\nLoop for each episode:\\nS, A initial state and action of episode (e.g.,\"-greedy)\\nLoop for each step of episode:\\nTake actionA, observeR, S0\\nIf S0 is terminal:\\nw  w + ↵\\n⇥\\nR \\x00 ˆq(S, A,w)\\n⇤\\nrˆq(S, A,w)\\nGo to next episode\\nChoose A0 as a function of ˆq(S0, · , w) (e.g.,\"-greedy)\\nw  w + ↵\\n⇥\\nR + \\x00ˆq(S0,A 0, w) \\x00 ˆq(S, A,w)\\n⇤\\nrˆq(S, A,w)\\nS  S0\\nA  A0\\nExample 10.1: Mountain Car TaskConsider the task of driving an underpowered\\ncar up a steep mountain road, as suggested by the diagram in the upper left of Figure 10.1.\\nThe di\\x00culty is that gravity is stronger than the car’s engine, and even at full throttle\\nthe car cannot accelerate up the steep slope. The only solution is to ﬁrst move away from\\nthe goal and up the opposite slope on the left. Then, by applying full throttle the car'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 266, 'page_label': '267'}, page_content='10.1. Episodic Semi-gradient Control 245\\n!1.2\\nPosition\\n0.6\\nStep 428\\nGoal\\nPosition\\n4\\n0\\n!.07\\n.07\\nVelocity\\nVelocity\\nVelocity\\nVelocity\\nVelocity\\nVelocity\\nPosition\\nPosition\\nPosition\\n0\\n2 7\\n0\\n120\\n0\\n104\\n0\\n4 6\\nEpisode 12\\nEpisode 104 Episode 1000 Episode 9000\\nMOUNTAIN   CAR Goal\\nFigure 10.1: The Mountain Car task (upper left panel) and the cost-to-go function\\n(\\x00maxa ˆq(s, a,w)) learned during one run.\\ncan build up enough inertia to carry it up the steep slope even though it is slowing down\\nthe whole way. This is a simple example of a continuous control task where things have\\nto get worse in a sense (farther from the goal) before they can get better. Many control\\nmethodologies have great di\\x00culties with tasks of this kind unless explicitly aided by a\\nhuman designer.\\nThe reward in this problem is\\x001 on all time steps until the car moves past its goal\\nposition at the top of the mountain, which ends the episode. There are three possible\\nactions: full throttle forward (+1), full throttle reverse (\\x001), and zero throttle (0). The\\ncar moves according to a simpliﬁed physics. Its position,xt, and velocity,˙xt, are updated\\nby\\nxt+1\\n.= bound\\n⇥\\nxt +˙xt+1\\n⇤\\n˙xt+1\\n.= bound\\n⇥\\n˙xt +0 .001At \\x00 0.0025 cos(3xt)\\n⇤\\n,\\nwhere the bound operation enforces \\x001.2 \\uf8ff xt+1 \\uf8ff 0.5 and \\x000.07 \\uf8ff ˙xt+1 \\uf8ff 0.07. In\\naddition, when xt+1 reached the left bound,˙xt+1 was reset to zero. When it reached\\nthe right bound, the goal was reached and the episode was terminated. Each episode\\nstarted from a random positionxt 2 [\\x000.6, \\x000.4) and zero velocity. To convert the two\\ncontinuous state variables to binary features, we used grid-tilings as in Figure 9.9. We\\nused 8 tilings, with each tile covering 1/8th of the bounded distance in each dimension,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 267, 'page_label': '268'}, page_content='246 Chapter 10: On-policy Control with Approximation\\nand asymmetrical o↵sets as described in Section 9.5.4.1 The feature vectorsx(s, a) created\\nby tile coding were then combined linearly with the parameter vector to approximate the\\naction-value function:\\nˆq(s, a,w) .= w>x(s, a)=\\ndX\\ni=1\\nwi · xi(s, a), (10.3)\\nfor each pair of state,s, and action,a.\\nFigure 10.1 shows what typically happens while learning to solve this task with this\\nform of function approximation.2 Shown is the negative of the value function (thecost-\\nto-go function) learned on a single run. The initial action values were all zero, which was\\noptimistic (all true values are negative in this task), causing extensive exploration to occur\\neven though the exploration parameter,\", was 0. This can be seen in the middle-top panel\\nof the ﬁgure, labeled “Step 428”. At this time not even one episode had been completed,\\nbut the car has oscillated back and forth in the valley, following circular trajectories in\\nstate space. All the states visited frequently are valued worse than unexplored states,\\nbecause the actual rewards have been worse than what was (unrealistically) expected.\\nThis continually drives the agent away from wherever it has been, to explore new states,\\nuntil a solution is found.\\nFigure 10.2 shows several learning curves for semi-gradient Sarsa on this problem, with\\nvarious step sizes.\\n100\\n200\\n400\\n1000\\n0\\nMountain CarSteps per episodelog scaleaveraged over 100 runs\\nEpisode 500\\n↵=0.5/8\\n↵=0.1/8↵=0.2/8\\nFigure 10.2: Mountain Car learning curves for the semi-gradient Sarsa method with tile-coding\\nfunction approximation and\"-greedy action selection.\\n1In particular, we used the tile-coding software, available athttp://incompleteideas.net/tiles/\\ntiles3.html,w i t hiht=IHT(4096) and tiles(iht,8,[8*x/(0.5+1.2),8*xdot/(0.07+0.07)],[A]) to get\\nthe indices of the ones in the feature vector for state (x, xdot)a n da c t i o nA.\\n2This data is actually from the “semi-gradient Sarsa(\\x00)” algorithm that we will not meet until\\nChapter 12, but semi-gradient Sarsa would behave similarly.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 268, 'page_label': '269'}, page_content='10.2. Semi-gradient n-step Sarsa 247\\n10.2 Semi-gradient n-step Sarsa\\nWe can obtain ann-step version of episodic semi-gradient Sarsa by using ann-step return\\nas the update target in the semi-gradient Sarsa update equation(10.1).T h en-step return\\nimmediately generalizes from its tabular form (7.4) to a function approximation form:\\nGt:t+n\\n.= Rt+1+\\x00Rt+2+··· +\\x00n\\x001Rt+n+\\x00n ˆq(St+n,A t+n, wt+n\\x001),t +n<T , (10.4)\\nwith Gt:t+n\\n.= Gt if t + n \\x00 T, as usual. Then-step update equation is\\nwt+n\\n.= wt+n\\x001 + ↵ [Gt:t+n \\x00 ˆq(St,A t, wt+n\\x001)] rˆq(St,A t, wt+n\\x001), 0 \\uf8ff t<T .\\n(10.5)\\nComplete pseudocode is given in the box below.\\nEpisodic semi-gradientn-step Sarsa for estimatingˆq ⇡ q⇤ or q⇡\\nInput: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\\nInput: a policy⇡ (if estimatingq⇡)\\nAlgorithm parameters: step size↵> 0, small\"> 0, a positive integern\\nInitialize value-function weightsw 2 Rd arbitrarily (e.g.,w = 0)\\nAll store and access operations (St, At, andRt) can take their index modn +1\\nLoop for each episode:\\nInitialize and storeS0 6= terminal\\nSelect and store an actionA0 ⇠ ⇡(·| S0) or\"-greedy wrt ˆq(S0, · , w)\\nT  1\\nLoop for t =0 , 1, 2,... :\\n| If t<T ,t h e n :\\n| Take actionAt\\n| Observe and store the next reward asRt+1 and the next state asSt+1\\n| If St+1 is terminal, then:\\n| T  t +1\\n| else:\\n| Select and storeAt+1 ⇠ ⇡(·| St+1) or\"-greedy wrt ˆq(St+1, · , w)\\n| ⌧  t \\x00 n +1 ( ⌧ is the time whose estimate is being updated)\\n| If ⌧ \\x00 0:\\n| G  Pmin(⌧+n,T)\\ni=⌧+1 \\x00i\\x00⌧\\x001Ri\\n| If ⌧ + n<T ,t h e nG  G + \\x00n ˆq(S⌧+n,A ⌧+n, w)( G⌧:⌧+n)\\n| w  w + ↵ [G \\x00 ˆq(S⌧ ,A ⌧ , w)] rˆq(S⌧ ,A ⌧ , w)\\nUntil ⌧ = T \\x00 1\\nAs we have seen before, performance is best if an intermediate level of bootstrapping\\nis used, corresponding to ann larger than 1. Figure 10.3 shows how this algorithm tends\\nto learn faster and obtain a better asymptotic performance atn=8 than atn=1 on the\\nMountain Car task. Figure 10.4 shows the results of a more detailed study of the e↵ect\\nof the parameters↵ and n on the rate of learning on this task.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 269, 'page_label': '270'}, page_content='248 Chapter 10: On-policy Control with Approximation\\n100\\n200\\n400\\n1000\\n0\\nMountain CarSteps per episodelog scaleaveraged over 100 runs\\nEpisode 500\\nn=1n=8\\nFigure 10.3: Performance of one-step vs 8-step semi-gradient Sarsa on the Mountain Car task.\\nGood step sizes were used:↵ =0 .5/8 forn =1a n d↵ =0 .3/8 forn =8 .\\n220\\n240\\n260\\n300\\n0 0.5 1 1.5\\nMountain CarSteps per episodeaveraged overﬁrst 50 episodesand 100 runs\\n↵×  number of tilings (8)\\n280\\nn=1n=2n=4n=8\\nn=16\\nn=8\\nn=4\\nn=2\\nn=16n=1\\nFigure 10.4: E↵ect of the↵ and n on early performance ofn-step semi-gradient Sarsa and\\ntile-coding function approximation on the Mountain Car task. As usual, an intermediate level of\\nbootstrapping (n = 4) performed best. These results are for selected↵ values, on a log scale,\\nand then connected by straight lines. The standard errors ranged from 0.5 (less than the line\\nwidth) forn = 1 to about 4 forn = 16, so the main e↵ects are all statistically signiﬁcant.\\nExercise 10.1 We have not explicitly considered or given pseudocode for any Monte Carlo\\nmethods in this chapter. What would they be like? Why is it reasonable not to give\\npseudocode for them? How would they perform on the Mountain Car task? ⇤\\nExercise 10.2 Give pseudocode for semi-gradient one-stepExpected Sarsa for control.⇤\\nExercise 10.3 Why do the results shown in Figure 10.4 have higher standard errors at\\nlarge n than at smalln? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 270, 'page_label': '271'}, page_content='10.3. Average Reward: A New Problem Setting for Continuing Tasks 249\\n10.3 Average Reward: A New Problem Setting for\\nContinuing Tasks\\nWe now introduce a third classical setting—alongside the episodic and discounted settings—\\nfor formulating the goal in Markov decision problems (MDPs). Like the discounted\\nsetting, theaverage rewardsetting applies to continuing problems, problems for which the\\ninteraction between agent and environment goes on and on forever without termination\\nor start states. Unlike that setting, however, there is no discounting—the agent cares just\\nas much about delayed rewards as it does about immediate reward. The average-reward\\nsetting is one of the major settings commonly considered in the classical theory of dynamic\\nprogramming and less-commonly in reinforcement learning. As we discuss in the next\\nsection, the discounted setting is problematic with function approximation, and thus the\\naverage-reward setting is needed to replace it.\\nIn the average-reward setting, the quality of a policy⇡ is deﬁned as the average rate of\\nreward, or simplyaverage reward, while following that policy, which we denote asr(⇡):\\nr(⇡) .=l i m\\nh!1\\n1\\nh\\nhX\\nt=1\\nE[Rt | S0,A 0:t\\x001 ⇠⇡] (10.6)\\n=l i m\\nt!1\\nE[Rt | S0,A 0:t\\x001 ⇠⇡] , (10.7)\\n=\\nX\\ns\\nµ⇡(s)\\nX\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)r,\\nwhere the expectations are conditioned on the initial state,S0, and on the subsequent\\nactions, A0,A 1,...,A t\\x001, being taken according to⇡. The second and third equations\\nhold if the steady-state distribution,µ⇡(s) .=l i mt!1 Pr{St =s |A0:t\\x001 ⇠⇡}, exists and\\nis independent ofS0, in other words, if the MDP isergodic. In an ergodic MDP, the\\nstarting state and any early decision made by the agent can have only a temporary e↵ect;\\nin the long run the expectation of being in a state depends only on the policy and the\\nMDP transition probabilities. Ergodicity is su\\x00cient but not necessary to guarantee the\\nexistence of the limit in (10.6).\\nThere are subtle distinctions that can be drawn between di↵erent kinds of optimality\\nin the undiscounted continuing case. Nevertheless, for most practical purposes it may\\nbe adequate simply to order policies according to their average reward per time step,\\nin other words, according to theirr(⇡). This quantity is essentially the average reward\\nunder ⇡, as suggested by(10.7), or thereward rate. In particular, we consider all policies\\nthat attain the maximal value ofr(⇡) to be optimal.\\nNote that the steady state distributionµ⇡ is the special distribution under which, if\\nyou select actions according to⇡, you remain in the same distribution. That is, for which\\nX\\ns\\nµ⇡(s)\\nX\\na\\n⇡(a|s)p(s0|s, a)= µ⇡(s0). (10.8)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 271, 'page_label': '272'}, page_content='250 Chapter 10: On-policy Control with Approximation\\nIn the average-reward setting, returns are deﬁned in terms of di↵erences between\\nrewards and the average reward:\\nGt\\n.= Rt+1 \\x00 r(⇡)+ Rt+2 \\x00 r(⇡)+ Rt+3 \\x00 r(⇡)+ ··· . (10.9)\\nThis is known as thedi↵erential return, and the corresponding value functions are\\nknown asdi↵erential value functions. Di↵erential value functions are deﬁned in terms\\nof the new return just as conventional value functions were deﬁned in terms of the\\ndiscounted return; thus we will use the same notation,v⇡(s) .= E⇡[Gt|St = s] and\\nq⇡(s, a) .= E⇡[Gt|St = s, At = a] (similarly for v⇤ and q⇤), for di↵erential value functions.\\nDi↵erential value functions also have Bellman equations, just slightly di↵erent from those\\nwe have seen earlier. We simply remove all\\x00s and replace all rewards by the di↵erence\\nbetween the reward and the true average reward:\\nv⇡(s)=\\nX\\na\\n⇡(a|s)\\nX\\nr,s0\\np(s0,r |s, a)\\nh\\nr \\x00 r(⇡)+ v⇡(s0)\\ni\\n,\\nq⇡(s, a)=\\nX\\nr,s0\\np(s0,r |s, a)\\nh\\nr \\x00 r(⇡)+\\nX\\na0\\n⇡(a0|s0)q⇡(s0,a 0)\\ni\\n,\\nv⇤(s) = max\\na\\nX\\nr,s0\\np(s0,r |s, a)\\nh\\nr \\x00 max\\n⇡\\nr(⇡)+ v⇤(s0)\\ni\\n, and\\nq⇤(s, a)=\\nX\\nr,s0\\np(s0,r |s, a)\\nh\\nr \\x00 max\\n⇡\\nr(⇡) + max\\na0\\nq⇤(s0,a 0)\\ni\\n(cf. (3.14), Exercise 3.17, (3.19), and (3.20)).\\nThere is also a di↵erential form of the two TD errors:\\n\\x00t\\n.= Rt+1\\x00 ¯Rt +ˆv(St+1,wt) \\x00 ˆv(St,wt), (10.10)\\nand\\n\\x00t\\n.= Rt+1\\x00 ¯Rt +ˆq(St+1,A t+1, wt) \\x00 ˆq(St,A t, wt), (10.11)\\nwhere ¯Rt is an estimate at timet of the average rewardr(⇡). With these alternate\\ndeﬁnitions, most of our algorithms and many theoretical results carry through to the\\naverage-reward setting without change.\\nFor example, an average reward version of semi-gradient Sarsa could be deﬁned just as\\nin (10.2) except with the di↵erential version of the TD error. That is, by\\nwt+1\\n.= wt + ↵\\x00trˆq(St,A t, wt), (10.12)\\nwith \\x00t given by(10.11). Pseudocode for a complete algorithm is given in the box on the\\nnext page. One limitation of this algorithm is that it does not converge to the di↵erential\\nvalues but to the di↵erential values plus an arbitrary o↵set. Notice that the Bellman\\nequations and TD errors given above are una↵ected if all the values are shifted by the\\nsame amount. Thus, the o↵set may not matter in practice. How this algorithm could be\\nchanged to eliminate the o↵set is an interesting question for future research.\\nExercise 10.4 Give pseudocode for a di↵erential version of semi-gradient Q-learning.⇤\\nExercise 10.5 What equations are needed (beyond 10.10) to specify the di↵erential\\nversion of TD(0)? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 272, 'page_label': '273'}, page_content='10.3. Average Reward: A New Problem Setting for Continuing Tasks 251\\nDi↵erential semi-gradient Sarsa for estimatingˆq ⇡ q⇤\\nInput: a di↵erentiable action-value function parameterization ˆq : S ⇥ A ⇥ Rd ! R\\nAlgorithm parameters: step sizes↵, \\x00 >0, small\"> 0\\nInitialize value-function weightsw 2 Rd arbitrarily (e.g.,w = 0)\\nInitialize average reward estimate¯R 2 R arbitrarily (e.g., ¯R = 0)\\nInitialize stateS, and actionA\\nLoop for each step:\\nTake actionA, observeR, S0\\nChoose A0 as a function of ˆq(S0, · , w) (e.g.,\"-greedy)\\n\\x00  R \\x00 ¯R +ˆq(S0,A 0, w) \\x00 ˆq(S, A,w)\\n¯R  ¯R + \\x00\\x00\\nw  w + ↵\\x00rˆq(S, A,w)\\nS  S0\\nA  A0\\nExercise 10.6 Suppose there is an MDP that under any policy produces the deterministic\\nsequence of rewards +1, 0, +1, 0, +1, 0,... going on forever. Technically, this violates\\nergodicity; there is no stationary limiting distributionµ⇡ and the limit(10.7) does not\\nexist. Nevertheless, the average reward(10.6) is well deﬁned. What is it? Now consider\\ntwo states in this MDP. FromA, the reward sequence is exactly as described above,\\nstarting with a +1, whereas, fromB, the reward sequence starts with a 0 and then\\ncontinues with +1, 0, +1, 0,... . We would like to compute the di↵erential values ofA and\\nB. Unfortunately, the di↵erential return(10.9) is not well deﬁned when starting from\\nthese states as the implicit limit does not exist. To repair this, one could alternatively\\ndeﬁne the di↵erential value of a state as\\nv⇡(s) .=l i m\\x00!1\\nlim\\nh!1\\nhX\\nt=0\\n\\x00t\\n⇣\\nE⇡[Rt+1|S0 =s] \\x00 r(⇡)\\n⌘\\n. (10.13)\\nUnder this deﬁnition, what are the di↵erential values of statesA and B? ⇤\\nExercise 10.7 Consider a Markov reward process consisting of a ring of three statesA, B,\\nand C, with state transitions going deterministically around the ring. A reward of +1 is\\nreceived upon arrival inA and otherwise the reward is 0. What are the di↵erential values\\nof the three states, using (10.13)? ⇤\\nExercise 10.8 The pseudocode in the box on page 251 updates¯Rt using \\x00t as an error\\nrather than simplyRt+1 \\x00 ¯Rt. Both errors work, but using\\x00t is better. To see why,\\nconsider the ring MRP of three states from Exercise 10.7. The estimate of the average\\nreward should tend towards its true value of1\\n3 . Suppose it was already there and was\\nheld stuck there. What would the sequence ofRt+1 \\x00 ¯Rt errors be? What would the\\nsequence of\\x00t errors be (using Equation 10.10)? Which error sequence would produce\\na more stable estimate of the average reward if the estimate were allowed to change in\\nresponse to the errors? Why? ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 273, 'page_label': '274'}, page_content='252 Chapter 10: On-policy Control with Approximation\\nExample 10.2: An Access-Control Queuing TaskThis is a decision task involving\\naccess control to a set of 10 servers. Customers of four di↵erent priorities arrive at a\\nsingle queue. If given access to a server, the customers pay a reward of 1, 2, 4, or 8 to\\nthe server, depending on their priority, with higher priority customers paying more. In\\neach time step, the customer at the head of the queue is either accepted (assigned to one\\nof the servers) or rejected (removed from the queue, with a reward of zero). In either\\ncase, on the next time step the next customer in the queue is considered. The queue\\nnever empties, and the priorities of the customers in the queue are uniformly randomly\\ndistributed. Of course a customer cannot be served if there is no free server; the customer\\nis always rejected in this case. Each busy server becomes free with probabilityp =0 .06\\non each time step. Although we have just described them for deﬁniteness, let us assume\\nthe statistics of arrivals and departures are unknown. The task is to decide on each step\\nwhether to accept or reject the next customer, on the basis of his priority and the number\\nof free servers, so as to maximize long-term reward without discounting.\\nIn this example we consider a tabular solution to this problem. Although there is no\\ngeneralization between states, we can still consider it in the general function approximation\\nsetting as this setting generalizes the tabular setting. Thus we have a di↵erential action-\\nvalue estimate for each pair of state (number of free servers and priority of the customer\\nat the head of the queue) and action (accept or reject). Figure 10.5 shows the solution\\nfound by di↵erential semi-gradient Sarsa with parameters↵ =0 .01, \\x00 =0 .01, and\" =0 .1.\\nThe initial action values and¯R were zero.\\n-10\\n-5\\n0\\n10\\n0\\nDiﬀerentialvalue of best action\\nNumber of free servers1 2 3 4 5 6 7 8 9 100\\n!15\\n!10\\n!5\\n0\\n57 priority 8priority 4\\npriority 2priority 1\\nNumber of free servers\\n4\\n2\\n8\\nACCEPT\\nREJECT\\n1 2 3 4 5 6 7 8 9 10Number of free servers\\nPriority\\n1 POLICY\\nValue ofbest action\\nVALUEFUNCTION5\\n1 2 3 4 5 6 7 8 9 10\\npriority 8priority 4\\npriority 2\\npriority 1\\nPOLICY\\nVALUEFUNCTION\\nFigure 10.5: The policy and value function found by di↵erential semi-gradient one-step Sarsa\\non the access-control queuing task after 2 million steps. The drop on the right of the graph\\nis probably due to insu\\x00cient data; many of these states were never experienced. The value\\nlearned for ¯R was about 2.31. (Note that priority 1 here is the lowest priority.)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 274, 'page_label': '275'}, page_content='10.4. Deprecating the Discounted Setting 253\\n10.4 Deprecating the Discounted Setting\\nThe continuing, discounted problem formulation has been very useful in the tabular case,\\nin which the returns from each state can be separately identiﬁed and averaged. But in the\\napproximate case it is questionable whether one should ever use this problem formulation.\\nTo see why, consider an inﬁnite sequence of returns with no beginning or end, and no\\nclearly identiﬁed states. The states might be represented only by feature vectors, which\\nmay do little to distinguish the states from each other. As a special case, all of the feature\\nvectors may be the same. Thus one really has only the reward sequence (and the actions),\\nand performance has to be assessed purely from these. How could it be done? One way\\nis by averaging the rewards over a long interval—this is the idea of the average-reward\\nsetting. How could discounting be used? Well, for each time step we could measure\\nthe discounted return. Some returns would be small and some big, so again we would\\nhave to average them over a su\\x00ciently large time interval. In the continuing setting\\nthere are no starts and ends, and no special time steps, so there is nothing else that\\ncould be done. However, if you do this, it turns out that the average of the discounted\\nreturns is proportional to the average reward. In fact, for policy⇡, the average of the\\ndiscounted returns is alwaysr(⇡)/(1 \\x00 \\x00), that is, it is essentially the average reward,\\nr(⇡). In particular, theordering of all policies in the average discounted return setting\\nwould be exactly the same as in the average-reward setting. The discount rate\\x00 thus has\\nno e↵ect on the problem formulation. It could in fact bezero and the ranking would be\\nunchanged.\\nThis surprising fact is proven in the box on the next page, but the basic idea can\\nbe seen via a symmetry argument. Each time step is exactly the same as every other.\\nWith discounting, every reward will appear exactly once in each position in some return.\\nThe tth reward will appear undiscounted in thet \\x00 1st return, discounted once in the\\nt \\x00 2nd return, and discounted 999 times in thet \\x00 1000th return. The weight on the\\ntth reward is thus 1 +\\x00 + \\x002 + \\x003 + ··· =1 /(1 \\x00 \\x00). Because all states are the same,\\nthey are all weighted by this, and thus the average of the returns will be this times the\\naverage reward, orr(⇡)/(1 \\x00 \\x00).\\nThis example and the more general argument in the box show that if we optimized\\ndiscounted value over the on-policy distribution, then the e↵ect would be identical to\\noptimizing undiscounted average reward; the actual value of\\x00 would have no e↵ect. This\\nstrongly suggests that discounting has no role to play in the deﬁnition of the control\\nproblem with function approximation. One can nevertheless go ahead and use discounting\\nin solution methods. The discounting parameter\\x00 changes from a problem parameter\\nto a solution method parameter! Unfortunately, discounting algorithms with function\\napproximation do not optimize discounted value over the on-policy distribution, and thus\\nare not guaranteed to optimize average reward.\\nThe root cause of the di\\x00culties with the discounted control setting is that with\\nfunction approximation we have lost the policy improvement theorem (Section 4.2). It is\\nno longer true that if we change the policy to improve the discounted value of one state\\nthen we are guaranteed to have improved the overall policy in any useful sense. That\\nguarantee was key to the theory of our reinforcement learning control methods. With'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 275, 'page_label': '276'}, page_content='254 Chapter 10: On-policy Control with Approximation\\nThe Futility of Discounting in Continuing Problems\\nPerhaps discounting can be saved by choosing an objective that sums discounted\\nvalues over the distribution with which states occur under the policy:\\nJ(⇡)=\\nX\\ns\\nµ⇡(s)v\\x00\\n⇡(s)( w h e r e v\\x00\\n⇡ is the discounted value function)\\n=\\nX\\ns\\nµ⇡(s)\\nX\\na\\n⇡(a|s)\\nX\\ns0\\nX\\nr\\np(s0,r |s, a)[ r + \\x00v\\x00\\n⇡(s0)] (Bellman Eq.)\\n= r(⇡)+\\nX\\ns\\nµ⇡(s)\\nX\\na\\n⇡(a|s)\\nX\\ns0\\nX\\nr\\np(s0,r |s, a)\\x00v\\x00\\n⇡(s0) (from (10.7))\\n= r(⇡)+ \\x00\\nX\\ns0\\nv\\x00\\n⇡(s0)\\nX\\ns\\nµ⇡(s)\\nX\\na\\n⇡(a|s)p(s0|s, a) (from (3.4))\\n= r(⇡)+ \\x00\\nX\\ns0\\nv\\x00\\n⇡(s0)µ⇡(s0) (from (10.8))\\n= r(⇡)+ \\x00J (⇡)\\n= r(⇡)+ \\x00r(⇡)+ \\x002J(⇡)\\n= r(⇡)+ \\x00r(⇡)+ \\x002r(⇡)+ \\x003r(⇡)+ ···\\n= 1\\n1 \\x00 \\x00 r(⇡).\\nThe proposed discounted objective orders policies identically to the undiscounted\\n(average reward) objective. The discount rate\\x00 does not inﬂuence the ordering!\\nfunction approximation we have lost it!\\nIn fact, the lack of a policy improvement theorem is also a theoretical lacuna for the\\ntotal-episodic and average-reward settings. Once we introduce function approximation\\nwe can no longer guarantee improvement for any setting. In Chapter 13 we introduce an\\nalternative class of reinforcement learning algorithms based on parameterized policies,\\nand there we have a theoretical guarantee called the “policy-gradient theorem” which\\nplays a similar role as the policy improvement theorem. But for methods that learn\\naction values we seem to be currently without a local improvement guarantee (possibly\\nthe approach taken by Perkins and Precup (2003) may provide a part of the answer). We\\ndo know that\"-greediﬁcation may sometimes result in an inferior policy, as policies may\\nchatter among good policies rather than converge (Gordon, 1996a). This is an area with\\nmultiple open theoretical questions.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 276, 'page_label': '277'}, page_content='10.5. Di↵erential Semi-gradient n-step Sarsa 255\\n10.5 Di↵erential Semi-gradient n-step Sarsa\\nIn order to generalize ton-step bootstrapping, we need ann-step version of the TD error.\\nWe begin by generalizing then-step return(7.4) to its di↵erential form, with function\\napproximation:\\nGt:t+n\\n.= Rt+1\\x00 ¯Rt+n\\x001 + ··· + Rt+n\\x00 ¯Rt+n\\x001 +ˆq(St+n,A t+n, wt+n\\x001), (10.14)\\nwhere ¯R is an estimate ofr(⇡), n \\x00 1, and t + n<T .I f t + n \\x00 T,t h e nw ed e ﬁ n e\\nGt:t+n\\n.= Gt as usual. Then-step TD error is then\\n\\x00t\\n.= Gt:t+n \\x00 ˆq(St,A t, w), (10.15)\\nafter which we can apply our usual semi-gradient Sarsa update(10.12). Pseudocode for\\nthe complete algorithm is given in the box.\\nDi↵erential semi-gradientn-step Sarsa for estimatingˆq ⇡ q⇡ or q⇤\\nInput: a di↵erentiable function ˆq : S ⇥ A ⇥ Rd ! R, a policy⇡\\nInitialize value-function weightsw 2 Rd arbitrarily (e.g.,w = 0)\\nInitialize average-reward estimate¯R 2 R arbitrarily (e.g., ¯R = 0)\\nAlgorithm parameters: step sizes↵, \\x00 >0, small\"> 0, a positive integern\\nAll store and access operations (St, At, andRt) can take their index modn +1\\nInitialize and storeS0 and A0\\nLoop for each step,t =0 , 1, 2,... :\\nTake actionAt\\nObserve and store the next reward asRt+1 and the next state asSt+1\\nSelect and store an actionAt+1 ⇠ ⇡(·| St+1), or\"-greedy wrt ˆq(St+1, · , w)\\n⌧  t \\x00 n +1 ( ⌧ is the time whose estimate is being updated)\\nIf ⌧ \\x00 0:\\n\\x00  P⌧+n\\ni=⌧+1(Ri \\x00 ¯R)+ˆq(S⌧+n,A ⌧+n, w) \\x00 ˆq(S⌧ ,A ⌧ , w)\\n¯R  ¯R + \\x00\\x00\\nw  w + ↵\\x00rˆq(S⌧ ,A ⌧ , w)\\nExercise 10.9 In the di↵erential semi-gradientn-step Sarsa algorithm, the step-size\\nparameter on the average reward,\\x00, needs to be quite small so that¯R becomes a good\\nlong-term estimate of the average reward. Unfortunately,¯R will then be biased by its\\ninitial value for many steps, which may make learning ine\\x00cient. Alternatively, one could\\nuse a sample average of the observed rewards for¯R. That would initially adapt rapidly\\nbut in the long run would also adapt slowly. As the policy slowly changed,¯R would also\\nchange; the potential for such long-term nonstationarity makes sample-average methods\\nill-suited. In fact, the step-size parameter on the average reward is a perfect place to use\\nthe unbiased constant-step-size trick from Exercise 2.7. Describe the speciﬁc changes\\nneeded to the boxed algorithm for di↵erential semi-gradientn-step Sarsa to use this\\ntrick. ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 277, 'page_label': '278'}, page_content='256 Chapter 10: On-policy Control with Approximation\\n10.6 Summary\\nIn this chapter we have extended the ideas of parameterized function approximation and\\nsemi-gradient descent, introduced in the previous chapter, to control. The extension is\\nimmediate for the episodic case, but for the continuing case we have to introduce a whole\\nnew problem formulation based on maximizing theaverage reward settingper time step.\\nSurprisingly, the discounted formulation cannot be carried over to control in the presence\\nof approximations. In the approximate case most policies cannot be represented by a\\nvalue function. The arbitrary policies that remain need to be ranked, and the scalar\\naverage rewardr(⇡) provides an e↵ective way to do this.\\nThe average reward formulation involves newdi↵erential versions of value functions,\\nBellman equations, and TD errors, but all of these parallel the old ones, and the\\nconceptual changes are small. There is also a new parallel set of di↵erential algorithms\\nfor the average-reward case.\\nBibliographical and Historical Remarks\\n10.1 Semi-gradient Sarsa with function approximation was ﬁrst explored by Rummery\\nand Niranjan (1994). Linear semi-gradient Sarsa with\"-greedy action selection\\ndoes not converge in the usual sense, but does enter a bounded region near\\nthe best solution (Gordon, 1996a, 2001). Precup and Perkins (2003) showed\\nconvergence in a di↵erentiable action selection setting. See also Perkins and\\nPendrith (2002) and Melo, Meyn, and Ribeiro (2008). The mountain–car example\\nis based on a similar task studied by Moore (1990), but the exact form used here\\nis from Sutton (1996).\\n10.2 Episodic n-step semi-gradient Sarsa is based on the forward Sarsa(\\x00) algorithm\\nof van Seijen (2016). The empirical results shown here are new to the second\\nedition of this text.\\n10.3 The average-reward formulation has been described for dynamic programming\\n(e.g., Puterman, 1994) and from the point of view of reinforcement learning\\n(Mahadevan, 1996; Tadepalli and Ok, 1994; Bertsekas and Tsitsiklis, 1996;\\nTsitsiklis and Van Roy, 1999). The algorithm described here is the on-policy\\nanalog of the “R-learning” algorithm introduced by Schwartz (1993). The name\\nR-learning was probably meant to be the alphabetic successor to Q-learning,\\nbut we prefer to think of it as a reference to the learning of di↵erential or\\nrelative values. The access-control queuing example was suggested by the work\\nof Carlstr¨ om and Nordstr¨ om (1997).\\n10.4 The recognition of the limitations of discounting as a formulation of the rein-\\nforcement learning problem with function approximation became apparent to\\nthe authors shortly after the publication of the ﬁrst edition of this text. Singh,\\nJaakkola, and Jordan (1994) may have been the ﬁrst to observe it in print.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 278, 'page_label': '279'}, page_content='Chapter 11\\n*O↵-policy Methods with\\nApproximation\\nThis book has treated on-policy and o↵-policy learning methods since Chapter 5 primarily\\nas two alternative ways of handling the conﬂict between exploitation and exploration\\ninherent in learning forms of generalized policy iteration. The two chapters preceding this\\nhave treated theon-policy case with function approximation, and in this chapter we treat\\nthe o↵ -policy case with function approximation. The extension to function approximation\\nturns out to be signiﬁcantly di↵erent and harder for o↵-policy learning than it is for\\non-policy learning. The tabular o↵-policy methods developed in Chapters 6 and 7 readily\\nextend to semi-gradient algorithms, but these algorithms do not converge as robustly as\\nthey do under on-policy training. In this chapter we explore the convergence problems,\\ntake a closer look at the theory of linear function approximation, introduce a notion of\\nlearnability, and then discuss new algorithms with stronger convergence guarantees for the\\no↵-policy case. In the end we will have improved methods, but the theoretical results will\\nnot be as strong, nor the empirical results as satisfying, as they are for on-policy learning.\\nAlong the way, we will gain a deeper understanding of approximation in reinforcement\\nlearning for on-policy learning as well as o↵-policy learning.\\nRecall that in o↵-policy learning we seek to learn a value function for atarget policy\\n⇡, given data due to a di↵erentbehavior policyb. In the prediction case, both policies\\nare static and given, and we seek to learn either state valuesˆv ⇡ v⇡ or action values\\nˆq ⇡ q⇡. In the control case, action values are learned, and both policies typically change\\nduring learning— ⇡ being the greedy policy with respect toˆq, and b being something\\nmore exploratory such as the\"-greedy policy with respect to ˆq.\\nThe challenge of o↵-policy learning can be divided into two parts, one that arises in\\nthe tabular case and one that arises only with function approximation. The ﬁrst part\\nof the challenge has to do with the target of the update (not to be confused with the\\ntarget policy), and the second part has to do with the distribution of the updates. The\\ntechniques related to importance sampling developed in Chapters 5 and 7 deal with\\nthe ﬁrst part; these may increase variance but are needed in all successful algorithms,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 279, 'page_label': '280'}, page_content='258 Chapter 11: O↵-policy Methods with Approximation\\ntabular and approximate. The extension of these techniques to function approximation\\nare quickly dealt with in the ﬁrst section of this chapter.\\nSomething more is needed for the second part of the challenge of o↵-policy learning\\nwith function approximation because the distribution of updates in the o↵-policy case is\\nnot according to the on-policy distribution. The on-policy distribution is important to\\nthe stability of semi-gradient methods. Two general approaches have been explored to\\ndeal with this. One is to use importance sampling methods again, this time to warp the\\nupdate distribution back to the on-policy distribution, so that semi-gradient methods\\nare guaranteed to converge (in the linear case). The other is to develop true gradient\\nmethods that do not rely on any special distribution for stability. We present methods\\nbased on both approaches. This is a cutting-edge research area, and it is not clear which\\nof these approaches is most e↵ective in practice.\\n11.1 Semi-gradient Methods\\nWe begin by describing how the methods developed in earlier chapters for the o↵-\\npolicy case extend readily to function approximation as semi-gradient methods. These\\nmethods address the ﬁrst part of the challenge of o↵-policy learning (changing the update\\ntargets) but not the second part (changing the update distribution). Accordingly, these\\nmethods may diverge in some cases, and in that sense are not sound, but still they\\nare often successfully used. Remember that these methodsare guaranteed stable and\\nasymptotically unbiased for the tabular case, which corresponds to a special case of\\nfunction approximation. So it may still be possible to combine them with feature selection\\nmethods in such a way that the combined system could be assured stable. In any event,\\nthese methods are simple and thus a good place to start.\\nIn Chapter 7 we described a variety of tabular o↵-policy algorithms. To convert them\\nto semi-gradient form, we simply replace the update to an array (V or Q) to an update\\nto a weight vector (w), using the approximate value function (ˆv or ˆq) and its gradient.\\nMany of these algorithms use the per-step importance sampling ratio:\\n⇢t\\n.= ⇢t:t = ⇡(At|St)\\nb(At|St) . (11.1)\\nFor example, the one-step, state-value algorithm is semi-gradient o↵-policy TD(0), which\\nis just like the corresponding on-policy algorithm (page 203) except for the addition of\\n⇢t:\\nwt+1\\n.= wt + ↵⇢t\\x00trˆv(St,wt), (11.2)\\nwhere \\x00t is deﬁned appropriately depending on whether the problem is episodic and\\ndiscounted, or continuing and undiscounted using average reward:\\n\\x00t\\n.= Rt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt), or (11.3)\\n\\x00t\\n.= Rt+1 \\x00 ¯Rt +ˆv(St+1,wt) \\x00 ˆv(St,wt). (11.4)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 280, 'page_label': '281'}, page_content='11.1. Semi-gradient Methods 259\\nFor action values, the one-step algorithm is semi-gradient Expected Sarsa:\\nwt+1\\n.= wt + ↵\\x00trˆq(St,A t, wt), with (11.5)\\n\\x00t\\n.= Rt+1 + \\x00\\nX\\na\\n⇡(a|St+1)ˆq(St+1,a ,wt) \\x00 ˆq(St,A t, wt), or (episodic)\\n\\x00t\\n.= Rt+1 \\x00 ¯Rt +\\nX\\na\\n⇡(a|St+1)ˆq(St+1,a ,wt) \\x00 ˆq(St,A t, wt). (continuing)\\nNote that this algorithm does not use importance sampling. In the tabular case it is clear\\nthat this is appropriate because the only sample action isAt, and in learning its value we\\ndo not have to consider any other actions. With function approximation it is less clear\\nbecause we might want to weight di↵erent state–action pairs di↵erently once they all\\ncontribute to the same overall approximation. Proper resolution of this issue awaits a\\nmore thorough understanding of the theory of function approximation in reinforcement\\nlearning.\\nIn the multi-step generalizations of these algorithms, both the state-value and action-\\nvalue algorithms involve importance sampling. Then-step version of semi-gradient Sarsa\\nis\\nwt+n\\n.= wt+n\\x001+↵⇢t+1 ··· ⇢t+n [Gt:t+n \\x00 ˆq(St,A t, wt+n\\x001)] rˆq(St,A t, wt+n\\x001) (11.6)\\nwith\\nGt:t+n\\n.= Rt+1 + ··· + \\x00n\\x001Rt+n + \\x00n ˆq(St+n,A t+n, wt+n\\x001), or (episodic)\\nGt:t+n\\n.= Rt+1 \\x00 ¯Rt + ··· + Rt+n \\x00 ¯Rt+n\\x001 +ˆq(St+n,A t+n, wt+n\\x001), (continuing)\\nwhere here we are being slightly informal in our treatment of the ends of episodes. In the\\nﬁrst equation, the⇢ks fork \\x00 T (where T is the last time step of the episode) should be\\ntaken to be 1, andGt:t+n should be taken to beGt if t + n \\x00 T.\\nRecall that we also presented in Chapter 7 an o↵-policy algorithm that does not involve\\nimportance sampling at all: then-step tree-backup algorithm. Here is its semi-gradient\\nversion:\\nwt+n\\n.= wt+n\\x001 + ↵ [Gt:t+n \\x00 ˆq(St,A t, wt+n\\x001)] rˆq(St,A t, wt+n\\x001), (11.7)\\nGt:t+n\\n.=ˆq(St,A t, wt+n\\x001)+\\nt+n\\x001X\\nk=t\\n\\x00k\\nkY\\ni=t+1\\n\\x00⇡(Ai|Si), (11.8)\\nwith \\x00t as deﬁned at the top of this page for Expected Sarsa. We also deﬁned in Chapter 7\\nan algorithm that uniﬁes all action-value algorithms:n-step Q(\\x00). We leave the semi-\\ngradient form of that algorithm, and also of then-step state-value algorithm, as exercises\\nfor the reader.\\nExercise 11.1 Convert the equation ofn-step o↵-policy TD(7.9) to semi-gradient form.\\nGive accompanying deﬁnitions of the return for both the episodic and continuing cases.⇤\\n⇤Exercise 11.2 Convert the equations ofn-step Q(\\x00) (7.11 and 7.17) to semi-gradient\\nform. Give deﬁnitions that cover both the episodic and continuing cases. ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 281, 'page_label': '282'}, page_content='260 Chapter 11: O↵-policy Methods with Approximation\\n11.2 Examples of O↵-policy Divergence\\nIn this section we begin to discuss the second part of the challenge of o↵-policy learning\\nwith function approximation—that the distribution of updates does not match the on-\\npolicy distribution. We describe some instructive counterexamples to o↵-policy learning—\\ncases where semi-gradient and other simple algorithms are unstable and diverge.\\nTo establish intuitions, it is best to consider ﬁrst a very simple example. Suppose,\\nperhaps as part of a larger MDP, there are two states whose estimated values are of\\nthe functional formw and 2w, where the parameter vectorw consists of only a single\\ncomponent w. This occurs under linear function approximation if the feature vectors\\nfor the two states are each simple numbers (single-component vectors), in this case 1\\nand 2. In the ﬁrst state, only one action is available, and it results deterministically in a\\ntransition to the second state with a reward of 0:\\n2w 0 2w\\nwhere the expressions inside the two circles indicate the two state’s values.\\nSuppose initially w = 10. The transition will then be from a state of estimated value\\n10 to a state of estimated value 20. It will look like a good transition, andw will be\\nincreased to raise the ﬁrst state’s estimated value. If\\x00 is nearly 1, then the TD error will\\nbe nearly 10, and, if↵ =0 .1, thenw will be increased to nearly 11 in trying to reduce the\\nTD error. However, the second state’s estimated value will also be increased, to nearly\\n22. If the transition occurs again, then it will be from a state of estimated value⇡11 to\\na state of estimated value⇡22, for a TD error of⇡11—larger, not smaller, than before.\\nIt will look even more like the ﬁrst state is undervalued, and its value will be increased\\nagain, this time to⇡12.1. This looks bad, and in fact with further updatesw will diverge\\nto inﬁnity.\\nTo see this deﬁnitively we have to look more carefully at the sequence of updates. The\\nTD error on a transition between the two states is\\n\\x00t = Rt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt)=0+ \\x002wt \\x00 wt =( 2\\x00 \\x00 1)wt,\\nand the o↵-policy semi-gradient TD(0) update (from (11.2)) is\\nwt+1 = wt + ↵⇢t\\x00trˆv(St,wt)= wt + ↵ · 1 · (2\\x00 \\x00 1)wt · 1=\\n\\x00\\n1+ ↵(2\\x00 \\x00 1)\\n\\x00\\nwt.\\nNote that the importance sampling ratio,⇢t, is 1 on this transition because there is\\nonly one action available from the ﬁrst state, so its probabilities of being taken under\\nthe target and behavior policies must both be 1. In the ﬁnal update above, the new\\nparameter is the old parameter times a scalar constant, 1 +↵(2\\x00 \\x00 1). If this constant is\\ngreater than 1, then the system is unstable andw will go to positive or negative inﬁnity\\ndepending on its initial value. Here this constant is greater than 1 whenever\\x00> 0.5.\\nNote that stability does not depend on the speciﬁc step size, as long as↵> 0. Smaller or\\nlarger step sizes would a↵ect the rate at whichw goes to inﬁnity, but not whether it goes\\nthere or not.\\nKey to this example is that the one transition occurs repeatedly withoutw being\\nupdated on other transitions. This is possible under o↵-policy training because the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 282, 'page_label': '283'}, page_content='11.2. Examples of O↵-policy Divergence 261\\nbehavior policy might select actions on those other transitions which the target policy\\nnever would. For these transitions,⇢t would be zero and no update would be made.\\nUnder on-policy training, however,⇢t is always one. Each time there is a transition from\\nthe w state to the 2w state, increasingw, there would also have to be a transition out\\nof the 2w state. That transition would reducew, unless it were to a state whose value\\nwas higher (because\\x00< 1) than 2w, and then that state would have to be followed by a\\nstate of even higher value, or else againw would be reduced. Each state can support the\\none before only by creating a higher expectation. Eventually the piper must be paid. In\\nthe on-policy case the promise of future reward must be kept and the system is kept in\\ncheck. But in the o↵-policy case, a promise can be made and then, after taking an action\\nthat the target policy never would, forgotten and forgiven.\\nThis simple example communicates much of the reason why o↵-policy training can lead\\nto divergence, but it is not completely convincing because it is not complete—it is just a\\nfragment of a complete MDP. Can there really be a complete system with instability? A\\nsimple complete example of divergence isBaird’s counterexample. Consider the episodic\\nseven-state, two-action MDP shown in Figure 11.1. Thedashed action takes the system\\nto one of the six upper states with equal probability, whereas thesolid action takes the\\nsystem to the seventh state. The behavior policyb selects thedashed and solid actions\\nwith probabilities 6\\n7 and 1\\n7 , so that the next-state distribution under it is uniform (the\\nsame for all nonterminal states), which is also the starting distribution for each episode.\\nThe target policy⇡ always takes the solid action, and so the on-policy distribution (for⇡)\\nis concentrated in the seventh state. The reward is zero on all transitions. The discount\\nrate is\\x00 =0 .99.\\nConsider estimating the state-value under the linear parameterization indicated by\\nthe expression shown in each state circle. For example, the estimated value of the\\nleftmost state is 2w1 +w8, where the subscript corresponds to the component of the\\n2w2+w82w1+w8 2w3+w8 2w4+w8 2w5+w8 2w6+w8\\nw7+2w8\\nb(dashed|· )=6 /7\\nb(solid|· )=1 /7\\n⇡(solid|· )=1\\n\\x00 =0.99\\nFigure 11.1: Baird’s counterexample. The approximate state-value function for this Markov\\nprocess is of the form shown by the linear expressions inside each state. Thesolid action usually\\nresults in the seventh state, and thedashed action usually results in one of the other six states,\\neach with equal probability. The reward is always zero.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 283, 'page_label': '284'}, page_content='262 Chapter 11: O↵-policy Methods with Approximation\\noverall weight vectorw 2 R8; this corresponds to a feature vector for the ﬁrst state\\nbeing x(1) = (2, 0, 0, 0, 0, 0, 0, 1)>. The reward is zero on all transitions, so the true value\\nfunction is v⇡(s) = 0, for alls, which can be exactly approximated ifw = 0. In fact,\\nthere are many solutions, as there are more components to the weight vector (8) than\\nthere are nonterminal states (7). Moreover, the set of feature vectors,{x(s): s 2 S},i s\\na linearly independent set. In all these ways this task seems a favorable case for linear\\nfunction approximation.\\nIf we apply semi-gradient TD(0) to this problem(11.2), then the weights diverge\\nto inﬁnity, as shown in Figure 11.2 (left). The instability occurs for any positive step\\nsize, no matter how small. In fact, it even occurs if an expected update is done as in\\ndynamic programming (DP), as shown in Figure 11.2 (right). That is, if the weight\\nvector, wk, is updated for all states at the same time in a semi-gradient way, using the\\nDP (expectation-based) target:\\nwk+1\\n.= wk + ↵\\n|S|\\nX\\ns\\n⇣\\nE⇡[Rt+1 + \\x00ˆv(St+1,wk) | St =s] \\x00 ˆv(s,wk)\\n⌘\\nrˆv(s,wk). (11.9)\\nIn this case, there is no randomness and no asynchrony, just as in a classical DP update.\\nThe method is conventional except in its use of semi-gradient function approximation.\\nYet still the system is unstable.\\nIf we alter just the distribution of DP updates in Baird’s counterexample, from the\\nuniform distribution to the on-policy distribution (which generally requires asynchronous\\nupdating), then convergence is guaranteed to a solution with error bounded by (9.14).\\nThis example is striking because the TD and DP methods used are arguably the simplest\\nw8\\nw8\\n300\\n200\\n100\\n101\\n10000 10000\\nw1–w6\\nSteps\\nw7\\nSweeps\\nSemi-gradient Off-policy TD Semi-gradient DP\\nw1–w6\\nw7\\nFigure 11.2: Demonstration of instability on Baird’s counterexample. Shown are the evolution\\nof the components of the parameter vectorw of the two semi-gradient algorithms. The step size\\nwas ↵ =0 .01, and the initial weights werew =( 1, 1, 1, 1, 1, 1, 10, 1)>.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 284, 'page_label': '285'}, page_content='11.2. Examples of O↵-policy Divergence 263\\nand best-understood bootstrapping methods, and the linear, semi-descent method used is\\narguably the simplest and best-understood kind of function approximation. The example\\nshows that even the simplest combination of bootstrapping and function approximation\\ncan be unstable if the updates are not done according to the on-policy distribution.\\nThere are also counterexamples similar to Baird’s showing divergence for Q-learning.\\nThis is cause for concern because otherwise Q-learning has the best convergence guarantees\\nof all control methods. Considerable e↵ort has gone into trying to ﬁnd a remedy to\\nthis problem or to obtain some weaker, but still workable, guarantee. For example, it\\nmay be possible to guarantee convergence of Q-learning as long as the behavior policy is\\nsu\\x00ciently close to the target policy, for example, when it is the\"-greedy policy. To the\\nbest of our knowledge, Q-learning has never been found to diverge in this case, but there\\nhas been no theoretical analysis. In the rest of this section we present several other ideas\\nthat have been explored.\\nSuppose that instead of taking just a step toward the expected one-step return on each\\niteration, as in Baird’s counterexample, we actually change the value function all the way\\nto the best, least-squares approximation. Would this solve the instability problem? Of\\ncourse it would if the feature vectors,{x(s): s 2 S}, formed a linearly independent set,\\nas they do in Baird’s counterexample, because then exact approximation is possible on\\neach iteration and the method reduces to standard tabular DP. But of course the point\\nhere is to consider the case when an exact solution isnot possible. In this case stability\\nis not guaranteed even when forming the best approximation at each iteration, as shown\\nin the example.\\nExample 11.1: Tsitsiklis and Van Roy’s CounterexampleThis example shows\\nthat linear function approximation would not work with DP even if the least-squares\\n1\\x00\"\\n\"\\nw 2w\\nsolution was found at each step. The counterexample is formed\\nby extending thew-to-2w example (from earlier in this section)\\nwith a terminal state, as shown to the right. As before, the\\nestimated value of the ﬁrst state isw, and the estimated value\\nof the second state is 2w. The reward is zero on all transitions,\\nso the true values are zero at both states, which is exactly\\nrepresentable with w = 0. If we set wk+1 at each step so\\nas to minimize theVE between the estimated value and the\\nexpected one-step return, then we have\\nwk+1 = argmin\\nw2R\\nX\\ns2S\\n⇣\\nˆv(s,w) \\x00 E⇡\\n⇥\\nRt+1 + \\x00ˆv(St+1,wk)\\n\\x00\\x00 St = s\\n⇤⌘2\\n= argmin\\nw2R\\n\\x00\\nw \\x00 \\x002wk\\n\\x002\\n+\\n\\x00\\n2w \\x00 (1 \\x00 \")\\x002wk\\n\\x002\\n= 6 \\x00 4\"\\n5 \\x00wk. (11.10)\\nThe sequence{wk} diverges when\\x00> 5\\n6\\x004\" and w0 6= 0.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 285, 'page_label': '286'}, page_content='264 Chapter 11: O↵-policy Methods with Approximation\\nAnother way to try to prevent instability is to use special methods for function\\napproximation. In particular, stability is guaranteed for function approximation methods\\nthat do not extrapolate from the observed targets. These methods, calledaveragers,\\ninclude nearest neighbor methods and locally weighted regression, but not popular\\nmethods such as tile coding and artiﬁcial neural networks (ANNs).\\nExercise 11.3 (programming) Apply one-step semi-gradient Q-learning to Baird’s coun-\\nterexample and show empirically that its weights diverge. ⇤\\n11.3 The Deadly Triad\\nOur discussion so far can be summarized by saying that the danger of instability and\\ndivergence arises whenever we combine all of the following three elements, making up\\nwhat we callthe deadly triad:\\nFunction approximationA powerful, scalable way of generalizing from a state space\\nmuch larger than the memory and computational resources (e.g., linear function\\napproximation or ANNs).\\nBootstrapping Update targets that include existing estimates (as in dynamic pro-\\ngramming or TD methods) rather than relying exclusively on actual rewards and\\ncomplete returns (as in MC methods).\\nO↵-policy training Training on a distribution of transitions other than that produced\\nby the target policy. Sweeping through the state space and updating all states\\nuniformly, as in dynamic programming, does not respect the target policy and is\\nan example of o↵-policy training.\\nIn particular, note that the danger isnot due to control or to generalized policy iteration.\\nThose cases are more complex to analyze, but the instability arises in the simpler prediction\\ncase whenever it includes all three elements of the deadly triad. The danger is alsonot\\ndue to learning or to uncertainties about the environment, because it occurs just as\\nstrongly in planning methods, such as dynamic programming, in which the environment\\nis completely known.\\nIf any two elements of the deadly triad are present, but not all three, then instability\\ncan be avoided. It is natural, then, to go through the three and see if there is any one\\nthat can be given up.\\nOf the three, function approximation most clearly cannot be given up. We need\\nmethods that scale to large problems and to great expressive power. We need at least\\nlinear function approximation with many features and parameters. State aggregation or\\nnonparametric methods whose complexity grows with data are too weak or too expensive.\\nLeast-squares methods such as LSTD are of quadratic complexity and are therefore too\\nexpensive for large problems.\\nDoing withoutbootstrapping is possible, at the cost of computational and data e\\x00ciency.\\nPerhaps most important are the losses in computational e\\x00ciency. Monte Carlo (non-\\nbootstrapping) methods require memory to save everything that happens between making'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 286, 'page_label': '287'}, page_content='11.3. The Deadly Triad 265\\neach prediction and obtaining the ﬁnal return, and all their computation is done once the\\nﬁnal return is obtained. The cost of these computational issues is not apparent on serial\\nvon Neumann computers, but would be on specialized hardware. With bootstrapping and\\neligibility traces (Chapter 12), data can be dealt with when and where it is generated,\\nthen need never be used again. The savings in communication and memory made possible\\nby bootstrapping are great.\\nThe losses in data e\\x00ciency by giving upbootstrapping are also signiﬁcant. We\\nhave seen this repeatedly, such as in Chapters 7 (Figure 7.2) and 9 (Figure 9.2), where\\nsome degree of bootstrapping performed much better than Monte Carlo methods on\\nthe random-walk prediction task, and in Chapter 10 where the same was seen on the\\nMountain-Car control task (Figure 10.4). Many other problems show much faster learning\\nwith bootstrapping (e.g., see Figure 12.14). Bootstrapping often results in faster learning\\nbecause it allows learning to take advantage of the state property, the ability to recognize\\na state upon returning to it. On the other hand, bootstrapping can impair learning on\\nproblems where the state representation is poor and causes poor generalization (e.g.,\\nthis seems to be the case on Tetris, seeS¸im¸ sek, Alg´ orta, and Kothiyal, 2016). A poor\\nstate representation can also result in bias; this is the reason for the poorer bound on\\nthe asymptotic approximation quality of bootstrapping methods (Equation 9.14). On\\nbalance, the ability to bootstrap has to be considered extremely valuable. One may\\nsometimes choose not to use it by selecting longn-step updates (or a large bootstrapping\\nparameter, \\x00 ⇡ 1; see Chapter 12) but often bootstrapping greatly increases e\\x00ciency. It\\nis an ability that we would very much like to keep in our toolkit.\\nFinally, there iso↵-policy learning; can we give that up? On-policy methods are often\\nadequate. For model-free reinforcement learning, one can simply use Sarsa rather than\\nQ-learning. O↵-policy methods free behavior from the target policy. This could be\\nconsidered an appealing convenience but not a necessity. However, o↵-policy learning\\nis essential to other anticipated use cases, cases that we have not yet mentioned in this\\nbook but may be important to the larger goal of creating a powerful intelligent agent.\\nIn these use cases, the agent learns not just a single value function and single policy,\\nbut large numbers of them in parallel. There is extensive psychological evidence that\\npeople and animals learn to predict many di↵erent sensory events, not just rewards. We\\ncan be surprised by unusual events, and correct our predictions about them, even if\\nthey are of neutral valence (neither good nor bad). This kind of prediction presumably\\nunderlies predictive models of the world such as are used in planning. We predict what\\nwe will see after eye movements, how long it will take to walk home, the probability of\\nmaking a jump shot in basketball, and the satisfaction we will get from taking on a new\\nproject. In all these cases, the events we would like to predict depend on our acting in\\na certain way. To learn them all, in parallel, requires learning from the one stream of\\nexperience. There are many target policies, and thus the one behavior policy cannot\\nequal all of them. Yet parallel learning is conceptually possible because the behavior\\npolicy may overlap in part with many of the target policies. To take full advantage of\\nthis requires o↵-policy learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 287, 'page_label': '288'}, page_content='266 Chapter 11: O↵-policy Methods with Approximation\\n11.4 Linear Value-function Geometry\\nTo better understand the stability challenge of o↵-policy learning, it is helpful to think\\nabout value function approximation more abstractly and independently of how learning\\nis done. We can imagine the space of all possible state-value functions—all functions\\nfrom states to real numbersv : S ! R. Most of these value functions do not correspond\\nto any policy. More important for our purposes is that most are not representable by the\\nfunction approximator, which by design has far fewer parameters than there are states.\\nGiven an enumeration of the state spaceS = {s1,s 2,...,s |S|}, any value functionv\\ncorresponds to a vector listing the value of each state in order [v(s1),v (s2),...,v (s|S|)]>.\\nThis vector representation of a value function has as many components as there are\\nstates. In most cases where we want to use function approximation, this would be far\\ntoo many components to represent the vector explicitly. Nevertheless, the idea of this\\nvector is conceptually useful. In the following, we treat a value function and its vector\\nrepresentation interchangeably.\\nTo develop intuitions, consider the case with three statesS = {s1,s 2,s 3} and two\\nparameters w =( w1,w 2)>. We can then view all value functions/vectors as points in\\na three-dimensional space. The parameters provide an alternative coordinate system\\nover a two-dimensional subspace. Any weight vectorw =( w1,w 2)> is a point in the\\ntwo-dimensional subspace and thus also a complete value functionvw that assigns values\\nto all three states. With general function approximation the relationship between the\\nfull space and the subspace of representable functions could be complex, but in the case\\nof linear value-function approximation the subspace is a simple plane, as suggested by\\nFigure 11.3.\\nNow consider a single ﬁxed policy⇡. We assume that its true value function,v⇡,i st o o\\ncomplex to be represented exactly as an approximation. Thusv⇡ is not in the subspace;\\nin the ﬁgure it is depicted as being above the planar subspace of representable functions.\\nIf v⇡ cannot be represented exactly, what representable value function is closest to\\nit? This turns out to be a subtle question with multiple answers. To begin, we need\\na measure of the distance between two value functions. Given two value functionsv1\\nand v2, we can talk about the vector di↵erence between them,v = v1 \\x00 v2.I f v is small,\\nthen the two value functions are close to each other. But how are we to measure the size\\nof this di↵erence vector? The conventional Euclidean norm is not appropriate because,\\nas discussed in Section 9.2, some states are more important than others because they\\noccur more frequently or because we are more interested in them (Section 9.11). As\\nin Section 9.2, let us use the distributionµ : S ! [0, 1] to specify the degree to which\\nwe care about di↵erent states being accurately valued (often taken to be the on-policy\\ndistribution). We can then deﬁne the distance between value functions using the norm\\nkvk2\\nµ\\n.=\\nX\\ns2S\\nµ(s)v(s)2. (11.11)\\nNote that theVE from Section 9.2 can be written simply using this norm asVE(w)=\\nkvw \\x00 v⇡k2\\nµ. For any value functionv, the operation of ﬁnding its closest value function\\nin the subspace of representable value functions is a projection operation. We deﬁne a'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 288, 'page_label': '289'}, page_content='11.4. Linear Value-function Geometry 267\\naccording to a stationary decision making policy ⇡ : S ⇥ A ! [0, 1] where ⇡(s, a)i st h e\\nprobability that At = a given that St = s, for all t. To solve the MDP is to ﬁnd an optimal\\npolicy ⇡\\x00, deﬁned as a policy that maximizes the expected \\x00-discounted reward received\\nfrom each state:\\n⇡\\x00 = argmax\\n⇡\\nv⇡ (s), \\x00s 2 S ,\\nwhere\\nv⇡ (s)= E⇡\\n⇥\\nRt+1 + \\x00Rt+2 + \\x002Rt+3 + ···\\n\\x00\\x00 St = s\\n⇤\\n, \\x00s 2 S , (1)\\nwhere \\x00 2 [0, 1) is known as the discount-rate parameter, and the subscript on the E\\nindicates that the expectation is conditional on the policy ⇡ being used to select actions.\\nThe function v⇡ is called the state-value function for policy ⇡.\\nA key subproblem underlying almost all e \\x00cient solution strategies for MDPs is policy\\nevaluation , the computation or estimation of v⇡ for a given policy ⇡. For example, the\\npopular DP algorithm known as policy iteration involves computing the value function for\\na sequence of policies, each of which is better than the previous, until an optimal policy is\\nfound. In TDL, algorithms such as TD( \\x00) are used to approximate the value function for\\nthe current policy, for example as part of actor–critic methods.\\nIf the state space is ﬁnite, then the estimated value function may be represented in a\\ncomputer as a large array with one entry for each state and the entries directly updated to\\nform the estimate. Such tabular methods can handle large state spaces, even continuous\\nones, through discretization, state aggregation, and interpolation, but as the dimensionality\\nof the state space increases, these methods rapidly become computationally infeasible or\\nine ↵ective. This is the e ↵ect which gave rise to the phrase “the curse of dimensionality.”\\nA more general and ﬂexible approach is to represent the value function by a functional\\nform of ﬁxed size and ﬁxed structure with many variable parameters or weights. The weights\\nare then changed to reshape the approximate value function to better match the true value\\nfunction. We denote the parameterized value function approximator as\\nv\\x00 (s) ⇡ v⇡ (s), \\x00s 2 S , (2)\\nwhere ✓ 2 Rn ,w i t hn ⌧ |S| , is the weight/parameter vector. The approximate value\\nfunction can have arbitrary form as long as it is everywhere di ↵erentiable with respect to\\nthe weights. For example, it could be a cubic spline, or it could implemented by a multi-\\nlayer neural network where ✓ is the concatenation of all the connection weights. Henceforth\\nrefer to ✓ exclusively as the weights, or weight vector, and reserve the word “parameter”\\nfor things like the discount-rate parameter, \\x00, and step-size parameters.\\nAn important special case is that in which the approximate value function is linear in\\nthe weights and in features of the state:\\nv\\x00 (s)= ✓\\x00 \\x00(s), (3)\\nwhere the \\x00(s) 2 Rn , \\x00s 2 S , are feature vectors characterizing each state s, and x\\x00 y\\ndenotes the inner product of two vectors x and y.\\n2\\nThe subspace of all value functions representable as \\nBellman error vector (BE)\\nThe other two goals for approximation are related to the Bellman equation , which can\\nbe written compactly in vector form as\\nv⇡ = B⇡ v⇡ , (7)\\nwhere B⇡ : R|S| ! R|S| is the Bellman operator for policy ⇡,d e ﬁ n e db y\\n(B⇡ v)(s)=\\nX\\na\\x00A\\n⇡(s, a)\\n\"\\nr(s, a)+ \\x00\\nX\\ns0 \\x00S\\np(s\\x00|s, a)v(s\\x00)\\n#\\n, \\x00s 2 S , \\x00v : S ! R. (8)\\n(If the state and action spaces are continuous, then the sums are replaced by integrals and\\nthe function p(·| s, a) is taken to be a probability density.) The true value function v⇡ is\\nthe unique solution to the Bellman equation; the Bellman equation can be viewed as an\\nalternate way of deﬁning v⇡ . For any value function v : S ! R not equal to v⇡ ,t h e r ew i l l\\nalways be at least one state s at which v(s) 6=( B⇡ v)(s).\\nThe discrepancy between the two sides of the Bellman equation, v⇡ \\x00 B⇡ v⇡ , is an error\\nvector, and reducing it is the basis for our second and third goals for approximation. The\\nsecond goal is to minimize the error vector’s length in the d-metric. That is, to minimize\\nthe mean-squared Bellman error :\\nBE( ✓)=\\nX\\ns\\x00S\\nd(s)\\n⇥\\n(B⇡ v\\x00 )(s) \\x00 v\\x00 (s)\\n⇤2. (9)\\nNote that if v⇡ is not representable, then it is not be possible to reduce the Bellman error\\nto zero. For any v\\x00 , the corresponding B⇡ v\\x00 will generally not be representable; it will lie\\noutside the space of representable functions, as suggested by the ﬁgure...\\nFinally, in our third goal of approximation, we ﬁrst project the Bellman error and then\\nminimize its length. That is, we minimize the error not in the Bellman equation (7) but in\\nits projected form:\\nv\\x00 = ⇧B⇡ v\\x00 , (10)\\nUnlike the original Bellman equation, for most function approximators (e.g., linear ones)\\nthe projected Bellman equation can be solved exactly. If it can’t be solved exactly, you can\\nminimize the mean-squared projected Bellman error :\\nPBE( ✓)=\\nX\\ns\\x00S\\nd(s)\\n⇥\\n(⇧(B⇡ v\\x00 \\x00 v\\x00 ))(s)\\n⇤2. (11)\\nThe minimum is achieved at the projection ﬁxpoint , at which\\nX\\ns\\x00S\\nd(s)\\n⇥\\n(B⇡ v\\x00 )(s) \\x00 v\\x00 (s)\\n⇤\\nr\\x00 v\\x00 (s)= \\x000. (12)\\np\\nVE\\np\\nBE\\np\\nPBE ⇧v⇡ = v\\x00⇤\\nVE v\\x00⇤\\nPBE v\\x00⇤\\nBE\\n4\\n2.2 Bellman error\\nThe second goal for approximation is to approximately solve the\\nBellman equation:\\nv⇡ =B⇡v⇡,\\n(8)\\nwhereB⇡ : R|S| !R|S| is theBellman operatorfor policy⇡,d e ﬁ n e db y\\n(B⇡v)(s)=\\nX\\na\\x00A\\n⇡(s, a)\\n\"\\nr(s, a)+ \\x00\\nX\\ns0\\x00S\\np(s\\x00|s, a)v(s\\x00)\\n#\\n, \\x00s 2S,\\x00v : S !R. (9)\\n(If the state and action spaces are continuous, then the sums are replaced by integrals and\\nthe functionp(·| s, a) is taken to be a probability density.) The true value function\\nv⇡ is\\nthe unique solution to the Bellman equation, and in this sense the Bellman equation can\\nbe viewed as an alternate way of deﬁning\\nv⇡. For any value function\\nv\\x00 not equal tov⇡,w e\\ncan ask the Bellman equation to hold approximately,\\nv\\x00 ⇡B⇡v\\x00. That is, we can minimize\\ntheBellman error: BE(✓)= ||v\\x00 \\x00B⇡v\\x00||,\\n(10)\\nthough we cannot expect to drive it to zero if\\nv⇡ is outside the representable subspace.\\nFigure 1 shows the geometric relationships; note that the Bellman operator is shown as\\ntaking value functions inside the subspace outside to something that is not representable,\\nand that the point of minimum BE is in general di\\n↵erent from that of minimum VE.\\nThe BE was ﬁrst proposed as an objective function for DP by Schweitzer and Seidmann\\n(1985). Baird (1995, 1999) extended it to TDL based on stochastic gradient descent, and\\nEngel, Mannor, and Meir (2003) extended it to least squares (\\nO(n2)) methods known as\\nGaussian Process TDL. In the literature, BE minimization is often referred to as Bellman\\nresidual minimization.\\n2.3 Projected Bellman error\\nThe third goal for approximation is to approximately solve the\\nprojectedBellman equation:\\nv\\x00 =⇧B⇡v\\x00.\\n(11)\\nUnlike the original Bellman equation, for most function approximators (e.g., linear ones) the\\nprojected Bellman equation can be solved exactly. The original TDL methods (Sutton 1988,\\nDayan 1992) converge to this solution, as does least-squares TDL (Bradke & Barto 1996,\\nBoyan 1999). The goal of achieving (11) exactly is common; less common is to consider\\napproximating it as an objective. The early work on gradient-TD (e.g., Sutton et al. 2009)\\nappears to be ﬁrst to have explicitly proposed minimizing the\\nd-weighted norm of the error\\nin (11), which we here call the\\nprojected Bellman error\\n:\\nPBE(✓)= ||v\\x00 \\x00⇧B⇡v\\x00||.\\n(12)\\nThis objective is best understood by looking at the left side of Figure 1. Starting at\\nv\\x00,\\nthe Bellman operator takes us outside the subspace, and the projection operator takes us\\nback into it. The distance between where we end up and where we started is the PBE. The\\ndistance is minimal (zero) when the trip up and back leaves us in the same place.8\\nValue error (VE)\\nw1\\nw2\\nvw\\nvw\\nB⇡ vw\\n⇧B⇡ vw\\nwTD\\nPBE =\\x000\\nThe 3D space of \\nall value functions \\nover 3 states\\nminBE\\nminTDE\\n(minVE)\\nFigure 11.3: The geometry of linear value-function approximation. Shown is the three-\\ndimensional space of all value functions over three states, while shown as a plane is the subspace of\\nall value functions representable by a linear function approximator with parameterw =( w1,w 2)>.\\nThe true value functionv⇡ is in the larger space and can be projected down (into the subspace,\\nusing a projection operator ⇧) to its best approximation in the value error (VE) sense. The\\nbest approximators in the Bellman error (BE), projected Bellman error (PBE), and temporal\\ndi↵erence error (TDE) senses are all potentially di↵erent and are shown in the lower right. (VE,\\nBE, and PBE are all treated as the corresponding vectors in this ﬁgure.) The Bellman operator\\ntakes a value function in the plane to one outside, which can then be projected back. If you\\niteratively applied the Bellman operator outside the space (shown in gray above) you would\\nreach the true value function, as in conventional dynamic programming. If instead you kept\\nprojecting back into the subspace at each step, as in the lower step shown in gray, then the ﬁxed\\npoint would be the point of vector-zero PBE.\\nprojection operator ⇧ that takes an arbitrary value function to the representable function\\nthat is closest in our norm:\\n⇧v .= vw where w = argmin\\nw2Rd\\nkv \\x00 vwk2\\nµ . (11.12)\\nThe representable value function that is closest to the true value functionv⇡ is thus its\\nprojection, ⇧v⇡, as suggested in Figure 11.3. This is the solution asymptotically found\\nby Monte Carlo methods, albeit often very slowly. The projection operation is discussed\\nmore fully in the box on the next page.\\nTD methods ﬁnd di↵erent solutions. To understand their rationale, recall that the\\nBellman equation for value functionv⇡ is\\nv⇡(s)=\\nX\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)[ r + \\x00v⇡(s0)] , for alls 2 S. (11.16)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 289, 'page_label': '290'}, page_content='268 Chapter 11: O↵-policy Methods with Approximation\\nThe projection matrix\\nFor a linear function approximator, the projection operation is linear, which implies\\nthat it can be represented as an|S|⇥| S| matrix:\\n⇧ .= X\\n\\x00\\nX>DX\\n\\x00\\x001\\nX>D, (11.13)\\nwhere, as in Section 9.4,D denotes the |S|⇥| S| diagonal matrix with theµ(s)\\non the diagonal, andX denotes the |S|⇥ d matrix whose rows are the feature\\nvectors x(s)>, one for each states.I f t h e i n v e r s e i n(11.13) does not exist, then the\\npseudoinverse is substituted. Using these matrices, the squared norm of a vector\\ncan be written\\nkvk2\\nµ = v>Dv, (11.14)\\nand the approximate linear value function can be written\\nvw = Xw. (11.15)\\nThe true value functionv⇡ is the only value function that solves(11.16) exactly. If an\\napproximate value functionvw were substituted forv⇡, the di↵erence between the right\\nand left sides of the modiﬁed equation could be used as a measure of how far o↵vw is\\nfrom v⇡. We call this theBellman error at states:\\n¯\\x00w(s) .=\\n0\\n@X\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)[ r + \\x00vw(s0)]\\n1\\nA \\x00 vw(s) (11.17)\\n= E⇡\\n⇥\\nRt+1 + \\x00vw(St+1) \\x00 vw(St)\\n\\x00\\x00 St = s, At ⇠ ⇡\\n⇤\\n, (11.18)\\nwhich shows clearly the relationship of the Bellman error to the TD error(11.3).T h e\\nBellman error is the expectation of the TD error.\\nThe vector of all the Bellman errors, at all states,¯\\x00w 2 R|S|, is called theBellman\\nerror vector (shown as BE in Figure 11.3). The overall size of this vector, in the norm, is\\nan overall measure of the error in the value function, called themean square Bellman\\nerror:\\nBE(w)=\\n\\x00\\x00¯\\x00w\\n\\x00\\x002\\nµ . (11.19)\\nIt is not possible in general to reduce theBE to zero (at which pointvw = v⇡), but for\\nlinear function approximation there is a unique value ofw for which theBE is minimized.\\nThis point in the representable-function subspace (labeledmin BE in Figure 11.3) is\\ndi↵erent in general from that which minimizes theVE (shown as ⇧v⇡). Methods that\\nseek to minimize theBE are discussed in the next two sections.\\nThe Bellman error vector is shown in Figure 11.3 as the result of applying theBellman\\noperator B⇡ : R|S| ! R|S| to the approximate value function. The Bellman operator is'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 290, 'page_label': '291'}, page_content='11.5. Gradient Descent in the Bellman Error 269\\ndeﬁned by\\n(B⇡v)(s) .=\\nX\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)[ r + \\x00v(s0)] , (11.20)\\nfor alls 2 S, v : S ! R. The Bellman error vector forvw can be written¯\\x00w = B⇡vw \\x00vw.\\nIf the Bellman operator is applied to a value function in the representable subspace,\\nthen, in general, it will produce a new value function that is outside the subspace, as\\nsuggested in the ﬁgure. In dynamic programming (without function approximation), this\\noperator is applied repeatedly to the points outside the representable space, as suggested\\nby the gray arrows in the top of Figure 11.3. Eventually that process converges to the\\ntrue value functionv⇡, the only ﬁxed point for the Bellman operator, the only value\\nfunction for which\\nv⇡ = B⇡v⇡, (11.21)\\nwhich is just another way of writing the Bellman equation for⇡ (11.16).\\nWith function approximation, however, the intermediate value functions lying outside\\nthe subspace cannot be represented. The gray arrows in the upper part of Figure 11.3\\ncannot be followed because after the ﬁrst update (dark line) the value function must\\nbe projected back into something representable. The next iteration then begins within\\nthe subspace; the value function is again taken outside of the subspace by the Bellman\\noperator and then mapped back by the projection operator, as suggested by the lower\\ngray arrow and line. Following these arrows is a DP-like process with approximation.\\nIn this case we are interested in the projection of the Bellman error vector back into\\nthe representable space. This is the projected Bellman error vector ⇧¯\\x00w,s h o w ni n\\nFigure 11.3 as PBE. The size of this vector, in the norm, is another measure of error in\\nthe approximate value function. For any approximate value functionvw,w ed e ﬁ n et h e\\nmean square Projected Bellman error, denotedPBE, as\\nPBE(w)=\\n\\x00\\x00⇧¯\\x00w\\n\\x00\\x002\\nµ . (11.22)\\nWith linear function approximation there always exists an approximate value function\\n(within the subspace) with zeroPBE; this is the TD ﬁxed point,wTD,i n t r o d u c e di n\\nSection 9.4. As we have seen, this point is not always stable under semi-gradient TD\\nmethods and o↵-policy training. As shown in the ﬁgure, this value function is generally\\ndi↵erent from those minimizingVE or BE. Methods that are guaranteed to converge to\\nit are discussed in Sections 11.7 and 11.8.\\n11.5 Gradient Descent in the Bellman Error\\nArmed with a better understanding of value function approximation and its various\\nobjectives, we return now to the challenge of stability in o↵-policy learning. We would\\nlike to apply the approach of stochastic gradient descent (SGD, Section 9.3), in which\\nupdates are made that in expectation are equal to the negative gradient of an objective'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 291, 'page_label': '292'}, page_content='270 Chapter 11: O↵-policy Methods with Approximation\\nfunction. These methods always go downhill (in expectation) in the objective and because\\nof this are typically stable with excellent convergence properties. Among the algorithms\\ninvestigated so far in this book, only the Monte Carlo methods are true SGD methods.\\nThese methods converge robustly under both on-policy and o↵-policy training as well\\nas for general nonlinear (di↵erentiable) function approximators, though they are often\\nslower than semi-gradient methods with bootstrapping, which are not SGD methods.\\nSemi-gradient methods may diverge under o↵-policy training, as we have seen earlier in\\nthis chapter, and under contrived cases of nonlinear function approximation (Tsitsiklis\\nand Van Roy, 1997). With a true SGD method such divergence would not be possible.\\nThe appeal of SGD is so strong that great e↵ort has gone into ﬁnding a practical\\nway of harnessing it for reinforcement learning. The starting place of all such e↵orts is\\nthe choice of an error or objective function to optimize. In this and the next section\\nwe explore the origins and limits of the most popular proposed objective function, that\\nbased on theBellman error introduced in the previous section. Although this has been a\\npopular and inﬂuential approach, the conclusion that we reach here is that it is a misstep\\nand yields no good learning algorithms. On the other hand, this approach fails in an\\ninteresting way that o↵ers insight into what might constitute a good approach.\\nTo begin, let us consider not the Bellman error, but something more immediate\\nand naive. Temporal di↵erence learning is driven by the TD error. Why not take the\\nminimization of the expected square of the TD error as the objective? In the general\\nfunction-approximation case, the one-step TD error with discounting is\\n\\x00t = Rt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt).\\nA possible objective function then is what one might call themean square TD error:\\nTDE(w)=\\nX\\ns2S\\nµ(s)E\\n⇥\\n\\x002\\nt\\n\\x00\\x00 St =s, At ⇠⇡\\n⇤\\n=\\nX\\ns2S\\nµ(s)E\\n⇥\\n⇢t\\x002\\nt\\n\\x00\\x00 St =s, At ⇠b\\n⇤\\n= Eb\\n⇥\\n⇢t\\x002\\nt\\n⇤\\n. (if µ is the distribution encountered underb)\\nThe last equation is of the form needed for SGD; it gives the objective as an expectation\\nthat can be sampled from experience (remember the experience is due to the behavior\\npolicy b). Thus, following the standard SGD approach, one can derive the per-step update\\nbased on a sample of this expected value:\\nwt+1 = wt \\x00 1\\n2↵r(⇢t\\x002\\nt )\\n= wt \\x00 ↵⇢t\\x00tr\\x00t\\n= wt + ↵⇢t\\x00t\\n\\x00\\nrˆv(St,wt) \\x00 \\x00rˆv(St+1,wt)\\n\\x00\\n, (11.23)\\nwhich you will recognize as the same as the semi-gradient TD algorithm(11.2) except for\\nthe additional ﬁnal term. This term completes the gradient and makes this a true SGD\\nalgorithm with excellent convergence guarantees. Let us call this algorithm thenaive'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 292, 'page_label': '293'}, page_content='11.5. Gradient Descent in the Bellman Error 271\\nresidual-gradient algorithm (after Baird, 1995). Although the naive residual-gradient\\nalgorithm converges robustly, it does not necessarily converge to a desirable place.\\nExample 11.2: A-split example,\\nshowing the naivet´ e of the naive residual-gradient algorithm\\nA\\nB\\nC0 0\\n0 1\\nConsider the three-state episodic MRP shown to the right.\\nEpisodes begin in stateA and then ‘split’ stochastically, half\\nthe time going toB (and then invariably going on to terminate\\nwith a reward of 1) and half the time going to stateC (and\\nthen invariably terminating with a reward of zero). Reward for\\nthe ﬁrst transition, out ofA, is always zero whichever way the\\nepisode goes. As this is an episodic problem, we can take\\x00 to\\nbe 1. We also assume on-policy training, so that⇢t is always\\n1, and tabular function approximation, so that the learning algorithms are free to\\ngive arbitrary, independent values to all three states. Thus, this should be an easy\\nproblem.\\nWhat should the values be? FromA, half the time the return is 1, and half the\\ntime the return is 0;A should have value1\\n2 . From B the return is always 1, so its\\nvalue should be 1, and similarly fromC the return is always 0, so its value should\\nbe 0. These are the true values and, as this is a tabular problem, all the methods\\npresented previously converge to them exactly.\\nHowever, the naive residual-gradient algorithm ﬁnds di↵erent values forB and\\nC. It converges withB having a value of3\\n4 and C having a value of1\\n4 (A converges\\ncorrectly to 1\\n2 ). These are in fact the values that minimize theTDE.\\nLet us compute theTDE for these values. The ﬁrst transition of each episode is\\neither up fromA’s 1\\n2 to B’s 3\\n4 , a change of1\\n4 , or down fromA’s 1\\n2 to C’s 1\\n4 , a change\\nof \\x001\\n4 . Because the reward is zero on these transitions, and\\x00 = 1, these changesare\\nthe TD errors, and thus the squared TD error is always1\\n16 on the ﬁrst transition.\\nThe second transition is similar; it is either up fromB’s 3\\n4 to a reward of 1 (and a\\nterminal state of value 0), or down fromC’s 1\\n4 to a reward of 0 (again with a terminal\\nstate of value 0). Thus, the TD error is always± 1\\n4 , for a squared error of1\\n16 on the\\nsecond step. Thus, for this set of values, theTDE on both steps is1\\n16 .\\nNow let’s compute theTDE for the true values (B at 1,C at 0, andA at 1\\n2 ). In this\\ncase the ﬁrst transition is either from1\\n2 up to 1, atB, or from1\\n2 down to 0, atC;i n\\neither case the absolute error is1\\n2 and the squared error is1\\n4 . The second transition\\nhas zero error because the starting value, either 1 or 0 depending on whether the\\ntransition is fromB or C, always exactly matches the immediate reward and return.\\nThus the squared TD error is1\\n4 on the ﬁrst transition and 0 on the second, for a\\nmean reward over the two transitions of1\\n8 . As 1\\n8 is bigger that 1\\n16 , this solution is\\nworse according to theTDE. On this simple problem, the true values do not have\\nthe smallestTDE.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 293, 'page_label': '294'}, page_content='272 Chapter 11: O↵-policy Methods with Approximation\\nA tabular representation is used in the A-split example, so the true state values can\\nbe exactly represented, yet the naive residual-gradient algorithm ﬁnds di↵erent values,\\nand these values have lowerTDE than do the true values. Minimizing theTDE is naive;\\nby penalizing all TD errors it achieves something more like temporal smoothing than\\naccurate prediction.\\nA better idea would seem to be minimizing the mean square Bellman error (BE). If\\nthe exact values are learned, the Bellman error is zero everywhere. Thus, a Bellman-\\nerror-minimizing algorithm should have no trouble with the A-split example. We cannot\\nexpect to achieve zero Bellman error in general, as it would involve ﬁnding the true value\\nfunction, which we presume is outside the space of representable value functions. But\\ngetting close to this ideal is a natural-seeming goal. As we have seen, the Bellman error\\nis also closely related to the TD error. The Bellman error for a state is the expected TD\\nerror in that state. So let’s repeat the derivation above with the expected TD error (all\\nexpectations here are implicitly conditional onSt):\\nwt+1 = wt \\x00 1\\n2↵r(E⇡[\\x00t]2)\\n= wt \\x00 1\\n2↵r(Eb[⇢t\\x00t]2)\\n= wt \\x00 ↵Eb[⇢t\\x00t] rEb[⇢t\\x00t]\\n= wt \\x00 ↵Eb\\n⇥\\n⇢t(Rt+1 + \\x00ˆv(St+1,w) \\x00 ˆv(St,w))\\n⇤\\nEb[⇢tr\\x00t]\\n= wt + ↵\\nh\\nEb\\n⇥\\n⇢t(Rt+1 + \\x00ˆv(St+1,w))\\n⇤\\n\\x00 ˆv(St,w)\\nih\\nrˆv(St,w) \\x00 \\x00Eb\\n⇥\\n⇢trˆv(St+1,w)\\n⇤i\\n.\\nThis update and various ways of sampling it are referred to as theresidual-gradient\\nalgorithm. If you simply used the sample values in all the expectations, then the equation\\nabove reduces almost exactly to(11.23), the naive residual-gradient algorithm.1 But\\nthis is naive, because the equation above involves the next state,St+1, appearing in two\\nexpectations that are multiplied together. To get an unbiased sample of the product,\\ntwo independent samples of the next state are required, but during normal interaction\\nwith an external environment only one is obtained. One expectation or the other can be\\nsampled, but not both.\\nThere are two ways to make the residual-gradient algorithm work. One is in the case\\nof deterministic environments. If the transition to the next state is deterministic, then\\nthe two samples will necessarily be the same, and the naive algorithm is valid. The\\nother way is to obtaintwo independent samples of the next state,St+1, fromSt, one for\\nthe ﬁrst expectation and another for the second expectation. In real interaction with\\nan environment, this would not seem possible, but when interacting with a simulated\\nenvironment, it is. One simply rolls back to the previous state and obtains an alternate\\nnext state before proceeding forward from the ﬁrst next state. In either of these cases the\\nresidual-gradient algorithm is guaranteed to converge to a minimum of theBE under the\\nusual conditions on the step-size parameter. As a true SGD method, this convergence is\\n1For state values there remains a small di↵erence in the treatment of the importance sampling ratio\\n⇢t.I n t h e a n a l a g o u s a c t i o n - v a l u e c a s e ( w h i c h i s t h e m o s t i m p o r t a n t c a s e f o r c o n t r o l a l g o r i t h m s ) , t h e\\nresidual-gradient algorithm would reduce exactly to the naive version.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 294, 'page_label': '295'}, page_content='11.5. Gradient Descent in the Bellman Error 273\\nrobust, applying to both linear and nonlinear function approximators. In the linear case,\\nconvergence is always to theunique w that minimizes theBE.\\nHowever, there remain at least three ways in which the convergence of the residual-\\ngradient method is unsatisfactory. The ﬁrst of these is that empirically it is slow, much\\nslower that semi-gradient methods. Indeed, proponents of this method have proposed\\nincreasing its speed by combining it with faster semi-gradient methods initially, then\\ngradually switching over to residual gradient for the convergence guarantee (Baird and\\nMoore, 1999). The second way in which the residual-gradient algorithm is unsatisfactory\\nis that it still seems to converge to the wrong values. It does get the right values in all\\ntabular cases, such as the A-split example, as for those an exact solution to the Bellman\\nExample 11.3: A-presplit example, a counterexample for theBE\\nA1 B\\nC0 0\\n0 1\\nA2\\nA\\nConsider the three-state episodic MRP shown to the\\nright: Episodes start in either A1 or A2, with equal\\nprobability. These two states look exactly the same to\\nthe function approximator, like a single stateA whose\\nfeature representation is distinct from and unrelated to\\nthe feature representation of the other two states,B and\\nC, which are also distinct from each other. Speciﬁcally,\\nthe parameter of the function approximator has three\\ncomponents, one giving the value of stateB, one giving the value of stateC, and one\\ngiving the value of both statesA1 and A2. Other than the selection of the initial\\nstate, the system is deterministic. If it starts inA1, then it transitions toB with a\\nreward of 0 and then on to termination with a reward of 1. If it starts inA2,t h e ni t\\ntransitions toC, and then to termination, with both rewards zero.\\nTo a learning algorithm, seeing only the features, the system looks identical to\\nthe A-split example. The system seems to always start inA, followed by either\\nB or C with equal probability, and then terminating with a 1 or a 0 depending\\ndeterministically on the previous state. As in the A-split example, the true values\\nof B and C are 1 and 0, and the best shared value ofA1 and A2 is 1\\n2 ,b ys y m m e t r y .\\nBecause this problem appears externally identical to the A-split example, we\\nalready know what values will be found by the algorithms. Semi-gradient TD\\nconverges to the ideal values just mentioned, while the naive residual-gradient\\nalgorithm converges to values of 3\\n4 and 1\\n4 for B and C respectively. All state\\ntransitions are deterministic, so the non-naive residual-gradient algorithm will also\\nconverge to these values (it is the same algorithm in this case). It follows then that\\nthis ‘naive’ solution must also be the one that minimizes theBE, and so it is. On a\\ndeterministic problem, the Bellman errors and TD errors are all the same, so the\\nBE is always the same as theTDE.O p t i m i z i n g t h eBE on this example gives rise to\\nthe same failure mode as with the naive residual-gradient algorithm on the A-split\\nexample.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 295, 'page_label': '296'}, page_content='274 Chapter 11: O↵-policy Methods with Approximation\\nequation is possible. But if we examine examples with genuine function approximation,\\nthen the residual-gradient algorithm, and indeed theBE objective, seem to ﬁnd the\\nwrong value functions. One of the most telling such examples is the variation on the\\nA-split example known as the A-presplit example, shown on the preceding page, in which\\nthe residual-gradient algorithm ﬁnds the same poor solution as its naive version. This\\nexample shows intuitively that minimizing theBE (which the residual-gradient algorithm\\nsurely does) may not be a desirable goal.\\nThe third way in which the convergence of the residual-gradient algorithm is not\\nsatisfactory is explained in the next section. Like the second way, the third way is also\\na problem with theBE objective itself rather than with any particular algorithm for\\nachieving it.\\n11.6 The Bellman Error is Not Learnable\\nThe concept of learnability that we introduce in this section is di↵erent from that\\ncommonly used in machine learning. There, a hypothesis is said to be “learnable” if\\nit is e\\x00ciently learnable, meaning that it can be learned within a polynomial rather\\nthan an exponential number of examples. Here we use the term in a more basic way,\\nto mean learnable at all, with any amount of experience. It turns out many quantities\\nof apparent interest in reinforcement learning cannot be learned even from an inﬁnite\\namount of experiential data. These quantities are well deﬁned and can be computed\\ngiven knowledge of the internal structure of the environment, but cannot be computed\\nor estimated from the observed sequence of feature vectors, actions, and rewards.2 We\\nsay that they are notlearnable. It will turn out that the Bellman error objective (BE)\\nintroduced in the last two sections is not learnable in this sense. That the Bellman error\\nobjective cannot be learned from the observable data is probably the strongest reason\\nnot to seek it.\\nTo make the concept of learnability clear, let’s start with some simple examples.\\nConsider the two Markov reward processes3 (MRPs) diagrammed below:\\n0 2 0 2\\n2\\n0\\nw w w\\nWhere two edges leave a state, both transitions are assumed to occur with equal probability,\\nand the numbers indicate the reward received. All the states appear the same; they all\\nproduce the same single-component feature vectorx = 1 and have approximated value\\nw. Thus, the only varying part of the data trajectory is the reward sequence. The left\\nMRP stays in the same state and emits an endless stream of 0s and 2s at random, each\\nwith 0.5 probability. The right MRP, on every step, either stays in its current state or\\n2They would of course be estimated if thestate sequence were observed rather than only the\\ncorresponding feature vectors.\\n3All MRPs can be considered MDPs with a single action in all states; what we conclude about MRPs\\nhere applies as well to MDPs.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 296, 'page_label': '297'}, page_content='11.6. The Bellman Error is Not Learnable 275\\nswitches to the other, with equal probability. The reward is deterministic in this MRP,\\nalways a 0 from one state and always a 2 from the other, but because the each state\\nis equally likely on each step, the observable data is again an endless stream of 0s and\\n2s at random, identical to that produced by the left MRP. (We can assume the right\\nMRP starts in one of two states at random with equal probability.) Thus, even given\\nan inﬁnite amount of data, it would not be possible to tell which of these two MRPs\\nwas generating it. In particular, we could not tell if the MRP has one state or two, is\\nstochastic or deterministic. These things are not learnable.\\nThis pair of MRPs also illustrates that theVE objective (9.1) is not learnable. If\\n\\x00 = 0, then the true values of the three states (in both MRPs), left to right, are 1, 0,\\nand 2. Suppose w = 1. Then theVE is 0 for the left MRP and 1 for the right MRP.\\nBecause the VE is di↵erent in the two problems, yet the data generated has the same\\ndistribution, the VE cannot be learned. TheVE is not a unique function of the data\\ndistribution. And if it cannot be learned, then how could theVE possibly be useful as\\nan objective for learning?\\nIf an objective cannot be learned, it does indeed draw its utility into question. In\\nthe case of theVE, however, there is a way out. Note that the same solution,w = 1,\\nis optimal for both MRPs above (assumingµ is the same for the two indistinguishable\\nstates in the right MRP). Is this a coincidence, or could it be generally true that all\\nMDPs with the same data distribution also have the same optimal parameter vector? If\\nthis is true—and we will show next that it is—then theVE remains a usable objective.\\nThe VE is not learnable, but the parameter that optimizes it is!\\nTo understand this, it is useful to bring in another natural objective function, this\\ntime one that is clearly learnable. One error that is always observable is that between the\\nvalue estimate at each time and the return from that time. Themean square return error,\\ndenoted RE, is the expectation, underµ, of the square of this error. In the on-policy case\\nthe RE can be written\\nRE(w)= E\\nh\\x00\\nGt \\x00 ˆv(St,w)\\n\\x002i\\n= VE(w)+ E\\nh\\x00\\nGt \\x00 v⇡(St)\\n\\x002i\\n. (11.24)\\nThus, the two objectives are the same except for a variance term that does not depend on\\nthe parameter vector. The two objectives must therefore have the same optimal parameter\\nvalue w⇤. The overall relationships are summarized in the left side of Figure 11.4.\\n⇤Exercise 11.4 Prove (11.24). Hint: Write theRE as an expectation over possible states\\ns of the expectation of the squared error given thatSt = s. Then add and subtract the\\ntrue value of states from the error (before squaring), grouping the subtracted true value\\nwith the return and the added true value with the estimated value. Then, if you expand\\nthe square, the most complex term will end up being zero, leaving you with (11.24).⇤\\nNow let us return to theBE.T h eBE is like theVE in that it can be computed from\\nknowledge of the MDP but is not learnable from data. But it is not like theVE in that its\\nminimum solution is not learnable. The box on the next page presents a counterexample—\\ntwo MRPs that generate the same data distribution but whose minimizing parameter\\nvector is di↵erent, proving that the optimal parameter vector is not a function of the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 297, 'page_label': '298'}, page_content='276 Chapter 11: O↵-policy Methods with Approximation\\nExample 11.4: Counterexample to the learnability of the Bellman error\\nTo show the full range of possibilities we need a slightly more complex pair of Markov\\nreward processes (MRPs) than those considered earlier. Consider the following two\\nMRPs:\\nBA\\n1\\n0\\n-1 BA\\n0\\n-1 B\\x01\\n0\\n1 -1\\nWhere two edges leave a state, both transitions are assumed to occur with equal\\nprobability, and the numbers indicate the reward received. The MRP on the left has\\ntwo states that are represented distinctly. The MRP on the right has three states,\\ntwo of which,B and B0, appear the same and must be given the same approximate\\nvalue. Speciﬁcally,w has two components and the value of stateA is given by the ﬁrst\\ncomponent and the value ofB and B0 is given by the second. The second MRP has\\nbeen designed so that equal time is spent in all three states, so we can takeµ(s)= 1\\n3 ,\\nfor alls.\\nNote that the observable data distribution is identical for the two MRPs. In both\\ncases the agent will see single occurrences ofA followed by a 0, then some number\\nof apparentBs, each followed by a\\x001 except the last, which is followed by a 1, then\\nwe start all over again with a singleA and a 0, etc. All the statistical details are the\\nsame as well; in both MRPs, the probability of a string ofk Bsi s2\\x00k.\\nNow suppose w = 0. In the ﬁrst MRP, this is an exact solution, and theBE is\\nzero. In the second MRP, this solution produces a squared error in bothB and B0 of\\n1, such thatBE = µ(B)1 +µ(B0)1 =2\\n3 . These two MRPs, which generate the same\\ndata distribution, have di↵erentBEs; theBE is not learnable.\\nMoreover (and unlike the earlier example for theVE) the minimizing value ofw\\nis di↵erent for the two MRPs. For the ﬁrst MRP,w = 0 minimizes theBE for any\\n\\x00. For the second MRP, the minimizingw is a complicated function of\\x00,b u ti n\\nthe limit, as\\x00 ! 1, it is (\\x001\\n2 , 0)>. Thus the solution that minimizesBE cannot be\\nestimated from data alone; knowledge of the MRP beyond what is revealed in the\\ndata is required. In this sense, it is impossible in principle to pursue theBE as an\\nobjective for learning.\\nIt may be surprising that in the second MRP theBE-minimizing value ofA is so far\\nfrom zero. Recall thatA has a dedicated weight and thus its value is unconstrained\\nby function approximation.A is followed by a reward of 0 and transition to a state\\nwith a value of nearly 0, which suggestsvw(A) should be 0; why is its optimal\\nvalue substantially negative rather than 0? The answer is that makingvw(A) negative\\nreduces the error upon arriving inA from B. The reward on this deterministic transition\\nis 1, which implies thatB should have a value 1 more thanA. Because B’s value is\\napproximately zero,A’s value is driven toward\\x001. The BE-minimizing value of⇡\\x00 1\\n2\\nfor A is a compromise between reducing the errors on leaving and on enteringA.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 298, 'page_label': '299'}, page_content='11.6. The Bellman Error is Not Learnable 277\\nMDP1 MDP2\\nMSBE1 MSBE2\\npolicy together completely determine the probability distribution over data trajectories.Assume for the moment that the state, action, and reward sets are all ﬁnite. Then,for any ﬁnite sequence\\x00=\\x000,a0,r1,...,rk,\\x00k, there is a well deﬁned probability (pos-sibly zero) of it occuring as the initial portion of a trajectory, which we may denotedP(\\x00)= Pr{\\x00(S0)=\\x000,A0=a0,R1=r1,...,Rk=rk,\\x00(Sk)=\\x00k}. The distributionPthen is a complete characterization of a source of data trajectories. To knowPis to knoweverything about the statistics of the data, but it is still less than knowing the MDP. Inparticular, the VE and BE objectives are readily computed from the MDP as described inSection 3, but these cannot be determined fromPalone.\\n✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\\n✓\\x001 ✓\\x002 ✓\\x003 ✓\\x004 BE1 BE2 MDP1 MDP2\\nTDE RE VEThe problem can be seen in very simple, POMDP-like examples, in which the observabledata produced by two di↵erent MDPs is identical in every respect, yet the BE is di↵erent.In such a case the BE is literally not a function of the data, and thus there is no way toestimate it from data. One of the simplest examples is the pair of MDPs shown below:\\nBA 1\\n0\\n-1 BA 0\\n-1 B\\x01\\n0\\n1 -1\\nThese MDPs have only one action (or, equivalently, no actions), so they are in e↵ect Markovchains. Where two edges leave a state, both possibilities are assumed to occur with equalprobability. The numbers on the edges indicate the reward emitted if that edge is traversed.The MDP on the left has two states that are represented distinctly; each has a separateweight so that they can take on any value. The MDP on the right has three states, twoof which,BandB\\x00, are represented identically and must be given the same approximatevalue. We can imagine that the value of stateAis given by the ﬁrst component of✓andthe value ofBandB\\x00 is given by the second. Notice that the observable data is identicalfor the two MDPs. In both cases the agent will see single occurrences ofAfollowed by a0, then some number ofBs each followed by a\\x001, except the last which is followed by a1, then we start all over again with a singleAand a 0, etc. All the details are the sameas well; in both MDPs, the probability of a string ofkBsi s2\\x00k. Now consider the valuefunctionv\\x00=\\x000. In the ﬁrst MDP, this is an exact solution, and the overall BE is zero. Inthe second MDP, this solution produces an error in bothBandB\\x00 of 1, for an overall BEofpd(B)+d(B\\x00), orp2/3 if the three states are equally weighted byd.T h e t w o M D P s ,which generate the same data, have di↵erent BEs. Thus, the BE cannot be estimated fromdata alone; knowledge of the MDP beyond what is revealed in the data is required.Moreover, the two MDPs have di↵erent minimal-BE value functions.2 For the ﬁrst MDP,the minimal-BE value function is the exact solutionv\\x00=\\x000 for any\\x00. For the second MDP,\\n2. This is a critical observation, as it is possible for an error function to be unobservable and yet still beperfectly satisfactory for use in learning settings because the value that minimizes itcanbe determinedfrom data. For example, this is what happens with the VE. The VE is not observable from data, but its\\n20\\npolicy together completely determine the probability distribution over data trajectories.Assume for the moment that the state, action, and reward sets are all ﬁnite. Then,for any ﬁnite sequence\\x00=\\x000,a0,r1,...,rk,\\x00k, there is a well deﬁned probability (pos-sibly zero) of it occuring as the initial portion of a trajectory, which we may denotedP(\\x00)= Pr{\\x00(S0)=\\x000,A0=a0,R1=r1,...,Rk=rk,\\x00(Sk)=\\x00k}. The distributionPthen is a complete characterization of a source of data trajectories. To knowPis to knoweverything about the statistics of the data, but it is still less than knowing the MDP. Inparticular, the VE and BE objectives are readily computed from the MDP as described inSection 3, but these cannot be determined fromPalone.\\n✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\\n✓\\x001 ✓\\x002 ✓\\x003 ✓\\x004 BE1 BE2 MDP1 MDP2\\nTDE RE VEThe problem can be seen in very simple, POMDP-like examples, in which the observabledata produced by two di↵erent MDPs is identical in every respect, yet the BE is di↵erent.In such a case the BE is literally not a function of the data, and thus there is no way toestimate it from data. One of the simplest examples is the pair of MDPs shown below:\\nBA 1\\n0\\n-1 BA 0\\n-1 B\\x01\\n0\\n1 -1\\nThese MDPs have only one action (or, equivalently, no actions), so they are in e↵ect Markovchains. Where two edges leave a state, both possibilities are assumed to occur with equalprobability. The numbers on the edges indicate the reward emitted if that edge is traversed.The MDP on the left has two states that are represented distinctly; each has a separateweight so that they can take on any value. The MDP on the right has three states, twoof which,BandB\\x00, are represented identically and must be given the same approximatevalue. We can imagine that the value of stateAis given by the ﬁrst component of✓andthe value ofBandB\\x00 is given by the second. Notice that the observable data is identicalfor the two MDPs. In both cases the agent will see single occurrences ofAfollowed by a0, then some number ofBs each followed by a\\x001, except the last which is followed by a1, then we start all over again with a singleAand a 0, etc. All the details are the sameas well; in both MDPs, the probability of a string ofkBsi s2\\x00k. Now consider the valuefunctionv\\x00=\\x000. In the ﬁrst MDP, this is an exact solution, and the overall BE is zero. Inthe second MDP, this solution produces an error in bothBandB\\x00 of 1, for an overall BEofpd(B)+d(B\\x00), orp2/3 if the three states are equally weighted byd.T h e t w o M D P s ,which generate the same data, have di↵erent BEs. Thus, the BE cannot be estimated fromdata alone; knowledge of the MDP beyond what is revealed in the data is required.Moreover, the two MDPs have di↵erent minimal-BE value functions.2 For the ﬁrst MDP,the minimal-BE value function is the exact solutionv\\x00=\\x000 for any\\x00. For the second MDP,\\n2. This is a critical observation, as it is possible for an error function to be unobservable and yet still beperfectly satisfactory for use in learning settings because the value that minimizes itcanbe determinedfrom data. For example, this is what happens with the VE. The VE is not observable from data, but its\\n20\\nMSPBE\\npolicy together completely determine the probability distribution over data trajectories.Assume for the moment that the state, action, and reward sets are all ﬁnite. Then,for any ﬁnite sequence\\x00=\\x000,a0,r1,...,rk,\\x00k, there is a well deﬁned probability (pos-sibly zero) of it occuring as the initial portion of a trajectory, which we may denotedP(\\x00)= Pr{\\x00(S0)=\\x000,A0=a0,R1=r1,...,Rk=rk,\\x00(Sk)=\\x00k}. The distributionPthen is a complete characterization of a source of data trajectories. To knowPis to knoweverything about the statistics of the data, but it is still less than knowing the MDP. Inparticular, the VE and BE objectives are readily computed from the MDP as described inSection 3, but these cannot be determined fromPalone.\\n✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\\n✓\\x001 ✓\\x002 ✓\\x003 ✓\\x004 BE1 BE2 MDP1 MDP2\\nTDE RE VEThe problem can be seen in very simple, POMDP-like examples, in which the observabledata produced by two di↵erent MDPs is identical in every respect, yet the BE is di↵erent.In such a case the BE is literally not a function of the data, and thus there is no way toestimate it from data. One of the simplest examples is the pair of MDPs shown below:\\nBA 1\\n0\\n-1 BA 0\\n-1 B\\x01\\n0\\n1 -1\\nThese MDPs have only one action (or, equivalently, no actions), so they are in e↵ect Markovchains. Where two edges leave a state, both possibilities are assumed to occur with equalprobability. The numbers on the edges indicate the reward emitted if that edge is traversed.The MDP on the left has two states that are represented distinctly; each has a separateweight so that they can take on any value. The MDP on the right has three states, twoof which,BandB\\x00, are represented identically and must be given the same approximatevalue. We can imagine that the value of stateAis given by the ﬁrst component of✓andthe value ofBandB\\x00 is given by the second. Notice that the observable data is identicalfor the two MDPs. In both cases the agent will see single occurrences ofAfollowed by a0, then some number ofBs each followed by a\\x001, except the last which is followed by a1, then we start all over again with a singleAand a 0, etc. All the details are the sameas well; in both MDPs, the probability of a string ofkBsi s2\\x00k. Now consider the valuefunctionv\\x00=\\x000. In the ﬁrst MDP, this is an exact solution, and the overall BE is zero. Inthe second MDP, this solution produces an error in bothBandB\\x00 of 1, for an overall BEofpd(B)+d(B\\x00), orp2/3 if the three states are equally weighted byd.T h e t w o M D P s ,which generate the same data, have di↵erent BEs. Thus, the BE cannot be estimated fromdata alone; knowledge of the MDP beyond what is revealed in the data is required.Moreover, the two MDPs have di↵erent minimal-BE value functions.2 For the ﬁrst MDP,the minimal-BE value function is the exact solutionv\\x00=\\x000 for any\\x00. For the second MDP,\\n2. This is a critical observation, as it is possible for an error function to be unobservable and yet still beperfectly satisfactory for use in learning settings because the value that minimizes itcanbe determinedfrom data. For example, this is what happens with the VE. The VE is not observable from data, but its\\n20\\nMSTDE\\npolicy together completely determine the probability distribution over data trajectories.Assume for the moment that the state, action, and reward sets are all ﬁnite. Then,for any ﬁnite sequence\\x00=\\x000,a0,r1,...,rk,\\x00k, there is a well deﬁned probability (pos-sibly zero) of it occuring as the initial portion of a trajectory, which we may denotedP(\\x00)= Pr{\\x00(S0)=\\x000,A0=a0,R1=r1,...,Rk=rk,\\x00(Sk)=\\x00k}. The distributionPthen is a complete characterization of a source of data trajectories. To knowPis to knoweverything about the statistics of the data, but it is still less than knowing the MDP. Inparticular, the VE and BE objectives are readily computed from the MDP as described inSection 3, but these cannot be determined fromPalone.\\n✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2\\n✓\\x001 ✓\\x002 ✓\\x003 ✓\\x004 BE1 BE2 MDP1 MDP2\\nTDE RE VEThe problem can be seen in very simple, POMDP-like examples, in which the observabledata produced by two di↵erent MDPs is identical in every respect, yet the BE is di↵erent.In such a case the BE is literally not a function of the data, and thus there is no way toestimate it from data. One of the simplest examples is the pair of MDPs shown below:\\nBA 1\\n0\\n-1 BA 0\\n-1 B\\x01\\n0\\n1 -1\\nThese MDPs have only one action (or, equivalently, no actions), so they are in e↵ect Markovchains. Where two edges leave a state, both possibilities are assumed to occur with equalprobability. The numbers on the edges indicate the reward emitted if that edge is traversed.The MDP on the left has two states that are represented distinctly; each has a separateweight so that they can take on any value. The MDP on the right has three states, twoof which,BandB\\x00, are represented identically and must be given the same approximatevalue. We can imagine that the value of stateAis given by the ﬁrst component of✓andthe value ofBandB\\x00 is given by the second. Notice that the observable data is identicalfor the two MDPs. In both cases the agent will see single occurrences ofAfollowed by a0, then some number ofBs each followed by a\\x001, except the last which is followed by a1, then we start all over again with a singleAand a 0, etc. All the details are the sameas well; in both MDPs, the probability of a string ofkBsi s2\\x00k. Now consider the valuefunctionv\\x00=\\x000. In the ﬁrst MDP, this is an exact solution, and the overall BE is zero. Inthe second MDP, this solution produces an error in bothBandB\\x00 of 1, for an overall BEofpd(B)+d(B\\x00), orp2/3 if the three states are equally weighted byd.T h e t w o M D P s ,which generate the same data, have di↵erent BEs. Thus, the BE cannot be estimated fromdata alone; knowledge of the MDP beyond what is revealed in the data is required.Moreover, the two MDPs have di↵erent minimal-BE value functions.2 For the ﬁrst MDP,the minimal-BE value function is the exact solutionv\\x00=\\x000 for any\\x00. For the second MDP,\\n2. This is a critical observation, as it is possible for an error function to be unobservable and yet still beperfectly satisfactory for use in learning settings because the value that minimizes itcanbe determinedfrom data. For example, this is what happens with the VE. The VE is not observable from data, but its\\n20\\nDatadistribution\\nMDP1 MDP2\\nMSVE1 MSVE2\\nMSRE\\nDatadistributionpolicy together completely determine the probability distribution over data trajectories.Assume for the moment that the state, action, and reward sets are all ﬁnite. Then,for any ﬁnite sequence\\x00=\\x000,a0,r1,...,rk,\\x00k, there is a well deﬁned probability (pos-sibly zero) of it occuring as the initial portion of a trajectory, which we may denotedP(\\x00)= Pr{\\x00(S0)=\\x000,A0=a0,R1=r1,...,Rk=rk,\\x00(Sk)=\\x00k}. The distributionPthen is a complete characterization of a source of data trajectories. To knowPis to knoweverything about the statistics of the data, but it is still less than knowing the MDP. Inparticular, the VE and BE objectives are readily computed from the MDP as described inSection 3, but these cannot be determined fromPalone.\\n✓1 ✓2 ✓3 ✓4 BE1 BE2 MDP1 MDP2 PBE✓\\x00\\n✓\\x001 ✓\\x002 ✓\\x003 ✓\\x004 BE1 BE2 MDP1 MDP2\\nTDE RE VEThe problem can be seen in very simple, POMDP-like examples, in which the observabledata produced by two di↵erent MDPs is identical in every respect, yet the BE is di↵erent.In such a case the BE is literally not a function of the data, and thus there is no way toestimate it from data. One of the simplest examples is the pair of MDPs shown below:\\nBA 1\\n0\\n-1 BA 0\\n-1 B\\x01\\n0\\n1 -1\\nThese MDPs have only one action (or, equivalently, no actions), so they are in e↵ect Markovchains. Where two edges leave a state, both possibilities are assumed to occur with equalprobability. The numbers on the edges indicate the reward emitted if that edge is traversed.The MDP on the left has two states that are represented distinctly; each has a separateweight so that they can take on any value. The MDP on the right has three states, twoof which,BandB\\x00, are represented identically and must be given the same approximatevalue. We can imagine that the value of stateAis given by the ﬁrst component of✓andthe value ofBandB\\x00 is given by the second. Notice that the observable data is identicalfor the two MDPs. In both cases the agent will see single occurrences ofAfollowed by a0, then some number ofBs each followed by a\\x001, except the last which is followed by a1, then we start all over again with a singleAand a 0, etc. All the details are the sameas well; in both MDPs, the probability of a string ofkBsi s2\\x00k. Now consider the valuefunctionv\\x00=\\x000. In the ﬁrst MDP, this is an exact solution, and the overall BE is zero. Inthe second MDP, this solution produces an error in bothBandB\\x00 of 1, for an overall BEofpd(B)+d(B\\x00), orp2/3 if the three states are equally weighted byd.T h e t w o M D P s ,which generate the same data, have di↵erent BEs. Thus, the BE cannot be estimated fromdata alone; knowledge of the MDP beyond what is revealed in the data is required.Moreover, the two MDPs have di↵erent minimal-BE value functions.2 For the ﬁrst MDP,the minimal-BE value function is the exact solutionv\\x00=\\x000 for any\\x00. For the second MDP,\\n2. This is a critical observation, as it is possible for an error function to be unobservable and yet still beperfectly satisfactory for use in learning settings because the value that minimizes itcanbe determinedfrom data. For example, this is what happens with the VE. The VE is not observable from data, but its\\n20\\nw⇤ w⇤1 w⇤2\\nw⇤3 w⇤4\\nVE1 VE2\\nRE\\nBE2BE1\\nPBE TDE\\nMonte Carloobjectives Bootstrappingobjectives\\nFigure 11.4: Causal relationships among the data distribution, MDPs, and various objectives.\\nLeft, Monte Carlo objectives:Two di↵erent MDPs can produce the same data distribution\\nyet also produce di↵erentVEs, proving that theVE objective cannot be determined from data\\nand is not learnable. However, all suchVEs must have the same optimal parameter vector,w⇤!\\nMoreover, this samew⇤ can be determined from another objective, theRE,w h i c his uniquely\\ndetermined from the data distribution. Thusw⇤ and theRE are learnable even though theVEs\\nare not. Right, Bootstrapping objectives:Two di↵erent MDPs can produce the same data\\ndistribution yet also produce di↵erentBEs and have di↵erent minimizing parameter vectors;\\nthese are not learnable from the data distribution. ThePBE and TDE objectives and their\\n(di↵erent) minima can be directly determined from data and thus are learnable.\\ndata and thus cannot be learned from it. The other bootstrapping objectives that we\\nhave considered, thePBE and TDE, can be determined from data (are learnable) and\\ndetermine optimal solutions that are in general di↵erent from each other and theBE\\nminimums. The general case is summarized in the right side of Figure 11.4.\\nThus, theBE is not learnable; it cannot be estimated from feature vectors and other\\nobservable data. This limits theBE to model-based settings. There can be no algorithm\\nthat minimizes theBE without access to the underlying MDP states beyond the feature\\nvectors. The residual-gradient algorithm is only able to minimizeBE because it is allowed\\nto double sample from the same state—not a state that has the same feature vector,\\nbut one that is guaranteed to be the same underlying state. We can see now that there\\nis no way around this. Minimizing theBE requires some such access to the nominal,\\nunderlying MDP. This is an important limitation of theBE beyond that identiﬁed in the\\nA-presplit example on page 273. All this directs more attention toward thePBE.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 299, 'page_label': '300'}, page_content='278 Chapter 11: O↵-policy Methods with Approximation\\n11.7 Gradient-TD Methods\\nWe now consider SGD methods for minimizing thePBE. As true SGD methods, these\\nGradient-TD methods have robust convergence properties even under o↵-policy training\\nand nonlinear function approximation. Remember that in the linear case there is always\\nan exact solution, the TD ﬁxed pointwTD, at which thePBE is zero. This solution could\\nbe found by least-squares methods (Section 9.8), but only by methods of quadraticO(d2)\\ncomplexity in the number of parameters. We seek instead an SGD method, which should\\nbe O(d) and have robust convergence properties. Gradient-TD methods come close to\\nachieving these goals, at the cost of a rough doubling of computational complexity.\\nTo derive an SGD method for thePBE (assuming linear function approximation) we\\nbegin by expanding and rewriting the objective (11.22) in matrix terms:\\nPBE(w)=\\n\\x00\\x00⇧¯\\x00w\\n\\x00\\x002\\nµ\\n=( ⇧¯\\x00w)>D⇧¯\\x00w (from (11.14))\\n= ¯\\x00>\\nw⇧>D⇧¯\\x00w\\n= ¯\\x00>\\nwDX\\n\\x00\\nX>DX\\n\\x00\\x001\\nX>D¯\\x00w (11.25)\\n(using (11.13) and the identity ⇧>D⇧= DX\\n\\x00\\nX>DX\\n\\x00\\x001\\nX>D)\\n=\\n\\x00\\nX>D¯\\x00w\\n\\x00>\\x00\\nX>DX\\n\\x00\\x001\\x00\\nX>D¯\\x00w\\n\\x00\\n. (11.26)\\nThe gradient with respect tow is\\nrPBE(w)=2 r\\n⇥\\nX>D¯\\x00w\\n⇤> \\x00\\nX>DX\\n\\x00\\x001\\x00\\nX>D¯\\x00w\\n\\x00\\n.\\nTo turn this into an SGD method, we have to sample something on every time step that\\nhas this quantity as its expected value. Let us takeµ to be the distribution of states\\nvisited under the behavior policy. All three of the factors above can then be written in\\nterms of expectations under this distribution. For example, the last factor can be written\\nX>D¯\\x00w =\\nX\\ns\\nµ(s)x(s)¯\\x00w(s)= E[⇢t\\x00txt] ,\\nwhich is just the expectation of the semi-gradient TD(0) update(11.2). The ﬁrst factor\\nis the transpose of the gradient of this update:\\nrE[⇢t\\x00txt]> = E\\n⇥\\n⇢tr\\x00>\\nt x>\\nt\\n⇤\\n= E\\n⇥\\n⇢tr(Rt+1 + \\x00w>xt+1 \\x00 w>xt)>x>\\nt\\n⇤\\n(using episodic \\x00t)\\n= E\\n⇥\\n⇢t(\\x00xt+1 \\x00 xt)x>\\nt\\n⇤\\n.\\nFinally, the middle factor is the inverse of the expected outer-product matrix of the\\nfeature vectors:\\nX>DX =\\nX\\ns\\nµ(s)x(s)x(s)> = E\\n⇥\\nxtx>\\nt\\n⇤\\n.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 300, 'page_label': '301'}, page_content='11.7. Gradient-TD Methods 279\\nSubstituting these expectations for the three factors in our expression for the gradient of\\nthe PBE, we get\\nrPBE(w)=2 E\\n⇥\\n⇢t(\\x00xt+1 \\x00 xt)x>\\nt\\n⇤\\nE\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt] . (11.27)\\nIt might not be obvious that we have made any progress by writing the gradient in this\\nform. It is a product of three expressions and the ﬁrst and last are not independent.\\nThey both depend on the next feature vectorxt+1; we cannot simply sample both of\\nthese expectations and then multiply the samples. This would give us a biased estimate\\nof the gradient just as in the residual-gradient algorithm.\\nAnother idea would be to estimate the three expectations separately and then combine\\nthem to produce an unbiased estimate of the gradient. This would work, but would\\nrequire a lot of computational resources, particularly to store the ﬁrst two expectations,\\nwhich ared ⇥ d matrices, and to compute the inverse of the second. This idea can be\\nimproved. If two of the three expectations are estimated and stored, then the third could\\nbe sampled and used in conjunction with the two stored quantities. For example, you\\ncould store estimates of the second two quantities (using the increment inverse-updating\\ntechniques in Section 9.8) and then sample the ﬁrst expression. Unfortunately, the overall\\nalgorithm would still be of quadratic complexity (of orderO(d2)).\\nThe idea of storing some estimates separately and then combining them with samples\\nis a good one and is also used in Gradient-TD methods. Gradient-TD methods estimate\\nand store the product of the second two factors in(11.27). These factors are ad ⇥ d\\nmatrix and ad-vector, so their product is just ad-vector, likew itself. We denote this\\nsecond learned vector asv:\\nv ⇡ E\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt] . (11.28)\\nThis form is familiar to students of linear supervised learning. It is the solution to a linear\\nleast-squares problem that tries to approximate⇢t\\x00t from the features. The standard\\nSGD method for incrementally ﬁnding the vectorv that minimizes the expected squared\\nerror\\n\\x00\\nv>xt \\x00 ⇢t\\x00t\\n\\x002\\nis known as the Least Mean Square (LMS) rule (here augmented\\nwith an importance sampling ratio):\\nvt+1\\n.= vt + \\x00⇢t\\n\\x00\\n\\x00t \\x00 v>\\nt xt\\n\\x00\\nxt,\\nwhere \\x00> 0 is another step-size parameter. We can use this method to e↵ectively achieve\\n(11.28) withO(d) storage and per-step computation.\\nGiven a stored estimatevt approximating (11.28), we can update our main parameter\\nvector wt using SGD methods based on (11.27). The simplest such rule is\\nwt+1 = wt \\x00 1\\n2↵rPBE(wt) (the general SGD rule)\\n= wt \\x00 1\\n2↵2E\\n⇥\\n⇢t(\\x00xt+1 \\x00 xt)x>\\nt\\n⇤\\nE\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt] (from (11.27))\\n= wt + ↵E\\n⇥\\n⇢t(xt \\x00 \\x00xt+1)x>\\nt\\n⇤\\nE\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt] (11.29)\\n⇡ wt + ↵E\\n⇥\\n⇢t(xt \\x00 \\x00xt+1)x>\\nt\\n⇤\\nvt (based on (11.28))\\n⇡ wt + ↵⇢t (xt \\x00 \\x00xt+1) x>\\nt vt. (sampling)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 301, 'page_label': '302'}, page_content='280 Chapter 11: O↵-policy Methods with Approximation\\nThis algorithm is calledGTD2. Note that if the ﬁnal inner product (x>\\nt vt) is done ﬁrst,\\nthen the entire algorithm is ofO(d) complexity.\\nA slightly better algorithm can be derived by doing a few more analytic steps before\\nsubstituting invt. Continuing from (11.29):\\nwt+1 = wt + ↵E\\n⇥\\n⇢t(xt \\x00 \\x00xt+1)x>\\nt\\n⇤\\nE\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt]\\n= wt + ↵\\n\\x00\\nE\\n⇥\\n⇢txtx>\\nt\\n⇤\\n\\x00 \\x00E\\n⇥\\n⇢txt+1x>\\nt\\n⇤\\x00\\nE\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt]\\n= wt + ↵\\n\\x00\\nE\\n⇥\\nxtx>\\nt\\n⇤\\n\\x00 \\x00E\\n⇥\\n⇢txt+1x>\\nt\\n⇤\\x00\\nE\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt]\\n= wt + ↵\\n⇣\\nE[⇢t\\x00txt] \\x00 \\x00E\\n⇥\\n⇢txt+1x>\\nt\\n⇤\\nE\\n⇥\\nxtx>\\nt\\n⇤\\x001\\nE[⇢t\\x00txt]\\n⌘\\n⇡ wt + ↵\\n\\x00\\nE[⇢t\\x00txt] \\x00 \\x00E\\n⇥\\n⇢txt+1x>\\nt\\n⇤\\nvt\\n\\x00\\n(based on (11.28))\\n⇡ wt + ↵⇢t\\n\\x00\\n\\x00txt \\x00 \\x00xt+1x>\\nt vt\\n\\x00\\n, (sampling)\\nwhich again isO(d) if the ﬁnal product (x>\\nt vt) is done ﬁrst. This algorithm is known as\\neither TD(0) with gradient correction (TDC)or, alternatively, asGTD(0).\\nFigure 11.5 shows a sample and the expected behavior of TDC on Baird’s counterex-\\nample. As intended, thePBE falls to zero, but note that the individual components\\nof the parameter vector do not approach zero. In fact, these values are still far from\\nw1–w6\\nw8 w8\\np\\nVE p\\nVE\\np\\nPBE\\np\\nPBE\\n10\\n5\\n2\\n0\\n-2.34\\n0 1000 0 1000Steps\\nw7\\nw1–w6\\nTDC Expected TDC\\nSweeps\\nw7\\nFigure 11.5: The behavior of the TDC algorithm on Baird’s counterexample. On the left is\\nshown a typical single run, and on the right is shown the expected behavior of this algorithm if\\nthe updates are done synchronously (analogous to(11.9), except for the two TDC parameter\\nvectors). The step sizes were↵ =0 .005 and\\x00 =0 .05.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 302, 'page_label': '303'}, page_content='11.8. Emphatic-TD Methods 281\\nan optimal solution,ˆv(s) = 0, for alls, for whichw would have to be proportional to\\n(1, 1, 1, 1, 1, 1, 4, \\x002)>. After 1000 iterations we are still far from an optimal solution, as\\nwe can see from theVE, which remains almost 2. The system is actually converging to\\nan optimal solution, but progress is extremely slow because thePBE is already so close\\nto zero.\\nGTD2 and TDC both involve two learning processes, a primary one forw and a\\nsecondary one forv. The logic of the primary learning process relies on the secondary\\nlearning process having ﬁnished, at least approximately, whereas the secondary learning\\nprocess proceeds without being inﬂuenced by the ﬁrst. We call this sort of asymmetrical\\ndependence a cascade. In cascades we often assume that the secondary learning process\\nis proceeding faster and thus is always at its asymptotic value, ready and accurate to\\nassist the primary learning process. The convergence proofs for these methods often make\\nthis assumption explicitly. These are calledtwo-time-scale proofs. The fast time scale is\\nthat of the secondary learning process, and the slower time scale is that of the primary\\nlearning process. If↵ is the step size of the primary learning process, and\\x00 is the step\\nsize of the secondary learning process, then these convergence proofs will typically require\\nthat in the limit\\x00 ! 0 and ↵\\n\\x00 ! 0.\\nGradient-TD methods are currently the most well understood and widely used stable\\no↵-policy methods. There are extensions to action values and control (GQ, Maei et al.,\\n2010), to eligibility traces (GTD(\\x00) and GQ(\\x00), Maei, 2011; Maei and Sutton, 2010), and\\nto nonlinear function approximation (Maei et al., 2009). There have also been proposed\\nhybrid algorithms midway between semi-gradient TD and gradient TD (Hackman, 2012;\\nWhite and White, 2016). Hybrid-TD algorithms behave like Gradient-TD algorithms in\\nstates where the target and behavior policies are very di↵erent, and behave like semi-\\ngradient algorithms in states where the target and behavior policies are the same. Finally,\\nthe Gradient-TD idea has been combined with the ideas of proximal methods and control\\nvariates to produce more e\\x00cient methods (Mahadevan et al., 2014; Du et al., 2017).\\n11.8 Emphatic-TD Methods\\nWe turn now to the second major strategy that has been extensively explored for obtaining\\na cheap and e\\x00cient o↵-policy learning method with function approximation. Recall\\nthat linear semi-gradient TD methods are e\\x00cient and stable when trained under the\\non-policy distribution, and that we showed in Section 9.4 that this has to do with the\\npositive deﬁniteness of the matrixA (9.11)4 and the match between the on-policy state\\ndistribution µ⇡ and the state-transition probabilitiesp(s|s, a) under the target policy. In\\no↵-policy learning, we reweight the state transitions using importance sampling so that\\nthey become appropriate for learning about the target policy, but the state distribution\\nis still that of the behavior policy. There is a mismatch. A natural idea is to somehow\\nreweight the states, emphasizing some and de-emphasizing others, so as to return the\\ndistribution of updates to the on-policy distribution. There would then be a match,\\nand stability and convergence would follow from existing results. This is the idea of\\n4In the o↵-policy case, the matrixA is generally deﬁned asEs⇠b\\n⇥\\nx(s)E\\n⇥\\nx(St+1)>\\x00\\x00 St =s, At ⇠⇡\\n⇤⇤\\n.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 303, 'page_label': '304'}, page_content='282 Chapter 11: O↵-policy Methods with Approximation\\nEmphatic-TD methods, ﬁrst introduced for on-policy training in Section 9.11.\\nActually, the notion of “the on-policy distribution” is not quite right, as there are many\\non-policy distributions, and any one of these is su\\x00cient to guarantee stability. Consider\\nan undiscounted episodic problem. The way episodes terminate is fully determined by the\\ntransition probabilities, but there may be several di↵erent ways the episodes might begin.\\nHowever the episodes start, if all state transitions are due to the target policy, then the\\nstate distribution that results is an on-policy distribution. You might start close to the\\nterminal state and visit only a few states with high probability before ending the episode.\\nOr you might start far away and pass through many states before terminating. Both are\\non-policy distributions, and training on both with a linear semi-gradient method would\\nbe guaranteed to be stable. However the process starts, an on-policy distribution results\\nas long as all states encountered are updated up until termination.\\nIf there is discounting, it can be treated as partial or probabilistic termination for these\\npurposes. If \\x00 =0 .9, then we can consider that with probability 0.1 the process terminates\\non every time step and then immediately restarts in the state that is transitioned to. A\\ndiscounted problem is one that is continually terminating and restarting with probability\\n1 \\x00 \\x00 on every step. This way of thinking about discounting is an example of a more\\ngeneral notion ofpseudo termination—termination that does not a↵ect the sequence of\\nstate transitions, but does a↵ect the learning process and the quantities being learned.\\nThis kind of pseudo termination is important to o↵-policy learning because the restarting\\nis optional—remember we can start any way we want to—and the termination relieves\\nthe need to keep including encountered states within the on-policy distribution. That is,\\nif we don’t consider the new states as restarts, then discounting quickly gives us a limited\\non-policy distribution.\\nThe one-step Emphatic-TD algorithm for learning episodic state values is deﬁned by:\\n\\x00t = Rt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt),\\nwt+1 = wt + ↵Mt⇢t\\x00trˆv(St,wt),\\nMt = \\x00⇢t\\x001Mt\\x001 + It,\\nwith It,t h einterest, being arbitrary andMt,t h eemphasis, being initialized toM\\x001 = 0.\\nHow does this algorithm perform on Baird’s counterexample? Figure 11.6 shows the\\ntrajectory in expectation of the components of the parameter vector (for the case in\\nwhich It =1, for allt). There are some oscillations but eventually everything converges\\nand theVE goes to zero. These trajectories are obtained by iteratively computing the\\nexpectation of the parameter vector trajectory without any of the variance due to sampling\\nof transitions and rewards. We do not show the results of applying the Emphatic-TD\\nalgorithm directly because its variance on Baird’s counterexample is so high that it is\\nnigh impossible to get consistent results in computational experiments. The algorithm\\nconverges to the optimal solution in theory on this problem, but in practice it does\\nnot. We turn to the topic of reducing the variance of all these algorithms in the next\\nsection.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 304, 'page_label': '305'}, page_content='11.9. Reducing Variance 283\\nSweeps\\nw1–w6\\nw7\\nw8\\np\\nVE\\n10\\n5\\n2\\n0\\n-5\\n0 1000\\nFigure 11.6: The behavior of the one-step Emphatic-TD algorithm in expectation on Baird’s\\ncounterexample. The step size was↵ =0 .03.\\n11.9 Reducing Variance\\nO↵-policy learning is inherently of greater variance than on-policy learning. This is not\\nsurprising; if you receive data less closely related to a policy, you should expect to learn\\nless about the policy’s values. In the extreme, one may be able to learn nothing. You\\ncan’t expect to learn how to drive by cooking dinner, for example. Only if the target and\\nbehavior policies are related, if they visit similar states and take similar actions, should\\none be able to make signiﬁcant progress in o↵-policy training.\\nOn the other hand, any policy has many neighbors, many similar policies with con-\\nsiderable overlap in states visited and actions chosen, and yet which are not identical.\\nThe raison d’ˆ etre of o↵-policy learning is to enable generalization to this vast number\\nof related-but-not-identical policies. The problem remains of how to make the best use\\nof the experience. Now that we have some methods that are stable in expected value\\n(if the step sizes are set right), attention naturally turns to reducing the variance of the\\nestimates. There are many possible ideas, and we can just touch on a few of them in this\\nintroductory text.\\nWhy is controlling variance especially critical in o↵-policy methods based on importance\\nsampling? As we have seen, importance sampling often involves products of policy ratios.\\nThe ratios are always one in expectation(5.13), but their actual values may be very high\\nor as low as zero. Successive ratios are uncorrelated, so their products are also always one\\nin expected value, but they can be of very high variance. Recall that these ratios multiply\\nthe step size in SGD methods, so high variance means taking steps that vary greatly in\\ntheir sizes. This is problematic for SGD because of the occasional very large steps. They\\nmust not be so large as to take the parameter to a part of the space with a very di↵erent\\ngradient. SGD methods rely on averaging over multiple steps to get a good sense of\\nthe gradient, and if they make large moves from single samples they become unreliable.\\nIf the step-size parameter is set small enough to prevent this, then the expected step'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 305, 'page_label': '306'}, page_content='284 Chapter 11: O↵-policy Methods with Approximation\\ncan end up being very small, resulting in very slow learning. The notions of momentum\\n(Derthick, 1984), of Polyak-Ruppert averaging (Polyak, 1990; Ruppert, 1988; Polyak and\\nJuditsky, 1992), or further extensions of these ideas may signiﬁcantly help. Methods for\\nadaptively setting separate step sizes for di↵erent components of the parameter vector\\nare also pertinent (e.g., Jacobs, 1988; Sutton, 1992b, c), as are the “importance weight\\naware” updates of Karampatziakis and Langford (2010).\\nIn Chapter 5 we saw how weighted importance sampling is signiﬁcantly better behaved,\\nwith lower variance updates, than ordinary importance sampling. However, adapting\\nweighted importance sampling to function approximation is challenging and can probably\\nonly be done approximately withO(d) complexity (Mahmood and Sutton, 2015).\\nThe Tree Backup algorithm (Section 7.5) shows that it is possible to perform some\\no↵-policy learning without using importance sampling. This idea has been extended to\\nthe o↵-policy case to produce stable and more e\\x00cient methods by Munos, Stepleton,\\nHarutyunyan, and Bellemare (2016) and by Mahmood, Yu and Sutton (2017).\\nAnother, complementary strategy is to allow the target policy to be determined in\\npart by the behavior policy, in such a way that it never can be so di↵erent from it to\\ncreate large importance sampling ratios. For example, the target policy can be deﬁned by\\nreference to the behavior policy, as in the “recognizers” proposed by Precup et al. (2006).\\n11.10 Summary\\nO↵-policy learning is a tempting challenge, testing our ingenuity in designing stable and\\ne\\x00cient learning algorithms. Tabular Q-learning makes o↵-policy learning seem easy,\\nand it has natural generalizations to Expected Sarsa and to the Tree Backup algorithm.\\nBut as we have seen in this chapter, the extension of these ideas to signiﬁcant function\\napproximation, even linear function approximation, involves new challenges and forces us\\nto deepen our understanding of reinforcement learning algorithms.\\nWhy go to such lengths? One reason to seek o↵-policy algorithms is to give ﬂexibility\\nin dealing with the tradeo↵ between exploration and exploitation. Another is to free\\nbehavior from learning, and avoid the tyranny of the target policy. TD learning appears\\nto hold out the possibility of learning about multiple things in parallel, of using one\\nstream of experience to solve many tasks simultaneously. We can certainly do this in\\nspecial cases, just not in every case that we would like to or as e\\x00ciently as we would\\nlike to.\\nIn this chapter we divided the challenge of o↵-policy learning into two parts. The\\nﬁrst part, correcting the targets of learning for the behavior policy, is straightforwardly\\ndealt with using the techniques devised earlier for the tabular case, albeit at the cost of\\nincreasing the variance of the updates and thereby slowing learning. High variance will\\nprobably always remains a challenge for o↵-policy learning.\\nThe second part of the challenge of o↵-policy learning emerges as the instability\\nof semi-gradient TD methods that involve bootstrapping. We seek powerful function\\napproximation, o↵-policy learning, and the e\\x00ciency and ﬂexibility of bootstrapping'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 306, 'page_label': '307'}, page_content='11.10. Summary 285\\nTD methods, but it is challenging to combine all three aspects of thisdeadly triad in\\none algorithm without introducing the potential for instability. There have been several\\nattempts. The most popular has been to seek to perform true stochastic gradient descent\\n(SGD) in the Bellman error (a.k.a. the Bellman residual). However, our analysis concludes\\nthat this is not an appealing goal in many cases, and that anyway it is impossible to\\nachieve with a learning algorithm—the gradient of theBE is not learnable from experience\\nthat reveals only feature vectors and not underlying states. Another approach, Gradient-\\nTD methods, performs SGD in theprojected Bellman error. The gradient of thePBE\\nis learnable withO(d) complexity, but at the cost of a second parameter vector with a\\nsecond step size. The newest family of methods, Emphatic-TD methods, reﬁne an old idea\\nfor reweighting updates, emphasizing some and de-emphasizing others. In this way they\\nrestore the special properties that make on-policy learning stable with computationally\\nsimple semi-gradient methods.\\nThe whole area of o↵-policy learning is relatively new and unsettled. Which methods\\nare best or even adequate is not yet clear. Are the complexities of the new methods\\nintroduced at the end of this chapter really necessary? Which of them can be combined\\ne↵ectively with variance reduction methods? The potential for o↵-policy learning remains\\ntantalizing, the best way to achieve it still a mystery.\\nBibliographical and Historical Remarks\\n11.1 The ﬁrst semi-gradient method was linear TD(\\x00) (Sutton, 1988). The name\\n“semi-gradient” is more recent (Sutton, 2015a). Semi-gradient o↵-policy TD(0)\\nwith general importance-sampling ratio may not have been explicitly stated until\\nSutton, Mahmood, and White (2016), but the action-value forms were introduced\\nby Precup, Sutton, and Singh (2000), who also did eligibility trace forms of these\\nalgorithms (see Chapter 12). Their continuing, undiscounted forms have not\\nbeen signiﬁcantly explored. Then-step forms given here are new.\\n11.2 The earliestw-to-2w example was given by Tsitsiklis and Van Roy (1996), who\\nalso introduced the speciﬁc counterexample in the box on page 263. Baird’s\\ncounterexample is due to Baird (1995), though the version we present here is\\nslightly modiﬁed. Averaging methods for function approximation were developed\\nby Gordon (1995, 1996b). Other examples of instability with o↵-policy DP\\nmethods and more complex methods of function approximation are given by\\nBoyan and Moore (1995). Bradtke (1993) gives an example in which Q-learning\\nusing linear function approximation in a linear quadratic regulation problem\\nconverges to a destabilizing policy.\\n11.3 The deadly triad was ﬁrst identiﬁed by Sutton (1995b) and thoroughly analyzed\\nby Tsitsiklis and Van Roy (1997). The name “deadly triad” is due to Sutton\\n(2015a).\\n11.4 This kind of linear analysis was pioneered by Tsitsiklis and Van Roy (1996; 1997),\\nincluding the dynamic programming operator. Diagrams like Figure 11.3 were'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 307, 'page_label': '308'}, page_content='286 Chapter 11: O↵-policy Methods with Approximation\\nintroduced by Lagoudakis and Parr (2003).\\nWhat we have called the Bellman operator, and denotedB⇡, is more commonly\\ndenoted T⇡ and called a “dynamic programming operator,” while a generalized\\nform, denotedT(\\x00), is called the “TD(\\x00) operator” (Tsitsiklis and Van Roy, 1996,\\n1997).\\n11.5 The BE was ﬁrst proposed as an objective function for dynamic programming by\\nSchweitzer and Seidmann (1985). Baird (1995, 1999) extended it to TD learning\\nbased on stochastic gradient descent. In the literature,BE minimization is often\\nreferred to as Bellman residual minimization.\\nThe earliest A-split example is due to Dayan (1992). The two forms given here\\nwere introduced by Sutton et al. (2009a).\\n11.6 The contents of this section are new to this text.\\n11.7 Gradient-TD methods were introduced by Sutton, Szepesv´ ari, and Maei (2009b).\\nThe methods highlighted in this section were introduced by Sutton et al. (2009a)\\nand Mahmood et al. (2014). A major extension to proximal TD methods\\nwas developed by Mahadevan et al. (2014). The most sensitive empirical\\ninvestigations to date of Gradient-TD and related methods are given by Geist\\nand Scherrer (2014), Dann, Neumann, and Peters (2014), White (2015), and\\nGhiassian, Patterson, White, Sutton, and White (2018). Recent developments in\\nthe theory of Gradient-TD methods are presented by Yu (2017).\\n11.8 Emphatic-TD methods were introduced by Sutton, Mahmood, and White (2016).\\nFull convergence proofs and other theory were later established by Yu (2015;\\n2016; Yu, Mahmood, and Sutton, 2017), Hallak, Tamar, and Mannor (2015), and\\nHallak, Tamar, Munos, and Mannor (2016).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 308, 'page_label': '309'}, page_content='Chapter 12\\nEligibility Traces\\nEligibility traces are one of the basic mechanisms of reinforcement learning. For example,\\nin the popular TD(\\x00) algorithm, the\\x00 refers to the use of an eligibility trace. Almost any\\ntemporal-di↵erence (TD) method, such as Q-learning or Sarsa, can be combined with\\neligibility traces to obtain a more general method that may learn more e\\x00ciently.\\nEligibility traces unify and generalize TD and Monte Carlo methods. When TD\\nmethods are augmented with eligibility traces, they produce a family of methods spanning\\na spectrum that has Monte Carlo methods at one end (\\x00=1) and one-step TD methods\\nat the other (\\x00 = 0). In between are intermediate methods that are often better than\\neither extreme method. Eligibility traces also provide a way of implementing Monte Carlo\\nmethods online and on continuing problems without episodes.\\nOf course, we have already seen one way of unifying TD and Monte Carlo methods: the\\nn-step TD methods of Chapter 7. What eligibility traces o↵er beyond these is an elegant\\nalgorithmic mechanism with signiﬁcant computational advantages. The mechanism is\\na short-term memory vector, theeligibility trace zt 2 Rd, that parallels the long-term\\nweight vectorwt 2 Rd. The rough idea is that when a component ofwt participates in\\nproducing an estimated value, then the corresponding component ofzt is bumped up and\\nthen begins to fade away. Learning will then occur in that component ofwt if a nonzero\\nTD error occurs before the trace falls back to zero. The trace-decay parameter\\x00 2 [0, 1]\\ndetermines the rate at which the trace falls.\\nThe primary computational advantage of eligibility traces overn-step methods is that\\nonly a single trace vector is required rather than a store of the lastn feature vectors.\\nLearning also occurs continually and uniformly in time rather than being delayed and\\nthen catching up at the end of the episode. In addition learning can occur and a↵ect\\nbehavior immediately after a state is encountered rather than being delayedn steps.\\nEligibility traces illustrate that a learning algorithm can sometimes be implemented in\\na di↵erent way to obtain computational advantages. Many algorithms are most naturally\\nformulated and understood as an update of a state’s value based on events that follow\\nthat state over multiple future time steps. For example, Monte Carlo methods (Chapter 5)\\nupdate a state based on all the future rewards, andn-step TD methods (Chapter 7)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 309, 'page_label': '310'}, page_content='288 Chapter 12: Eligibility Traces\\nupdate based on the nextn rewards and staten steps in the future. Such formulations,\\nbased on looking forward from the updated state, are calledforward views. Forward views\\nare always somewhat complex to implement because the update depends on later things\\nthat are not available at the time. However, as we show in this chapter it is often possible\\nto achieve nearly the same updates—and sometimesexactly the same updates—with an\\nalgorithm that uses the current TD error, looking backward to recently visited states\\nusing an eligibility trace. These alternate ways of looking at and implementing learning\\nalgorithms are calledbackward views. Backward views, transformations between forward\\nviews and backward views, and equivalences between them, date back to the introduction\\nof temporal di↵erence learning but have become much more powerful and sophisticated\\nsince 2014. Here we present the basics of the modern view.\\nAs usual, ﬁrst we fully develop the ideas for state values and prediction, then extend\\nthem to action values and control. We develop them ﬁrst for the on-policy case then\\nextend them to o↵-policy learning. Our treatment pays special attention to the case of\\nlinear function approximation, for which the results with eligibility traces are stronger.\\nAll these results apply also to the tabular and state aggregation cases because these are\\nspecial cases of linear function approximation.\\n12.1 The \\x00-return\\nIn Chapter 7 we deﬁned ann-step return as the sum of the ﬁrstn rewards plus the\\nestimated value of the state reached inn steps, each appropriately discounted(7.1).T h e\\ngeneral form of that equation, for any parameterized function approximator, is\\nGt:t+n\\n.= Rt+1 +\\x00Rt+2 +··· +\\x00n\\x001Rt+n +\\x00nˆv(St+n,wt+n\\x001), 0 \\uf8ff t \\uf8ff T \\x00n, (12.1)\\nwhere ˆv(s,w) is the approximate value of states given weight vectorw (Chapter 9), and\\nT is the time of episode termination, if any. We noted in Chapter 7 that eachn-step\\nreturn, forn \\x00 1, is a valid update target for a tabular learning update, just as it is for\\nan approximate SGD learning update such as (9.7).\\nNow we note that a valid update can be done not just toward anyn-step return, but\\ntoward anyaverage of n-step returns for di↵erentns. For example, an update can be\\ndone toward a target that is half of a two-step return and half of a four-step return:\\n1\\n2 Gt:t+2 + 1\\n2 Gt:t+4. Any set ofn-step returns can be averaged in this way, even an inﬁnite\\nset, as long as the weights on the component returns are positive and sum to 1. The\\ncomposite return possesses an error reduction property similar to that of individualn-step\\nreturns (7.3) and thus can be used to construct updates with guaranteed convergence\\nproperties. Averaging produces a substantial new range of algorithms. For example, one\\ncould average one-step and inﬁnite-step returns to obtain another way of interrelating TD\\nand Monte Carlo methods. In principle, one could even average experience-based updates\\nwith DP updates to get a simple combination of experience-based and model-based\\nmethods (cf. Chapter 8).\\nAn update that averages simpler component updates is called acompound update.T h e\\nbackup diagram for a compound update consists of the backup diagrams for each of the\\ncomponent updates with a horizontal line above them and the weighting fractions below.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 310, 'page_label': '311'}, page_content='12.1. The \\x00-return 289\\n12\\n12\\nFor example, the compound update for the case mentioned at the start of\\nthis section, mixing half of a two-step return and half of a four-step return,\\nhas the diagram shown to the right. A compound update can only be done\\nwhen the longest of its component updates is complete. The update at the\\nright, for example, could only be done at timet+4 for the estimate formed at\\ntime t. In general one would like to limit the length of the longest component\\nupdate because of the corresponding delay in the updates.\\nThe TD(\\x00) algorithm can be understood as one particular way of averaging\\nn-step updates. This average contains all then-step updates, each weighted\\nproportionally to \\x00n\\x001 (where \\x00 2 [0, 1)), and is normalized by a factor of\\n1 \\x00\\x00 to ensure that the weights sum to 1 (Figure 12.1). The resulting update\\nis toward a return, called the\\x00-return, deﬁned in its state-based form by\\nG\\x00\\nt\\n.=( 1\\x00 \\x00)\\n1X\\nn=1\\n\\x00n\\x001Gt:t+n. (12.2)\\nFigure 12.2 further illustrates the weighting on the sequence ofn-step returns in the\\n\\x00-return. The one-step return is given the largest weight, 1\\x00 \\x00; the two-step return is\\ngiven the next largest weight, (1\\x00\\x00)\\x00; the three-step return is given the weight (1\\x00\\x00)\\x002;\\nand so on. The weight fades by\\x00 with each additional step. After a terminal state has\\nbeen reached, all subsequentn-step returns are equal to the conventional return,Gt.I f\\n1 \\x00\\x00\\n(1\\x00\\x00)\\x00\\n(1\\x00\\x00)\\x002\\n\\x00T\\x00t\\x001\\n···\\n···\\nSt\\nAt\\nAt+1\\nAT\\x001\\nSt+1 Rt+1\\nST RT\\n···\\nSt+2 Rt+2\\nAt+2\\nTD( \\x00)\\nX\\n=1\\nFigure 12.1: The backup diagram for TD(\\x00). If \\x00 = 0, then the overall update reduces to its\\nﬁrst component, the one-step TD update, whereas if\\x00 = 1, then the overall update reduces to\\nits last component, the Monte Carlo update.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 311, 'page_label': '312'}, page_content='290 Chapter 12: Eligibility Traces\\n1!\"\\nweight given to\\nthe 3-step return\\ndecay by \"\\nweight given to\\nactual, final return\\nt T\\nTime\\nWeight\\ntotal area = 1\\nis(1\\x00\\x00)\\x002\\nis\\x00T\\x00t\\x001\\nWeighting\\nFigure 12.2: Weighting given in the\\x00-return to each of then-step returns.\\nwe want, we can separate these post-termination terms from the main sum, yielding\\nG\\x00\\nt =( 1 \\x00 \\x00)\\nT\\x00t\\x001X\\nn=1\\n\\x00n\\x001Gt:t+n + \\x00T\\x00t\\x001Gt, (12.3)\\nas indicated in the ﬁgures. This equation makes it clearer what happens when\\x00 = 1. In\\nthis case the main sum goes to zero, and the remaining term reduces to the conventional\\nreturn. Thus, for\\x00 = 1, updating according to the\\x00-return is a Monte Carlo algorithm.\\nOn the other hand, if\\x00 = 0, then the\\x00-return reduces toGt:t+1, the one-step return.\\nThus, for\\x00 = 0, updating according to the\\x00-return is a one-step TD method.\\nExercise 12.1 Just as the return can be written recursively in terms of the ﬁrst reward and\\nitself one-step later(3.9), so can the\\x00-return. Derive the analogous recursive relationship\\nfrom (12.2) and (12.1). ⇤\\nExercise 12.2 The parameter \\x00 characterizes how fast the exponential weighting in\\nFigure 12.2 falls o↵, and thus how far into the future the\\x00-return algorithm looks in\\ndetermining its update. But a rate factor such as\\x00 is sometimes an awkward way of\\ncharacterizing the speed of the decay. For some purposes it is better to specify a time\\nconstant, or half-life. What is the equation relating\\x00 and the half-life,⌧\\x00,t h et i m eb y\\nwhich the weighting sequence will have fallen to half of its initial value? ⇤\\nWe are now ready to deﬁne our ﬁrst learning algorithm based on the\\x00-return: the\\no↵-line \\x00-return algorithm. As an o↵-line algorithm, it makes no changes to the weight\\nvector during the episode. Then, at the end of the episode, a whole sequence of o↵-line\\nupdates are made according to our usual semi-gradient rule, using the\\x00-return as the\\ntarget:\\nwt+1\\n.= wt + ↵\\nh\\nG\\x00\\nt \\x00 ˆv(St,wt)\\ni\\nrˆv(St,wt),t =0 ,...,T \\x00 1. (12.4)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 312, 'page_label': '313'}, page_content='12.1. The \\x00-return 291\\nThe \\x00-return gives us an alternative way of moving smoothly between Monte Carlo\\nand one-step TD methods that can be compared with then-step bootstrapping way\\ndeveloped in Chapter 7. There we assessed e↵ectiveness on a 19-state random walk task\\n(Example 7.1, page 144). Figure 12.3 shows the performance of the o↵-line\\x00-return\\nalgorithm on this task alongside that of then-step methods (repeated from Figure 7.2).\\nThe experiment was just as described earlier except that for the\\x00-return algorithm we\\nvaried \\x00 instead ofn. The performance measure used is the estimated root-mean-square\\nerror between the correct and estimated values of each state measured at the end of\\nthe episode, averaged over the ﬁrst 10 episodes and the 19 states. Note that overall\\nperformance of the o↵-line \\x00-return algorithms is comparable to that of then-step\\nalgorithms. In both cases we get best performance with an intermediate value of the\\nbootstrapping parameter, n for n-step methods and\\x00 for the o↵-line\\x00-return algorithm.\\nn-step TD methods\\n(from Chapter 7)\\n↵\\nAverage\\nRMS error\\nover 19 states\\nand ﬁrst 10 \\nepisodes n=1\\nn=2\\nn=4n=8\\nn=16\\nn=32\\nn=32n=64128512\\n256\\n0.55\\n0.5\\n0.45\\n0.35\\n0.3\\n0.25\\n0.4\\n0.40.20 0.80.6 1\\n0.55\\n0.5\\n0.45\\n0.35\\n0.3\\n0.25\\n0.4\\n0.40.20 0.80.6 1\\nOff\\x03line λ-return algorithm\\n↵\\nRMS error\\nat the end \\nof the episode\\nover the ﬁrst\\n10 episodes λ=0\\nλ=.4λ=.8\\nλ=.9\\nλ=.95\\nλ=.975\\nλ=.99\\nλ=1\\nλ=.95\\n-\\nFigure 12.3: 19-state Random walk results (Example 7.1): Performance of the o↵-line\\x00-return\\nalgorithm alongside that of then-step TD methods. In both case, intermediate values of the\\nbootstrapping parameter (\\x00 or n) performed best. The results with the o↵-line\\x00-return algorithm\\nare slightly better at the best values of↵ and \\x00,a n da th i g h↵.\\nThe approach that we have been taking so far is what we call the theoretical, or\\nforward, view of a learning algorithm. For each state visited, we look forward in time to\\nall the future rewards and decide how best to combine them. We might imagine ourselves\\nriding the stream of states, looking forward from each state to determine its update, as\\nsuggested by Figure 12.4. After looking forward from and updating one state, we move\\non to the next and never have to work with the preceding state again. Future states,\\non the other hand, are viewed and processed repeatedly, once from each vantage point\\npreceding them.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 313, 'page_label': '314'}, page_content='292 Chapter 12: Eligibility Traces\\nTime\\nrt+3\\nrt+2\\nrt+1\\nrT\\nst+1\\nst+2\\nst+3\\nst\\nS t +1\\nS t\\nS t +2\\nSt+3Rt+3\\nR t +2\\nR t +1\\nRT\\nFigure 12.4: The forward view. We decide how to update each state by looking forward to\\nfuture rewards and states.\\n12.2 TD( \\x00)\\nTD(\\x00) is one of the oldest and most widely used algorithms in reinforcement learning.\\nIt was the ﬁrst algorithm for which a formal relationship was shown between a more\\ntheoretical forward view and a more computationally-congenial backward view using\\neligibility traces. Here we will show empirically that it approximates the o↵-line\\x00-return\\nalgorithm presented in the previous section.\\nTD(\\x00) improves over the o↵-line\\x00-return algorithm in three ways. First it updates\\nthe weight vector on every step of an episode rather than only at the end, and thus\\nits estimates may be better sooner. Second, its computations are equally distributed\\nin time rather than all at the end of the episode. And third, it can be applied to\\ncontinuing problems rather than just to episodic problems. In this section we present the\\nsemi-gradient version of TD(\\x00) with function approximation.\\nWith function approximation, the eligibility trace is a vectorzt 2 Rd with the same\\nnumber of components as the weight vectorwt. Whereas the weight vector is a long-term\\nmemory, accumulating over the lifetime of the system, the eligibility trace is a short-term\\nmemory, typically lasting less time than the length of an episode. Eligibility traces assist\\nin the learning process; their only consequence is that they a↵ect the weight vector, and\\nthen the weight vector determines the estimated value.\\nIn TD(\\x00), the eligibility trace vector is initialized to zero at the beginning of the\\nepisode, is incremented on each time step by the value gradient, and then fades away by\\n\\x00\\x00:\\nz\\x001\\n.= 0,\\nzt\\n.= \\x00\\x00 zt\\x001 + rˆv(St,wt), 0 \\uf8ff t \\uf8ff T, (12.5)\\nwhere \\x00 is the discount rate and\\x00 is the parameter introduced in the previous section,\\nwhich we henceforth call the trace-decay parameter. The eligibility trace keeps track\\nof which components of the weight vector have contributed, positively or negatively, to\\nrecent state valuations, where “recent” is deﬁned in terms of\\x00\\x00. (Recall that in linear\\nfunction approximation,rˆv(St,wt) is the feature vector,xt, in which case the eligibility\\ntrace vector is just a sum of past, fading, input vectors.) The trace is said to indicate\\nthe eligibility of each component of the weight vector for undergoing learning changes'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 314, 'page_label': '315'}, page_content='12.2. TD( \\x00) 293\\nshould a reinforcing event occur. The reinforcing events we are concerned with are the\\nmoment-by-moment one-step TD errors. The TD error for state-value prediction is\\n\\x00t\\n.= Rt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt). (12.6)\\nIn TD(\\x00), the weight vector is updated on each step proportional to the scalar TD error\\nand the vector eligibility trace:\\nwt+1\\n.= wt + ↵\\x00t zt. (12.7)\\nSemi-gradient TD(\\x00) for estimatingˆv ⇡ v⇡\\nInput: the policy⇡ to be evaluated\\nInput: a di↵erentiable function ˆv : S+ ⇥ Rd ! R such that ˆv(terminal,· )=0\\nAlgorithm parameters: step size↵> 0, trace decay rate\\x00 2 [0, 1]\\nInitialize value-function weightsw arbitrarily (e.g.,w = 0)\\nLoop for each episode:\\nInitialize S\\nz  0 (a d-dimensional vector)\\nLoop for each step of episode:\\n| Choose A ⇠ ⇡(·| S)\\n| Take actionA, observeR, S0\\n| z  \\x00\\x00z + rˆv(S,w)\\n| \\x00  R + \\x00ˆv(S0,w) \\x00 ˆv(S,w)\\n| w  w + ↵\\x00 z\\n| S  S0\\nuntil S0 is terminal\\n!tet et et et\\nTime\\nst\\nst+1\\nst-1\\nst-2\\nst-3\\nS t\\nS t + 1\\nSt -1\\nSt-2\\nSt-3\\n\\x00 tzt\\nzt\\nzt\\nz t\\nFigure 12.5: The backward or mechanistic view of TD(\\x00). Each update depends on the current\\nTD error combined with the current eligibility traces of past events.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 315, 'page_label': '316'}, page_content='294 Chapter 12: Eligibility Traces\\nTD(\\x00) is oriented backward in time. At each moment we look at the current TD error\\nand assign it backward to each prior state according to how much that state contributed\\nto the current eligibility trace at that time. We might imagine ourselves riding along the\\nstream of states, computing TD errors, and shouting them back to the previously visited\\nstates, as suggested by Figure 12.5. Where the TD error and traces come together, we\\nget the update given by (12.7), changing the values of those past states for when they\\noccur again in the future.\\nTo better understand the backward view of TD(\\x00), consider what happens at various\\nvalues of \\x00.I f \\x00 = 0, then by (12.5) the trace at t is exactly the value gradient\\ncorresponding toSt.T h u s t h e T D (\\x00) update (12.7) reduces to the one-step semi-gradient\\nTD update treated in Chapter 9 (and, in the tabular case, to the simple TD rule (6.2)).\\nThis is why that algorithm was called TD(0). In terms of Figure 12.5, TD(0) is the\\ncase in which only the one state preceding the current one is updated by the TD error\\n(other states may have their value estimates changed by generalization due to function\\napproximation). For larger values of\\x00,b u ts t i l l\\x00< 1, more of the preceding states are\\nupdated, but each more temporally distant state is updated less because the corresponding\\neligibility trace is smaller, as suggested by the ﬁgure. We say that the earlier states are\\ngiven lesscredit for the TD error.\\nIf \\x00 = 1, then the credit given to earlier states falls only by\\x00 per step. This turns out\\nto be just the right thing to do to achieve Monte Carlo behavior. For example, remember\\nthat the TD error,\\x00t, includes an undiscounted term ofRt+1. In passing this backk\\nsteps it needs to be discounted, like any reward in a return, by\\x00k, which is just what\\nthe falling eligibility trace achieves. If\\x00 = 1 and\\x00 = 1, then the eligibility traces do not\\ndecay at all with time. In this case the method behaves like a Monte Carlo method for\\nan undiscounted, episodic task. If\\x00 = 1, the algorithm is also known as TD(1).\\nTD(1) is a way of implementing Monte Carlo algorithms that is more general than those\\npresented earlier and that signiﬁcantly increases their range of applicability. Whereas\\nthe earlier Monte Carlo methods were limited to episodic tasks, TD(1) can be applied to\\ndiscounted continuing tasks as well. Moreover, TD(1) can be performed incrementally\\nand online. One disadvantage of Monte Carlo methods is that they learn nothing from\\nan episode until it is over. For example, if a Monte Carlo control method takes an action\\nthat produces a very poor reward but does not end the episode, then the agent’s tendency\\nto repeat the action will be undiminished during the episode. Online TD(1), on the other\\nhand, learns in ann-step TD way from the incomplete ongoing episode, where then\\nsteps are all the way up to the current step. If something unusually good or bad happens\\nduring an episode, control methods based on TD(1) can learn immediately and alter their\\nbehavior on that same episode.\\nIt is revealing to revisit the 19-state random walk example (Example 7.1) to see how\\nwell TD(\\x00) does in approximating the o↵-line\\x00-return algorithm. The results for both\\nalgorithms are shown in Figure 12.6. For each\\x00 value, if↵ is selected optimally for it (or\\nsmaller), then the two algorithms perform virtually identically. If↵ is chosen larger than\\nis optimal, however, then the\\x00-return algorithm is only a little worse whereas TD(\\x00)\\nis much worse and may even be unstable. This is not catastrophic for TD(\\x00) on this\\nproblem, as these higher parameter values are not what one would want to use anyway,\\nbut for other problems it can be a signiﬁcant weakness.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 316, 'page_label': '317'}, page_content='12.3. n-step Truncated \\x00-return Methods 295\\nOff\\x03line λ-return algorithm(from the previous section)\\n↵\\nλ=0\\nλ=.4\\nλ=.8\\nλ=.9\\nλ=.95.975.991\\nTD(λ)\\n↵\\nλ=.8\\nλ=.9\\nRMS error\\nat the end \\nof the episode\\nover the ﬁrst\\n10 episodes\\n0.40.20 0.80.6 1\\nλ=0\\nλ=.4λ=.8\\nλ=.9\\nλ=.95\\nλ=.975\\nλ=.99\\nλ=1\\nλ=.95\\n0.55\\n0.5\\n0.45\\n0.35\\n0.3\\n0.25\\n0.4\\n0.40.20 0.80.6 1\\n-\\nFigure 12.6: 19-state Random walk results (Example 7.1): Performance of TD(\\x00)a l o n g s i d e\\nthat of the o↵-line\\x00-return algorithm. The two algorithms performed virtually identically at\\nlow (less than optimal)↵ values, but TD(\\x00) was worse at high↵ values.\\nLinear TD(\\x00) has been proved to converge in the on-policy case if the step-size\\nparameter is reduced over time according to the usual conditions(2.7). Just as discussed\\nin Section 9.4, convergence is not to the minimum-error weight vector, but to a nearby\\nweight vector that depends on\\x00. The bound on solution quality presented in that section\\n(9.14) can now be generalized to apply for any\\x00. For the continuing discounted case,\\nVE(w1) \\uf8ff 1 \\x00 \\x00\\x00\\n1 \\x00 \\x00 min\\nw\\nVE(w). (12.8)\\nThat is, the asymptotic error is no more than1\\x00\\x00\\x00\\n1\\x00\\x00 times the smallest possible error. As\\n\\x00 approaches 1, the bound approaches the minimum error (and it is loosest at\\x00 = 0).\\nIn practice, however,\\x00 = 1 is often the poorest choice, as will be illustrated later in\\nFigure 12.14.\\nExercise 12.3 Some insight into how TD(\\x00) can closely approximate the o↵-line\\x00-return\\nalgorithm can be gained by seeing that the latter’s error term (in brackets in(12.4)) can\\nbe written as the sum of TD errors(12.6) for a single ﬁxedw. Show this, following the\\npattern of (6.6), and using the recursive relationship for the\\x00-return you obtained in\\nExercise 12.1. ⇤\\nExercise 12.4 Use your result from the preceding exercise to show that, if the weight\\nupdates over an episode were computed on each step but not actually used to change the\\nweights (w remained ﬁxed), then the sum of TD(\\x00)’s weight updates would be the same\\nas the sum of the o↵-line\\x00-return algorithm’s updates. ⇤\\n12.3 n-step Truncated\\x00-return Methods\\nThe o↵-line\\x00-return algorithm is an important ideal, but it is of limited utility because\\nit uses the \\x00-return (12.2), which is not known until the end of the episode. In the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 317, 'page_label': '318'}, page_content='296 Chapter 12: Eligibility Traces\\ncontinuing case, the\\x00-return is technically never known, as it depends onn-step returns\\nfor arbitrarily largen, and thus on rewards arbitrarily far in the future. However, the\\ndependence becomes weaker for longer-delayed rewards, falling by\\x00\\x00 for each step of\\ndelay. A natural approximation, then, would be to truncate the sequence after some\\nnumber of steps. Our existing notion ofn-step returns provides a natural way to do this\\nin which the missing rewards are replaced with estimated values.\\nIn general, we deﬁne thetruncated \\x00-return for time t, given data only up to some\\nlater horizon,h, as\\nG\\x00\\nt:h\\n.=( 1 \\x00 \\x00)\\nh\\x00t\\x001X\\nn=1\\n\\x00n\\x001Gt:t+n + \\x00h\\x00t\\x001Gt:h, 0 \\uf8ff t<h \\uf8ff T. (12.9)\\nIf you compare this equation with the\\x00-return (12.3), it is clear that the horizonh is\\nplaying the same role as was previously played byT, the time of termination. Whereas\\nin the\\x00-return there is a residual weight given to the conventional returnGt,h e r ei ti s\\ngiven to the longest availablen-step return,Gt:h (Figure 12.2).\\nThe truncated\\x00-return immediately gives rise to a family ofn-step \\x00-return algorithms\\nsimilar to the n-step methods of Chapter 7. In all of these algorithms, updates are\\ndelayed byn steps and only take into account the ﬁrstn rewards, but now all thek-step\\nreturns are included for 1\\uf8ff k \\uf8ff n (whereas the earliern-step algorithms used only the\\nn-step return), weighted geometrically as in Figure 12.2. In the state-value case, this\\nfamily of algorithms is known as Truncated TD(\\x00), or TTD(\\x00). The compound backup\\ndiagram, shown in Figure 12.7, is similar to that for TD(\\x00) (Figure 12.1) except that the\\nlongest component update is at mostn steps rather than always going all the way to the\\n1\\x00\\x00\\n(1\\x00\\x00)\\x00\\n(1\\x00\\x00)\\x002 \\x00T\\x00t\\x001\\nor,if t + n\\x00T···\\n···\\n···\\n\\x00n\\x001\\nSt\\nAt\\nAt+1\\nAT\\x001\\nSt+nRt+n\\nSt+1Rt+1\\nSTRT\\nAt+n\\x001\\nn-step truncated TD(\\x00)\\nFigure 12.7: The backup diagram for Truncated TD(\\x00).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 318, 'page_label': '319'}, page_content='12.4. Redoing Updates: Online\\x00-return Algorithm 297\\nend of the episode. TTD(\\x00) is deﬁned by (cf. (9.15)):\\nwt+n\\n.= wt+n\\x001 + ↵\\n⇥\\nG\\x00\\nt:t+n \\x00 ˆv(St,wt+n\\x001)\\n⇤\\nrˆv(St,wt+n\\x001), 0 \\uf8ff t<T .\\nThis algorithm can be implemented e\\x00ciently so that per-step computation does not scale\\nwith n (though of course memory must). Much as inn-step TD methods, no updates are\\nmade on the ﬁrstn \\x001 time steps of each episode, andn \\x001 additional updates are made\\nupon termination. E\\x00cient implementation relies on the fact that thek-step \\x00-return\\ncan be written exactly as\\nG\\x00\\nt:t+k =ˆv(St,wt\\x001)+\\nt+k\\x001X\\ni=t\\n(\\x00\\x00)i\\x00t\\x000\\ni, (12.10)\\nwhere\\n\\x000\\nt\\n.= Rt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt\\x001).\\nExercise 12.5 Several times in this book (often in exercises) we have established that\\nreturns can be written as sums of TD errors if the value function is held constant. Why\\nis (12.10) another instance of this? Prove (12.10). ⇤\\n12.4 Redoing Updates: Online \\x00-return Algorithm\\nChoosing the truncation parametern in Truncated TD(\\x00) involves a tradeo↵.n should\\nbe large so that the method closely approximates the o↵-line\\x00-return algorithm, but it\\nshould also be small so that the updates can be made sooner and can inﬂuence behavior\\nsooner. Can we get the best of both? Well, yes, in principle we can, albeit at the cost of\\ncomputational complexity.\\nThe idea is that, on each time step as you gather a new increment of data, you go back\\nand redo all the updates since the beginning of the current episode. The new updates\\nwill be better than the ones you previously made because now they can take into account\\nthe time step’s new data. That is, the updates are always towards ann-step truncated\\n\\x00-return target, but they always use the latest horizon. In each pass over that episode\\nyou can use a slightly longer horizon and obtain slightly better results. Recall that the\\ntruncated \\x00-return is deﬁned in (12.9) as\\nG\\x00\\nt:h\\n.=( 1 \\x00 \\x00)\\nh\\x00t\\x001X\\nn=1\\n\\x00n\\x001Gt:t+n + \\x00h\\x00t\\x001Gt:h.\\nLet us step through how this target could ideally be used if computational complexity was\\nnot an issue. The episode begins with an estimate at time 0 using the weightsw0 from\\nthe end of the previous episode. Learning begins when the data horizon is extended to\\ntime step 1. The target for the estimate at step 0, given the data up to horizon 1, could\\nonly be the one-step returnG0:1,w h i c hi n c l u d e sR1 and bootstraps from the estimate\\nˆv(S1,w0). Note that this is exactly whatG\\x00\\n0:1 is, with the sum in the ﬁrst term of the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 319, 'page_label': '320'}, page_content='298 Chapter 12: Eligibility Traces\\nequation degenerating to zero. Using this update target, we constructw1. Then, after\\nadvancing the data horizon to step 2, what do we do? We have new data in the form of\\nR2 and S2, as well as the neww1, so now we can construct a better update targetG\\x00\\n0:2\\nfor the ﬁrst update fromS0 as well as a better update targetG\\x00\\n1:2 for the second update\\nfrom S1. Using these improved targets, we redo the updates atS1 and S2, starting again\\nfrom w0,t op r o d u c ew2. Now we advance the horizon to step 3 and repeat, going all the\\nway back to produce three new targets, redoing all updates starting from the originalw0\\nto produce w3, and so on. Each time the horizon is advanced, all the updates are redone\\nstarting fromw0 using the weight vector from the preceding horizon.\\nThis conceptual algorithm involves multiple passes over the episode, one at each\\nhorizon, each generating a di↵erent sequence of weight vectors. To describe it clearly we\\nhave to distinguish between the weight vectors computed at the di↵erent horizons. Let us\\nuse wh\\nt to denote the weights used to generate the value at timet in the sequence up to\\nhorizon h. The ﬁrst weight vectorwh\\n0 in each sequence is that inherited from the previous\\nepisode (so they are the same for allh), and the last weight vectorwh\\nh in each sequence\\ndeﬁnes the ultimate weight-vector sequence of the algorithm. At the ﬁnal horizonh = T\\nwe obtain the ﬁnal weightswT\\nT which will be passed on to form the initial weights of the\\nnext episode. With these conventions, the three ﬁrst sequences described in the previous\\nparagraph can be given explicitly:\\nh =1: w1\\n1\\n.= w1\\n0 + ↵\\n⇥\\nG\\x00\\n0:1 \\x00 ˆv(S0,w1\\n0)\\n⇤\\nrˆv(S0,w1\\n0),\\nh =2: w2\\n1\\n.= w2\\n0 + ↵\\n⇥\\nG\\x00\\n0:2 \\x00 ˆv(S0,w2\\n0)\\n⇤\\nrˆv(S0,w2\\n0),\\nw2\\n2\\n.= w2\\n1 + ↵\\n⇥\\nG\\x00\\n1:2 \\x00 ˆv(S1,w2\\n1)\\n⇤\\nrˆv(S1,w2\\n1),\\nh =3: w3\\n1\\n.= w3\\n0 + ↵\\n⇥\\nG\\x00\\n0:3 \\x00 ˆv(S0,w3\\n0)\\n⇤\\nrˆv(S0,w3\\n0),\\nw3\\n2\\n.= w3\\n1 + ↵\\n⇥\\nG\\x00\\n1:3 \\x00 ˆv(S1,w3\\n1)\\n⇤\\nrˆv(S1,w3\\n1),\\nw3\\n3\\n.= w3\\n2 + ↵\\n⇥\\nG\\x00\\n2:3 \\x00 ˆv(S2,w3\\n2)\\n⇤\\nrˆv(S2,w3\\n2).\\nThe general form for the update is\\nwh\\nt+1\\n.= wh\\nt + ↵\\n⇥\\nG\\x00\\nt:h \\x00 ˆv(St,wh\\nt )\\n⇤\\nrˆv(St,wh\\nt ), 0 \\uf8ff t<h \\uf8ff T.\\nThis update, together withwt\\n.= wt\\nt deﬁnes the online \\x00-return algorithm.\\nThe online \\x00-return algorithm is fully online, determining a new weight vectorwt\\nat each stept during an episode, using only information available at timet. Its main\\ndrawback is that it is computationally complex, passing over the portion of the episode\\nexperienced so far on every step. Note that it is strictly more complex than the o↵-line\\n\\x00-return algorithm, which passes through all the steps at the time of termination but does\\nnot make any updates during the episode. In return, the online algorithm can be expected\\nto perform better than the o↵-line one, not only during the episode when it makes an\\nupdate while the o↵-line algorithm makes none, but also at the end of the episode because\\nthe weight vector used in bootstrapping (inG\\x00\\nt:h) has had a larger number of informative'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 320, 'page_label': '321'}, page_content='12.5. True Online TD(\\x00)2 9 9\\nupdates. This e↵ect can be seen if one looks carefully at Figure 12.8, which compares the\\ntwo algorithms on the 19-state random walk task.\\nOff\\x03line λ-return algorithm\\n(from Section 12.1)\\n↵↵\\nRMS error\\nat the end \\nof the episode\\nover the ﬁrst\\n10 episodes\\n0.40.20 0.80.6 1\\nλ=0\\nλ=.4λ=.8\\nλ=.9\\nλ=.95\\nλ=.975\\nλ=.99\\nλ=1\\nλ=.95\\n0.55\\n0.5\\n0.45\\n0.35\\n0.3\\n0.25\\n0.4\\n0.40.20 0.80.6 1\\nOn-line λ-return algorithm\\n= true online TD(λ)\\nλ=0\\nλ=.4\\nλ=.8\\nλ=.9\\nλ=.95\\nλ=.975\\nλ=.99\\nλ=1\\nλ=.95\\n-\\nFigure 12.8: 19-state Random walk results (Example 7.1): Performance of online and o↵-line\\n\\x00-return algorithms. The performance measure here is theVE at the end of the episode, which\\nshould be the best case for the o↵-line algorithm. Nevertheless, the online algorithm performs\\nsubtly better. For comparison, the\\x00=0 line is the same for both methods.\\n12.5 True Online TD(\\x00)\\nThe online\\x00-return algorithm just presented is currently the best performing temporal-\\ndi↵erence algorithm. It is an ideal which online TD(\\x00) only approximates. As presented,\\nhowever, the online\\x00-return algorithm is very complex. Is there a way to invert this\\nforward-view algorithm to produce an e\\x00cient backward-view algorithm using eligibility\\ntraces? It turns out that there is indeed an exact computationally congenial implementa-\\ntion of the online\\x00-return algorithm for the case of linear function approximation. This\\nimplementation is known as the true online TD(\\x00) algorithm because it is “truer” to the\\nideal of the online\\x00-return algorithm than the TD(\\x00) algorithm is.\\nThe derivation of true online TD(\\x00) is a little too complex to present here (see the\\nnext section and the appendix to the paper by van Seijen et al., 2016) but its strategy is\\nsimple. The sequence of weight vectors produced by the online\\x00-return algorithm can\\nbe arranged in a triangle:\\nw0\\n0\\nw1\\n0 w1\\n1\\nw2\\n0 w2\\n1 w2\\n2\\nw3\\n0 w3\\n1 w3\\n2 w3\\n3\\n... ... ... ... ...\\nwT\\n0 wT\\n1 wT\\n2 wT\\n3 ··· wT\\nT\\nOne row of this triangle is produced on each time step. It turns out that the weight vectors\\non the diagonal, thewt\\nt, are the only ones really needed. The ﬁrst,w0\\n0, is the initial weight'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 321, 'page_label': '322'}, page_content='300 Chapter 12: Eligibility Traces\\nvector of the episode, the last,wT\\nT , is the ﬁnal weight vector, and each weight vector\\nalong the way,wt\\nt, plays a role in bootstrapping in then-step returns of the updates.\\nIn the ﬁnal algorithm the diagonal weight vectors are renamed without a superscript,\\nwt\\n.= wt\\nt. The strategy then is to ﬁnd a compact, e\\x00cient way of computing eachwt\\nt\\nfrom the one before. If this is done, for the linear case in whichˆv(s,w)= w>x(s), then\\nwe arrive at the true online TD(\\x00) algorithm:\\nwt+1\\n.= wt + ↵\\x00t zt + ↵\\n\\x00\\nw>\\nt xt \\x00 w>\\nt\\x001xt\\n\\x00\\n(zt \\x00 xt),\\nwhere we have used the shorthandxt\\n.= x(St), \\x00t is deﬁned as in TD(\\x00) (12.6), and zt is\\ndeﬁned by\\nzt\\n.= \\x00\\x00 zt\\x001 +\\n\\x00\\n1 \\x00 ↵\\x00\\x00 z>\\nt\\x001xt\\n\\x00\\nxt. (12.11)\\nThis algorithm has been proven to produce exactly the same sequence of weight vectors,\\nwt, 0 \\uf8ff t \\uf8ff T, as the online\\x00-return algorithm (van Seijen et al. 2016). Thus the results\\non the random walk task on the left of Figure 12.8 are also its results on that task. Now,\\nhowever, the algorithm is much less expensive. The memory requirements of true online\\nTD(\\x00) are identical to those of conventional TD(\\x00), while the per-step computation is\\nincreased by about 50% (there is one more inner product in the eligibility-trace update).\\nOverall, the per-step computational complexity remains ofO(d), the same as TD(\\x00).\\nPseudocode for the complete algorithm is given in the box.\\nTrue online TD(\\x00) for estimating w>x ⇡ v⇡\\nInput: the policy⇡ to be evaluated\\nInput: a feature functionx : S+ ! Rd such thatx(terminal, · )= 0\\nAlgorithm parameters: step size↵> 0, trace decay rate\\x00 2 [0, 1]\\nInitialize value-function weightsw 2 Rd (e.g., w = 0)\\nLoop for each episode:\\nInitialize state and obtain initial feature vectorx\\nz  0 (a d-dimensional vector)\\nVold  0 (a temporary scalar variable)\\nLoop for each step of episode:\\n| Choose A ⇠ ⇡\\n| Take actionA, observeR, x0 (feature vector of the next state)\\n| V  w>x\\n| V 0  w>x0\\n| \\x00  R + \\x00V 0 \\x00 V\\n| z  \\x00\\x00z +\\n\\x00\\n1 \\x00 ↵\\x00\\x00 z>x\\n\\x00\\nx\\n| w  w + ↵(\\x00 + V \\x00 Vold)z \\x00 ↵(V \\x00 Vold)x\\n| Vold  V 0\\n| x  x0\\nuntil x0 = 0 (signaling arrival at a terminal state)\\nThe eligibility trace (12.11) used in true online TD(\\x00) is called a dutch trace to\\ndistinguish it from the trace(12.5) used in TD(\\x00), which is called anaccumulating trace.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 322, 'page_label': '323'}, page_content='12.6. *Dutch Traces in Monte Carlo Learning 301\\nEarlier work often used a third kind of trace called thereplacing trace, deﬁned only for\\nthe tabular case or for binary feature vectors such as those produced by tile coding. The\\nreplacing trace is deﬁned on a component-by-component basis depending on whether the\\ncomponent of the feature vector was 1 or 0:\\nzi,t\\n.=\\n⇢ 1i f xi,t =1\\n\\x00\\x00zi,t\\x001 otherwise. (12.12)\\nNowadays, we see replacing traces as crude approximations to dutch traces, which largely\\nsupersede them. Dutch traces usually perform better than replacing traces and have a\\nclearer theoretical basis. Accumulating traces remain of interest for nonlinear function\\napproximations where dutch traces are not available.\\n12.6 *Dutch Traces in Monte Carlo Learning\\nAlthough eligibility traces are closely associated historically with TD learning, in fact\\nthey have nothing to do with it. In fact, eligibility traces arise even in Monte Carlo\\nlearning, as we show in this section. We show that the linear MC algorithm (Chapter 9),\\ntaken as a forward view, can be used to derive an equivalent yet computationally cheaper\\nbackward-view algorithm using dutch traces. This is the only equivalence of forward- and\\nbackward-views that we explicitly demonstrate in this book. It gives some of the ﬂavor\\nof the proof of equivalence of true online TD(\\x00) and the online\\x00-return algorithm, but is\\nmuch simpler.\\nThe linear version of the gradient Monte Carlo prediction algorithm (page 202) makes\\nthe following sequence of updates, one for each time step of the episode:\\nwt+1\\n.= wt + ↵\\n⇥\\nG \\x00 w>\\nt xt\\n⇤\\nxt, 0 \\uf8ff t<T . (12.13)\\nTo simplify the example, we assume here that the returnG is a single reward received at\\nthe end of the episode (this is whyG is not subscripted by time) and that there is no\\ndiscounting. In this case the update is also known as the Least Mean Square (LMS) rule.\\nAs a Monte Carlo algorithm, all the updates depend on the ﬁnal reward/return, so none\\ncan be made until the end of the episode. The MC algorithm is an o↵-line algorithm and\\nwe do not seek to improve this aspect of it. Rather we seek merely an implementation of\\nthis algorithm with computational advantages. We will still update the weight vector\\nonly at the end of the episode, but we will do some computation during each step of the\\nepisode and less at its end. This will give a more equal distribution of computation—O(d)\\nper step—and also remove the need to store the feature vectors at each step for use later\\nat the end of each episode. Instead, we will introduce an additional vector memory, the\\neligibility trace, keeping in it a summary of all the feature vectors seen so far. This will\\nbe su\\x00cient to e\\x00ciently recreate exactly the same overall update as the sequence of MC'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 323, 'page_label': '324'}, page_content='302 Chapter 12: Eligibility Traces\\nupdates (12.13), by the end of the episode:\\nwT = wT\\x001 + ↵\\n\\x00\\nG \\x00 w>\\nT\\x001xT\\x001\\n\\x00\\nxT\\x001\\n= wT\\x001 + ↵xT\\x001\\n\\x00\\n\\x00x>\\nT\\x001wT\\x001\\n\\x00\\n+ ↵GxT\\x001\\n=\\n\\x00\\nI \\x00 ↵xT\\x001x>\\nT\\x001\\n\\x00\\nwT\\x001 + ↵GxT\\x001\\n= FT\\x001wT\\x001 + ↵GxT\\x001\\nwhere Ft\\n.= I \\x00 ↵xtx>\\nt is aforgetting, orfading, matrix. Now, recursing,\\n= FT\\x001 (FT\\x002wT\\x002 + ↵GxT\\x002)+ ↵GxT\\x001\\n= FT\\x001FT\\x002wT\\x002 + ↵G (FT\\x001xT\\x002 + xT\\x001)\\n= FT\\x001FT\\x002 (FT\\x003wT\\x003 + ↵GxT\\x003)+ ↵G (FT\\x001xT\\x002 + xT\\x001)\\n= FT\\x001FT\\x002FT\\x003wT\\x003 + ↵G (FT\\x001FT\\x002xT\\x003 + FT\\x001xT\\x002 + xT\\x001)\\n...\\n= FT\\x001FT\\x002 ··· F0w0| {z }\\naT\\x001\\n+ ↵G\\nT\\x001X\\nk=0\\nFT\\x001FT\\x002 ··· Fk+1xk\\n| {z }\\nzT\\x001\\n= aT\\x001 + ↵GzT\\x001 , (12.14)\\nwhere aT\\x001 and zT\\x001 are the values at timeT \\x00 1 of two auxiliary memory vectors that\\ncan be updated incrementally without knowledge ofG and withO(d) complexity per time\\nstep. The zt vector is in fact a dutch-style eligibility trace. It is initialized toz0 = x0\\nand then updated according to\\nzt\\n.=\\ntX\\nk=0\\nFtFt\\x001 ··· Fk+1xk, 1 \\uf8ff t<T\\n=\\nt\\x001X\\nk=0\\nFtFt\\x001 ··· Fk+1xk + xt\\n= Ft\\nt\\x001X\\nk=0\\nFt\\x001Ft\\x002 ··· Fk+1xk + xt\\n= Ftzt\\x001 + xt\\n=\\n\\x00\\nI \\x00 ↵xtx>\\nt\\n\\x00\\nzt\\x001 + xt\\n= zt\\x001 \\x00 ↵xtx>\\nt zt\\x001 + xt\\n= zt\\x001 \\x00 ↵\\n\\x00\\nz>\\nt\\x001xt\\n\\x00\\nxt + xt\\n= zt\\x001 +\\n\\x00\\n1 \\x00 ↵z>\\nt\\x001xt\\n\\x00\\nxt,\\nwhich is the dutch trace for the case of\\x00\\x00 =1 (cf. Eq. 12.11). Theat auxiliary vector is\\ninitialized toa0 = w0 and then updated according to\\nat\\n.= FtFt\\x001 ··· F0w0 = Ftat\\x001 = at\\x001 \\x00 ↵xtx>\\nt at\\x001, 1 \\uf8ff t<T .'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 324, 'page_label': '325'}, page_content='12.7. Sarsa( \\x00) 303\\nThe auxiliary vectors,at and zt, are updated on each time stept<T and then, at time\\nT when G is observed, they are used in(12.14) to computewT . In this way we achieve\\nexactly the same ﬁnal result as the MC/LMS algorithm that has poor computational\\nproperties (12.13), but now with an incremental algorithm whose time and memory\\ncomplexity per step isO(d). This is surprising and intriguing because the notion of\\nan eligibility trace (and the dutch trace in particular) has arisen in a setting without\\ntemporal-di↵erence (TD) learning (in contrast to van Seijen and Sutton, 2014). It seems\\neligibility traces are not speciﬁc to TD learning at all; they are more fundamental than\\nthat. The need for eligibility traces seems to arise whenever one tries to learn long-term\\npredictions in an e\\x00cient manner.\\n12.7 Sarsa( \\x00)\\nVery few changes in the ideas already presented in this chapter are required in order to\\nextend eligibility-traces to action-value methods. To learn approximate action values,\\nˆq(s, a,w), rather than approximate state values,ˆv(s,w), we need to use the action-value\\nform of then-step return, from Chapter 10:\\nGt:t+n\\n.= Rt+1 + ··· + \\x00n\\x001Rt+n + \\x00n ˆq(St+n,A t+n, wt+n\\x001),t + n<T ,\\nwith Gt:t+n\\n.= Gt if t + n \\x00 T. Using this, we can form the action-value form of the\\n\\x00-return, which is otherwise identical to the state-value form(12.3). The action-value\\nform of the o↵-line\\x00-return algorithm (12.4) simply uses ˆq rather than ˆv:\\nwt+1\\n.= wt + ↵\\nh\\nG\\x00\\nt \\x00 ˆq(St,A t, wt)\\ni\\nrˆq(St,A t, wt),t =0 ,...,T \\x00 1, (12.15)\\nwhere G\\x00\\nt\\n.= G\\x00\\nt:1. The compound backup diagram for this forward view is shown in\\nFigure 12.9. Notice the similarity to the diagram of the TD(\\x00) algorithm (Figure 12.1).\\nThe ﬁrst update looks ahead one full step, to the next state–action pair, the second looks\\nahead two steps, to the second state–action pair, and so on. A ﬁnal update is based on\\nthe complete return. The weighting of eachn-step update in the\\x00-return is just as in\\nTD(\\x00) and the\\x00-return algorithm (12.3).\\nThe temporal-di↵erence method for action values, known asSarsa(\\x00), approximates\\nthis forward view. It has the same update rule as given earlier for TD(\\x00):\\nwt+1\\n.= wt + ↵\\x00t zt,\\nexcept, naturally, using the action-value form of the TD error:\\n\\x00t\\n.= Rt+1 + \\x00ˆq(St+1,A t+1, wt) \\x00 ˆq(St,A t, wt), (12.16)\\nand the action-value form of the eligibility trace:\\nz\\x001\\n.= 0,\\nzt\\n.= \\x00\\x00 zt\\x001 + rˆq(St,A t, wt), 0 \\uf8ff t \\uf8ff T.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 325, 'page_label': '326'}, page_content='304 Chapter 12: Eligibility Traces\\n1\\x00\\x00\\n(1\\x00\\x00)\\x00\\n(1\\x00\\x00)\\x002\\n\\x00T\\x00t\\x001\\n···\\n···\\nSt At\\nAt+1\\nAT\\x001\\nSt+1 Rt+1\\nST RT\\n···\\nSt+2 Rt+2\\nAt+2\\nX\\n=1\\nSarsa( \\x00)\\nFigure 12.9: Sarsa(\\x00)’s backup diagram. Compare with Figure 12.1.\\nComplete pseudocode for Sarsa(\\x00) with linear function approximation, binary features,\\nand either accumulating or replacing traces is given in the box on the next page. This\\npseudocode highlights a few optimizations possible in the special case of binary features\\n(features are either active (=1) or inactive (=0)).\\nExample 12.1: Traces in GridworldThe use of eligibility traces can substantially\\nincrease the e\\x00ciency of control algorithms over one-step methods and even overn-step\\nmethods. The reason for this is illustrated by the gridworld example below.\\nPath taken Action values increasedby one-step SarsaAction values increasedby Sarsa( ) with =0.9\\nG\\nG\\nG\\nPath taken Action values increasedby one-step SarsaAction values increasedby Sarsa(\\x04) with \\x04=0.9by 10-step Sarsa\\nG G G\\nPath taken Action values increasedby one-step Sarsa Action values increased by 10-step Sarsa\\nG G G\\nhh\\nThe ﬁrst panel shows the path taken by an agent in a single episode. The initial estimated\\nvalues were zero, and all rewards were zero except for a positive reward at the goal\\nlocation marked byG. The arrows in the other panels show, for various algorithms, which\\naction-values would be increased, and by how much, upon reaching the goal. A one-step\\nmethod would increment only the last action value, whereas ann-step method would\\nequally increment the lastn actions’ values (assuming\\x00 = 1), and an eligibility trace\\nmethod would update all the action values up to the beginning of the episode, to di↵erent\\ndegrees, fading with recency. The fading strategy is often the best.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 326, 'page_label': '327'}, page_content='12.7. Sarsa( \\x00) 305\\nSarsa(\\x00) with binary features and linear function approximation\\nfor estimating w>x ⇡ q⇡ or q⇤\\nInput: a functionF(s, a) returning the set of (indices of) active features fors, a\\nInput: a policy⇡\\nAlgorithm parameters: step size↵> 0, trace decay rate\\x00 2 [0, 1], small\"> 0\\nInitialize: w =( w1,...,w d)> 2 Rd (e.g., w = 0), z =( z1,...,z d)> 2 Rd\\nLoop for each episode:\\nInitialize S\\nChoose A ⇠ ⇡(·| S) or\"-greedy according to ˆq(S, · , w)\\nz  0\\nLoop for each step of episode:\\nTake actionA, observeR, S0\\n\\x00  R\\nLoop for i in F(S, A):\\n\\x00  \\x00 \\x00 wi\\nzi  zi + 1 (accumulating traces)\\nor zi  1 (replacing traces)\\nIf S0 is terminal then:\\nw  w + ↵\\x00 z\\nGo to next episode\\nChoose A0 ⇠ ⇡(·| S0) or\"-greedy according to ˆq(S0, · , w)\\nLoop for i in F(S0,A 0): \\x00  \\x00 + \\x00wi\\nw  w + ↵\\x00 z\\nz  \\x00\\x00z\\nS  S0; A  A0\\nExercise 12.6 Modify the pseudocode for Sarsa(\\x00) to use dutch traces(12.11) without the\\nother distinctive features of a true online algorithm. Assume linear function approximation\\nand binary features. ⇤\\nExample 12.2: Sarsa(\\x00) on Mountain Car Figure 12.10 (left) on the next page\\nshows results with Sarsa(\\x00) on the Mountain Car task introduced in Example 10.1. The\\nfunction approximation, action selection, and environmental details were exactly as in\\nChapter 10, and thus it is appropriate to numerically compare these results with the\\nChapter 10 results forn-step Sarsa (right side of the ﬁgure). The earlier results varied the\\nupdate length n whereas here for Sarsa(\\x00) we vary the trace parameter\\x00,w h i c hp l a y s\\na similar role. The fading-trace bootstrapping strategy of Sarsa(\\x00) appears to result in\\nmore e\\x00cient learning on this problem.\\nThere is also an action-value version of our ideal TD method, the online\\x00-return algo-\\nrithm (Section 12.4) and its e\\x00cient implementation as true online TD(\\x00) (Section 12.5).\\nEverything in Section 12.4 goes through without change other than to use the action-value\\nform of then-step return given at the beginning of the current section. The analyses in\\nSections 12.5 and 12.6 also carry through for action values, the only change being the use'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 327, 'page_label': '328'}, page_content='306 Chapter 12: Eligibility Traces\\n220\\n240\\n260\\n300\\n0 0.5 1 1.5\\nMountain CarSteps per episodeaveraged overﬁrst 50 episodesand 100 runs\\n280\\n200\\n180\\n×  number of tilings (8)↵\\nλ=.96\\nλ=.92\\nλ=.99λ=.84\\nλ=.68\\nλ=0\\n220\\n240\\n260\\n300\\n0 0.5 1 1.5\\n280\\n0 0.5 1 1.5\\nn=1n=2n=4n=8\\nn=16\\nn=8\\nn=4n=2\\nn=16n=1\\nλ=.98\\n×  number of tilings (8)↵\\nSarsa(λ) with replacing tracesn-step Sarsa\\nλ=.92\\nλ=.84\\nFigure 12.10: Early performance on the Mountain Car task of Sarsa(\\x00) with replacing traces\\nand n-step Sarsa (copied from Figure 10.4) as a function of the step size,↵.\\nof state–action feature vectorsxt = x(St,A t) instead of state feature vectorsxt = x(St).\\nPseudocode for the resulting e\\x00cient algorithm, calledtrue online Sarsa(\\x00) is given in\\nthe box on the next page. The ﬁgure below compares the performance of various versions\\nof Sarsa(\\x00) on the Mountain Car example.\\nTrue Online TD(\\x00)\\n0 0.5 1 1.50\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nstep−size\\nRMS           error         \\nTD(λ), accumulating traces − Task 1\\nλ = 0λ = 0.975λ = 1\\nλ = 0.95\\nλ = 0.2\\nλ = 0.1\\n0 0.5 1 1.50\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nstep−size\\nRMS           error         \\nTD(λ), replacing traces − Task 1\\nλ = 0\\nλ = 1\\n0 0.5 1 1.50\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nstep−size\\nRMS           error         \\ntrue online TD(λ) − Task 1\\nλ = 0\\nλ = 1\\n0 0.5 1 1.50\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nstep−size\\nRMS           error         \\nTD(λ), accumulating traces − Task 2\\nλ = 1λ = 0\\n0 0.5 1 1.50\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nstep−size\\nRMS          error         \\nTD(λ), replacing traces − Task 2\\n0 0.5 1 1.50\\n0.2\\n0.4\\n0.6\\n0.8\\n1\\nstep−size\\nRMS           error         \\ntrue online TD(λ) − Task 2\\nλ = 1\\nλ = 0\\nFigure 2.RMS error of state values at the end of each episode, averaged over the ﬁrst 10 episodes, as well as 100 independent runs, fordifferent values of↵and\\x00.\\nFigure 4 compares true online Sarsa(\\x00) with the traditionalSarsa(\\x00) implementation on the standard mountain car task(Sutton & Barto, 1998) using 10 tilings of each10⇥10tiles.Results are plotted for\\x00=0.9and↵=↵0/10, for↵0from0.2 to 2.0 with steps of 0.2. Clearing/no clearing refersto whether the trace values of non-selected actions are setto 0 (clearing) or not (no clearing), in case of replacingtraces. The results suggest that the true online principle isalso effective in a control setting.\\n6. ConclusionWe presented for the ﬁrst time an online version of the for-ward view which forms the theoretical and intuitive foun-dation for the TD(\\x00) algorithm. In addition, we have pre-sented a new variant of TD(\\x00), with the same compu-tational complexity as the classical algorithm, which wecall true online TD(\\x00). We proved that true online TD(\\x00)matches the new online forward view exactly, in contrastto classical online TD(\\x00), which only approximates itsforward view. In addition, we demonstrated empiricallythat true online TD(\\x00) outperforms conventional TD(\\x00) onthree benchmark problems. It seems, by adhering moretruly to the original goal of TD(\\x00)—matching an intuitivelyclear forward view even in the online case—that we havefound a new algorithm that simply improves on TD(\\x00).\\n0.20.40.60.81 1.21.41.61.82−550\\n−500\\n−450\\n−400\\n−350\\n−300\\n−250\\n−200\\n−150\\nα0\\nreturn\\nSarsa(λ), replacing, clearing\\nSarsa(λ), replacing, no clearing\\nSarsa(λ), accumulating\\nFigure 4.Average return over ﬁrst 20 episodes on mountain cartask for\\x00=0.9and different↵0. Results are averaged over 100independent runs.\\nAcknowledgementsThe authors thank Hado van Hasselt and Rupam Mahmoodfor extensive discussions leading to the reﬁnement of theseideas. This work was supported by grants from AlbertaInnovates – Technology Futures and the National Scienceand Engineering Research Council of Canada.\\nMountain CarReward per episodeaveraged overﬁrst 20 episodesand 100 runs\\n×  number of tilings (8)↵\\n7UXH\\x03RQOLQH\\x036DUVD\\x0bλ\\x0c\\nSarsa(λ) with replacing traces\\nSarsa(λ) with replacing tracesand clearing the traces of other actions\\nSarsa(λ) with accumulating traces\\nFigure 12.11: Summary comparison of Sarsa(\\x00) algorithms on the Mountain Car task. True\\nonline Sarsa(\\x00) performed better than regular Sarsa(\\x00) with both accumulating and replacing\\ntraces. Also included is a version of Sarsa(\\x00) with replacing traces in which, on each time step,\\nthe traces for the state and the actions not selected were set to zero.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 328, 'page_label': '329'}, page_content='12.8. Variable \\x00 and \\x00 307\\nTrue online Sarsa(\\x00) for estimating w>x ⇡ q⇡ or q⇤\\nInput: a feature functionx : S+ ⇥ A ! Rd such thatx(terminal, · )= 0\\nInput: a policy⇡ (if estimatingq⇡)\\nAlgorithm parameters: step size↵> 0, trace decay rate\\x00 2 [0, 1], small\"> 0\\nInitialize: w 2 Rd (e.g., w = 0)\\nLoop for each episode:\\nInitialize S\\nChoose A ⇠ ⇡(·| S) or\"-greedy according to ˆq(S, · , w)\\nx  x(S, A)\\nz  0\\nQold  0\\nLoop for each step of episode:\\n| Take actionA, observeR, S0\\n| Choose A0 ⇠ ⇡(·| S0) or\"-greedy according to ˆq(S0, · , w)\\n| x0  x(S0,A 0)\\n| Q  w>x\\n| Q0  w>x0\\n| \\x00  R + \\x00Q0 \\x00 Q\\n| z  \\x00\\x00z +\\n\\x00\\n1 \\x00 ↵\\x00\\x00 z>x\\n\\x00\\nx\\n| w  w + ↵(\\x00 + Q \\x00 Qold)z \\x00 ↵(Q \\x00 Qold)x\\n| Qold  Q0\\n| x  x0\\n| A  A0\\nuntil S0 is terminal\\nFinally, there is also a truncated version of Sarsa(\\x00), called forward Sarsa(\\x00) (van\\nSeijen, 2016), which appears to be a particularly e↵ective model-free control method for\\nuse in conjunction with multi-layer artiﬁcial neural networks.\\n12.8 Variable \\x00 and \\x00\\nWe are starting now to reach the end of our development of fundamental TD learning\\nalgorithms. To present the ﬁnal algorithms in their most general forms, it is useful to\\ngeneralize the degree of bootstrapping and discounting beyond constant parameters to\\nfunctions potentially dependent on the state and action. That is, each time step will have\\nad i ↵ e r e n t\\x00 and \\x00, denoted\\x00t and \\x00t. We change notation now so that\\x00 : S⇥A ! [0, 1]\\nis now a function from states and actions to the unit interval such that\\x00t\\n.= \\x00(St,A t), and\\nsimilarly, \\x00 : S ! [0, 1] is a function from states to the unit interval such that\\x00t\\n.= \\x00(St).\\nIntroducing the function\\x00,t h etermination function, is particularly signiﬁcant because\\nit changes the return, the fundamental random variable whose expectation we seek to'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 329, 'page_label': '330'}, page_content='308 Chapter 12: Eligibility Traces\\nestimate. Now the return is deﬁned more generally as\\nGt\\n.= Rt+1 + \\x00t+1Gt+1\\n= Rt+1 + \\x00t+1Rt+2 + \\x00t+1\\x00t+2Rt+3 + \\x00t+1\\x00t+2\\x00t+3Rt+4 + ···\\n=\\n1X\\nk=t\\n kY\\ni=t+1\\n\\x00i\\n!\\nRk+1, (12.17)\\nwhere, to assure the sums are ﬁnite, we require thatQ1\\nk=t \\x00k = 0 with probability one for\\nall t. One convenient aspect of this deﬁnition is that it enables the episodic setting and\\nits algorithms to be presented in terms of a single stream of experience, without special\\nterminal states, start distributions, or termination times. An erstwhile terminal state\\nbecomes a state at which\\x00(s) = 0 and which transitions to the start distribution. In that\\nway (and by choosing\\x00(· ) as a constant in all other states) we can recover the classical\\nepisodic setting as a special case. State dependent termination includes other prediction\\ncases such aspseudo termination, in which we seek to predict a quantity without altering\\nthe ﬂow of the Markov process. Discounted returns can be thought of as such a quantity,\\nin which case state-dependent termination uniﬁes the episodic and discounted-continuing\\ncases. (The undiscounted-continuing case still needs some special treatment.)\\nThe generalization to variable bootstrapping is not a change in the problem, like\\ndiscounting, but a change in the solution strategy. The generalization a↵ects the\\x00-\\nreturns for states and actions. The new state-based\\x00-return can be written recursively\\nas\\nG\\x00s\\nt\\n.= Rt+1 + \\x00t+1\\n\\x00\\n(1 \\x00 \\x00t+1)ˆv(St+1,wt)+ \\x00t+1G\\x00s\\nt+1\\n\\x00\\n, (12.18)\\nwhere now we have added the “s”t ot h es u p e r s c r i p t\\x00 to remind us that this is a return\\nthat bootstraps from state values, distinguishing it from returns that bootstrap from\\naction values, which we present below with “a” in the superscript. This equation says\\nthat the \\x00-return is the ﬁrst reward, undiscounted and una↵ected by bootstrapping, plus\\npossibly a second term to the extent that we are not discounting at the next state (that\\nis, according to\\x00t+1; recall that this is zero if the next state is terminal). To the extent\\nthat we aren’t terminating at the next state, we have a second term which is itself divided\\ninto two cases depending on the degree of bootstrapping in the state. To the extent we\\nare bootstrapping, this term is the estimated value at the state, whereas, to the extent\\nthat we are not bootstrapping, the term is the\\x00-return for the next time step. The\\naction-based \\x00-return is either the Sarsa form\\nG\\x00a\\nt\\n.= Rt+1 + \\x00t+1\\n⇣\\n(1 \\x00 \\x00t+1)ˆq(St+1,A t+1, wt)+ \\x00t+1G\\x00a\\nt+1\\n⌘\\n, (12.19)\\nor the Expected Sarsa form,\\nG\\x00a\\nt\\n.= Rt+1 + \\x00t+1\\n⇣\\n(1 \\x00 \\x00t+1) ¯Vt(St+1)+ \\x00t+1G\\x00a\\nt+1\\n⌘\\n, (12.20)\\nwhere (7.8) is generalized to function approximation as\\n¯Vt(s) .=\\nX\\na\\n⇡(a|s)ˆq(s, a,wt). (12.21)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 330, 'page_label': '331'}, page_content='12.9. O↵-policy Traces with Control Variates 309\\nExercise 12.7 Generalize the three recursive equations above to their truncated versions,\\ndeﬁning G\\x00s\\nt:h and G\\x00a\\nt:h. ⇤\\n12.9 O↵-policy Traces with Control Variates\\nThe ﬁnal step is to incorporate importance sampling. For methods using non-truncated\\n\\x00-returns, there is not a practical option in which the importance-sampling weighting is\\napplied to the target return (as there is forn-step methods as explained in Section 7.3).\\nInstead, we move directly to the bootstrapping generalization of per-decision importance\\nsampling with control variates (Section 7.4).\\nIn the state case, our ﬁnal deﬁnition of the\\x00-return generalizes(12.18), after the model\\nof (7.13), to\\nG\\x00s\\nt\\n.= ⇢t\\n⇣\\nRt+1 +\\x00t+1\\n\\x00\\n(1\\x00\\x00t+1)ˆv(St+1,wt)+ \\x00t+1G\\x00s\\nt+1\\n\\x00⌘\\n+(1 \\x00⇢t)ˆv(St,wt), (12.22)\\nwhere ⇢t = ⇡(At|St)\\nb(At|St) is the usual single-step importance sampling ratio. Much like the\\nother returns we have seen in this book, this ﬁnal\\x00-return can be approximated simply\\nin terms of sums of the state-based TD error,\\n\\x00s\\nt\\n.= Rt+1 + \\x00t+1ˆv(St+1,wt) \\x00 ˆv(St,wt), (12.23)\\nas\\nG\\x00s\\nt ⇡ ˆv(St,wt)+ ⇢t\\n1X\\nk=t\\n\\x00s\\nk\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i, (12.24)\\nwith the approximation becoming exact if the approximate value function does not change.\\nExercise 12.8 Prove that(12.24) becomes exact if the value function does not change.\\nTo save writing, consider the case oft = 0, and use the notationVk\\n.=ˆv(Sk,w). ⇤\\nExercise 12.9 The truncated version of the general o↵-policy return is denotedG\\x00s\\nt:h.\\nGuess the correct equation, based on (12.24). ⇤\\nThe above form of the\\x00-return (12.24) is convenient to use in a forward-view update,\\nwt+1 = wt + ↵\\n\\x00\\nG\\x00s\\nt \\x00 ˆv(St,wt)\\n\\x00\\nrˆv(St,wt)\\n⇡ wt + ↵⇢t\\n 1X\\nk=t\\n\\x00s\\nk\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i\\n!\\nrˆv(St,wt),\\nwhich to the experienced eye looks like an eligibility-based TD update—the product is\\nlike an eligibility trace and it is multiplied by TD errors. But this is just one time step of\\na forward view. The relationship that we are looking for is that the forward-view update,\\nsummed over time, is approximately equal to a backward-view update, summed over\\ntime (this relationship is only approximate because again we ignore changes in the value'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 331, 'page_label': '332'}, page_content='310 Chapter 12: Eligibility Traces\\nfunction). The sum of the forward-view update over time is\\n1X\\nt=0\\n(wt+1 \\x00 wt) ⇡\\n1X\\nt=0\\n1X\\nk=t\\n↵⇢t\\x00s\\nkrˆv(St,wt)\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i\\n=\\n1X\\nk=0\\nkX\\nt=0\\n↵⇢trˆv(St,wt)\\x00s\\nk\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i\\n(using the summation rule:Py\\nt=x\\nPy\\nk=t = Py\\nk=x\\nPk\\nt=x)\\n=\\n1X\\nk=0\\n↵\\x00s\\nk\\nkX\\nt=0\\n⇢trˆv(St,wt)\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i,\\nwhich would be in the form of the sum of a backward-view TD update if the entire\\nexpression from the second sum on could be written and updated incrementally as an\\neligibility trace, which we now show can be done. That is, we show that if this expression\\nwas the trace at timek, then we could update it from its value at timek \\x00 1b y :\\nzk =\\nkX\\nt=0\\n⇢trˆv(St,wt)\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i\\n=\\nk\\x001X\\nt=0\\n⇢trˆv(St,wt)\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i + ⇢krˆv(Sk,wk)\\n= \\x00k\\x00k⇢k\\nk\\x001X\\nt=0\\n⇢trˆv(St,wt)\\nk\\x001Y\\ni=t+1\\n\\x00i\\x00i⇢i\\n| {z }\\nzk\\x001\\n+ ⇢krˆv(Sk,wk)\\n= ⇢k\\n\\x00\\n\\x00k\\x00kzk\\x001 + rˆv(Sk,wk)\\n\\x00\\n,\\nwhich, changing the index fromk to t, is the general accumulating trace update for state\\nvalues:\\nzt\\n.= ⇢t\\n\\x00\\n\\x00t\\x00tzt\\x001 + rˆv(St,wt)\\n\\x00\\n, (12.25)\\nThis eligibility trace, together with the usual semi-gradient parameter-update rule for\\nTD(\\x00) (12.7), forms a general TD(\\x00) algorithm that can be applied to either on-policy or\\no↵-policy data. In the on-policy case, the algorithm is exactly TD(\\x00) because ⇢t is alway\\n1 and(12.25) becomes the usual accumulating trace(12.5) (extended to variable\\x00 and\\n\\x00). In the o↵-policy case, the algorithm often works well but, as a semi-gradient method,\\nis not guaranteed to be stable. In the next few sections we will consider extensions of it\\nthat do guarantee stability.\\nA very similar series of steps can be followed to derive the o↵-policy eligibility traces\\nfor action-value methods and corresponding general Sarsa(\\x00) algorithms. One could start\\nwith either recursive form for the general action-based\\x00-return, (12.19) or (12.20),b u t\\nthe latter (the Expected Sarsa form) works out to be simpler. We extend(12.20) to the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 332, 'page_label': '333'}, page_content='12.9. O↵-policy Traces with Control Variates 311\\no↵-policy case after the model of (7.14) to produce\\nG\\x00a\\nt\\n.= Rt+1+\\x00t+1\\n⇣\\n(1\\x00\\x00t+1) ¯Vt(St+1)+\\x00t+1\\n⇥\\n⇢t+1G\\x00a\\nt+1+ ¯Vt(St+1)\\x00⇢t+1 ˆq(St+1,A t+1, wt)\\n⇤⌘\\n= Rt+1 + \\x00t+1\\n⇣\\n¯Vt(St+1)+ \\x00t+1⇢t+1\\n⇥\\nG\\x00a\\nt+1 \\x00 ˆq(St+1,A t+1, wt)\\n⇤⌘\\n(12.26)\\nwhere ¯Vt(St+1) is as given by(12.21). Again the\\x00-return can be written approximately\\nas the sum of TD errors,\\nG\\x00a\\nt ⇡ ˆq(St,A t, wt)+\\n1X\\nk=t\\n\\x00a\\nk\\nkY\\ni=t+1\\n\\x00i\\x00i⇢i, (12.27)\\nusing the expectation form of the action-based TD error:\\n\\x00a\\nt = Rt+1 + \\x00t+1 ¯Vt(St+1) \\x00 ˆq(St,A t, wt). (12.28)\\nAs before, the approximation becomes exact if the approximate value function does not\\nchange.\\nExercise 12.10 Prove that(12.27) becomes exact if the value function does not change.\\nTo save writing, consider the case oft = 0, and use the notationQk = ˆq(Sk,A k, w). Hint:\\nStart by writing out\\x00a\\n0 and G\\x00a\\n0 ,t h e nG\\x00a\\n0 \\x00 Q0. ⇤\\nExercise 12.11 The truncated version of the general o↵-policy return is denotedG\\x00a\\nt:h.\\nGuess the correct equation for it, based on (12.27). ⇤\\nUsing steps entirely analogous to those for the state case, one can write a forward-view\\nupdate based on(12.27), transform the sum of the updates using the summation rule,\\nand ﬁnally derive the following form for the eligibility trace for action values:\\nzt\\n.= \\x00t\\x00t⇢tzt\\x001 + rˆq(St,A t, wt). (12.29)\\nThis eligibility trace, together with the expectation-based TD error(12.28) and the usual\\nsemi-gradient parameter-update rule(12.7), forms an elegant, e\\x00cient Expected Sarsa(\\x00)\\nalgorithm that can be applied to either on-policy or o↵-policy data. It is probably the\\nbest algorithm of this type at the current time (though of course it is not guaranteed to\\nbe stable until combined in some way with one of the methods presented in the following\\nsections). In the on-policy case with constant\\x00 and \\x00, and the usual state–action TD\\nerror (12.16), the algorithm would be identical to the Sarsa(\\x00) algorithm presented in\\nSection 12.7.\\nExercise 12.12 Show in detail the steps outlined above for deriving(12.29) from (12.27).\\nStart with the update(12.15),s u b s t i t u t eG\\x00a\\nt from (12.26) for G\\x00\\nt , then follow similar\\nsteps as led to (12.25). ⇤\\nAt \\x00 = 1, these algorithms become closely related to corresponding Monte Carlo\\nalgorithms. One might expect that an exact equivalence would hold for episodic problems\\nand o↵-line updating, but in fact the relationship is subtler and slightly weaker than that.\\nUnder these most favorable conditions still there is not an episode by episode equiva-\\nlence of updates, only of their expectations. This should not be surprising as these methods'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 333, 'page_label': '334'}, page_content='312 Chapter 12: Eligibility Traces\\nmake irrevocable updates as a trajectory unfolds, whereas true Monte Carlo methods\\nwould make no update for a trajectory if any action within it has zero probability under\\nthe target policy. In particular, all of these methods, even at\\x00 = 1, still bootstrap in\\nthe sense that their targets depend on the current value estimates—it’s just that the\\ndependence cancels out in expected value. Whether this is a good or bad property in\\npractice is another question. Recently, methods have been proposed that do achieve an\\nexact equivalence (Sutton, Mahmood, Precup and van Hasselt, 2014). These methods\\nrequire an additional vector of “provisional weights” that keep track of updates which\\nhave been made but may need to be retracted (or emphasized) depending on the actions\\ntaken later. The state and state–action versions of these methods are called PTD(\\x00) and\\nPQ(\\x00) respectively, where the ‘P’ stands for Provisional.\\nThe practical consequences of all these new o↵-policy methods have not yet been\\nestablished. Undoubtedly, issues of high variance will arise as they do in all o↵-policy\\nmethods using importance sampling (Section 11.9).\\nIf \\x00< 1, then all these o↵-policy algorithms involve bootstrapping and the deadly\\ntriad applies (Section 11.3), meaning that they can be guaranteed stable only for the\\ntabular case, for state aggregation, and for other limited forms of function approximation.\\nFor linear and more-general forms of function approximation the parameter vector may\\ndiverge to inﬁnity as in the examples in Chapter 11. As we discussed there, the challenge\\nof o↵-policy learning has two parts. O↵-policy eligibility traces deal e↵ectively with the\\nﬁrst part of the challenge, correcting for the expected value of the targets, but not at\\nall with the second part of the challenge, having to do with the distribution of updates.\\nAlgorithmic strategies for meeting the second part of the challenge of o↵-policy learning\\nwith eligibility traces are summarized in Section 12.11.\\nExercise 12.13 What are the dutch-trace and replacing-trace versions of o↵-policy\\neligibility traces for state-value and action-value methods? ⇤\\n12.10 Watkins’s Q( \\x00) to Tree-Backup(\\x00)\\nSeveral methods have been proposed over the years to extend Q-learning to eligibility\\ntraces. The original isWatkins’s Q(\\x00), which decays its eligibility traces in the usual way\\nas long as a greedy action was taken, then cuts the traces to zero after the ﬁrst non-greedy\\naction. The backup diagram for Watkins’s Q(\\x00) is shown in Figure 12.12. In Chapter 6,\\nwe uniﬁed Q-learning and Expected Sarsa in the o↵-policy version of the latter, which\\nincludes Q-learning as a special case, and generalizes it to arbitrary target policies, and\\nin the previous section of this chapter we completed our treatment of Expected Sarsa by\\ngeneralizing it to o↵-policy eligibility traces. In Chapter 7, however, we distinguished\\nn-step Expected Sarsa fromn-step Tree Backup, where the latter retained the property\\nof not using importance sampling. It remains then to present the eligibility trace version\\nof Tree Backup, which we will callTree-Backup(\\x00), or TB(\\x00) for short. This is arguably\\nthe true successor to Q-learning because it retains its appealing absence of importance\\nsampling even though it can be applied to o↵-policy data.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 334, 'page_label': '335'}, page_content='12.10. Watkins’s Q( \\x00)t oT r e e - B a c k u p (\\x00)3 1 3\\n1\\x00\\x00\\n(1\\x00\\x00)\\x00\\n(1\\x00\\x00)\\x002\\n\\x00T\\x00t\\x001\\n···\\nSt At\\nAt+1\\nSt+1Rt+1\\nSTRT\\n···\\nSt+2Rt+2\\nAt+2\\nX=1\\nOR\\n···\\n···\\nSt+nRt+n\\nFirst non-greedy action\\n\\x00n\\x001\\nWatkins’s Q(\\x00)\\nFigure 12.12: The backup diagram for Watkins’s Q(\\x00). The series of component updates ends\\neither with the end of the episode or with the ﬁrst nongreedy action, whichever comes ﬁrst.\\nThe concept of TB(\\x00) is straightforward. As shown in its backup diagram in Fig-\\nure 12.13, the tree-backup updates of each length (from Section 7.5) are weighted in the\\nusual way dependent on the bootstrapping parameter\\x00. To get the detailed equations,\\nwith the right indices on the general bootstrapping and discounting parameters, it is\\nbest to start with a recursive form(12.20) for the \\x00-return using action values, and then\\nexpand the bootstrapping case of the target after the model of (7.16):\\nG\\x00a\\nt\\n.= Rt+1+\\x00t+1\\n✓\\n(1\\x00\\x00t+1) ¯Vt(St+1)+\\x00t+1\\nh X\\na6=At+1\\n⇡(a|St+1)ˆq(St+1,a ,wt)+⇡(At+1|St+1)G\\x00a\\nt+1\\ni◆\\n= Rt+1 + \\x00t+1\\n✓\\n¯Vt(St+1)+ \\x00t+1⇡(At+1|St+1)\\n⇣\\nG\\x00a\\nt+1 \\x00 ˆq(St+1,A t+1, wt)\\n⌘◆\\nAs per the usual pattern, it can also be written approximately (ignoring changes in the\\napproximate value function) as a sum of TD errors,\\nG\\x00a\\nt ⇡ ˆq(St,A t, wt)+\\n1X\\nk=t\\n\\x00a\\nk\\nkY\\ni=t+1\\n\\x00i\\x00i⇡(Ai|Si),\\nusing the expectation form of the action-based TD error (12.28).\\nFollowing the same steps as in the previous section, we arrive at a special eligibility\\ntrace update involving the target-policy probabilities of the selected actions,\\nzt\\n.= \\x00t\\x00t⇡(At|St)zt\\x001 + rˆq(St,A t, wt).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 335, 'page_label': '336'}, page_content='314 Chapter 12: Eligibility Traces\\n1\\x00\\x00\\n(1\\x00\\x00)\\x00\\n(1\\x00\\x00)\\x002\\n\\x00T\\x00t\\x001\\n···\\nSt At\\nAt+1\\nAT\\x001\\nSt+1 Rt+1\\nST RT\\n···\\nSt+2 Rt+2\\nAt+2\\nX\\n=1\\n···\\nTree Backup( \\x00)\\nST\\x001\\nFigure 12.13: The backup diagram for the\\x00 version of the Tree Backup algorithm.\\nThis, together with the usual parameter-update rule(12.7),d e ﬁ n e st h eT B (\\x00) algorithm.\\nLike all semi-gradient algorithms, TB(\\x00) is not guaranteed to be stable when used with\\no↵-policy data and with a powerful function approximator. To obtain those assurances,\\nTB(\\x00) would have to be combined with one of the methods presented in the next section.\\n⇤Exercise 12.14 How might Double Expected Sarsa be extended to eligibility traces?⇤\\n12.11 Stable O↵-policy Methods with Traces\\nSeveral methods using eligibility traces have been proposed that achieve guarantees\\nof stability under o↵-policy training, and here we present four of the most important\\nusing this book’s standard notation, including general bootstrapping and discounting\\nfunctions. All are based on either the Gradient-TD or the Emphatic-TD ideas presented\\nin Sections 11.7 and 11.8. All the algorithms assume linear function approximation,\\nthough extensions to nonlinear function approximation can also be found in the literature.\\nGTD(\\x00) is the eligibility-trace algorithm analogous to TDC, the better of the two\\nstate-value Gradient-TD prediction algorithms discussed in Section 11.7. Its goal is to\\nlearn a parameterwt such that ˆv(s,w) .= w>\\nt x(s) ⇡ v⇡(s), even from data that is due to\\nfollowing another policyb. Its update is\\nwt+1\\n.= wt + ↵\\x00s\\nt zt \\x00 ↵\\x00t+1(1 \\x00 \\x00t+1)\\n\\x00\\nz>\\nt vt\\n\\x00\\nxt+1,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 336, 'page_label': '337'}, page_content='12.11. Stable O↵-policy Methods with Traces 315\\nwith \\x00s\\nt , zt, and⇢t deﬁned in the usual ways for state values(12.23) (12.25) (11.1), and\\nvt+1\\n.= vt + \\x00\\x00s\\nt zt \\x00 \\x00\\n\\x00\\nv>\\nt xt\\n\\x00\\nxt, (12.30)\\nwhere, as in Section 11.7,v 2 Rd is a vector of the same dimension asw, initialized to\\nv0 = 0, and\\x00> 0 is a second step-size parameter.\\nGQ(\\x00) is the Gradient-TD algorithm for action values with eligibility traces. Its goal\\nis to learn a parameterwt such that ˆq(s, a,wt) .= w>\\nt x(s, a) ⇡ q⇡(s, a) from o↵-policy\\ndata. If the target policy is\"-greedy, or otherwise biased toward the greedy policy forˆq,\\nthen GQ(\\x00) can be used as a control algorithm. Its update is\\nwt+1\\n.= wt + ↵\\x00a\\nt zt \\x00 ↵\\x00t+1(1 \\x00 \\x00t+1)\\n\\x00\\nz>\\nt vt\\n\\x00 ¯xt+1,\\nwhere ¯xt is the average feature vector forSt under the target policy,\\n¯xt\\n.=\\nX\\na\\n⇡(a|St)x(St,a ),\\n\\x00a\\nt is the expectation form of the TD error, which can be written\\n\\x00a\\nt\\n.= Rt+1 + \\x00t+1w>\\nt ¯xt+1 \\x00 w>\\nt xt,\\nzt is deﬁned in the usual way for action values(12.29), and the rest is as in GTD(\\x00),\\nincluding the update forvt (12.30).\\nHTD(\\x00) is a hybrid state-value algorithm combining aspects of GTD(\\x00) and TD(\\x00). Its\\nmost appealing feature is that it is a strict generalization of TD(\\x00) to o↵-policy learning,\\nmeaning that if the behavior policy happens to be the same as the target policy, then\\nHTD(\\x00) becomes the same as TD(\\x00), which is not true for GTD(\\x00). This is appealing\\nbecause TD(\\x00) is often faster than GTD(\\x00) when both algorithms converge, and TD(\\x00)\\nrequires setting only a single step size. HTD(\\x00)i sd e ﬁ n e db y\\nwt+1\\n.= wt + ↵\\x00s\\nt zt + ↵\\n\\x00\\n(zt \\x00 zb\\nt)>vt\\n\\x00\\n(xt \\x00 \\x00t+1xt+1),\\nvt+1\\n.= vt + \\x00\\x00s\\nt zt \\x00 \\x00\\n⇣\\nzb\\nt\\n>\\nvt\\n⌘\\n(xt \\x00 \\x00t+1xt+1), with v0\\n.= 0,\\nzt\\n.= ⇢t\\n\\x00\\n\\x00t\\x00tzt\\x001 + xt\\n\\x00\\n, with z\\x001\\n.= 0,\\nzb\\nt\\n.= \\x00t\\x00tzb\\nt\\x001 + xt, with zb\\n\\x001\\n.= 0,\\nwhere \\x00> 0 again is a second step-size parameter. In addition to the second set of\\nweights, vt, HTD(\\x00) also has a second set of eligibility traces,zb\\nt. These are conventional\\naccumulating eligibility traces for the behavior policy and become equal tozt if all the⇢t\\nare 1, which causes the last term in thewt update to be zero and the overall update to\\nreduce to TD(\\x00).\\nEmphatic TD(\\x00) is the extension of the one-step Emphatic-TD algorithm (Sections\\n9.11 and 11.8) to eligibility traces. The resultant algorithm retains strong o↵-policy\\nconvergence guarantees while enabling any degree of bootstrapping, albeit at the cost of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 337, 'page_label': '338'}, page_content='316 Chapter 12: Eligibility Traces\\nhigh variance and potentially slow convergence. Emphatic TD(\\x00)i sd e ﬁ n e db y\\nwt+1\\n.= wt + ↵\\x00tzt\\n\\x00t\\n.= Rt+1 + \\x00t+1w>\\nt xt+1 \\x00 w>\\nt xt\\nzt\\n.= ⇢t\\n\\x00\\n\\x00t\\x00tzt\\x001 + Mtxt\\n\\x00\\n, with z\\x001\\n.= 0,\\nMt\\n.= \\x00t It +( 1\\x00 \\x00t)Ft\\nFt\\n.= ⇢t\\x001\\x00tFt\\x001 + It, with F0\\n.= i(S0),\\nwhere Mt \\x00 0 is the general form ofemphasis, Ft \\x00 0i st e r m e dt h efollowon trace, and\\nIt \\x00 0i st h einterest, as described in Section 11.8. Note thatMt,l i k e\\x00t, is not really an\\nadditional memory variable. It can be removed from the algorithm by substituting its\\ndeﬁnition into the eligibility-trace equation. Pseudocode and software for the true online\\nversion of Emphatic-TD(\\x00) are available on the web (Sutton, 2015b).\\nIn the on-policy case (⇢t = 1, for allt), Emphatic-TD(\\x00) is similar to conventional\\nTD(\\x00), but still signiﬁcantly di↵erent. In fact, whereas Emphatic-TD(\\x00) is guaranteed\\nto converge for all state-dependent\\x00 functions, TD(\\x00) is not. TD(\\x00) is guaranteed\\nconvergent only for all constant\\x00. See Yu’s counterexample (Ghiassian, Raﬁee, and\\nSutton, 2016).\\n12.12 Implementation Issues\\nIt might at ﬁrst appear that tabular methods using eligibility traces are much more\\ncomplex than one-step methods. A naive implementation would require every state (or\\nstate–action pair) to update both its value estimate and its eligibility trace on every time\\nstep. This would not be a problem for implementations on single-instruction, multiple-\\ndata, parallel computers or in plausible artiﬁcial neural network (ANN) implementations,\\nbut it is a problem for implementations on conventional serial computers. Fortunately,\\nfor typical values of\\x00 and \\x00 the eligibility traces of almost all states are almost always\\nnearly zero; only those states that have recently been visited will have traces signiﬁcantly\\ngreater than zero and only these few states need to be updated to closely approximate\\nthese algorithms.\\nIn practice, then, implementations on conventional computers may keep track of and\\nupdate only the few traces that are signiﬁcantly greater than zero. Using this trick, the\\ncomputational expense of using traces in tabular methods is typically just a few times\\nthat of a one-step method. The exact multiple of course depends on\\x00 and \\x00 and on the\\nexpense of the other computations. Note that the tabular case is in some sense the worst\\ncase for the computational complexity of eligibility traces. When function approximation\\nis used, the computational advantages of not using traces generally decrease. For example,\\nif ANNs and backpropagation are used, then eligibility traces generally cause only a\\ndoubling of the required memory and computation per step. Truncated\\x00-return methods\\n(Section 12.3) can be computationally e\\x00cient on conventional computers though they\\nalways require some additional memory.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 338, 'page_label': '339'}, page_content='12.13. Conclusions 317\\n12.13 Conclusions\\nEligibility traces in conjunction with TD errors provide an e\\x00cient, incremental way of\\nshifting and choosing between Monte Carlo and TD methods. Then-step methods of\\nChapter 7 also enabled this, but eligibility trace methods are more general, often faster to\\nlearn, and o↵er di↵erent computational complexity tradeo↵s. This chapter has o↵ered an\\nintroduction to the elegant, emerging theoretical understanding of eligibility traces for on-\\nand o↵-policy learning and for variable bootstrapping and discounting. One aspect of this\\nelegant theory is true online methods, which exactly reproduce the behavior of expensive\\nideal methods while retaining the computational congeniality of conventional TD methods.\\nAnother aspect is the possibility of derivations that automatically convert from intuitive\\nforward-view methods to more e\\x00cient incremental backward-view algorithms. We\\nillustrated this general idea in a derivation that started with a classical, expensive Monte\\nCarlo algorithm and ended with a cheap incremental non-TD implementation using the\\nsame novel eligibility trace used in true online TD methods.\\nAs we mentioned in Chapter 5, Monte Carlo methods may have advantages in non-\\nMarkov tasks because they do not bootstrap. Because eligibility traces make TD methods\\nmore like Monte Carlo methods, they also can have advantages in these cases. If one\\nwants to use TD methods because of their other advantages, but the task is at least\\npartially non-Markov, then the use of an eligibility trace method is indicated. Eligibility\\ntraces are the ﬁrst line of defense against both long-delayed rewards and non-Markov\\ntasks.\\nBy adjusting\\x00, we can place eligibility trace methods anywhere along a continuum\\nfrom Monte Carlo to one-step TD methods. Where shall we place them? We do not yet\\nhave a good theoretical answer to this question, but a clear empirical answer appears to\\nbe emerging. On tasks with many steps per episode, or many steps within the half-life of\\ndiscounting, it appears signiﬁcantly better to use eligibility traces than not to (e.g., see\\nFigure 12.14). On the other hand, if the traces are so long as to produce a pure Monte\\nCarlo method, or nearly so, then performance degrades sharply. An intermediate mixture\\nappears to be the best choice. Eligibility traces should be used to bring us toward Monte\\nCarlo methods, but not all the way there. In the future it may be possible to more ﬁnely\\nvary the trade-o↵ between TD and Monte Carlo methods by using variable\\x00, but at\\npresent it is not clear how this can be done reliably and usefully.\\nMethods using eligibility traces require more computation than one-step methods, but\\nin return they o↵er signiﬁcantly faster learning, particularly when rewards are delayed by\\nmany steps. Thus it often makes sense to use eligibility traces when data are scarce and\\ncannot be repeatedly processed, as is often the case in online applications. On the other\\nhand, in o↵-line applications in which data can be generated cheaply, perhaps from an\\ninexpensive simulation, then it often does not pay to use eligibility traces. In these cases\\nthe objective is not to get more out of a limited amount of data, but simply to process as\\nmuch data as possible as quickly as possible. In these cases the speedup per datum due to\\ntraces is typically not worth their computational cost, and one-step methods are favored.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 339, 'page_label': '340'}, page_content='318 Chapter 12: Eligibility Traces\\naccumulating\\ntraces  \\n0.2\\n0.3\\n0.4\\n0.5\\n0 0.2 0.4 0.6 0.8 1\\n!\\nRANDOM WALK\\n50\\n100\\n150\\n200\\n250\\n300\\nFailures per\\n100,000 steps\\n0 0.2 0.4 0.6 0.8 1\\n!\\nCART AND POLE\\n400\\n450\\n500\\n550\\n600\\n650\\n700\\n Steps per\\nepisode\\n0 0.2 0.4 0.6 0.8 1\\n!\\nMOUNTAIN CAR\\nreplacing\\ntraces\\n150\\n160\\n170\\n180\\n190\\n200\\n210\\n220\\n230\\n240\\nCost per\\nepisode\\n0 0.2 0.4 0.6 0.8 1\\n!\\nPUDDLE WORLD\\nreplacing\\ntraces\\naccumulating\\ntraces \\nreplacing\\ntraces\\naccumulating\\ntraces\\nRMS error\\nFigure 12.14: The e↵ect of\\x00 on reinforcement learning performance in four di↵erent test\\nproblems. In all cases, performance is generally best (alower number in the graph) at an\\nintermediate value of\\x00. The two left panels are applications to simple continuous-state control\\ntasks using the Sarsa(\\x00) algorithm and tile coding, with either replacing or accumulating traces\\n(Sutton, 1996). The upper-right panel is for policy evaluation on a random walk task using TD(\\x00)\\n(Singh and Sutton, 1996). The lower right panel is unpublished data for the pole-balancing task\\n(Example 3.4) from an earlier study (Sutton, 1984).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 340, 'page_label': '341'}, page_content='12.13. Conclusions 319\\nBibliographical and Historical Remarks\\nEligibility traces came into reinforcement learning via the fecund ideas of Klopf (1972).\\nOur use of eligibility traces is based on Klopf’s work (Sutton, 1978a, 1978b, 1978c; Barto\\nand Sutton, 1981a, 1981b; Sutton and Barto, 1981a; Barto, Sutton, and Anderson, 1983;\\nSutton, 1984). We may have been the ﬁrst to use the term “eligibility trace” (Sutton\\nand Barto, 1981a). The idea that stimuli produce after e↵ects in the nervous system\\nthat are important for learning is very old (see Chapter 14). Some of the earliest uses of\\neligibility traces were in the actor–critic methods discussed in Chapter 13 (Barto, Sutton,\\nand Anderson, 1983; Sutton, 1984).\\n12.1 Compound updates were called “complex backups” in the ﬁrst edition of this\\nbook.\\nThe \\x00-return and its error-reduction properties were introduced by Watkins (1989)\\nand further developed by Jaakkola, Jordan, and Singh (1994). The random walk\\nresults in this and subsequent sections are new to this text, as are the terms\\n“forward view” and “backward view.” The notion of a\\x00-return algorithm was\\nintroduced in the ﬁrst edition of this text. The more reﬁned treatment presented\\nhere was developed in conjunction with Harm van Seijen (e.g., van Seijen and\\nSutton, 2014).\\n12.2 TD(\\x00) with accumulating traces was introduced by Sutton (1988, 1984). Con-\\nvergence in the mean was proved by Dayan (1992), and with probability 1 by\\nmany researchers, including Peng (1993), Dayan and Sejnowski (1994), Tsitsiklis\\n(1994), and Gurvits, Lin, and Hanson (1994). The bound on the error of the\\nasymptotic \\x00-dependent solution of linear TD(\\x00) is due to Tsitsiklis and Van\\nRoy (1997).\\n12.3 Truncated TD methods were developed by Cichosz (1995) and van Seijen (2016).\\n12.4 The idea of redoing updates was extensively developed by van Seijen, originally\\nunder the name “best-match learning” (van Seijen, 2011; van Seijen, Whiteson,\\nvan Hasselt, and Weiring, 2011).\\n12.5 True online TD(\\x00) is primarily due to Harm van Seijen (van Seijen and Sutton,\\n2014; van Seijen et al., 2016) though some of its key ideas were discovered\\nindependently by Hado van Hasselt (personal communication). The name “dutch\\ntraces” is in recognition of the contributions of both scientists. Replacing traces\\nare due to Singh and Sutton (1996).\\n12.6 The material in this section is from van Hasselt and Sutton (2015).\\n12.7 Sarsa(\\x00) with accumulating traces was ﬁrst explored as a control method by\\nRummery and Niranjan (1994; Rummery, 1995). True Online Sarsa(\\x00) was'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 341, 'page_label': '342'}, page_content='320 Chapter 12: Eligibility Traces\\nintroduced by van Seijen and Sutton (2014). The algorithm on page 307 was\\nadapted from van Seijen et al. (2016). The Mountain Car results were made for\\nthis text, except for Figure 12.11 which is adapted from van Seijen and Sutton\\n(2014).\\n12.8 Perhaps the ﬁrst published discussion of variable\\x00 was by Watkins (1989), who\\npointed out that the cutting o↵ of the update sequence (Figure 12.12) in his\\nQ(\\x00) when a nongreedy action was selected could be implemented by temporarily\\nsetting \\x00 to 0.\\nVariable \\x00 was introduced in the ﬁrst edition of this text. The roots of variable\\x00\\nare in the work on options (Sutton, Precup, and Singh, 1999) and its precursors\\n(Sutton, 1995a), becoming explicit in the GQ(\\x00) paper (Maei and Sutton, 2010),\\nwhich also introduced some of these recursive forms for the\\x00-returns.\\nA di↵erent notion of variable\\x00 has been developed by Yu (2012).\\n12.9 O↵-policy eligibility traces were introduced by Precup et al. (2000, 2001), then\\nfurther developed by Bertsekas and Yu (2009), Maei (2011; Maei and Sutton,\\n2010), Yu (2012), and by Sutton, Mahmood, Precup, and van Hasselt (2014).\\nThe last reference in particular gives a powerful forward view for o↵-policy TD\\nmethods with general state-dependent\\x00 and \\x00. The presentation here seems to\\nbe new.\\nThis section ends with an elegant Expected Sarsa(\\x00) algorithm. Although it is\\na natural algorithm, to our knowledge it has not previously been described or\\ntested in the literature.\\n12.10 Watkins’s Q(\\x00) is due to Watkins (1989). The tabular, episodic, o↵-line version\\nhas been proven convergent by Munos, Stepleton, Harutyunyan, and Bellemare\\n(2016). Alternative Q(\\x00) algorithms were proposed by Peng and Williams (1994,\\n1996) and by Sutton, Mahmood, Precup, and van Hasselt (2014). Tree Backup(\\x00)\\nis due to Precup, Sutton, and Singh (2000).\\n12.11 GTD(\\x00) is due to Maei (2011). GQ(\\x00) is due to Maei and Sutton (2010).\\nHTD(\\x00) is due to White and White (2016) based on the one-step HTD algorithm\\nintroduced by Hackman (2012). The latest developments in the theory of\\nGradient-TD methods are by Yu (2017). Emphatic TD(\\x00) was introduced by\\nSutton, Mahmood, and White (2016), who proved its stability. Yu (2015, 2016)\\nproved its convergence, and the algorithm was developed further by Hallak et\\nal. (2015, 2016).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 342, 'page_label': '343'}, page_content='Chapter 13\\nPolicy Gradient Methods\\nIn this chapter we consider something new. So far in this book almost all the methods\\nhave been action-value methods; they learned the values of actions and then selected\\nactions based on their estimated action values1; their policies would not even exist without\\nthe action-value estimates. In this chapter we consider methods that instead learn a\\nparameterized policythat can select actions without consulting a value function. A value\\nfunction may still be used tolearn the policy parameter, but is not required for action\\nselection. We use the notation✓ 2 Rd0\\nfor the policy’s parameter vector. Thus we write\\n⇡(a|s, ✓)= Pr{At =a | St =s, ✓t =✓} for the probability that actiona is taken at timet\\ngiven that the environment is in states at timet with parameter✓.I fam e t h o du s e sa\\nlearned value function as well, then the value function’s weight vector is denotedw 2 Rd\\nas usual, as in ˆv(s,w).\\nIn this chapter we consider methods for learning the policy parameter based on the\\ngradient of some scalar performance measureJ(✓) with respect to the policy parameter.\\nThese methods seek tomaximize performance, so their updates approximate gradient\\nascent in J:\\n✓t+1 = ✓t + ↵ \\\\rJ(✓t), (13.1)\\nwhere \\\\rJ(✓t) 2 Rd0\\nis a stochastic estimate whose expectation approximates the gradient\\nof the performance measure with respect to its argument✓t. All methods that follow\\nthis general schema we callpolicy gradient methods, whether or not they also learn an\\napproximate value function. Methods that learn approximations to both policy and value\\nfunctions are often calledactor–critic methods, where ‘actor’ is a reference to the learned\\npolicy, and ‘critic’ refers to the learned value function, usually a state-value function.\\nFirst we treat the episodic case, in which performance is deﬁned as the value of the start\\nstate under the parameterized policy, before going on to consider the continuing case, in\\n1The lone exception is the gradient bandit algorithms of Section 2.8. In fact, that section goes through\\nmany of the same steps, in the single-state bandit case, as we go through here for full MDPs. Reviewing\\nthat section would be good preparation for fully understanding this chapter.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 343, 'page_label': '344'}, page_content='322 Chapter 13: Policy Gradient Methods\\nwhich performance is deﬁned as the average reward rate, as in Section 10.3. In the end,\\nwe are able to express the algorithms for both cases in very similar terms.\\n13.1 Policy Approximation and its Advantages\\nIn policy gradient methods, the policy can be parameterized in any way, as long as\\n⇡(a|s, ✓) is di↵erentiable with respect to its parameters, that is, as long asr⇡(a|s, ✓)( t h e\\ncolumn vector of partial derivatives of⇡(a|s, ✓) with respect to the components of✓)e x i s t s\\nand is ﬁnite for alls 2 S, a 2 A(s), and✓ 2 Rd0\\n. In practice, to ensure exploration we\\ngenerally require that the policy never becomes deterministic (i.e., that⇡(a|s, ✓) 2 (0, 1),\\nfor all s, a,✓). In this section we introduce the most common parameterization for\\ndiscrete action spaces and point out the advantages it o↵ers over action-value methods.\\nPolicy-based methods also o↵er useful ways of dealing with continuous action spaces, as\\nwe describe later in Section 13.7.\\nIf the action space is discrete and not too large, then a natural and common kind of\\nparameterization is to form parameterized numerical preferencesh(s, a,✓) 2 R for each\\nstate–action pair. The actions with the highest preferences in each state are given the\\nhighest probabilities of being selected, for example, according to an exponential soft-max\\ndistribution:\\n⇡(a|s, ✓) .= eh(s,a,✓)\\nP\\nb eh(s,b,✓) , (13.2)\\nwhere e ⇡ 2.71828 is the base of the natural logarithm. Note that the denominator here\\nis just what is required so that the action probabilities in each state sum to one. We call\\nthis kind of policy parameterizationsoft-max in action preferences.\\nThe action preferences themselves can be parameterized arbitrarily. For example, they\\nmight be computed by a deep artiﬁcial neural network (ANN), where✓ is the vector\\nof all the connection weights of the network (as in the AlphaGo system described in\\nSection 16.6). Or the preferences could simply be linear in features,\\nh(s, a,✓)= ✓>x(s, a), (13.3)\\nusing feature vectors x(s, a) 2 Rd0\\nconstructed by any of the methods described in\\nSection 9.5.\\nOne advantage of parameterizing policies according to the soft-max in action preferences\\nis that the approximate policy can approach a deterministic policy, whereas with\"-greedy\\naction selection over action values there is always an\" probability of selecting a random\\naction. Of course, one could select according to a soft-max distribution based on action\\nvalues, but this alone would not allow the policy to approach a deterministic policy.\\nInstead, the action-value estimates would converge to their corresponding true values,\\nwhich would di↵er by a ﬁnite amount, translating to speciﬁc probabilities other than 0 and\\n1. If the soft-max distribution included a temperature parameter, then the temperature\\ncould be reduced over time to approach determinism, but in practice it would be di\\x00cult\\nto choose the reduction schedule, or even the initial temperature, without more prior\\nknowledge of the true action values than we would like to assume. Action preferences'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 344, 'page_label': '345'}, page_content='13.1. Policy Approximation and its Advantages 323\\nare di↵erent because they do not approach speciﬁc values; instead they are driven to\\nproduce the optimal stochastic policy. If the optimal policy is deterministic, then the\\npreferences of the optimal actions will be driven inﬁnitely higher than all suboptimal\\nactions (if permitted by the parameterization).\\nA second advantage of parameterizing policies according to the soft-max in action\\npreferences is that it enables the selection of actions with arbitrary probabilities. In\\nproblems with signiﬁcant function approximation, the best approximate policy may be\\nstochastic. For example, in card games with imperfect information the optimal play is\\noften to do two di↵erent things with speciﬁc probabilities, such as when blu\\x00ng in Poker.\\nAction-value methods have no natural way of ﬁnding stochastic optimal policies, whereas\\npolicy approximating methods can, as shown in Example 13.1.\\nExample 13.1 Short corridor with switched actions\\nConsider the small corridor gridworld shown inset in the graph below. The reward\\nis \\x001 per step, as usual. In each of the three nonterminal states there are only\\ntwo actions, right and left. These actions have their usual consequences in the\\nﬁrst and third states (left causes no movement in the ﬁrst state), but in the\\nsecond state they are reversed, so thatright moves to the left andleft moves to\\nthe right. The problem is di\\x00cult because all the states appear identical under\\nthe function approximation. In particular, we deﬁne x(s, right)=[ 1 , 0]> and\\nx(s, left)=[ 0, 1]>, for alls. An action-value method with\"-greedy action selection\\nis forced to choose between just two policies: choosingright with high probability\\n1 \\x00 \"/2 on all steps or choosingleft with the same high probability on all time\\nsteps. If \" =0 .1, then these two policies achieve a value (at the start state)\\nof less than\\x0044 and \\x0082, respectively, as shown in the graph. A method can\\ndo signiﬁcantly better if it can learn a speciﬁc probability with which to select\\nright. The best probability is about 0.59, which achieves a value of about\\x0011.6.\\nprobability of right action\\n-11.6\\n0.1 0.2\\n-20\\n-40\\n-60\\n-80\\n-100 0.3 0.40 0.6 0.7 0.8 0.90.5 1\\n\\x00-greedy left \\n\\x00-greedy right \\noptimalstochasticpolicy \\nJ(✓)=v⇡✓(S)\\nGS'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 345, 'page_label': '346'}, page_content='324 Chapter 13: Policy Gradient Methods\\nPerhaps the simplest advantage that policy parameterization may have over action-\\nvalue parameterization is that the policy may be a simpler function to approximate.\\nProblems vary in the complexity of their policies and action-value functions. For some,\\nthe action-value function is simpler and thus easier to approximate. For others, the policy\\nis simpler. In the latter case a policy-based method will typically learn faster and yield a\\nsuperior asymptotic policy (as in Tetris; see S ¸im¸ sek, Alg´ orta, and Kothiyal, 2016).\\nFinally, we note that the choice of policy parameterization is sometimes a good way\\nof injecting prior knowledge about the desired form of the policy into the reinforcement\\nlearning system. This is often the most important reason for using a policy-based learning\\nmethod.\\nExercise 13.1 Use your knowledge of the gridworld and its dynamics to determine an\\nexact symbolic expression for the optimal probability of selecting theright action in\\nExample 13.1. ⇤\\n13.2 The Policy Gradient Theorem\\nIn addition to the practical advantages of policy parameterization over\"-greedy action\\nselection, there is also an important theoretical advantage. With continuous policy\\nparameterization the action probabilities change smoothly as a function of the learned\\nparameter, whereas in\"-greedy selection the action probabilities may change dramatically\\nfor an arbitrarily small change in the estimated action values, if that change results in a\\ndi↵erent action having the maximal value. Largely because of this, stronger convergence\\nguarantees are available for policy-gradient methods than for action-value methods. In\\nparticular, it is the continuity of the policy dependence on the parameters that enables\\npolicy-gradient methods to approximate gradient ascent (13.1).\\nThe episodic and continuing cases deﬁne the performance measure,J(✓), di↵erently\\nand thus have to be treated separately to some extent. Nevertheless, we will try to\\npresent both cases uniformly, and we develop a notation so that the major theoretical\\nresults can be described with a single set of equations.\\nIn this section we treat the episodic case, for which we deﬁne the performance measure\\nas the value of the start state of the episode. We can simplify the notation without\\nlosing any meaningful generality by assuming that every episode starts in some particular\\n(non-random) states0. Then, in the episodic case we deﬁne performance as\\nJ(✓) .= v⇡✓ (s0), (13.4)\\nwhere v⇡✓ is the true value function for⇡✓, the policy determined by✓. From here on in\\nour discussion we will assume no discounting (\\x00 = 1) for the episodic case, although for\\ncompleteness we do include the possibility of discounting in the boxed algorithms.\\nWith function approximation it may seem challenging to change the policy parameter\\nin a way that ensures improvement. The problem is that performance depends on both\\nthe action selections and the distribution of states in which those selections are made,\\nand that both of these are a↵ected by the policy parameter. Given a state, the e↵ect of\\nthe policy parameter on the actions, and thus on reward, can be computed in a relatively\\nstraightforward way from knowledge of the parameterization. But the e↵ect of the policy'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 346, 'page_label': '347'}, page_content='13.2. The Policy Gradient Theorem 325\\nProof of the Policy Gradient Theorem (episodic case)\\nWith just elementary calculus and re-arranging of terms, we can prove the policy\\ngradient theorem from ﬁrst principles. To keep the notation simple, we leave it\\nimplicit in all cases that⇡ is a function of✓, and all gradients are also implicitly\\nwith respect to✓. First note that the gradient of the state-value function can be\\nwritten in terms of the action-value function as\\nrv⇡(s)= r\\n\"X\\na\\n⇡(a|s)q⇡(s, a)\\n#\\n, for alls 2 S (Exercise 3.18)\\n=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)rq⇡(s, a)\\ni\\n(product rule of calculus)\\n=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)r\\nX\\ns0,r\\np(s0,r |s, a)\\n\\x00\\nr + v⇡(s0)\\n\\x00i\\n(Exercise 3.19 and Equation 3.2)\\n=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)\\nX\\ns0\\np(s0|s, a)rv⇡(s0)\\ni\\n(Eq. 3.4)\\n=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)\\nX\\ns0\\np(s0|s, a) (unrolling)\\nX\\na0\\n⇥\\nr⇡(a0|s0)q⇡(s0,a 0)+ ⇡(a0|s0)\\nX\\ns00\\np(s00|s0,a 0)rv⇡(s00)\\n⇤i\\n=\\nX\\nx2S\\n1X\\nk=0\\nPr(s!x, k, ⇡)\\nX\\na\\nr⇡(a|x)q⇡(x, a),\\nafter repeated unrolling, wherePr(s !x, k, ⇡) is the probability of transitioning\\nfrom states to statex in k steps under policy⇡. It is then immediate that\\nrJ(✓)= rv⇡(s0)\\n=\\nX\\ns\\n 1X\\nk=0\\nPr(s0 !s, k, ⇡)\\n!X\\na\\nr⇡(a|s)q⇡(s, a)\\n=\\nX\\ns\\n⌘(s)\\nX\\na\\nr⇡(a|s)q⇡(s, a) (box page 199)\\n=\\nX\\ns0\\n⌘(s0)\\nX\\ns\\n⌘(s)P\\ns0 ⌘(s0)\\nX\\na\\nr⇡(a|s)q⇡(s, a)\\n=\\nX\\ns0\\n⌘(s0)\\nX\\ns\\nµ(s)\\nX\\na\\nr⇡(a|s)q⇡(s, a) (Eq. 9.3)\\n/\\nX\\ns\\nµ(s)\\nX\\na\\nr⇡(a|s)q⇡(s, a)( Q . E . D . )'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 347, 'page_label': '348'}, page_content='326 Chapter 13: Policy Gradient Methods\\non the state distribution is a function of the environment and is typically unknown. How\\ncan we estimate the performance gradient with respect to the policy parameter when the\\ngradient depends on the unknown e↵ect of policy changes on the state distribution?\\nFortunately, there is an excellent theoretical answer to this challenge in the form of\\nthe policy gradient theorem, which provides an analytic expression for the gradient of\\nperformance with respect to the policy parameter (which is what we need to approximate\\nfor gradient ascent(13.1)) that doesnot involve the derivative of the state distribution.\\nThe policy gradient theorem for the episodic case establishes that\\nrJ(✓) /\\nX\\ns\\nµ(s)\\nX\\na\\nq⇡(s, a)r⇡(a|s, ✓), (13.5)\\nwhere the gradients are column vectors of partial derivatives with respect to the compo-\\nnents of✓, and⇡ denotes the policy corresponding to parameter vector✓. The symbol/\\nhere means “proportional to”. In the episodic case, the constant of proportionality is the\\naverage length of an episode, and in the continuing case it is 1, so that the relationship is\\nactually an equality. The distributionµ here (as in Chapters 9 and 10) is the on-policy\\ndistribution under ⇡ (see page 199). The policy gradient theorem is proved for the\\nepisodic case in the box on the previous page.\\n13.3 REINFORCE: Monte Carlo Policy Gradient\\nWe are now ready to derive our ﬁrst policy-gradient learning algorithm. Recall our overall\\nstrategy of stochastic gradient ascent(13.1), which requires a way to obtain samples such\\nthat the expectation of the sample gradient is proportional to the actual gradient of the\\nperformance measure as a function of the parameter. The sample gradients need only be\\nproportional to the gradient because any constant of proportionality can be absorbed\\ninto the step size↵, which is otherwise arbitrary. The policy gradient theorem gives an\\nexact expression proportional to the gradient; all that is needed is some way of sampling\\nwhose expectation equals or approximates this expression. Notice that the right-hand\\nside of the policy gradient theorem is a sum over states weighted by how often the states\\noccur under the target policy⇡;i f⇡ is followed, then states will be encountered in these\\nproportions. Thus\\nrJ(✓) /\\nX\\ns\\nµ(s)\\nX\\na\\nq⇡(s, a)r⇡(a|s, ✓)\\n= E⇡\\n\"X\\na\\nq⇡(St,a )r⇡(a|St, ✓)\\n#\\n. (13.6)\\nWe could stop here and instantiate our stochastic gradient-ascent algorithm (13.1) as\\n✓t+1\\n.= ✓t + ↵\\nX\\na\\nˆq(St,a ,w)r⇡(a|St, ✓), (13.7)\\nwhere ˆq is some learned approximation toq⇡. This algorithm, which has been called\\nan all-actions method because its update involves all of the actions, is promising and'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 348, 'page_label': '349'}, page_content='13.3. REINFORCE: Monte Carlo Policy Gradient 327\\ndeserving of further study, but our current interest is the classical REINFORCE algorithm\\n(Willams, 1992) whose update at timet involves justAt, the one action actually taken at\\ntime t.\\nWe continue our derivation of REINFORCE by introducingAt in the same way as we\\nintroduced St in (13.6)—by replacing a sum over the random variable’s possible values\\nby an expectation under⇡, and then sampling the expectation. Equation(13.6) involves\\nan appropriate sum over actions, but each term is not weighted by⇡(a|St, ✓) as is needed\\nfor an expectation under⇡. So we introduce such a weighting, without changing the\\nequality, by multiplying and then dividing the summed terms by⇡(a|St, ✓). Continuing\\nfrom (13.6), we have\\nrJ(✓) / E⇡\\n\"X\\na\\n⇡(a|St, ✓)q⇡(St,a )r⇡(a|St, ✓)\\n⇡(a|St, ✓)\\n#\\n= E⇡\\n\\uf8ff\\nq⇡(St,A t)r⇡(At|St, ✓)\\n⇡(At|St, ✓)\\n\\x00\\n(replacing a by the sampleAt ⇠ ⇡)\\n= E⇡\\n\\uf8ff\\nGt\\nr⇡(At|St, ✓)\\n⇡(At|St, ✓)\\n\\x00\\n, (because E⇡[Gt|St,A t]= q⇡(St,A t))\\nwhere Gt is the return as usual. The ﬁnal expression in brackets is exactly what is needed,\\na quantity that can be sampled on each time step whose expectation is proportional\\nto the gradient. Using this sample to instantiate our generic stochastic gradient ascent\\nalgorithm (13.1) yields the REINFORCE update:\\n✓t+1\\n.= ✓t + ↵Gt\\nr⇡(At|St, ✓t)\\n⇡(At|St, ✓t) . (13.8)\\nThis update has an intuitive appeal. Each increment is proportional to the product of a\\nreturn Gt and a vector, the gradient of the probability of taking the action actually taken\\ndivided by the probability of taking that action. The vector is the direction in parameter\\nspace that most increases the probability of repeating the actionAt on future visits\\nto state St. The update increases the parameter vector in this direction proportional\\nto the return, and inversely proportional to the action probability. The former makes\\nsense because it causes the parameter to move most in the directions that favor actions\\nthat yield the highest return. The latter makes sense because otherwise actions that are\\nselected frequently are at an advantage (the updates will be more often in their direction)\\nand might win out even if they do not yield the highest return.\\nNote that REINFORCE uses the complete return from timet, which includes all\\nfuture rewards up until the end of the episode. In this sense REINFORCE is a Monte\\nCarlo algorithm and is well deﬁned only for the episodic case with all updates made in\\nretrospect after the episode is completed (like the Monte Carlo algorithms in Chapter 5).\\nThis is shown explicitly in the boxed algorithm on the next page.\\nNotice that the update in the last line of pseudocode appears rather di↵erent from\\nthe REINFORCE update rule(13.8). One di↵erence is that the pseudocode uses the\\ncompact expressionrln ⇡(At|St, ✓t) for the fractional vectorr⇡(At|St,✓t)\\n⇡(At|St,✓t) in (13.8). That\\nthese two expressions for the vector are equivalent follows from the identityrln x = rx\\nx .'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 349, 'page_label': '350'}, page_content='328 Chapter 13: Policy Gradient Methods\\nThis vector has been given several names and notations in the literature; we will refer\\nto it simply as theeligibility vector. Note that it is the only place that the policy\\nparameterization appears in the algorithm.\\nREINFORCE: Monte-Carlo Policy-Gradient Control (episodic) for⇡⇤\\nInput: a di↵erentiable policy parameterization⇡(a|s, ✓)\\nAlgorithm parameter: step size↵> 0\\nInitialize policy parameter✓ 2 Rd0\\n(e.g., to0)\\nLoop forever (for each episode):\\nGenerate an episodeS0,A 0,R 1,...,S T\\x001,A T\\x001,R T , following⇡(·|· , ✓)\\nLoop for each step of the episodet =0 , 1,...,T \\x00 1:\\nG  PT\\nk=t+1 \\x00k\\x00t\\x001Rk (Gt)\\n✓  ✓ + ↵\\x00 t Grln ⇡(At|St, ✓)\\nThe second di↵erence between the pseudocode update and the REINFORCE update\\nequation (13.8) is that the former includes a factor of\\x00t. This is because, as mentioned\\nearlier, in the text we are treating the non-discounted case (\\x00 = 1) while in the boxed\\nalgorithms we are giving the algorithms for the general discounted case. All of the ideas\\ngo through in the discounted case with appropriate adjustments (including to the box on\\npage 199) but involve additional complexity that distracts from the main ideas.\\n⇤Exercise 13.2 Generalize the box on page 199, the policy gradient theorem(13.5),t h e\\nproof of the policy gradient theorem (page 325), and the steps leading to the REINFORCE\\nupdate equation (13.8), so that(13.8) ends up with a factor of\\x00t and thus aligns with\\nthe general algorithm given in the pseudocode. ⇤\\nFigure 13.1 shows the performance of REINFORCE on the short-corridor gridworld\\nfrom Example 13.1.\\n↵=2\\x0013\\n↵=2\\x0012\\nEpisode\\n10008006004002001\\n-80\\n-90\\n-60\\n-40\\n-20\\n-10\\nTotal rewardon episodeaveraged over 100 runs\\nG0\\nv⇤(s0)\\n↵=2\\x0014\\nFigure 13.1: REINFORCE on the short-corridor gridworld (Example 13.1). With a good step\\nsize, the total reward per episode approaches the optimal value of the start state.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 350, 'page_label': '351'}, page_content='13.4. REINFORCE with Baseline 329\\nAs a stochastic gradient method, REINFORCE has good theoretical convergence\\nproperties. By construction, the expected update over an episode is in the same direction\\nas the performance gradient. This assures an improvement in expected performance for\\nsu\\x00ciently small↵, and convergence to a local optimum under standard stochastic approx-\\nimation conditions for decreasing↵. However, as a Monte Carlo method REINFORCE\\nmay be of high variance and thus produce slow learning.\\nExercise 13.3 In Section 13.1 we considered policy parameterizations using the soft-max in\\naction preferences (13.2) with linear action preferences(13.3). For this parameterization,\\nprove that the eligibility vector is\\nrln ⇡(a|s, ✓)= x(s, a) \\x00\\nX\\nb\\n⇡(b|s, ✓)x(s, b), (13.9)\\nusing the deﬁnitions and elementary calculus. ⇤\\n13.4 REINFORCE with Baseline\\nThe policy gradient theorem(13.5) can be generalized to include a comparison of the\\naction value to an arbitrarybaseline b(s):\\nrJ(✓) /\\nX\\ns\\nµ(s)\\nX\\na\\n⇣\\nq⇡(s, a) \\x00 b(s)\\n⌘\\nr⇡(a|s, ✓). (13.10)\\nThe baseline can be any function, even a random variable, as long as it does not vary\\nwith a; the equation remains valid because the subtracted quantity is zero:\\nX\\na\\nb(s)r⇡(a|s, ✓)= b(s)r\\nX\\na\\n⇡(a|s, ✓)= b(s)r1=0 .\\nThe policy gradient theorem with baseline(13.10) can be used to derive an update\\nrule using similar steps as in the previous section. The update rule that we end up with\\nis a new version of REINFORCE that includes a general baseline:\\n✓t+1\\n.= ✓t + ↵\\n⇣\\nGt \\x00 b(St)\\n⌘r⇡(At|St, ✓t)\\n⇡(At|St, ✓t) . (13.11)\\nBecause the baseline could be uniformly zero, this update is a strict generalization of\\nREINFORCE. In general, the baseline leaves the expected value of the update unchanged,\\nbut it can have a large e↵ect on its variance. For example, we saw in Section 2.8 that an\\nanalogous baseline can signiﬁcantly reduce the variance (and thus speed the learning) of\\ngradient bandit algorithms. In the bandit algorithms the baseline was just a number (the\\naverage of the rewards seen so far), but for MDPs the baseline should vary with state.\\nIn some states all actions have high values and we need a high baseline to di↵erentiate\\nthe higher valued actions from the less highly valued ones; in other states all actions will\\nhave low values and a low baseline is appropriate.\\nOne natural choice for the baseline is an estimate of the state value,ˆv(St,w), where\\nw 2 Rd is a weight vector learned by one of the methods presented in previous chapters.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 351, 'page_label': '352'}, page_content='330 Chapter 13: Policy Gradient Methods\\nBecause REINFORCE is a Monte Carlo method for learning the policy parameter,✓,\\nit seems natural to also use a Monte Carlo method to learn the state-value weights,w.\\nA complete pseudocode algorithm for REINFORCE with baseline using such a learned\\nstate-value function as the baseline is given in the box below.\\nREINFORCE with Baseline (episodic), for estimating⇡✓ ⇡ ⇡⇤\\nInput: a di↵erentiable policy parameterization⇡(a|s, ✓)\\nInput: a di↵erentiable state-value function parameterization ˆv(s,w)\\nAlgorithm parameters: step sizes↵✓ > 0, ↵w > 0\\nInitialize policy parameter✓ 2 Rd0\\nand state-value weightsw 2 Rd (e.g., to0)\\nLoop forever (for each episode):\\nGenerate an episodeS0,A 0,R 1,...,S T\\x001,A T\\x001,R T , following⇡(·|· , ✓)\\nLoop for each step of the episodet =0 , 1,...,T \\x00 1:\\nG  PT\\nk=t+1 \\x00k\\x00t\\x001Rk (Gt)\\n\\x00  G \\x00 ˆv(St,w)\\nw  w + ↵w \\x00rˆv(St,w)\\n✓  ✓ + ↵✓ \\x00t \\x00rln ⇡(At|St, ✓)\\nThis algorithm has two step sizes, denoted↵✓ and ↵w (where ↵✓ is the↵ in (13.11)).\\nChoosing the step size for values (here↵w) is relatively easy; in the linear case we have\\nrules of thumb for setting it, such as↵w =0 .1/E\\n⇥\\nkrˆv(St,w)k2\\nµ\\n⇤\\n(see Section 9.6). It is\\nmuch less clear how to set the step size for the policy parameters,↵✓, whose best value\\ndepends on the range of variation of the rewards and on the policy parameterization.\\n↵=2\\x0013\\nEpisode\\n10008006004002001\\n-80\\n-90\\n-60\\n-40\\n-20\\n-10 v⇤(s0)\\nREINFORCE\\nREINFORCE with baseline\\n↵=2\\x009\\n↵✓=2\\x009, ↵w=2\\x006\\nTotal rewardon episodeaveraged over 100 runs\\nG0\\nFigure 13.2: Adding a baseline to REINFORCE can make it learn much faster, as illus-\\ntrated here on the short-corridor gridworld (Example 13.1). The step size used here for plain\\nREINFORCE is that at which it performs best (to the nearest power of two; see Figure 13.1).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 352, 'page_label': '353'}, page_content='13.5. Actor–Critic Methods 331\\nFigure 13.2 compares the behavior of REINFORCE with and without a baseline on\\nthe short-corridor gridword (Example 13.1). Here the approximate state-value function\\nused in the baseline is ˆv(s,w)= w. That is,w is a single component,w.\\n13.5 Actor–Critic Methods\\nIn REINFORCE with baseline, the learned state-value function estimates the value of\\nthe ﬁrst state of each state transition. This estimate sets a baseline for the subsequent\\nreturn, but is made prior to the transition’s action and thus cannot be used to assess that\\naction. In actor–critic methods, on the other hand, the state-value function is applied\\nalso to thesecond state of the transition. The estimated value of the second state, when\\ndiscounted and added to the reward, constitutes the one-step return,Gt:t+1,w h i c hi sa\\nuseful estimate of the actual return and thusis a way of assessing the action. As we have\\nseen in the TD learning of value functions throughout this book, the one-step return is\\noften superior to the actual return in terms of its variance and computational congeniality,\\neven though it introduces bias. We also know how we can ﬂexibly modulate the extent\\nof the bias withn-step returns and eligibility traces (Chapters 7 and 12). When the\\nstate-value function is used to assess actions in this way it is called acritic, and the\\noverall policy-gradient method is termed anactor–critic method. Note that the bias in\\nthe gradient estimate is not due to bootstrapping as such; the actor would be biased even\\nif the critic was learned by a Monte Carlo method.\\nFirst consider one-step actor–critic methods, the analog of the TD methods introduced\\nin Chapter 6 such as TD(0), Sarsa(0), and Q-learning. The main appeal of one-step\\nmethods is that they are fully online and incremental, yet avoid the complexities of\\neligibility traces. They are a special case of the eligibility trace methods, but easier\\nto understand. One-step actor–critic methods replace the full return of REINFORCE\\n(13.11) with the one-step return (and use a learned state-value function as the baseline)\\nas follows:\\n✓t+1\\n.= ✓t + ↵\\n⇣\\nGt:t+1 \\x00 ˆv(St,w)\\n⌘r⇡(At|St, ✓t)\\n⇡(At|St, ✓t) (13.12)\\n= ✓t + ↵\\n⇣\\nRt+1 + \\x00ˆv(St+1,w) \\x00 ˆv(St,w)\\n⌘r⇡(At|St, ✓t)\\n⇡(At|St, ✓t) (13.13)\\n= ✓t + ↵\\x00t\\nr⇡(At|St, ✓t)\\n⇡(At|St, ✓t) . (13.14)\\nThe natural state-value-function learning method to pair with this is semi-gradient TD(0).\\nPseudocode for the complete algorithm is given in the box at the top of the next page.\\nNote that it is now a fully online, incremental algorithm, with states, actions, and rewards\\nprocessed as they occur and then never revisited.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 353, 'page_label': '354'}, page_content='332 Chapter 13: Policy Gradient Methods\\nOne-step Actor–Critic (episodic), for estimating⇡✓ ⇡ ⇡⇤\\nInput: a di↵erentiable policy parameterization⇡(a|s, ✓)\\nInput: a di↵erentiable state-value function parameterization ˆv(s,w)\\nParameters: step sizes↵✓ > 0, ↵w > 0\\nInitialize policy parameter✓ 2 Rd0\\nand state-value weightsw 2 Rd (e.g., to0)\\nLoop forever (for each episode):\\nInitialize S (ﬁrst state of episode)\\nI  1\\nLoop while S is not terminal (for each time step):\\nA ⇠ ⇡(·| S, ✓)\\nTake actionA, observeS0,R\\n\\x00  R + \\x00 ˆv(S0,w) \\x00 ˆv(S,w)( i f S0 is terminal, then ˆv(S0,w) .= 0)\\nw  w + ↵w \\x00rˆv(S,w)\\n✓  ✓ + ↵✓ I\\x00 rln ⇡(A|S, ✓)\\nI  \\x00I\\nS  S0\\nThe generalizations to the forward view ofn-step methods and then to a\\x00-return\\nalgorithm are straightforward. The one-step return in(13.12) is merely replaced byGt:t+n\\nor G\\x00\\nt respectively. The backward view of the\\x00-return algorithm is also straightforward,\\nusing separate eligibility traces for the actor and critic, each after the patterns in\\nChapter 12. Pseudocode for the complete algorithm is given in the box below.\\nActor–Critic with Eligibility Traces (episodic), for estimating⇡✓ ⇡ ⇡⇤\\nInput: a di↵erentiable policy parameterization⇡(a|s, ✓)\\nInput: a di↵erentiable state-value function parameterization ˆv(s,w)\\nParameters: trace-decay rates\\x00✓ 2 [0, 1], \\x00w 2 [0, 1]; step sizes↵✓ > 0, ↵w > 0\\nInitialize policy parameter✓ 2 Rd0\\nand state-value weightsw 2 Rd (e.g., to0)\\nLoop forever (for each episode):\\nInitialize S (ﬁrst state of episode)\\nz✓  0 (d0-component eligibility trace vector)\\nzw  0 (d-component eligibility trace vector)\\nI  1\\nLoop while S is not terminal (for each time step):\\nA ⇠ ⇡(·| S, ✓)\\nTake actionA, observeS0,R\\n\\x00  R + \\x00 ˆv(S0,w) \\x00 ˆv(S,w)( i f S0 is terminal, then ˆv(S0,w) .= 0)\\nzw  \\x00\\x00wzw + rˆv(S,w)\\nz✓  \\x00\\x00✓z✓ + I rln ⇡(A|S, ✓)\\nw  w + ↵w \\x00zw\\n✓  ✓ + ↵✓ \\x00z✓\\nI  \\x00I\\nS  S0'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 354, 'page_label': '355'}, page_content='13.6. Policy Gradient for Continuing Problems 333\\n13.6 Policy Gradient for Continuing Problems\\nAs discussed in Section 10.3, for continuing problems without episode boundaries we need\\nto deﬁne performance in terms of the average rate of reward per time step:\\nJ(✓) .= r(⇡) .=l i m\\nh!1\\n1\\nh\\nhX\\nt=1\\nE[Rt | S0,A 0:t\\x001 ⇠⇡] (13.15)\\n=l i m\\nt!1\\nE[Rt | S0,A 0:t\\x001 ⇠⇡]\\n=\\nX\\ns\\nµ(s)\\nX\\na\\n⇡(a|s)\\nX\\ns0,r\\np(s0,r |s, a)r,\\nwhere µ is the steady-state distribution under⇡, µ(s) .=l i mt!1 Pr{St =s|A0:t ⇠⇡},\\nwhich is assumed to exist and to be independent ofS0 (an ergodicity assumption).\\nRemember that this is the special distribution under which, if you select actions according\\nto ⇡, you remain in the same distribution:\\nX\\ns\\nµ(s)\\nX\\na\\n⇡(a|s, ✓)p(s0|s, a)= µ(s0), for alls0 2 S. (13.16)\\nComplete pseudocode for the actor–critic algorithm in the continuing case (backward\\nview) is given in the box below.\\nActor–Critic with Eligibility Traces (continuing), for estimating⇡✓ ⇡ ⇡⇤\\nInput: a di↵erentiable policy parameterization⇡(a|s, ✓)\\nInput: a di↵erentiable state-value function parameterization ˆv(s,w)\\nAlgorithm parameters: \\x00w 2 [0, 1], \\x00✓ 2 [0, 1], ↵w > 0, ↵✓ > 0, ↵ ¯R > 0\\nInitialize ¯R 2 R (e.g., to 0)\\nInitialize state-value weightsw 2 Rd and policy parameter✓ 2 Rd0\\n(e.g., to0)\\nInitialize S 2 S (e.g., tos0)\\nzw  0 (d-component eligibility trace vector)\\nz✓  0 (d0-component eligibility trace vector)\\nLoop forever (for each time step):\\nA ⇠ ⇡(·| S, ✓)\\nTake actionA, observeS0,R\\n\\x00  R \\x00 ¯R +ˆv(S0,w) \\x00 ˆv(S,w)\\n¯R  ¯R + ↵ ¯R \\x00\\nzw  \\x00wzw + rˆv(S,w)\\nz✓  \\x00✓z✓ + rln ⇡(A|S, ✓)\\nw  w + ↵w \\x00zw\\n✓  ✓ + ↵✓ \\x00z✓\\nS  S0'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 355, 'page_label': '356'}, page_content='334 Chapter 13: Policy Gradient Methods\\nNaturally, in the continuing case, we deﬁne values,v⇡(s) .= E⇡[Gt|St =s] and q⇡(s, a) .=\\nE⇡[Gt|St =s, At =a], with respect to the di↵erential return:\\nGt\\n.= Rt+1 \\x00 r(⇡)+ Rt+2 \\x00 r(⇡)+ Rt+3 \\x00 r(⇡)+ ··· . (13.17)\\nWith these alternate deﬁnitions, the policy gradient theorem as given for the episodic\\ncase (13.5) remains true for the continuing case. A proof is given in the box on the next\\npage. The forward and backward view equations also remain the same.\\nProof of the Policy Gradient Theorem (continuing case)\\nThe proof of the policy gradient theorem for the continuing case begins similarly\\nto the episodic case. Again we leave it implicit in all cases that⇡ is a function\\nof ✓ and that the gradients are with respect to✓. Recall that in the continuing\\ncase J(✓)= r(⇡) (13.15) and that v⇡ and q⇡ denote values with respect to the\\ndi↵erential return(13.17). The gradient of the state-value function can be written,\\nfor anys 2 S, as\\nrv⇡(s)= r\\n\"X\\na\\n⇡(a|s)q⇡(s, a)\\n#\\n, for alls 2 S (Exercise 3.18)\\n=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)rq⇡(s, a)\\ni\\n(product rule of calculus)\\n=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)r\\nX\\ns0,r\\np(s0,r |s, a)\\n\\x00\\nr \\x00 r(✓)+ v⇡(s0)\\n\\x00i\\n=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)\\n⇥\\n\\x00rr(✓)+\\nX\\ns0\\np(s0|s, a)rv⇡(s0)\\n⇤i\\n.\\nAfter re-arranging terms, we obtain\\nrr(✓)=\\nX\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)\\nX\\ns0\\np(s0|s, a)rv⇡(s0)\\ni\\n\\x00r v⇡(s).\\nNotice that the left-hand side can be writtenrJ(✓), and that it does not depend\\non s. Thus the right-hand side does not depend ons either, and we can safely sum\\nit over alls 2 S, weighted byµ(s), without changing it (becauseP\\ns µ(s) = 1):\\nrJ(✓)=\\nX\\ns\\nµ(s)\\n X\\na\\nh\\nr⇡(a|s)q⇡(s, a)+ ⇡(a|s)\\nX\\ns0\\np(s0|s, a)rv⇡(s0)\\ni\\n\\x00r v⇡(s)\\n!\\n=\\nX\\ns\\nµ(s)\\nX\\na\\nr⇡(a|s)q⇡(s, a)\\n+\\nX\\ns\\nµ(s)\\nX\\na\\n⇡(a|s)\\nX\\ns0\\np(s0|s, a)rv⇡(s0) \\x00\\nX\\ns\\nµ(s)rv⇡(s)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 356, 'page_label': '357'}, page_content='13.7. Policy Parameterization for Continuous Actions 335\\n=\\nX\\ns\\nµ(s)\\nX\\na\\nr⇡(a|s)q⇡(s, a)\\n+\\nX\\ns0\\nX\\ns\\nµ(s)\\nX\\na\\n⇡(a|s)p(s0|s, a)\\n| {z }\\nµ(s0)( 1 3 . 1 6 )\\nrv⇡(s0) \\x00\\nX\\ns\\nµ(s)rv⇡(s)\\n=\\nX\\ns\\nµ(s)\\nX\\na\\nr⇡(a|s)q⇡(s, a)+\\nX\\ns0\\nµ(s0)rv⇡(s0) \\x00\\nX\\ns\\nµ(s)rv⇡(s)\\n=\\nX\\ns\\nµ(s)\\nX\\na\\nr⇡(a|s)q⇡(s, a). Q.E.D.\\n13.7 Policy Parameterization for Continuous Actions\\nPolicy-based methods o↵er practical ways of dealing with large action spaces, even\\ncontinuous spaces with an inﬁnite number of actions. Instead of computing learned\\nprobabilities for each of the many actions, we instead learn statistics of the probability\\ndistribution. For example, the action set might be the real numbers, with actions chosen\\nfrom a normal (Gaussian) distribution.\\nThe probability density function for the normal distribution is conventionally written\\np(x) .= 1\\n\\x00\\np\\n2⇡ exp\\n✓\\n\\x00(x \\x00 µ)2\\n2\\x002\\n◆\\n, (13.18)\\np(x).= 1\\x00p2⇡exp✓\\x00(x\\x00µ)2\\n2\\x002\\n◆\\nφµ,σ2(\\n0.8\\n0.6\\n0.4\\n0.2\\n0.0\\n−5 −3 1 3 5x\\n1.0\\n−1 0 2 4−2−4\\nx)\\n0,µ=0,µ=0,µ=−2,µ=\\n2 0.2,σ=2 1.0,σ=2 5.0,σ=2 0.5,σ=\\np(x).= 1\\x00p2⇡exp✓\\x00(x\\x00µ)2\\n2\\x002\\n◆\\nwhere µ and \\x00 here are the mean and stan-\\ndard deviation of the normal distribution,\\nand of course ⇡ here is just the number\\n⇡ ⇡ 3.14159. The probability density func-\\ntions for several di↵erent means and stan-\\ndard deviations are shown to the right. The\\nvalue p(x)i st h edensity of the probability\\nat x, not the probability. It can be greater\\nthan 1; it is the total area underp(x) that\\nmust sum to 1. In general, one can take\\nthe integral underp(x) for any range ofx\\nvalues to get the probability ofx falling\\nwithin that range.\\nTo produce a policy parameterization, the policy can be deﬁned as the normal proba-\\nbility density over a real-valued scalar action, with mean and standard deviation given\\nby parametric function approximators that depend on the state. That is,\\n⇡(a|s, ✓) .= 1\\n\\x00(s, ✓)\\np\\n2⇡ exp\\n✓\\n\\x00(a \\x00 µ(s, ✓))2\\n2\\x00(s, ✓)2\\n◆\\n, (13.19)\\nwhere µ : S⇥Rd0\\n! R and \\x00 : S⇥Rd0\\n! R+ are two parameterized function approximators.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 357, 'page_label': '358'}, page_content='336 Chapter 13: Policy Gradient Methods\\nTo complete the example we need only give a form for these approximators. For this we\\ndivide the policy’s parameter vector into two parts,✓ =[ ✓µ, ✓\\x00]>, one part to be used\\nfor the approximation of the mean and one part for the approximation of the standard\\ndeviation. The mean can be approximated as a linear function. The standard deviation\\nmust always be positive and is better approximated as the exponential of a linear function.\\nThus\\nµ(s, ✓) .= ✓µ\\n>xµ(s) and \\x00(s, ✓) .=e x p\\n⇣\\n✓\\x00\\n>x\\x00(s)\\n⌘\\n, (13.20)\\nwhere xµ(s) andx\\x00(s) are state feature vectors perhaps constructed by one of the methods\\ndescribed in Section 9.5. With these deﬁnitions, all the algorithms described in the rest\\nof this chapter can be applied to learn to select real-valued actions.\\nExercise 13.4 Show that for the Gaussian policy parameterization (Equations 13.19 and\\n13.20) the eligibility vector has the following two parts:\\nrln ⇡(a|s, ✓µ)= r⇡(a|s, ✓µ)\\n⇡(a|s, ✓) = 1\\n\\x00(s, ✓)2\\n\\x00\\na \\x00 µ(s, ✓)\\n\\x00\\nxµ(s), and\\nrln ⇡(a|s, ✓\\x00)= r⇡(a|s, ✓\\x00)\\n⇡(a|s, ✓) =\\n \\x00\\na \\x00 µ(s, ✓)\\n\\x002\\n\\x00(s, ✓)2 \\x00 1\\n!\\nx\\x00(s). ⇤\\nExercise 13.5 A Bernoulli-logistic unit is a stochastic neuron-like unit used in some ANNs\\n(Section 9.7). Its input at timet is a feature vectorx(St); its output,At, is a random\\nvariable having two values, 0 and 1, withPr{At =1 } = Pt and Pr{At =0 } =1 \\x00Pt (the\\nBernoulli distribution). Leth(s, 0, ✓) andh(s, 1, ✓) be the preferences in states for the\\nunit’s two actions given policy parameter✓. Assume that the di↵erence between the\\naction preferences is given by a weighted sum of the unit’s input vector, that is, assume\\nthat h(s, 1, ✓) \\x00 h(s, 0, ✓)= ✓>x(s), where✓ is the unit’s weight vector.\\n(a) Show that if the exponential soft-max distribution (13.2) is used to convert action\\npreferences to policies, thenPt = ⇡(1|St, ✓t)=1 /(1+e x p (\\x00✓>\\nt x(St))) (the logistic\\nfunction).\\n(b) What is the Monte-Carlo REINFORCE update of✓t to ✓t+1 upon receipt of return\\nGt?\\n(c) Express the eligibilityrln ⇡(a|s, ✓) for a Bernoulli-logistic unit, in terms ofa, x(s),\\nand ⇡(a|s, ✓) by calculating the gradient.\\nHint for part (c): DeﬁneP = ⇡(1|s, ✓) and compute the derivative of the logarithm, for\\neach action, using the chain rule onP. Combine the two results into one expression that\\ndepends on a and P, and then use the chain rule again, this time on✓>x(s), noting that\\nthe derivative of the logistic functionf(x)=1 /(1 +e\\x00x)i sf(x)(1 \\x00 f(x)). ⇤'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 358, 'page_label': '359'}, page_content='13.8. Summary 337\\n13.8 Summary\\nPrior to this chapter, this book focused onaction-value methods—meaning methods that\\nlearn action values and then use them to determine action selections. In this chapter, on\\nthe other hand, we considered methods that learn a parameterized policy that enables\\nactions to be taken without consulting action-value estimates. In particular, we have\\nconsidered policy-gradient methods—meaning methods that update the policy parameter\\non each step in the direction of an estimate of the gradient of performance with respect\\nto the policy parameter.\\nMethods that learn and store a policy parameter have many advantages. They can\\nlearn speciﬁc probabilities for taking the actions. They can learn appropriate levels\\nof exploration and approach deterministic policies asymptotically. They can naturally\\nhandle continuous action spaces. All these things are easy for policy-based methods but\\nawkward or impossible for\"-greedy methods and for action-value methods in general. In\\naddition, on some problems the policy is just simpler to represent parametrically than\\nthe value function; these problems are more suited to parameterized policy methods.\\nParameterized policy methods also have an important theoretical advantage over\\naction-value methods in the form of thepolicy gradient theorem, which gives an exact\\nformula for how performance is a↵ected by the policy parameter that does not involve\\nderivatives of the state distribution. This theorem provides a theoretical foundation for\\nall policy gradient methods.\\nThe REINFORCE method follows directly from the policy gradient theorem. Adding\\na state-value function as abaseline reduces REINFORCE’s variance without introducing\\nbias. If the state-value function is also used to assess—or criticize—the policy’s action\\nselections, then the value function is called acritic and the policy is called anactor;\\nthe overall method is called anactor–critic method. The critic introduces bias into the\\nactor’s gradient estimates, but is often desirable for the same reason that bootstrapping\\nTD methods are often superior to Monte Carlo methods (substantially reduced variance).\\nOverall, policy-gradient methods provide a signiﬁcantly di↵erent set of strengths and\\nweaknesses than action-value methods. Today they are less well understood in some\\nrespects, but a subject of excitement and ongoing research.\\nBibliographical and Historical Remarks\\nMethods that we now see as related to policy gradients were actually some of the earliest\\nto be studied in reinforcement learning (Witten, 1977; Barto, Sutton, and Anderson,\\n1983; Sutton, 1984; Williams, 1987, 1992) and in predecessor ﬁelds (see Phansalkar\\nand Thathachar, 1995). They were largely supplanted in the 1990s by the action-value\\nmethods that are the focus of the other chapters of this book. In recent years, however,\\nattention has returned to actor–critic methods and to policy-gradient methods in general.\\nAmong the further developments beyond what we cover here are natural-gradient methods\\n(Amari, 1998; Kakade, 2002, Peters, Vijayakumar and Schaal, 2005; Peters and Schaal,\\n2008; Park, Kim and Kang, 2005; Bhatnagar, Sutton, Ghavamzadeh and Lee, 2009; see\\nGrondman, Busoniu, Lopes and Babuska, 2012), deterministic policy-gradient methods'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 359, 'page_label': '360'}, page_content='338 Chapter 13: Policy Gradient Methods\\n(Silver et al., 2014), o↵-policy policy-gradient methods (Degris, White, and Sutton, 2012;\\nMaei, 2018), and entropy regularization (see Schulman, Chen, and Abbeel, 2017). Major\\napplications include acrobatic helicopter autopilots and AlphaGo (Section 16.6).\\nOur presentation in this chapter is based primarily on that by Sutton, McAllester,\\nSingh, and Mansour (2000), who introduced the term “policy gradient methods.” A\\nuseful overview is provided by Bhatnagar et al. (2009). One of the earliest related works\\nis by Aleksandrov, Sysoyev, and Shemeneva (1968). Thomas (2014) ﬁrst realized that\\nthe factor of\\x00t, as speciﬁed in the boxed algorithms of this chapter, was needed in the\\ncase of discounted episodic problems.\\n13.1 Example 13.1 and the results with it in this chapter were developed with Eric\\nGraves.\\n13.2 The policy gradient theorem here and on page 334 was ﬁrst obtained by Marbach\\nand Tsitsiklis (1998, 2001) and then independently by Sutton et al. (2000). A\\nsimilar expression was obtained by Cao and Chen (1997). Other early results\\nare due to Konda and Tsitsiklis (2000, 2003), Baxter and Bartlett (2001), and\\nBaxter, Bartlett, and Weaver (2001). Some additional results are developed by\\nSutton, Singh, and McAllester (2000).\\n13.3 REINFORCE is due to Williams (1987, 1992). Phansalkar and Thathachar\\n(1995) proved both local and global convergence theorems for modiﬁed versions\\nof REINFORCE algorithms.\\nThe all-actions algorithm was ﬁrst presented in an unpublished but widely\\ncirculated incomplete paper (Sutton, Singh, and McAllester, 2000) and then\\ndeveloped further by Ciosek and Whiteson (2017, 2018), who termed it “expected\\npolicy gradients,” and by Asadi, Allen, Roderick, Mohamed, Konidaris, and\\nLittman (2017), who called it “mean actor critic.”\\n13.4 The baseline was introduced in Williams’s (1987, 1992) original work. Greensmith,\\nBartlett, and Baxter (2004) analyzed an arguably better baseline (see Dick, 2015).\\nThomas and Brunskill (2017) argue that an action-dependent baseline can be\\nused without incurring bias.\\n13.5–6 Actor–critic methods were among the earliest to be investigated in reinforcement\\nlearning (Witten, 1977; Barto, Sutton, and Anderson, 1983; Sutton, 1984). The\\nalgorithms presented here are based on the work of Degris, White, and Sutton\\n(2012). Actor–critic methods are sometimes referred to as advantage actor–critic\\n(“A2C”) methods in the literature.\\n13.7 The ﬁrst to show how continuous actions could be handled this way appears\\nto have been Williams (1987, 1992). The ﬁgure on page 335 is adapted from\\nWikipedia.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 360, 'page_label': '361'}, page_content='Part III: Looking Deeper\\nIn this last part of the book we look beyond the standard reinforcement learning ideas\\npresented in the ﬁrst two parts of the book to brieﬂy survey their relationships with\\npsychology and neuroscience, a sampling of reinforcement learning applications, and some\\nof the active frontiers for future reinforcement learning research.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 361, 'page_label': '362'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 362, 'page_label': '363'}, page_content='Chapter 14\\nPsychology\\nIn previous chapters we developed ideas for algorithms based on computational con-\\nsiderations alone. In this chapter we look at some of these algorithms from another\\nperspective: the perspective of psychology and its study of how animals learn. The goals\\nof this chapter are, ﬁrst, to discuss ways that reinforcement learning ideas and algorithms\\ncorrespond to what psychologists have discovered about animal learning, and second, to\\nexplain the inﬂuence reinforcement learning is having on the study of animal learning.\\nThe clear formalism provided by reinforcement learning that systemizes tasks, returns,\\nand algorithms is proving to be enormously useful in making sense of experimental data,\\nin suggesting new kinds of experiments, and in pointing to factors that may be critical to\\nmanipulate and to measure. The idea of optimizing return over the long term that is\\nat the core of reinforcement learning is contributing to our understanding of otherwise\\npuzzling features of animal learning and behavior.\\nSome of the correspondences between reinforcement learning and psychological theories\\nare not surprising because the development of reinforcement learning drew inspiration\\nfrom psychological learning theories. However, as developed in this book, reinforcement\\nlearning explores idealized situations from the perspective of an artiﬁcial intelligence\\nresearcher or engineer, with the goal of solving computational problems with e\\x00cient\\nalgorithms, rather than to replicate or explain in detail how animals learn. As a result,\\nsome of the correspondences we describe connect ideas that arose independently in their\\nrespective ﬁelds. We believe these points of contact are specially meaningful because they\\nexpose computational principles important to learning, whether it is learning by artiﬁcial\\nor by natural systems.\\nFor the most part, we describe correspondences between reinforcement learning and\\nlearning theories developed to explain how animals like rats, pigeons, and rabbits learn\\nin controlled laboratory experiments. Thousands of these experiments were conducted\\nthroughout the 20th century, and many are still being conducted today. Although\\nsometimes dismissed as irrelevant to wider issues in psychology, these experiments probe\\nsubtle properties of animal learning, often motivated by precise theoretical questions.\\nAs psychology shifted its focus to more cognitive aspects of behavior, that is, to mental'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 363, 'page_label': '364'}, page_content='342 Chapter 14: Psychology\\nprocesses such as thought and reasoning, animal learning experiments came to play\\nless of a role in psychology than they once did. But this experimentation led to the\\ndiscovery of learning principles that are elemental and widespread throughout the animal\\nkingdom, principles that should not be neglected in designing artiﬁcial learning systems.\\nIn addition, as we shall see, some aspects of cognitive processing connect naturally to the\\ncomputational perspective provided by reinforcement learning.\\nThis chapter’s ﬁnal section includes references relevant to the connections we discuss as\\nwell as to connections we neglect. We hope this chapter encourages readers to probe all\\nof these connections more deeply. Also included in this ﬁnal section is a discussion of how\\nthe terminology used in reinforcement learning relates to that of psychology. Many of\\nthe terms and phrases used in reinforcement learning are borrowed from animal learning\\ntheories, but the computational/engineering meanings of these terms and phrases do not\\nalways coincide with their meanings in psychology.\\n14.1 Prediction and Control\\nThe algorithms we describe in this book fall into two broad categories: algorithms\\nfor prediction and algorithms forcontrol.1 These categories arise naturally in solution\\nmethods for the reinforcement learning problem presented in Chapter 3. In many ways\\nthese categories respectively correspond to categories of learning extensively studied\\nby psychologists: classical, or Pavlovian, conditioning and instrumental, or operant,\\nconditioning. These correspondences are not completely accidental because of psychology’s\\ninﬂuence on reinforcement learning, but they are nevertheless striking because they connect\\nideas arising from di↵erent objectives.\\nThe prediction algorithms presented in this book estimate quantities that depend\\non how features of an agent’s environment are expected to unfold over the future. We\\nspeciﬁcally focus on estimating the amount of reward an agent can expect to receive over\\nthe future while it interacts with its environment. In this role, prediction algorithms are\\npolicy evaluation algorithms, which are integral components of algorithms for improving\\npolicies. But prediction algorithms are not limited to predicting future reward; they can\\npredict any feature of the environment (see, for example, Modayil, White, and Sutton,\\n2014). The correspondence between prediction algorithms and classical conditioning rests\\non their common property of predicting upcoming stimuli, whether or not those stimuli\\nare rewarding (or punishing).\\nThe situation in an instrumental, or operant, conditioning experiment is di↵erent.\\nHere, the experimental apparatus is set up so that an animal is given something it likes\\n(a reward) or something it dislikes (a penalty) depending on what the animal did. The\\nanimal learns to increase its tendency to produce rewarded behavior and to decrease its\\ntendency to produce penalized behavior. The reinforcing stimulus is said to becontingent\\non the animal’s behavior, whereas in classical conditioning it is not (although it is di\\x00cult\\nto remove all behavior contingencies in a classical conditioning experiment). Instrumental\\n1What control means for us is di↵erent from what it typically means in animal learning theories; there\\nthe environment controls the agent instead of the other way around. See our comments on terminology\\nat the end of this chapter.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 364, 'page_label': '365'}, page_content='14.2. Classical Conditioning 343\\nconditioning experiments are like those that inspired Thorndike’s Law of E↵ect that\\nwe brieﬂy discuss in Chapter 1.Control is at the core of this form of learning, which\\ncorresponds to the operation of reinforcement learning’s policy-improvement algorithms.\\nThinking of classical conditioning in terms of prediction, and instrumental conditioning\\nin terms of control, is a starting point for connecting our computational view of rein-\\nforcement learning to animal learning, but in reality, the situation is more complicated\\nthan this. There is more to classical conditioning than prediction; it also involves action,\\nand so is a mode of control, sometimes calledPavlovian control. Further, classical and\\ninstrumental conditioning interact in interesting ways, with both sorts of learning likely\\nbeing engaged in most experimental situations. Despite these complications, aligning the\\nclassical/instrumental distinction with the prediction/control distinction is a convenient\\nﬁrst approximation in connecting reinforcement learning to animal learning.\\nIn psychology, the term reinforcement is used to describe learning in both classical and\\ninstrumental conditioning. Originally referring only to the strengthening of a pattern of\\nbehavior, it is frequently also used for the weakening of a pattern of behavior. A stimulus\\nconsidered to be the cause of the change in behavior is called a reinforcer, whether or\\nnot it is contingent on the animal’s previous behavior. At the end of this chapter we\\ndiscuss this terminology in more detail and how it relates to terminology used in machine\\nlearning.\\n14.2 Classical Conditioning\\nWhile studying the activity of the digestive system, the celebrated Russian physiologist\\nIvan Pavlov found that an animal’s innate responses to certain triggering stimuli can\\ncome to be triggered by other stimuli that are quite unrelated to the inborn triggers. His\\nexperimental subjects were dogs that had undergone minor surgery to allow the intensity\\nof their salivary reﬂex to be accurately measured. In one case he describes, the dog did\\nnot salivate under most circumstances, but about 5 seconds after being presented with\\nfood it produced about six drops of saliva over the next several seconds. After several\\nrepetitions of presenting another stimulus, one not related to food, in this case the sound\\nof a metronome, shortly before the introduction of food, the dog salivated in response to\\nthe sound of the metronome in the same way it did to the food. “The activity of the\\nsalivary gland has thus been called into play by impulses of sound—a stimulus quite\\nalien to food” (Pavlov, 1927, p. 22). Summarizing the signiﬁcance of this ﬁnding, Pavlov\\nwrote:\\nIt is pretty evident that under natural conditions the normal animal must\\nrespond not only to stimuli which themselves bring immediate beneﬁt or\\nharm, but also to other physical or chemical agencies—waves of sound, light,\\nand the like—which in themselves onlysignal the approach of these stimuli;\\nthough it is not the sight and sound of the beast of prey which is in itself\\nharmful to the smaller animal, but its teeth and claws. (Pavlov, 1927, p. 14)\\nConnecting new stimuli to innate reﬂexes in this way is now called classical, or Pavlovian,\\nconditioning. Pavlov (or more exactly, his translators) called inborn responses (e.g.,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 365, 'page_label': '366'}, page_content='344 Chapter 14: Psychology\\nsalivation in his demonstration described above) “unconditioned responses” (URs), their\\nnatural triggering stimuli (e.g., food) “unconditioned stimuli” (USs), and new responses\\ntriggered by predictive stimuli (e.g., here also salivation) “conditioned responses” (CRs).\\nA stimulus that is initially neutral, meaning that it does not normally elicit strong\\nresponses (e.g., the metronome sound), becomes a “conditioned stimulus” (CS) as the\\nanimal learns that it predicts the US and so comes to produce a CR in response to the CS.\\nThese terms are still used in describing classical conditioning experiments (though better\\ntranslations would have been “conditional” and “unconditional” instead of conditioned\\nand unconditioned). The US is called a reinforcer because it reinforces producing a CR\\nin response to the CS.\\nt\\nTrace Conditioning\\nDelay ConditioningCS\\nUS\\nCS\\nUS\\nISI\\nThe arrangement of stimuli in two\\ncommon types of classical condition-\\ning experiments is shown to the right.\\nIn delay conditioning,t h eC Se x t e n d s\\nthroughout the interstimulus interval, or\\nISI, which is the time interval between\\nthe CS onset and the US onset (with\\nthe CS ending when the US ends in a\\ncommon version shown here). Intrace\\nconditioning, the US begins after the CS\\nends, and the time interval between CS\\no↵set and US onset is called the trace\\ninterval.\\nThe salivation of Pavlov’s dogs to the\\nsound of a metronome is just one exam-\\nple of classical conditioning, which has\\nbeen intensively studied across many response systems of many species of animals. URs\\nare often preparatory in some way, like the salivation of Pavlov’s dog, or protective in\\nsome way, like an eye blink in response to something irritating to the eye, or freezing\\nin response to seeing a predator. Experiencing the CS-US predictive relationship over\\na series of trials causes the animal to learn that the CS predicts the US so that the\\nanimal can respond to the CS with a CR that prepares the animal for, or protects it\\nfrom, the predicted US. Some CRs are similar to the UR but begin earlier and di↵er in\\nways that increase their e↵ectiveness. In one intensively studied type of experiment, for\\nexample, a tone CS reliably predicts a pu↵ of air (the US) to a rabbit’s eye, triggering a\\nUR consisting of the closure of a protective inner eyelid called the nictitating membrane.\\nAfter one or more trials, the tone comes to trigger a CR consisting of membrane closure\\nthat begins before the air pu↵ and eventually becomes timed so that peak closure occurs\\njust when the air pu↵ is likely to occur. This CR, being initiated in anticipation of the\\nair pu↵ and appropriately timed, o↵ers better protection than simply initiating closure\\nas a reaction to the irritating US. The ability to act in anticipation of important events\\nby learning about predictive relationships among stimuli is so beneﬁcial that it is widely\\npresent across the animal kingdom.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 366, 'page_label': '367'}, page_content='14.2. Classical Conditioning 345\\n14.2.1 Blocking and Higher-order Conditioning\\nMany interesting properties of classical conditioning have been observed in experiments.\\nBeyond the anticipatory nature of CRs, two widely observed properties ﬁgured prominently\\nin the development of classical conditioning models:blocking and higher-order conditioning.\\nBlocking occurs when an animal fails to learn a CR when a potential CS is presented along\\nwith another CS that had been used previously to condition the animal to produce that\\nCR. For example, in the ﬁrst stage of a blocking experiment involving rabbit nictitating\\nmembrane conditioning, a rabbit is ﬁrst conditioned with a tone CS and an air pu↵ US\\nto produce the CR of closing its nictitating membrane in anticipation of the air pu↵. The\\nexperiment’s second stage consists of additional trials in which a second stimulus, say\\na light, is added to the tone to form a compound tone/light CS followed by the same\\nair pu↵ US. In the experiment’s third phase, the second stimulus alone—the light—is\\npresented to the rabbit to see if the rabbit has learned to respond to it with a CR. It\\nturns out that the rabbit produces very few, or no, CRs in response to the light: learning\\nto the light had beenblocked by the previous learning to the tone.2 Blocking results like\\nthis challenged the idea that conditioning depends only on simple temporal contiguity,\\nthat is, that a necessary and su\\x00cient condition for conditioning is that a US frequently\\nfollows a CS closely in time. In the next section we describe theRescorla–Wagner model\\n(Rescorla and Wagner, 1972) that o↵ered an inﬂuential explanation for blocking.\\nHigher-order conditioning occurs when a previously-conditioned CS acts as a US\\nin conditioning another initially neutral stimulus. Pavlov described an experiment in\\nwhich his assistant ﬁrst conditioned a dog to salivate to the sound of a metronome that\\npredicted a food US, as described above. After this stage of conditioning, a number of\\ntrials were conducted in which a black square, to which the dog was initially indi↵erent,\\nwas placed in the dog’s line of vision followed by the sound of the metronome—and\\nthis wasnot followed by food. In just ten trials, the dog began to salivate merely upon\\nseeing the black square, despite the fact that the sight of it had never been followed by\\nfood. The sound of the metronome itself acted as a US in conditioning a salivation CR\\nto the black square CS. This was second-order conditioning. If the black square had\\nbeen used as a US to establish salivation CRs to another otherwise neutral CS, it would\\nhave been third-order conditioning, and so on. Higher-order conditioning is di\\x00cult to\\ndemonstrate, especially above the second order, in part because a higher-order reinforcer\\nloses its reinforcing value due to not being repeatedly followed by the original US during\\nhigher-order conditioning trials. But under the right conditions, such as intermixing\\nﬁrst-order trials with higher-order trials or by providing a general energizing stimulus,\\nhigher-order conditioning beyond the second order can be demonstrated. As we describe\\nbelow, the TD model of classical conditioninguses the bootstrapping idea that is central\\nto our approach to extend the Rescorla–Wagner model’s account of blocking to include\\nboth the anticipatory nature of CRs and higher-order conditioning.\\n2Comparison with a control group is necessary to show that the previous conditioning to the tone is\\nresponsible for blocking learning to the light. This is done by trials with the tone/light CS but with no\\nprior conditioning to the tone. Learning to the light in this case is unimpaired. Moore and Schmajuk\\n(2008) give a full account of this procedure.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 367, 'page_label': '368'}, page_content='346 Chapter 14: Psychology\\nHigher-order instrumental conditioning occurs as well. In this case, a stimulus that\\nconsistently predicts primary reinforcement becomes a reinforcer itself, where reinforce-\\nment is primary if its rewarding or penalizing quality has been built into the animal by\\nevolution. The predicting stimulus becomes asecondary reinforcer, or more generally, a\\nhigher-order or conditioned reinforcer—the latter being a better term when the predicted\\nreinforcing stimulus is itself a secondary, or an even higher-order, reinforcer. A condi-\\ntioned reinforcer deliversconditioned reinforcement: conditioned reward or conditioned\\npenalty. Conditioned reinforcement acts like primary reinforcement in increasing an\\nanimal’s tendency to produce behavior that leads to conditioned reward, and to decrease\\nan animal’s tendency to produce behavior that leads to conditioned penalty. (See our\\ncomments at the end of this chapter that explain how our terminology sometimes di↵ers,\\nas it does here, from terminology used in psychology.)\\nConditioned reinforcement is a key phenomenon that explains, for instance, why we\\nwork for the conditioned reinforcer money, whose worth derives solely from what is\\npredicted by having it. In actor–critic methods described in Section 13.5 (and discussed\\nin the context of neuroscience in Sections 15.7 and 15.8), the critic uses a TD method\\nto evaluate the actor’s policy, and its value estimates provide conditioned reinforcement\\nto the actor, allowing the actor to improve its policy. This analog of higher-order\\ninstrumental conditioning helps address the credit-assignment problem mentioned in\\nSection 1.7 because the critic gives moment-by-moment reinforcement to the actor when\\nthe primary reward signal is delayed. We discuss this more below in Section 14.4.\\n14.2.2 The Rescorla–Wagner Model\\nRescorla and Wagner created their model mainly to account for blocking. The core\\nidea of the Rescorla–Wagner model is that an animal only learns when events violate\\nits expectations, in other words, only when the animal is surprised (although without\\nnecessarily implying anyconscious expectation or emotion). We ﬁrst present Rescorla and\\nWagner’s model using their terminology and notation before shifting to the terminology\\nand notation we use to describe the TD model.\\nHere is how Rescorla and Wagner described their model. The model adjusts the\\n“associative strength” of each component stimulus of a compound CS, which is a number\\nrepresenting how strongly or reliably that component is predictive of a US. When\\na compound CS consisting of several component stimuli is presented in a classical\\nconditioning trial, the associative strength of each component stimulus changes in a way\\nthat depends on an associative strength associated with the entire stimulus compound,\\ncalled the “aggregate associative strength,” and not just on the associative strength of\\neach component itself.\\nRescorla and Wagner considered a compound CS AX, consisting of component stimuli\\nA and X, where the animal may have already experienced stimulus A, and stimulus X\\nmight be new to the animal. LetVA, VX, and VAX respectively denote the associative\\nstrengths of stimuli A, X, and the compound AX. Suppose that on a trial the compound\\nCS AX is followed by a US, which we label stimulus Y. Then the associative strengths of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 368, 'page_label': '369'}, page_content='14.2. Classical Conditioning 347\\nthe stimulus components change according to these expressions:\\n\\x00VA = ↵A\\x00Y(RY \\x00 VAX)\\n\\x00VX = ↵X\\x00Y(RY \\x00 VAX),\\nwhere ↵A\\x00Y and ↵X\\x00Y are the step-size parameters, which depend on the identities of\\nthe CS components and the US, andRY is the asymptotic level of associative strength\\nthat the US Y can support. (Rescorla and Wagner used\\x00 here instead ofR,b u tw e\\nuse R to avoid confusion with our use of\\x00 and because we usually think of this as the\\nmagnitude of a reward signal, with the caveat that the US in classical conditioning is not\\nnecessarily rewarding or penalizing.) A key assumption of the model is that the aggregate\\nassociative strengthVAX is equal toVA + VX. The associative strengths as changed by\\nthese \\x00s become the associative strengths at the beginning of the next trial.\\nTo be complete, the model needs a response-generation mechanism, which is a way\\nof mapping values ofV s to CRs. Because this mapping would depend on details of\\nthe experimental situation, Rescorla and Wagner did not specify a mapping but simply\\nassumed that largerV s would produce stronger or more likely CRs, and that negative\\nV s would mean that there would be no CRs.\\nThe Rescorla–Wagner model accounts for the acquisition of CRs in a way that explains\\nblocking. As long as the aggregate associative strength,VAX, of the stimulus compound\\nis below the asymptotic level of associative strength,RY, that the US Y can support, the\\nprediction errorRY \\x00VAX is positive. This means that over successive trials the associative\\nstrengths VA and VX of the component stimuli increase until the aggregate associative\\nstrength VAX equals RY, at which point the associative strengths stop changing (unless the\\nUS changes). When a new component is added to a compound CS to which the animal has\\nalready been conditioned, further conditioning with the augmented compound produces\\nlittle or no increase in the associative strength of the added CS component because the\\nerror has already been reduced to zero, or to a low value. The occurrence of the US is\\nalready predicted nearly perfectly, so little or no error—or surprise—is introduced by the\\nnew CS component. Prior learning blocks learning to the new component.\\nTo transition from Rescorla and Wagner’s model to the TD model of classical condi-\\ntioning (which we just call the TD model), we ﬁrst recast their model in terms of the\\nconcepts that we are using throughout this book. Speciﬁcally, we match the notation\\nwe use for learning with linear function approximation (Section 9.4), and we think of\\nthe conditioning process as one of learning to predict the “magnitude of the US” on a\\ntrial on the basis of the compound CS presented on that trial, where the magnitude of\\na US Y is theRY of the Rescorla–Wagner model as given above. We also introduce\\nstates. Because the Rescorla–Wagner model is atrial-level model, meaning that it deals\\nwith how associative strengths change from trial to trial without considering any details\\nabout what happens within and between trials, we do not have to consider how states\\nchange during a trial until we present the full TD model in the following section. Instead,\\nhere we simply think of a state as a way of labeling a trial in terms of the collection of\\ncomponent CSs that are present on the trial.\\nTherefore, assume that trial-type, or state,s is described by a real-valued vector of\\nfeatures x(s)=( x1(s),x 2(s),...,x d(s))> where xi(s)=1i f CSi,t h eith component of a'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 369, 'page_label': '370'}, page_content='348 Chapter 14: Psychology\\ncompound CS, is present on the trial and 0 otherwise. Then if thed-dimensional vector\\nof associative strengths isw, the aggregate associative strength for trial-types is\\nˆv(s,w)= w>x(s). (14.1)\\nThis corresponds to avalue estimate in reinforcement learning, and we think of it as the\\nUS prediction.\\nNow temporally lett denote the number of a complete trial and not its usual meaning\\nas a time step (we revert tot’s usual meaning when we extend this to the TD model\\nbelow), and assume thatSt is the state corresponding to trialt. Conditioning trial t\\nupdates the associative strength vectorwt to wt+1 as follows:\\nwt+1 = wt + ↵\\x00t x(St), (14.2)\\nwhere ↵ is the step-size parameter, and—because here we are describing the Rescorla–\\nWagner model— \\x00t is theprediction error\\n\\x00t = Rt \\x00 ˆv(St,wt). (14.3)\\nRt is the target of the prediction on trialt, that is, the magnitude of the US, or in Rescorla\\nand Wagner’s terms, the associative strength that the US on the trial can support. Note\\nthat because of the factorx(St) in (14.2), only the associative strengths of CS components\\npresent on a trial are adjusted as a result of that trial. You can think of the prediction\\nerror as a measure of surprise, and the aggregate associative strength as the animal’s\\nexpectation that is violated when it does not match the target US magnitude.\\nFrom the perspective of machine learning, the Rescorla–Wagner model is an error-\\ncorrection supervised learning rule. It is essentially the same as the Least Mean Square\\n(LMS), or Widrow-Ho↵, learning rule (Widrow and Ho↵, 1960) that ﬁnds the weights—\\nhere the associative strengths—that make the average of the squares of all the errors as\\nclose to zero as possible. It is a “curve-ﬁtting,” or regression, algorithm that is widely\\nused in engineering and scientiﬁc applications (see Section 9.4).3\\nThe Rescorla–Wagner model was very inﬂuential in the history of animal learning\\ntheory because it showed that a “mechanistic” theory could account for the main facts\\nabout blocking without resorting to more complex cognitive theories involving, for\\nexample, an animal’s explicit recognition that another stimulus component had been\\nadded and then scanning its short-term memory backward to reassess the predictive\\nrelationships involving the US. The Rescorla–Wagner model showed how traditional\\ncontiguity theories of conditioning—that temporal contiguity of stimuli was a necessary\\nand su\\x00cient condition for learning—could be adjusted in a simple way to account for\\nblocking (Moore and Schmajuk, 2008).\\nThe Rescorla–Wagner model provides a simple account of blocking and some other\\nfeatures of classical conditioning, but it is not a complete or perfect model of classical\\n3The only di↵erences between the LMS rule and the Rescorla–Wagner model are that for LMS the\\ninput vectorsxt can have any real numbers as components, and—at least in the simplest version of the\\nLMS rule—the step-size parameter↵ does not depend on the input vector or the identity of the stimulus\\nsetting the prediction target.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 370, 'page_label': '371'}, page_content='14.2. Classical Conditioning 349\\nconditioning. Di↵erent ideas account for a variety of other observed e↵ects, and progress\\nis still being made toward understanding the many subtleties of classical conditioning.\\nThe TD model, which we describe next, though also not a complete or perfect model\\nmodel of classical conditioning, extends the Rescorla–Wagner model to address how\\nwithin-trial and between-trial timing relationships among stimuli can inﬂuence learning\\nand how higher-order conditioning might arise.\\n14.2.3 The TD Model\\nThe TD model is areal-time model, as opposed to a trial-level model like the Rescorla–\\nWagner model. A single step t in the Rescorla–Wagner model represents an entire\\nconditioning trial. The model does not apply to details about what happens during the\\ntime a trial is taking place, or what might happen between trials. Within each trial an\\nanimal might experience various stimuli whose onsets occur at particular times and that\\nhave particular durations. These timing relationships strongly inﬂuence learning. The\\nRescorla–Wagner model also does not include a mechanism for higher-order conditioning,\\nwhereas for the TD model, higher-order conditioning is a natural consequence of the\\nbootstrapping idea that is at the base of TD algorithms.\\nTo describe the TD model we begin with the formulation of the Rescorla–Wagner\\nmodel above, butt now labels time steps within or between trials instead of complete\\ntrials. Think of the time betweent and t + 1 as a small time interval, say .01 second, and\\nthink of a trial as a sequences of states, one associated with each time step, where the\\nstate at stept now represents details of how stimuli are represented att instead of just\\na label for the CS components present on a trial. In fact, we can completely abandon\\nthe idea of trials. From the point of view of the animal, a trial is just a fragment of its\\ncontinuing experience interacting with its world. Following our usual view of an agent\\ninteracting with its environment, imagine that the animal is experiencing an endless\\nsequence of statess, each represented by a feature vectorx(s). That said, it is still often\\nconvenient to refer to trials as fragments of time during which patterns of stimuli repeat\\nin an experiment.\\nState features are not restricted to describing the external stimuli that an animal\\nexperiences; they can describe neural activity patterns that external stimuli produce in\\nan animal’s brain, and these patterns can be history-dependent, meaning that they can\\nbe persistent patterns produced by sequences of external stimuli. Of course, we do not\\nknow exactly what these neural activity patterns are, but a real-time model like the TD\\nmodel allows one to explore the consequences on learning of di↵erent hypotheses about\\nthe internal representations of external stimuli. For these reasons, the TD model does\\nnot commit to any particular state representation. In addition, because the TD model\\nincludes discounting and eligibility traces that span time intervals between stimuli, the\\nmodel also makes it possible to explore how discounting and eligibility traces interact with\\nstimulus representations in making predictions about the results of classical conditioning\\nexperiments.\\nBelow we describe some of the state representations that have been used with the\\nTD model and some of their implications, but for the moment we stay agnostic about'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 371, 'page_label': '372'}, page_content='350 Chapter 14: Psychology\\nthe representation and just assume that each states is represented by a feature vector\\nx(s)=( x1(s),x 2(s),...,x n(s))>. Then the aggregate associative strength corresponding\\nto a states is given by (14.1), the same as for the Rescorla-Wagner model, but the TD\\nmodel updates the associative strength vector,w,d i ↵ e r e n t l y .W i t ht now labeling a time\\nstep instead of a complete trial, the TD model governs learning according to this update:\\nwt+1 = wt + ↵\\x00t zt, (14.4)\\nwhich replacesxt(St) in the Rescorla–Wagner update (14.2) withzt, a vector of eligibility\\ntraces, and instead of the\\x00t of (14.3), here\\x00t is a TD error:\\n\\x00t = Rt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt), (14.5)\\nwhere \\x00 is a discount factor (between 0 and 1),Rt is the prediction target at timet, and\\nˆv(St+1,wt) and ˆv(St,wt) are aggregate associative strengths att + 1 andt as deﬁned by\\n(14.1).\\nEach componenti of the eligibility-trace vectorzt increments or decrements according\\nto the componentxi(St) of the feature vectorx(St), and otherwise decays with a rate\\ndetermined by\\x00\\x00:\\nzt = \\x00\\x00 zt\\x001 + x(St). (14.6)\\nHere \\x00 is the usual eligibility trace decay parameter.\\nNote that if\\x00 = 0, the TD model reduces to the Rescorla–Wagner model with the\\nexceptions that: the meaning of t is di↵erent in each case (a trial number for the\\nRescorla–Wagner model and a time step for the TD model), and in the TD model there\\nis a one-time-step lead in the prediction targetR. The TD model is equivalent to the\\nbackward view of the semi-gradient TD(\\x00) algorithm with linear function approximation\\n(Chapter 12), except thatRt in the model does not have to be a reward signal as it does\\nwhen the TD algorithm is used to learn a value function for policy-improvement.\\n14.2.4 TD Model Simulations\\nReal-time conditioning models like the TD model are interesting primarily because they\\nmake predictions for a wide range of situations that cannot be represented by trial-level\\nmodels. These situations involve the timing and durations of conditionable stimuli, the\\ntiming of these stimuli in relation to the timing of the US, and the timing and shapes\\nof CRs. For example, the US generally must begin after the onset of a neutral stimulus\\nfor conditioning to occur, with the rate and e↵ectiveness of learning depending on the\\ninter-stimulus interval, or ISI, the interval between the onsets of the CS and the US. When\\nCRs appear, they generally begin before the appearance of the US and their temporal\\nproﬁles change during learning. In conditioning with compound CSs, the component\\nstimuli of the compound CSs may not all begin and end at the same time, sometimes\\nforming what is called aserial compound in which the component stimuli occur in a\\nsequence over time. Timing considerations like these make it important to consider how'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 372, 'page_label': '373'}, page_content='14.2. Classical Conditioning 351\\nPresenceComplete Serial Compound\\nStimulus Representation\\nMicrostimuli\\nCS\\nUS\\nFigure 14.1: Three stimulus representations (in columns) sometimes used with the TD model.\\nEach row represents one element of the stimulus representation. The three representations vary\\nalong a temporal generalization gradient, with no generalization between nearby time points in\\nthe complete serial compound (left column) and complete generalization between nearby time\\npoints in the presence representation (right column). The microstimulus representation occupies\\na middle ground. The degree of temporal generalization determines the temporal granularity\\nwith which US predictions are learned. Adapted with minor changes fromLearning & Behavior,\\nEvaluating the TD Model of Classical Conditioning, volume 40, 2012, p. 311, E. A. Ludvig, R. S.\\nSutton, E. J. Kehoe. With permission of Springer.\\nstimuli are represented, how these representations unfold over time during and between\\ntrials, and how they interact with discounting and eligibility traces.\\nFigure 14.1 shows three of the stimulus representations that have been used in exploring\\nthe behavior of the TD model: thecomplete serial compound(CSC), themicrostimulus\\n(MS), and the presence representations (Ludvig, Sutton, and Kehoe, 2012). These\\nrepresentations di↵er in the degree to which they force generalization among nearby time\\npoints during which a stimulus is present.\\nThe simplest of the representations shown in Figure 14.1 is the presence representation\\nin the ﬁgure’s right column. This representation has a single feature for each component\\nCS present on a trial, where the feature has value 1 whenever that component is present,\\nand 0 otherwise.4 The presence representation is not a realistic hypothesis about how\\nstimuli are represented in an animal’s brain, but as we describe below, the TD model\\nwith this representation can produce many of the timing phenomena seen in classical\\nconditioning.\\n4In our formalism, there is a di↵erent state,St,f o re a c ht i m es t e pt during a trial, and for a trial\\nin which a compound CS consists ofn component CSs of various durations occurring at various times\\nthroughout the trial, there is a feature,xi,f o re a c hc o m p o n e n tCSi, i =1 ,...,n ,w h e r exi(St)=1f o r\\nall timest when the CSi is present, and equals zero otherwise.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 373, 'page_label': '374'}, page_content='352 Chapter 14: Psychology\\nFor the CSC representation (left column of Figure 14.1), the onset of each external\\nstimulus initiates a sequence of precisely-timed short-duration internal signals that\\ncontinues until the external stimulus ends.5 This is like assuming the animal’s nervous\\nsystem has a clock that keeps precise track of time during stimulus presentations; it is\\nwhat engineers call a “tapped delay line.” Like the presence representation, the CSC\\nrepresentation is unrealistic as a hypothesis about how the brain internally represents\\nstimuli, but Ludvig et al. (2012) call it a “useful ﬁction” because it can reveal details of\\nhow the TD model works when relatively unconstrained by the stimulus representation.\\nThe CSC representation is also used in most TD models of dopamine-producing neurons\\nin the brain, a topic we take up in Chapter 15. The CSC representation is often viewed\\nas an essential part of the TD model, although this view is mistaken.\\nThe MS representation (center column of Figure 14.1) is like the CSC representation\\nin that each external stimulus initiates a cascade of internal stimuli, but in this case the\\ninternal stimuli—the microstimuli—are not of such limited and non-overlapping form;\\nthey are extended over time and overlap. As time elapses from stimulus onset, di↵erent\\nsets of microstimuli become more or less active, and each subsequent microstimulus\\nbecomes progressively wider in time and reaches a lower maximal level. Of course, there\\nare many MS representations depending on the nature of the microstimuli, and a number\\nof examples of MS representations have been studied in the literature, in some cases along\\nwith proposals for how an animal’s brain might generate them (see the Bibliographic and\\nHistorical Comments at the end of this chapter). MS representations are more realistic\\nthan the presence or CSC representations as hypotheses about neural representations of\\nstimuli, and they allow the behavior of the TD model to be related to a broader collection\\nof phenomena observed in animal experiments. In particular, by assuming that cascades\\nof microstimuli are initiated by USs as well as by CSs, and by studying the signiﬁcant\\ne↵ects on learning of interactions between microstimuli, eligibility traces, and discounting,\\nthe TD model is helping to frame hypotheses to account for many of the subtle phenomena\\nof classical conditioning and how an animal’s brain might produce them. We say more\\nabout this below, particularly in Chapter 15 where we discuss reinforcement learning and\\nneuroscience.\\nEven with the simple presence representation, however, the TD model produces all the\\nbasic properties of classical conditioning that are accounted for by the Rescorla–Wagner\\nmodel, plus features of conditioning that are beyond the scope of trial-level models. For\\nexample, as we have already mentioned, a conspicuous feature of classical conditioning is\\nthat the US generally must beginafter the onset of a neutral stimulus for conditioning\\nto occur, and that after conditioning, the CR beginsbefore the appearance of the US.\\nIn other words, conditioning generally requires a positive ISI, and the CR generally\\nanticipates the US. How the strength of conditioning (e.g., the percentage of CRs elicited\\nby a CS) depends on the ISI varies substantially across species and response systems, but\\nit typically has the following properties: it is negligible for a zero or negative ISI, i.e., when\\n5In our formalism, for each CS componentCSi present on a trial, and for each time stept during a\\ntrial, there is a separate featurext\\ni,w h e r ext\\ni(St0 )=1i f t = t0 for anyt0 at whichCSi is present, and\\nequals 0 otherwise. This is di↵erent from the CSC representation in Sutton and Barto (1990) in which\\nthere are the same distinct features for each time step but no reference to external stimuli; hence the\\nname complete serial compound.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 374, 'page_label': '375'}, page_content='14.2. Classical Conditioning 353\\nthe US onset occurs simultaneously with, or earlier than, the CS onset (although research\\nhas found that associative strengths sometimes increase slightly or become negative with\\nnegative ISIs); it increases to a maximum at a positive ISI where conditioning is most\\ne↵ective; and it then decreases to zero after an interval that varies widely with response\\nsystems. The precise shape of this dependency for the TD model depends on the values\\nof its parameters and details of the stimulus representation, but these basic features of\\nISI-dependency are core properties of the TD model.\\nFacilitation of remote associations in the TD model\\nOne of the theoretical issues arising\\nwith serial-compound conditioning, that\\nis, conditioning with a compound CS\\nwhose components occur in a sequence,\\nconcerns the facilitation of remote asso-\\nciations. It has been found that if the\\nempty trace interval between a ﬁrst CS\\n(CSA) and the US is ﬁlled with a second\\nCS (CSB) to form a serial-compound\\nstimulus, then conditioning to CSA is\\nfacilitated. Shown to the right is the\\nbehavior of the TD model with the pres-\\nence representation in a simulation of\\nsuch an experiment whose timing details\\nare shown above. Consistent with the\\nexperimental results (Kehoe, 1982), the\\nmodel shows facilitation of both the rate\\nof conditioning and the asymptotic level\\nof conditioning of the ﬁrst CS due to the\\npresence of the second CS.\\nwCSB\\nThe Egger-Miller e↵ect in the TD model\\nA well-known demonstration of the\\ne↵ects on conditioning of temporal re-\\nlationships among stimuli within a trial\\nis an experiment by Egger and Miller\\n(1962) that involved two overlapping\\nCSs in a delay conﬁguration as shown\\nto the right (top). Although CSB was\\nin a better temporal relationship with\\nthe US, the presence of CSA substan-\\ntially reduced conditioning to CSB as\\ncompared to controls in which CSA was\\nabsent. Directly to the right is shown\\nthe same result being generated by the\\nTD model in a simulation of this exper-\\niment with the presence representation.\\nThe TD model accounts for blocking\\nbecause it is an error-correctinglearning'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 375, 'page_label': '376'}, page_content='354 Chapter 14: Psychology\\nrule like the Rescorla–Wagner model. Beyond accounting for basic blocking results,\\nhowever, the TD model predicts (with the presence representation and more complex\\nrepresentations as well) that blocking is reversed if the blocked stimulus is moved earlier\\nwCSB\\nFigure 14.2: Temporal primacy overriding\\nblocking in the TD model.\\nin time (like CSA in the diagram to the\\nright) so that its onset occurs before the\\nonset of the blocking stimulus. This feature\\nof the TD model’s behavior deserves atten-\\ntion because it had not been observed at\\nthe time of the model’s introduction. Recall\\nthat in blocking, if an animal has already\\nlearned that one CS predicts a US, then\\nlearning that a newly-added second CS also\\npredicts the US is much reduced, i.e., is\\nblocked. But if the newly-added second CS\\nbegins earlier than the pretrained CS, then—\\naccording to the TD model—learning to the\\nnewly-added CS is not blocked. In fact, as\\ntraining continues and the newly-added CS\\ngains associative strength, the pretrained\\nCS loses associative strength. The behavior\\nof the TD model under these conditions\\nis shown in the lower part of Figure 14.2.\\nThis simulation experiment di↵ered from the Egger-Miller experiment (bottom of the\\npreceding page) in that the shorter CS with the later onset was given prior training\\nuntil it was fully associated with the US. This surprising prediction led Kehoe, Schreurs,\\nand Graham (1987) to conduct the experiment using the well-studied rabbit nictitating\\nmembrane preparation. Their results conﬁrmed the model’s prediction, and they noted\\nthat non-TD models have considerable di\\x00culty explaining their data.\\nWith the TD model, an earlier predictive stimulus takes precedence over a later\\npredictive stimulus because, like all the prediction methods described in this book, the\\nTD model is based on the backing-up or bootstrapping idea: updates to associative\\nstrengths shift the strengths at a particular state toward the strength at later states.\\nAnother consequence of bootstrapping is that the TD model provides an account of higher-\\norder conditioning, a feature of classical conditioning that is beyond the scope of the\\nRescorla-Wagner and similar models. As we described above, higher-order conditioning\\nis the phenomenon in which a previously-conditioned CS can act as a US in conditioning\\nanother initially neutral stimulus. Figure 14.3 shows the behavior of the TD model (again\\nwith the presence representation) in a higher-order conditioning experiment—in this case\\nit is second-order conditioning. In the ﬁrst phase (not shown in the ﬁgure), CSB is trained\\nto predict a US so that its associative strength increases, here to 1.65. In the second\\nphase, CSA is paired with CSB in the absence of the US, in the sequential arrangement\\nshown at the top of the ﬁgure. CSA acquires associative strength even though it is never\\npaired with the US. With continued training, CSA’s associative strength reaches a peak\\nand then decreases because the associative strength of CSB, the secondary reinforcer,\\ndecreases so that it loses its ability to provide secondary reinforcement. CSB’s associative'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 376, 'page_label': '377'}, page_content='14.2. Classical Conditioning 355\\nw\\nw\\nFigure 14.3: Second-order conditioning with\\nthe TD model.\\nstrength decreases because the US does not\\noccur in these higher-order conditioning tri-\\nals. These are extinction trials for CSB\\nbecause its predictive relationship to the\\nUS is disrupted so that its ability to act as\\na reinforcer decreases. This same pattern\\nis seen in animal experiments. This extinc-\\ntion of conditioned reinforcement in higher-\\norder conditioning trials makes it di\\x00cult\\nto demonstrate higher-order conditioning\\nunless the original predictive relationships\\nare periodically refreshed by occasionally\\ninserting ﬁrst-order trials.\\nThe TD model produces an analog of\\nsecond- and higher-order conditioning be-\\ncause \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt) appears in\\nthe TD error \\x00t (14.5). Due to the ﬁrst\\nphase of learning,\\x00ˆv(St+1,wt)m a yd i ↵ e r\\nfrom ˆv(St,wt), making\\x00t non-zero (a temporal di↵erence). This di↵erence has the same\\nstatus asRt+1 in (14.5), implying that as far as learning is concerned there is no di↵erence\\nbetween a temporal di↵erence and the occurrence of a US. In fact, this feature of the\\nTD algorithm is one of the major reasons for its development, which we now understand\\nthrough its connection to dynamic programming as described in Chapter 6. Bootstrapping\\nvalues is intimately related to second-order, and higher-order, conditioning.\\nIn the examples of the TD model’s behavior described above, we examined only the\\nchanges in the associative strengths of the CS components; we did not look at what\\nthe model predicts about properties of an animal’s conditioned responses (CRs): their\\ntiming, shape, and how they develop over conditioning trials. These properties depend\\non the species, the response system being observed, and parameters of the conditioning\\ntrials, but in many experiments with di↵erent animals and di↵erent response systems, the\\nmagnitude of the CR, or the probability of a CR, increases as the expected time of the\\nUS approaches. For example, in classical conditioning of a rabbit’s nictitating membrane\\nresponse that we mentioned above, over conditioning trials the delay from CS onset to\\nwhen the nictitating membrane begins to move across the eye decreases over trials, and\\nthe amplitude of this anticipatory closure gradually increases over the interval between\\nthe CS and the US until the membrane reaches maximal closure at the expected time of\\nthe US. The timing and shape of this CR is critical to its adaptive signiﬁcance—covering\\nthe eye too early reduces vision (even though the nictitating membrane is translucent),\\nwhile covering it too late is of little protective value. Capturing CR features like these is\\nchallenging for models of classical conditioning.\\nThe TD model does not include as part of its deﬁnition any mechanism for translat-\\ning the time course of the US prediction,ˆv(St,wt), into a proﬁle that can be compared'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 377, 'page_label': '378'}, page_content='356 Chapter 14: Psychology\\nwith the properties of an animal’s CR. The simplest choice is to let the time course of\\na simulated CR equal the time course of the US prediction. In this case, features of\\nsimulated CRs and how they change over trials depend only on the stimulus representation\\nchosen and the values of the model’s parameters↵, \\x00, and\\x00.\\nFigure 14.4 shows the time courses of US predictions at di↵erent points during learning\\nwith the three representations shown in Figure 14.1. For these simulations the US\\noccurred 25 time steps after the onset of the CS, and↵ = .05, \\x00 = .95 and \\x00 = .97.\\nWith the CSC representation (Figure 14.4 left), the curve of the US prediction formed\\nby the TD model increases exponentially throughout the interval between the CS and\\nthe US until it reaches a maximum exactly when the US occurs (at time step 25). This\\nexponential increase is the result of discounting in the TD model learning rule. With the\\npresence representation (Figure 14.4 middle), the US prediction is nearly constant while\\nthe stimulus is present because there is only one weight, or associative strength, to be\\nlearned for each stimulus. Consequently, the TD model with the presence representation\\ncannot recreate many features of CR timing. With an MS representation (Figure 14.4\\nright), the development of the TD model’s US prediction is more complicated. After 200\\ntrials the prediction’s proﬁle is a reasonable approximation of the US prediction curve\\nproduced with the CSC representation.\\nˆv ˆv ˆv\\nFigure 14.4: Time course of US prediction over the course of acquisition for the TD model\\nwith three di↵erent stimulus representations. Left: With the complete serial compound (CSC),\\nthe US prediction increases exponentially through the interval, peaking at the time of the US.\\nAt asymptote (trial 200), the US prediction peaks at the US intensity (1 in these simulations).\\nMiddle: With the presence representation, the US prediction converges to an almost constant\\nlevel. This constant level is determined by the US intensity and the length of the CS–US interval.\\nRight: With the microstimulus representation, at asymptote, the TD model approximates the\\nexponentially increasing time course depicted with the CSC through a linear combination of the\\ndi↵erent microstimuli. Adapted with minor changes fromLearning & Behavior,E v a l u a t i n gt h e\\nTD Model of Classical Conditioning, volume 40, 2012, E. A. Ludvig, R. S. Sutton, E. J. Kehoe.\\nWith permission of Springer.\\nThe US prediction curves shown in Figure 14.4 were not intended to precisely match\\nproﬁles of CRs as they develop during conditioning in any particular animal experiment,\\nbut they illustrate the strong inﬂuence that the stimulus representation has on predictions\\nderived from the TD model. Further, although we can only mention it here, how the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 378, 'page_label': '379'}, page_content='14.3. Instrumental Conditioning 357\\nstimulus representation interacts with discounting and eligibility traces is important\\nin determining properties of the US prediction proﬁles produced by the TD model.\\nAnother dimension beyond what we can discuss here is the inﬂuence of di↵erent response-\\ngeneration mechanisms that translate US predictions into CR proﬁles; the proﬁles shown\\nin Figure 14.4 are “raw” US prediction proﬁles. Even without any special assumption\\nabout how an animal’s brain might produce overt responses from US predictions, however,\\nthe proﬁles in Figure 14.4 for the CSC and MS representations increase as the time of the\\nUS approaches and reach a maximum at the time of the US, as is seen in many animal\\nconditioning experiments.\\nThe TD model, when combined with particular stimulus representations and response-\\ngeneration mechanisms, is able to account for a surprisingly wide range of phenomena\\nobserved in animal classical conditioning experiments, but it is far from being a perfect\\nmodel. To generate other details of classical conditioning the model needs to be extended,\\nperhaps by adding model-based elements and mechanisms for adaptively altering some of\\nits parameters. Other approaches to modeling classical conditioning depart signiﬁcantly\\nfrom the Rescorla–Wagner-style error-correction process. Bayesian models, for example,\\nwork within a probabilistic framework in which experience revises probability estimates.\\nAll of these models usefully contribute to our understanding of classical conditioning.\\nPerhaps the most notable feature of the TD model is that it is based on a theory—the\\ntheory we have described in this book—that suggests an account of what an animal’s\\nnervous system istrying to dowhile undergoing conditioning: it is trying to form accurate\\nlong-term predictions, consistent with the limitations imposed by the way stimuli are\\nrepresented and how the nervous system works. In other words, it suggests anormative\\naccount of classical conditioning in which long-term, instead of immediate, prediction is a\\nkey feature.\\nThe development of the TD model of classical conditioning is one instance in which the\\nexplicit goal was to model some of the details of animal learning behavior. In addition to\\nits standing as analgorithm, then, TD learning is also the basis of thismodel of aspects\\nof biological learning. As we discuss in Chapter 15, TD learning has also turned out\\nto underlie an inﬂuential model of the activity of neurons that produce dopamine, a\\nchemical in the brain of mammals that is deeply involved in reward processing. These\\nare instances in which reinforcement learning theory makes detailed contact with animal\\nbehavioral and neural data.\\nWe now turn to considering correspondences between reinforcement learning and animal\\nbehavior in instrumental conditioning experiments, the other major type of laboratory\\nexperiment studied by animal learning psychologists.\\n14.3 Instrumental Conditioning\\nIn instrumental conditioning experiments learning depends on the consequences of be-\\nhavior: the delivery of a reinforcing stimulus is contingent on what the animal does.\\nIn classical conditioning experiments, in contrast, the reinforcing stimulus—the US—is\\ndelivered independently of the animal’s behavior. Instrumental conditioning is usually\\nconsidered to be the same asoperant conditioning, the term B. F. Skinner (1938, 1963)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 379, 'page_label': '380'}, page_content='358 Chapter 14: Psychology\\nintroduced for experiments with behavior-contingent reinforcement, though the experi-\\nments and theories of those who use these two terms di↵er in a number of ways, some of\\nwhich we touch on below. We will exclusively use the term instrumental conditioning for\\nexperiments in which reinforcement is contingent upon behavior. The roots of instrumen-\\ntal conditioning go back to experiments performed by the American psychologist Edward\\nThorndike one hundred years before publication of the ﬁrst edition of this book.\\nOne of Thorndike’s puzzle boxes.\\nReprinted from Thorndike, Animal Intelligence: An\\nExperimental Study of the Associative Processes in\\nAnimals, The Psychological Review, Series of Mono-\\ngraph SupplementsII(4), Macmillan, New York, 1898.\\nThorndike observed the behavior of cats\\nwhen they were placed in “puzzle boxes,”\\nsuch as the one at the right, from which\\nthey could escape by appropriate actions.\\nFor example, a cat could open the door\\nof one box by performing a sequence of\\nthree separate actions: depressing a plat-\\nform at the back of the box, pulling a string\\nby clawing at it, and pushing a bar up or\\ndown. When ﬁrst placed in a puzzle box,\\nwith food visible outside, all but a few of\\nThorndike’s cats displayed “evident signs\\nof discomfort” and extraordinarily vigorous\\nactivity “to strive instinctively to escape\\nfrom conﬁnement” (Thorndike, 1898).\\nIn experiments with di↵erent cats and\\nboxes with di↵erent escape mechanisms, Thorndike recorded the amounts of time each\\ncat took to escape over multiple experiences in each box. He observed that the time\\nalmost invariably decreased with successive experiences, for example, from 300 seconds\\nto 6 or 7 seconds. He described cats’ behavior in a puzzle box like this:\\nThe cat that is clawing all over the box in her impulsive struggle will probably\\nclaw the string or loop or button so as to open the door. And gradually all the\\nother non-successful impulses will be stamped out and the particular impulse\\nleading to the successful act will be stamped in by the resulting pleasure,\\nuntil, after many trials, the cat will, when put in the box, immediately claw\\nthe button or loop in a deﬁnite way. (Thorndike 1898, p. 13)\\nThese and other experiments (some with dogs, chicks, monkeys, and even ﬁsh) led\\nThorndike to formulate a number of “laws” of learning, the most inﬂuential being the\\nLaw of E↵ect. This law describes what is generally known as learning by trial and\\nerror. As we mentioned in Chapter 1, many aspects of the Law of E↵ect have generated\\ncontroversy, and its details have been modiﬁed over the years. Still the law—in one form\\nor another—expresses an enduring principle of learning.\\nEssential features of reinforcement learning algorithms correspond to features of animal\\nlearning described by the Law of E↵ect. First, reinforcement learning algorithms are\\nselectional, meaning that they try alternatives and select among them by comparing their\\nconsequences. Second, reinforcement learning algorithms areassociative, meaning that\\nthe alternatives found by selection are associated with particular situations, or states,\\nto form the agent’s policy. Like learning described by the Law of E↵ect, reinforcement'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 380, 'page_label': '381'}, page_content='14.3. Instrumental Conditioning 359\\nlearning is not just the process ofﬁnding actions that produce a lot of reward, but also\\nof connecting these actions to situations or states. Thorndike used the phrase learning\\nby “selecting and connecting” (Hilgard, 1956). Natural selection in evolution is a prime\\nexample of a selectional process, but it is not associative (at least as it is commonly\\nunderstood); supervised learning is associative, but it is not selectional because it relies\\non instructions that directly tell the agent how to change its behavior.\\nIn computational terms, the Law of E↵ect describes an elementary way of combining\\nsearch and memory: search in the form of trying and selecting among many actions\\nin each situation, and memory in the form of associations linking situations with the\\nactions found—so far—to work best in those situations. Search and memory are essential\\ncomponents of all reinforcement learning algorithms, whether memory takes the form of\\nan agent’s policy, value function, or environment model.\\nA reinforcement learning algorithm’s need to search means that it has to explore in\\nsome way. Animals clearly explore as well, and early animal learning researchers disagreed\\nabout the degree of guidance an animal uses in selecting its actions in situations like\\nThorndike’s puzzle boxes. Are actions the result of “absolutely random, blind groping”\\n(Woodworth, 1938, p. 777), or is there some degree of guidance, either from prior learning,\\nreasoning, or other means? Although some thinkers, including Thorndike, seem to have\\ntaken the former position, others favored more deliberate exploration. Reinforcement\\nlearning algorithms allow wide latitude for how much guidance an agent can employ in\\nselecting actions. The forms of exploration we have used in the algorithms presented\\nin this book, such as\"-greedy and upper-conﬁdence-bound action selection, are merely\\namong the simplest. More sophisticated methods are possible, with the only stipulation\\nbeing that there has to besome form of exploration for the algorithms to work e↵ectively.\\nThe feature of our treatment of reinforcement learning allowing the set of actions\\navailable at any time to depend on the environment’s current state echoes something\\nThorndike observed in his cats’ puzzle-box behaviors. The cats selected actions from\\nthose that they instinctively perform in their current situation, which Thorndike called\\ntheir “instinctual impulses.” First placed in a puzzle box, a cat instinctively scratches,\\nclaws, and bites with great energy: a cat’s instinctual responses to ﬁnding itself in a\\nconﬁned space. Successful actions are selected from these and not from every possible\\naction or activity. This is like the feature of our formalism where the action selected\\nfrom a states belongs to a set of admissible actions,A(s). Specifying these sets is an\\nimportant aspect of reinforcement learning because it can radically simplify learning.\\nThey are like an animal’s instinctual impulses. On the other hand, Thorndike’s cats might\\nhave been exploring according to an instinctual context-speciﬁcordering over actions\\nrather than by just selecting from a set of instinctual impulses. This is another way to\\nmake reinforcement learning easier.\\nAmong the most prominent animal learning researchers inﬂuenced by the Law of E↵ect\\nwere Clark Hull (e.g., Hull, 1943) and B. F. Skinner (e.g., Skinner, 1938). At the center\\nof their research was the idea of selecting behavior on the basis of its consequences.\\nReinforcement learning has features in common with Hull’s theory, which included\\neligibility-like mechanisms and secondary reinforcement to account for the ability to learn\\nwhen there is a signiﬁcant time interval between an action and the consequent reinforcing'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 381, 'page_label': '382'}, page_content='360 Chapter 14: Psychology\\nstimulus (see Section 14.4). Randomness also played a role in Hull’s theory through what\\nhe called “behavioral oscillation” to introduce exploratory behavior.\\nSkinner did not fully subscribe to the memory aspect of the Law of E↵ect. Being averse\\nto the idea of associative linkages, he instead emphasized selection from spontaneously-\\nemitted behavior. He introduced the term “operant” to emphasize the key role of an\\naction’s e↵ects on an animal’s environment. Unlike the experiments of Thorndike and\\nothers, which consisted of sequences of separate trials, Skinner’s operant conditioning\\nexperiments allowed animal subjects to behave for extended periods of time without\\ninterruption. He invented the operant conditioning chamber, now called a “Skinner box,”\\nthe most basic version of which contains a lever or key that an animal can press to obtain\\na reward, such as food or water, which would be delivered according to a well-deﬁned rule,\\ncalled a reinforcement schedule. By recording the cumulative number of lever presses\\nas a function of time, Skinner and his followers could investigate the e↵ect of di↵erent\\nreinforcement schedules on the animal’s rate of lever-pressing. Modeling results from\\nexperiments likes these using the reinforcement learning principles we present in this\\nbook is not well developed, but we mention some exceptions in the Bibliographic and\\nHistorical Remarks section at the end of this chapter.\\nAnother of Skinner’s contributions resulted from his recognition of the e↵ectiveness of\\ntraining an animal by reinforcing successive approximations of the desired behavior, a\\nprocess he calledshaping. Although this technique had been used by others, including\\nSkinner himself, its signiﬁcance was impressed upon him when he and colleagues were\\nattempting to train a pigeon to bowl by swiping a wooden ball with its beak. After\\nwaiting for a long time without seeing any swipe that they could reinforce, they\\n... decided to reinforce any response that had the slightest resemblance to\\na swipe—perhaps, at ﬁrst, merely the behavior of looking at the ball—and\\nthen to select responses which more closely approximated the ﬁnal form. The\\nresult amazed us. In a few minutes, the ball was caroming o↵ the walls of\\nthe box as if the pigeon had been a champion squash player. (Skinner, 1958,\\np. 94)\\nNot only did the pigeon learn a behavior that is unusual for pigeons, it learned quickly\\nthrough an interactive process in which its behavior and the reinforcement contingencies\\nchanged in response to each other. Skinner compared the process of altering reinforcement\\ncontingencies to the work of a sculptor shaping clay into a desired form. Shaping is a\\npowerful technique for computational reinforcement learning systems as well. When it is\\ndi\\x00cult for an agent to receive any non-zero reward signal at all, either due to sparseness\\nof rewarding situations or their inaccessibility given initial behavior, starting with an\\neasier problem and incrementally increasing its di\\x00culty as the agent learns can be an\\ne↵ective, and sometimes indispensable, strategy.\\nA concept from psychology that is especially relevant in the context of instrumental\\nconditioning is motivation, which refers to processes that inﬂuence the direction and\\nstrength, or vigor, of behavior. Thorndike’s cats, for example, were motivated to escape\\nfrom puzzle boxes because they wanted the food that was sitting just outside. Obtaining\\nthis goal was rewarding to them and reinforced the actions allowing them to escape. It\\nis di\\x00cult to link the concept of motivation, which has many dimensions, in a precise'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 382, 'page_label': '383'}, page_content='14.4. Delayed Reinforcement 361\\nway to reinforcement learning’s computational perspective, but there are clear links with\\nsome of its dimensions.\\nIn one sense, a reinforcement learning agent’s reward signal is at the base of its\\nmotivation: the agent is motivated to maximize the total reward it receives over the long\\nrun. A key facet of motivation, then, is what makes an agent’s experience rewarding. In\\nreinforcement learning, reward signals depend on the state of the reinforcement learning\\nagent’s environment and the agent’s actions. Further, as pointed out in Chapter 1, the\\nstate of the agent’s environment not only includes information about what is external to\\nthe machine, like an organism or a robot, that houses the agent, but also what is internal\\nto this machine. Some internal state components correspond to what psychologists call\\nan animal’smotivational state, which inﬂuences what is rewarding to the animal. For\\nexample, an animal will be more rewarded by eating when it is hungry than when it has\\njust ﬁnished a satisfying meal. The concept of state dependence is broad enough to allow\\nfor many types of modulating inﬂuences on the generation of reward signals.\\nValue functions provide a further link to psychologists’ concept of motivation. If the\\nmost basic motive for selecting an action is to obtain as much reward as possible, for a\\nreinforcement learning agent that selects actions using a value function, a more proximal\\nmotive is toascend the gradient of its value function, that is, to select actions expected\\nto lead to the most highly-valued next states (or what is essentially the same thing, to\\nselect actions with the greatest action-values). For these agents, value functions are the\\nmain driving force determining the direction of their behavior.\\nAnother dimension of motivation is that an animal’s motivational state not only\\ninﬂuences learning, but also inﬂuences the strength, or vigor, of the animal’s behavior\\nafter learning. For example, after learning to ﬁnd food in the goal box of a maze, a hungry\\nrat will run faster to the goal box than one that is not hungry. This aspect of motivation\\ndoes not link so cleanly to the reinforcement learning framework we present here, but\\nin the Bibliographical and Historical Remarks section at the end of this chapter we cite\\nseveral publications that propose theories of behavioral vigor based on reinforcement\\nlearning.\\nWe turn now to the subject of learning when reinforcing stimuli occur well after the\\nevents they reinforce. The mechanisms used by reinforcement learning algorithms to\\nenable learning with delayed reinforcement—eligibility traces and TD learning—closely\\ncorrespond to psychologists’ hypotheses about how animals can learn under these condi-\\ntions.\\n14.4 Delayed Reinforcement\\nThe Law of E↵ect requires a backward e↵ect on connections, and some early critics of the\\nlaw could not conceive of how the present could a↵ect something that was in the past. This\\nconcern was ampliﬁed by the fact that learning can even occur when there is a considerable\\ndelay between an action and the consequent reward or penalty. Similarly, in classical\\nconditioning, learning can occur when US onset follows CS o↵set by a non-negligible time\\ninterval. We call this the problem of delayed reinforcement, which is related to what\\nMinsky (1961) called the “credit-assignment problem for learning systems”: how do you'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 383, 'page_label': '384'}, page_content='362 Chapter 14: Psychology\\ndistribute credit for success among the many decisions that may have been involved in\\nproducing it? The reinforcement learning algorithms presented in this book include two\\nbasic mechanisms for addressing this problem. The ﬁrst is the use of eligibility traces,\\nand the second is the use of TD methods to learn value functions that provide nearly\\nimmediate evaluations of actions (in tasks like instrumental conditioning experiments) or\\nthat provide immediate prediction targets (in tasks like classical conditioning experiments).\\nBoth of these methods correspond to similar mechanisms proposed in theories of animal\\nlearning.\\nPavlov (1927) pointed out that every stimulus must leave a trace in the nervous system\\nthat persists for some time after the stimulus ends, and he proposed that stimulus traces\\nmake learning possible when there is a temporal gap between the CS o↵set and the\\nUS onset. To this day, conditioning under these conditions is calledtrace conditioning\\n(page 344). Assuming a trace of the CS remains when the US arrives, learning occurs\\nthrough the simultaneous presence of the trace and the US. We discuss some proposals\\nfor trace mechanisms in the nervous system in Chapter 15.\\nStimulus traces were also proposed as a means for bridging the time interval between\\nactions and consequent rewards or penalties in instrumental conditioning. In Hull’s\\ninﬂuential learning theory, for example, “molar stimulus traces” accounted for what\\nhe called an animal’sgoal gradient, a description of how the maximum strength of an\\ninstrumentally-conditioned response decreases with increasing delay of reinforcement\\n(Hull, 1932, 1943). Hull hypothesized that an animal’s actions leave internal stimuli whose\\ntraces decay exponentially as functions of time since an action was taken. Looking at the\\nanimal learning data available at the time, he hypothesized that the traces e↵ectively\\nreach zero after 30 to 40 seconds.\\nThe eligibility traces used in the algorithms described in this book are like Hull’s\\ntraces: they are decaying traces of past state visitations, or of past state–action pairs.\\nEligibility traces were introduced by Klopf (1972) in his neuronal theory in which they\\nare temporally-extended traces of past activity at synapses, the connections between\\nneurons. Klopf’s traces are more complex than the exponentially-decaying traces our\\nalgorithms use, and we discuss this more when we take up his theory in Section 15.9.\\nTo account for goal gradients that extend over longer time periods than spanned\\nby stimulus traces, Hull (1943) proposed that longer gradients result from conditioned\\nreinforcement passing backwards from the goal, a process acting in conjunction with\\nhis molar stimulus traces. Animal experiments showed that if conditions favor the\\ndevelopment of conditioned reinforcement during a delay period, learning does not\\ndecrease with increased delay as much as it does under conditions that obstruct secondary\\nreinforcement. Conditioned reinforcement is favored if there are stimuli that regularly\\noccur during the delay interval. Then it is as if reward is not actually delayed because\\nthere is more immediate conditioned reinforcement. Hull therefore envisioned that there\\nis a primary gradient based on the delay of the primary reinforcement mediated by\\nstimulus traces, and that this is progressively modiﬁed, and lengthened, by conditioned\\nreinforcement.\\nAlgorithms presented in this book that use both eligibility traces and value functions\\nto enable learning with delayed reinforcement correspond to Hull’s hypothesis about how\\nanimals are able to learn under these conditions. The actor–critic architecture discussed'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 384, 'page_label': '385'}, page_content='14.5. Cognitive Maps 363\\nin Sections 13.5, 15.7, and 15.8 illustrates this correspondence most clearly. The critic uses\\na TD algorithm to learn a value function associated with the system’s current behavior,\\nthat is, to predict the current policy’s return. The actor updates the current policy based\\non the critic’s predictions, or more exactly, on changes in the critic’s predictions. The\\nTD error produced by the critic acts as a conditioned reinforcement signal for the actor,\\nproviding an immediate evaluation of performance even when the primary reward signal\\nitself is considerably delayed. Algorithms that estimate action-value functions, such as\\nQ-learning and Sarsa, similarly use TD learning principles to enable learning with delayed\\nreinforcement by means of conditioned reinforcement. The close parallel between TD\\nlearning and the activity of dopamine producing neurons that we discuss in Chapter 15\\nlends additional support to links between reinforcement learning algorithms and this\\naspect of Hull’s learning theory.\\n14.5 Cognitive Maps\\nModel-based reinforcement learning algorithms use environment models that have elements\\nin common with what psychologists callcognitive maps. Recall from our discussion of\\nplanning and learning in Chapter 8 that by an environment model we mean anything\\nan agent can use to predict how its environment will respond to its actions in terms of\\nstate transitions and rewards, and by planning we mean any process that computes a\\npolicy from such a model. Environment models consist of two parts: the state-transition\\npart encodes knowledge about the e↵ect of actions on state changes, and the reward-\\nmodel part encodes knowledge about the reward signals expected for each state or each\\nstate–action pair. A model-based algorithm selects actions by using a model to predict\\nthe consequences of possible courses of action in terms of future states and the reward\\nsignals expected to arise from those states. The simplest kind of planning is to compare\\nthe predicted consequences of collections of “imagined” sequences of decisions.\\nQuestions about whether or not animals use environment models, and if so, what are the\\nmodels like and how are they learned, have played inﬂuential roles in the history of animal\\nlearning research. Some researchers challenged the then-prevailing stimulus-response\\n(S–R) view of learning and behavior, which corresponds to the simplest model-free way\\nof learning policies, by demonstratinglatent learning. In the earliest latent learning\\nexperiment, two groups of rats were run in a maze. For the experimental group, there\\nwas no reward during the ﬁrst stage of the experiment, but food was suddenly introduced\\ninto the goal box of the maze at the start of the second stage. For the control group, food\\nwas in the goal box throughout both stages. The question was whether or not rats in the\\nexperimental group would have learned anything during the ﬁrst stage in the absence\\nof food reward. Although the experimental rats did notappear to learn much during\\nthe ﬁrst, unrewarded, stage, as soon as they discovered the food that was introduced\\nin the second stage, they rapidly caught up with the rats in the control group. It was\\nconcluded that “during the non-reward period, the rats [in the experimental group] were\\ndeveloping a latent learning of the maze which they were able to utilize as soon as reward\\nwas introduced” (Blodgett, 1929).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 385, 'page_label': '386'}, page_content='364 Chapter 14: Psychology\\nLatent learning is most closely associated with the psychologist Edward Tolman, who\\ninterpreted this result, and others like it, as showing that animals could learn a “cognitive\\nmap of the environment” in the absence of rewards or penalties, and that they could use\\nthe map later when they were motivated to reach a goal (Tolman, 1948). A cognitive map\\ncould also allow a rat to plan a route to the goal that was di↵erent from the route the rat\\nhad used in its initial exploration. Explanations of results like these led to the enduring\\ncontroversy lying at the heart of the behaviorist/cognitive dichotomy in psychology. In\\nmodern terms, cognitive maps are not restricted to models of spatial layouts but are\\nmore generally environment models, or models of an animal’s “task space” (e.g., Wilson,\\nTakahashi, Schoenbaum, and Niv, 2014). The cognitive map explanation of latent learning\\nexperiments is analogous to the claim that animals use model-based algorithms, and that\\nenvironment models can be learned even without explicit rewards or penalties. Models\\nare then used for planning when the animal is motivated by the appearance of rewards or\\npenalties.\\nTolman’s account of how animals learn cognitive maps was that they learn stimulus-\\nstimulus, or S–S, associations by experiencing successions of stimuli as they explore an\\nenvironment. In psychology this is calledexpectancy theory: given S–S associations, the\\noccurrence of a stimulus generates an expectation about the stimulus to come next. This\\nis much like what control engineers callsystem identiﬁcation, in which a model of a\\nsystem with unknown dynamics is learned from labeled training examples. In the simplest\\ndiscrete-time versions, training examples are S–S0 pairs, where S is a state and S0,t h e\\nsubsequent state, is the label. When S is observed, the model creates the “expectation”\\nthat S0 will be observed next. Models more useful for planning involve actions as well,\\nso that examples look like SA–S0,w h e r eS0 is expected when action A is executed in\\nstate S. It is also useful to learn how the environment generates rewards. In this case,\\nexamples are of the form S–R or SA–R,w h e r eR is a reward signal associated with S or\\nthe SA pair. These are all forms of supervised learning by which an agent can acquire\\ncognitive-like maps whether or not it receives any non-zero reward signals while exploring\\nits environment.\\n14.6 Habitual and Goal-directed Behavior\\nThe distinction between model-free and model-based reinforcement learning algorithms\\ncorresponds to the distinction psychologists make betweenhabitual and goal-directed\\ncontrol of learned behavioral patterns. Habits are behavior patterns triggered by appro-\\npriate stimuli and then performed more-or-less automatically. Goal-directed behavior,\\naccording to how psychologists use the phrase, is purposeful in the sense that it is con-\\ntrolled by knowledge of the value of goals and the relationship between actions and their\\nconsequences. Habits are sometimes said to be controlled by antecedent stimuli, whereas\\ngoal-directed behavior is said to be controlled by its consequences (Dickinson, 1980,\\n1985). Goal-directed control has the advantage that it can rapidly change an animal’s\\nbehavior when the environment changes its way of reacting to the animal’s actions. While\\nhabitual behavior responds quickly to input from an accustomed environment, it is unable'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 386, 'page_label': '387'}, page_content='14.6. Habitual and Goal-directed Behavior 365\\nto quickly adjust to changes in the environment. The development of goal-directed\\nbehavioral control was likely a major advance in the evolution of animal intelligence.\\nFigure 14.5 illustrates the di↵erence between model-free and model-based decision\\nstrategies in a hypothetical task in which a rat has to navigate a maze that has distinctive\\ngoal boxes, each delivering an associated reward of the magnitude shown (Figure 14.5\\ntop). Starting at S1, the rat has to ﬁrst select left (L) or right (R) and then has to select\\nL or R again at S2 or S3 to reach one of the goal boxes. The goal boxes are the terminal\\nstates of each episode of the rat’s episodic task. A model-free strategy (Figure 14.5 lower\\nleft) relies on stored values for state–action pairs. These action values are estimates of\\nthe highest return the rat can expect for each action taken from each (nonterminal) state.\\nThey are obtained over many trials of running the maze from start to ﬁnish. When the\\naction values have become good enough estimates of the optimal returns, the rat just has\\nS1, L\\nS2, L\\nS2, R\\nS3, L\\nS3, R\\nS1, R\\n4\\n3\\n0\\n4\\n3\\n2\\nModel-Free\\n= 4\\n= 0\\n= 2\\n= 3\\nReward\\nModel-Based\\nS1\\nL\\nRS2\\nS3 L\\nR\\nL\\nR\\n(4)S2 S3\\nS1\\n40 2 3\\nFigure 14.5: Model-based and model-free strategies to solve a hypothetical sequential action-\\nselection problem. Top: a rat navigates a maze with distinctive goal boxes, each associated\\nwith a reward having the value shown. Lower left: a model-free strategy relies on stored action\\nvalues for all the state–action pairs obtained over many learning trials. To make decisions the\\nrat just has to select at each state the action with the largest action value for that state. Lower\\nright: in a model-based strategy, the rat learns an environment model, consisting of knowledge\\nof state–action-next-state transitions and a reward model consisting of knowledge of the reward\\nassociated with each distinctive goal box. The rat can decide which way to turn at each state\\nby using the model to simulate sequences of action choices to ﬁnd a path yielding the highest\\nreturn. Adapted fromTrends in Cognitive Science, volume 10, number 8, Y. Niv, D. Joel, and P.\\nDayan, A Normative Perspective on Motivation, p. 376, 2006, with permission from Elsevier.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 387, 'page_label': '388'}, page_content='366 Chapter 14: Psychology\\nto select at each state the action with the largest action value in order to make optimal\\ndecisions. In this case, when the action-value estimates become accurate enough, the\\nrat selects L from S1 and R from S2 to obtain the maximum return of 4. A di↵erent\\nmodel-free strategy might simply rely on a cached policy instead of action values, making\\ndirect links from S1 to L and from S2 to R. In neither of these strategies do decisions\\nrely on an environment model. There is no need to consult a state-transition model, and\\nno connection is required between the features of the goal boxes and the rewards they\\ndeliver.\\nFigure 14.5 (lower right) illustrates a model-based strategy. It uses an environment\\nmodel consisting of a state-transition model and a reward model. The state-transition\\nmodel is shown as a decision tree, and the reward model associates the distinctive features\\nof the goal boxes with the rewards to be found in each. (The rewards associated with\\nstates S1,S 2, and S3 are also part of the reward model, but here they are zero and are\\nnot shown.) A model-based agent can decide which way to turn at each state by using\\nthe model to simulate sequences of action choices to ﬁnd a path yielding the highest\\nreturn. In this case the return is the reward obtained from the outcome at the end of\\nthe path. Here, with a su\\x00ciently accurate model, the rat would select L and then R\\nto obtain a return of 4. Comparing the predicted returns of simulated paths is a simple\\nform of planning, which can be done in a variety of ways as discussed in Chapter 8.\\nWhen the environment of a model-free agent changes the way it reacts to the agent’s\\nactions, the agent has to acquire new experience in the changed environment during\\nwhich it can update its policy and/or value function. In the model-free strategy shown\\nin Figure 14.5 (lower left), for example, if one of the goal boxes were to somehow shift\\nto delivering a di↵erent reward, the rat would have to traverse the maze, possibly many\\ntimes, to experience the new reward upon reaching that goal box, all the while updating\\neither its policy or its action-value function (or both) based on this experience. The key\\npoint is that for a model-free agent to change the action its policy speciﬁes for a state, or\\nto change an action value associated with a state, it has to move to that state, act from\\nit, possibly many times, and experience the consequences of its actions.\\nA model-based agent can accommodate changes in its environment without this kind\\nof ‘personal experience’ with the states and actions a↵ected by the change. A change in\\nits model automatically (through planning) changes its policy. Planning can determine\\nthe consequences of changes in the environment that have never been linked together in\\nthe agent’s own experience. For example, again referring to the maze task of Figure 14.5,\\nimagine that a rat with a previously learned transition and reward model is placed directly\\nin the goal box to the right of S2 to ﬁnd that the reward available there now has value 1\\ninstead of 4. The rat’s reward model will change even though the action choices required\\nto ﬁnd that goal box in the maze were not involved. The planning process will bring\\nknowledge of the new reward to bear on maze running without the need for additional\\nexperience in the maze; in this case changing the policy to right turns at both S1 and S3\\nto obtain a return of 3.\\nExactly this logic is the basis ofoutcome-devaluation experimentswith animals. Results\\nfrom these experiments provide insight into whether an animal has learned a habit or if\\nits behavior is under goal-directed control. Outcome-devaluation experiments are like\\nlatent-learning experiments in that the reward changes from one stage to the next. After'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 388, 'page_label': '389'}, page_content='14.6. Habitual and Goal-directed Behavior 367\\nan initial rewarded stage of learning, the reward value of an outcome is changed, including\\nbeing shifted to zero or even to a negative value.\\nAn early important experiment of this type was conducted by Adams and Dickinson\\n(1981). They trained rats via instrumental conditioning until the rats energetically pressed\\na lever for sucrose pellets in a training chamber. The rats were then placed in the same\\nchamber with the lever retracted and allowed non-contingent food, meaning that pellets\\nwere made available to them independently of their actions. After 15-minutes of this\\nfree-access to the pellets, rats in one group were injected with the nausea-inducing poison\\nlithium chloride. This was repeated for three sessions, in the last of which none of the\\ninjected rats consumed any of the non-contingent pellets, indicating that the reward\\nvalue of the pellets had been decreased—the pellets had been devalued. In the next stage\\ntaking place a day later, the rats were again placed in the chamber and given a session of\\nextinction training, meaning that the response lever was back in place but disconnected\\nfrom the pellet dispenser so that pressing it did not release pellets. The question was\\nwhether the rats that had the reward value of the pellets decreased would lever-press\\nless than rats that did not have the reward value of the pellets decreased, even without\\nexperiencing the devalued reward as a result of lever-pressing. It turned out that the\\ninjected rats had signiﬁcantly lower response rates than the non-injected ratsright from\\nthe start of the extinction trials.\\nAdams and Dickinson concluded that the injected rats associated lever pressing with\\nconsequent nausea by means of a cognitive map linking lever pressing to pellets, and\\npellets to nausea. Hence, in the extinction trials, the rats “knew” that the consequences\\nof pressing the lever would be something they did not want, and so they reduced their\\nlever-pressing right from the start. The important point is that they reduced lever-pressing\\nwithout ever having experienced lever-pressing directly followed by being sick: no lever\\nwas present when they were made sick. They seemed able to combine knowledge of the\\noutcome of a behavioral choice (pressing the lever will be followed by getting a pellet)\\nwith the reward value of the outcome (pellets are to be avoided) and hence could alter\\ntheir behavior accordingly. Not every psychologist agrees with this “cognitive” account\\nof this kind of experiment, and it is not the only possible way to explain these results,\\nbut the model-based planning explanation is widely accepted.\\nNothing prevents an agent from using both model-free and model-based algorithms, and\\nthere are good reasons for using both. We know from our own experience that with enough\\nrepetition, goal-directed behavior tends to turn into habitual behavior. Experiments show\\nthat this happens for rats too. Adams (1982) conducted an experiment to see if extended\\ntraining would convert goal-directed behavior into habitual behavior. He did this by\\ncomparing the e↵ect of outcome devaluation on rats that experienced di↵erent amounts\\nof training. If extended training made the rats less sensitive to devaluation compared to\\nrats that received less training, this would be evidence that extended training made the\\nbehavior more habitual. Adams’ experiment closely followed the Adams and Dickinson\\n(1981) experiment just described. Simplifying a bit, rats in one group were trained until\\nthey made 100 rewarded lever-presses, and rats in the other group—the overtrained\\ngroup—were trained until they made 500 rewarded lever-presses. After this training,\\nthe reward value of the pellets was decreased (using lithium chloride injections) for rats'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 389, 'page_label': '390'}, page_content='368 Chapter 14: Psychology\\nin both groups. Then both groups of rats were given a session of extinction training.\\nAdams’ question was whether devaluation would e↵ect the rate of lever-pressing for the\\novertrained rats less than it would for the non-overtrained rats, which would be evidence\\nthat extended training reduces sensitivity to outcome devaluation. It turned out that\\ndevaluation strongly decreased the lever-pressing rate of the non-overtrained rats. For\\nthe overtrained rats, in contrast, devaluation had little e↵ect on their lever-pressing; in\\nfact, if anything, it made it more vigorous. (The full experiment included control groups\\nshowing that the di↵erent amounts of training did not by themselves signiﬁcantly e↵ect\\nlever-pressing rates after learning.) This result suggested that while the non-overtrained\\nrats were acting in a goal-directed manner sensitive to their knowledge of the outcome of\\ntheir actions, the overtrained rats had developed a lever-pressing habit.\\nViewing this and other results like it from a computational perspective provides insight\\nas to why one might expect animals to behave habitually in some circumstances but in a\\ngoal-directed way in others, and why they shift from one mode of control to another as\\nthey continue to learn. While animals undoubtedly use algorithms that do not exactly\\nmatch those we have presented in this book, one can gain insight into animal behavior by\\nconsidering the tradeo↵s that various reinforcement learning algorithms imply. An idea\\ndeveloped by computational neuroscientists Daw, Niv, and Dayan (2005) is that animals\\nuse both model-free and model-based processes. Each process proposes an action, and\\nthe action chosen for execution is the one proposed by the process judged to be the more\\ntrustworthy of the two as determined by measures of conﬁdence that are maintained\\nthroughout learning. Early in learning the planning process of a model-based system is\\nmore trustworthy because it chains together short-term predictions which can become\\naccurate with less experience than long-term predictions of the model-free process. But\\nwith continued experience, the model-free process becomes more trustworthy because\\nplanning is prone to making mistakes due to model inaccuracies and short-cuts necessary\\nto make planning feasible, such as various forms of “tree-pruning”: the removal of\\nunpromising search tree branches. According to this idea one would expect a shift from\\ngoal-directed behavior to habitual behavior as more experience accumulates. Other ideas\\nhave been proposed for how animals arbitrate between goal-directed and habitual control,\\nand both behavioral and neuroscience research continues to examine this and related\\nquestions.\\nThe distinction between model-free and model-based algorithms is proving to be useful\\nfor this research. One can examine the computational implications of these types of\\nalgorithms in abstract settings that expose basic advantages and limitations of each\\ntype. This serves both to suggest and to sharpen questions that guide the design\\nof experiments necessary for increasing psychologists’ understanding of habitual and\\ngoal-directed behavioral control.\\n14.7 Summary\\nOur goal in this chapter has been to discuss correspondences between reinforcement\\nlearning and the experimental study of animal learning in psychology. We emphasized\\nat the outset that reinforcement learning as described in this book is not intended'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 390, 'page_label': '391'}, page_content='14.7. Summary 369\\nto model details of animal behavior. It is an abstract computational framework that\\nexplores idealized situations from the perspective of artiﬁcial intelligence and engineering.\\nBut many of the basic reinforcement learning algorithms were inspired by psychological\\ntheories, and in some cases, these algorithms have contributed to the development of\\nnew animal learning models. This chapter described the most conspicuous of these\\ncorrespondences.\\nThe distinction in reinforcement learning between algorithms for prediction and al-\\ngorithms for control parallels animal learning theory’s distinction between classical, or\\nPavlovian, conditioning and instrumental conditioning. The key di↵erence between\\ninstrumental and classical conditioning experiments is that in the former the reinforcing\\nstimulus is contingent upon the animal’s behavior, whereas in the latter it is not. Learning\\nto predict via a TD algorithm corresponds to classical conditioning, and we described\\nthe TD model of classical conditioningas one instance in which reinforcement learning\\nprinciples account for some details of animal learning behavior. This model generalizes\\nthe inﬂuential Rescorla–Wagner model by including the temporal dimension where events\\nwithin individual trials inﬂuence learning, and it provides an account of second-order\\nconditioning, where predictors of reinforcing stimuli become reinforcing themselves. It\\nalso is the basis of an inﬂuential view of the activity of dopamine neurons in the brain,\\nsomething we take up in Chapter 15.\\nLearning by trial and error is at the base of the control aspect of reinforcement learning.\\nWe presented some details about Thorndike’s experiments with cats and other animals that\\nled to hisLaw of E↵ect, which we discussed here and in Chapter 1 (page 15). We pointed\\nout that in reinforcement learning, exploration does not have to be limited to “blind\\ngroping”; trials can be generated by sophisticated methods using innate and previously\\nlearned knowledge as long as there issome exploration. We discussed the training method\\nB. F. Skinner calledshaping in which reward contingencies are progressively altered to\\ntrain an animal to successively approximate a desired behavior. Shaping is not only\\nindispensable for animal training, it is also an e↵ective tool for training reinforcement\\nlearning agents. There is also a connection to the idea of an animal’s motivational state,\\nwhich inﬂuences what an animal will approach or avoid and what events are rewarding\\nor punishing for the animal.\\nThe reinforcement learning algorithms presented in this book include two basic mecha-\\nnisms for addressing the problem of delayed reinforcement: eligibility traces and value\\nfunctions learned via TD algorithms. Both mechanisms have antecedents in theories of\\nanimal learning. Eligibility traces are similar to stimulus traces of early theories, and\\nvalue functions correspond to the role of secondary reinforcement in providing nearly\\nimmediate evaluative feedback.\\nThe next correspondence the chapter addressed is that between reinforcement learning’s\\nenvironment models and what psychologists callcognitive maps. Experiments conducted\\nin the mid 20th century purported to demonstrate the ability of animals to learn cognitive\\nmaps as alternatives to, or as additions to, state–action associations, and later use them\\nto guide behavior, especially when the environment changes unexpectedly. Environment\\nmodels in reinforcement learning are like cognitive maps in that they can be learned by\\nsupervised learning methods without relying on reward signals, and then they can be\\nused later to plan behavior.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 391, 'page_label': '392'}, page_content='370 Chapter 14: Psychology\\nReinforcement learning’s distinction betweenmodel-free and model-based algorithms\\ncorresponds to the distinction in psychology betweenhabitual and goal-directed behavior.\\nModel-free algorithms make decisions by accessing information that has been stored in a\\npolicy or an action-value function, whereas model-based methods select actions as the\\nresult of planning ahead using a model of the agent’s environment. Outcome-devaluation\\nexperiments provide information about whether an animal’s behavior is habitual or under\\ngoal-directed control. Reinforcement learning theory has helped clarify thinking about\\nthese issues.\\nAnimal learning clearly informs reinforcement learning, but as a type of machine\\nlearning, reinforcement learning is directed toward designing and understanding e↵ective\\nlearning algorithms, not toward replicating or explaining details of animal behavior.\\nWe focused on aspects of animal learning that relate in clear ways to methods for\\nsolving prediction and control problems, highlighting the fruitful two-way ﬂow of ideas\\nbetween reinforcement learning and psychology without venturing deeply into many of the\\nbehavioral details and controversies that have occupied the attention of animal learning\\nresearchers. Future development of reinforcement learning theory and algorithms will\\nlikely exploit links to many other features of animal learning as the computational utility\\nof these features becomes better appreciated. We expect that a ﬂow of ideas between\\nreinforcement learning and psychology will continue to bear fruit for both disciplines.\\nMany connections between reinforcement learning and areas of psychology and other\\nbehavioral sciences are beyond the scope of this chapter. We largely omit discussing\\nlinks to the psychology of decision making, which focuses on how actions are selected,\\nor how decisions are made,after learning has taken place. We also do not discuss links\\nto ecological and evolutionary aspects of behavior studied by ethologists and behavioral\\necologists: how animals relate to one another and to their physical surroundings, and how\\ntheir behavior contributes to evolutionary ﬁtness. Optimization, MDPs, and dynamic\\nprogramming ﬁgure prominently in these ﬁelds, and our emphasis on agent interaction\\nwith dynamic environments connects to the study of agent behavior in complex “ecologies.”\\nMulti-agent reinforcement learning, omitted in this book, has connections to social aspects\\nof behavior. Despite the lack of treatment here, reinforcement learning should by no means\\nbe interpreted as dismissing evolutionary perspectives. Nothing about reinforcement\\nlearning implies atabula rasa view of learning and behavior. Indeed, experience with\\nengineering applications has highlighted the importance of building into reinforcement\\nlearning systems knowledge that is analogous to what evolution provides to animals.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 392, 'page_label': '393'}, page_content='14.7. Summary 371\\nBibliographical and Historical Remarks\\nLudvig, Bellemare, and Pearson (2011) and Shah (2012) review reinforcement learning in\\nthe contexts of psychology and neuroscience. These publications are useful companions\\nto this chapter and the following chapter on reinforcement learning and neuroscience.\\n14.1 Dayan, Niv, Seymour, and Daw (2006) focused on interactions between clas-\\nsical and instrumental conditioning, particularly situations where classically-\\nconditioned and instrumental responses are in conﬂict. They proposed a Q-\\nlearning framework for modeling aspects of this interaction. Modayil and Sutton\\n(2014) used a mobile robot to demonstrate the e↵ectiveness of a control method\\ncombining a ﬁxed response with online prediction learning. Calling thisPavlo-\\nvian control, they emphasized that it di↵ers from the usual control methods of\\nreinforcement learning, being based on predictively executing ﬁxed responses\\nand not on reward maximization. The electro-mechanical machine of Ross (1933)\\nand especially the learning version of Walter’s turtle (Walter, 1951) were very\\nearly illustrations of Pavlovian control.\\n14.2.1 Kamin (1968) ﬁrst reported blocking, now commonly known as Kamin blocking,\\nin classical conditioning. Moore and Schmajuk (2008) provide an excellent\\nsummary of the blocking phenomenon, the research it stimulated, and its lasting\\ninﬂuence on animal learning theory. Gibbs, Cool, Land, Kehoe, and Gormezano\\n(1991) describe second-order conditioning of the rabbit’s nictitating membrane\\nresponse and its relationship to conditioning with serial-compound stimuli. Finch\\nand Culler (1934) reported obtaining ﬁfth-order conditioning of a dog’s foreleg\\nwithdrawal “when themotivation of the animal is maintained through the various\\norders.”\\n14.2.2 The idea built into the Rescorla–Wagner model that learning occurs when animals\\nare surprised is derived from Kamin (1969). Models of classical conditioning\\nother than Rescorla and Wagner’s include the models of Klopf (1988), Grossberg\\n(1975), Mackintosh (1975), Moore and Stickney (1980), Pearce and Hall (1980),\\nand Courville, Daw, and Touretzky (2006). Schmajuk (2008) reviews models of\\nclassical conditioning. Wagner (2008) provides a modern psychological perspective\\non the Rescorla-Wagner model and similar elemental theories of learning.\\n14.2.3 An early version of the TD model of classical conditioning appeared in Sutton and\\nBarto (1981a), which also included the early model’s prediction that temporal\\nprimacy overrides blocking, later shown by Kehoe, Schreurs, and Graham (1987)\\nto occur in the rabbit nictitating membrane preparation. Sutton and Barto\\n(1981a) contains the earliest recognition of the near identity between the Rescorla–\\nWagner model and the Least-Mean-Square (LMS), or Widrow-Ho↵, learning\\nrule (Widrow and Ho↵, 1960). This early model was revised following Sutton’s\\ndevelopment of the TD algorithm (Sutton, 1984, 1988) and was ﬁrst presented as\\nthe TD model in Sutton and Barto (1987) and more completely in Sutton and\\nBarto (1990), upon which this section is largely based. Additional exploration'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 393, 'page_label': '394'}, page_content='372 Chapter 14: Psychology\\nof the TD model and its possible neural implementation was conducted by\\nMoore and colleagues (Moore, Desmond, Berthier, Blazis, Sutton, and Barto,\\n1986; Moore and Blazis, 1989; Moore, Choi, and Brunzell, 1998; Moore, Marks,\\nCastagna, and Polewan, 2001). Klopf’s (1988) drive-reinforcement theory of\\nclassical conditioning extends the TD model to address additional experimental\\ndetails, such as the S-shape of acquisition curves. In some of these publications\\nTD is taken to mean Time Derivative instead of Temporal Di↵erence.\\n14.2.4 Ludvig, Sutton, and Kehoe (2012) evaluated the performance of the TD model\\nin previously unexplored tasks involving classical conditioning and examined\\nthe inﬂuence of various stimulus representations, including the microstimulus\\nrepresentation that they introduced earlier (Ludvig, Sutton, and Kehoe, 2008).\\nEarlier investigations of the inﬂuence of various stimulus representations and their\\npossible neural implementations on response timing and topography in the context\\nof the TD model are those of Moore and colleagues cited above. Although not in\\nthe context of the TD model, representations like the microstimulus representation\\nof Ludvig et al. (2012) have been proposed and studied by Grossberg and\\nSchmajuk (1989), Brown, Bullock, and Grossberg (1999), Buhusi and Schmajuk\\n(1999), and Machado (1997). The ﬁgures on pages 353–355 are adapted from\\nSutton and Barto (1990).\\n14.3 Section 1.7 includes comments on the history of trial-and-error learning and\\nthe Law of E↵ect. The idea that Thorndike’s cats might have been exploring\\naccording to an instinctual context-speciﬁc ordering over actions rather than by\\njust selecting from a set of instinctual impulses was suggested by Peter Dayan\\n(personal communication). Selfridge, Sutton, and Barto (1985) illustrated the\\ne↵ectiveness of shaping in a pole-balancing reinforcement learning task. Other\\nexamples of shaping in reinforcement learning are Gullapalli and Barto (1992),\\nMahadevan and Connell (1992), Mataric (1994), Dorigo and Colombette (1994),\\nSaksida, Raymond, and Touretzky (1997), and Randløv and Alstrøm (1998). Ng\\n(2003) and Ng, Harada, and Russell (1999) used the term shaping in a sense\\nsomewhat di↵erent from Skinner’s, focusing on the problem of how to alter the\\nreward signal without altering the set of optimal policies.\\nDickinson and Balleine (2002) discuss the complexity of the interaction between\\nlearning and motivation. Wise (2004) provides an overview of reinforcement\\nlearning and its relation to motivation. Daw and Shohamy (2008) link motivation\\nand learning to aspects of reinforcement learning theory. See also McClure,\\nDaw, and Montague (2003), Niv, Joel, and Dayan (2006), Rangel, Camerer, and\\nMontague (2008), and Dayan and Berridge (2014). McClure et al. (2003), Niv,\\nDaw, and Dayan (2006), and Niv, Daw, Joel, and Dayan (2007) present theories\\nof behavioral vigor related to the reinforcement learning framework.\\n14.4 Spence, Hull’s student and collaborator at Yale, elaborated the role of higher-\\norder reinforcement in addressing the problem of delayed reinforcement (Spence,\\n1947). Learning over very long delays, as in taste-aversion conditioning with'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 394, 'page_label': '395'}, page_content='14.7. Summary 373\\ndelays up to several hours, led to interference theories as alternatives to decaying-\\ntrace theories (e.g., Revusky and Garcia, 1970; Boakes and Costa, 2014). Other\\nviews of learning under delayed reinforcement invoke roles for awareness and\\nworking memory (e.g., Clark and Squire, 1998; Seo, Barraclough, and Lee, 2007).\\n14.5 Thistlethwaite (1951) provides an extensive review of latent learning experiments\\nup to the time of its publication. Ljung (1998) provides an overview of model\\nlearning, or system identiﬁcation, techniques in engineering. Gopnik, Glymour,\\nSobel, Schulz, Kushnir, and Danks (2004) present a Bayesian theory about how\\nchildren learn models.\\n14.6 Connections between habitual and goal-directed behavior and model-free and\\nmodel-based reinforcement learning were ﬁrst proposed by Daw, Niv, and Dayan\\n(2005). The hypothetical maze task used to explain habitual and goal-directed\\nbehavioral control is based on the explanation of Niv, Joel, and Dayan (2006).\\nDolan and Dayan (2013) review four generations of experimental research related\\nto this issue and discuss how it can move forward on the basis of reinforcement\\nlearning’s model-free/model-based distinction. Dickinson (1980, 1985) and Dick-\\ninson and Balleine (2002) discuss experimental evidence related to this distinction.\\nDonahoe and Burgos (2000) alternatively argue that model-free processes can\\naccount for the results of outcome-devaluation experiments. Dayan and Berridge\\n(2014) argue that classical conditioning involves model-based processes. Rangel,\\nCamerer, and Montague (2008) review many of the outstanding issues involving\\nhabitual, goal-directed, and Pavlovian modes of control.\\nComments on Terminology— The traditional meaning ofreinforcement in psychol-\\nogy is the strengthening of a pattern of behavior (by increasing either its intensity or\\nfrequency) as a result of an animal receiving a stimulus (or experiencing the omission\\nof a stimulus) in an appropriate temporal relationship with another stimulus or with a\\nresponse. Reinforcement produces changes that remain in future behavior. Sometimes in\\npsychology reinforcement refers to the process of producing lasting changes in behavior,\\nwhether the changes strengthen or weaken a behavior pattern (Mackintosh, 1983). Letting\\nreinforcement refer to weakening in addition to strengthening is at odds with the everyday\\nmeaning of reinforce, and its traditional use in psychology, but it is a useful extension\\nthat we have adopted here. In either case, a stimulus considered to be the cause of the\\nbehavioral change is called areinforcer.\\nPsychologists do not generally use the speciﬁc phrasereinforcement learningas we\\ndo. Animal learning pioneers probably regarded reinforcement and learning as being\\nsynonymous, so it would be redundant to use both words. Our use of the phrase follows\\nits use in computational and engineering research, inﬂuenced mostly by Minsky (1961).\\nBut the phrase is lately gaining currency in psychology and neuroscience, likely because\\nstrong parallels have surfaced between reinforcement learning algorithms and animal\\nlearning—parallels described in this chapter and the next.\\nAccording to common usage, areward is an object or event that an animal will\\napproach and work for. A reward may be given to an animal in recognition of its ‘good’'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 395, 'page_label': '396'}, page_content='374 Chapter 14: Psychology\\nbehavior, or given in order to make the animal’s behavior ‘better.’ Similarly, apenalty is\\nan object or event that the animal usually avoids and that is given as a consequence of\\n‘bad’ behavior, usually in order to change that behavior.Primary reward is reward due\\nto machinery built into an animal’s nervous system by evolution to improve its chances\\nof survival and reproduction, for example, reward produced by the taste of nourishing\\nfood, sexual contact, successful escape, and many other stimuli and events that predicted\\nreproductive success over the animal’s ancestral history. As explained in Section 14.2.1,\\nhigher-order rewardis reward delivered by stimuli that predict primary reward, either\\ndirectly or indirectly by predicting other stimuli that predict primary reward. Reward is\\nsecondary if its rewarding quality is the result of directly predicting primary reward.\\nIn this book we callRt the ‘reward signal at timet,’ or sometimes just the ‘reward\\nat time t,’ but we do not think of it as an object or event in the agent’s environment.\\nBecause Rt is a number—not an object or an event—it is more like a reward signal in\\nneuroscience, which is a signal internal to the brain, like the activity of neurons, that\\ninﬂuences decision making and learning. This signal might be triggered when the animal\\nperceives an attractive (or an aversive) object, but it can also be triggered by things that\\ndo not physically exist in the animal’s external environment, such as memories, ideas, or\\nhallucinations. Because ourRt can be positive, negative, or zero, it might be better to\\ncall a negativeRt a penalty, and anRt equal to zero a neutral signal, but for simplicity\\nwe generally avoid these terms.\\nIn reinforcement learning, the process that generates all theRts deﬁnes the problem\\nthe agent is trying to solve. The agent’s objective is to keep the magnitude ofRt as large\\nas possible over time. In this respect,Rt is like primary reward for an animal if we think\\nof the problem the animal faces as the problem of obtaining as much primary reward as\\npossible over its lifetime (and thereby, through the prospective “wisdom” of evolution,\\nimprove its chances of solving its real problem, which is to pass its genes on to future\\ngenerations). However, as we suggest in Chapter 15, it is unlikely that there is a single\\n“master” reward signal likeRt in an animal’s brain.\\nNot all reinforcers are rewards or penalties. Sometimes reinforcement is not the result\\nof an animal receiving a stimulus that evaluates its behavior by labeling the behavior\\ngood or bad. A behavior pattern can be reinforced by a stimulus that arrives to an animal\\nno matter how the animal behaved. As described in Section 14.1, whether the delivery of\\nreinforcement depends, or does not depend, on preceding behavior is the deﬁning di↵erence\\nbetween instrumental, or operant, conditioning experiments and classical, or Pavlovian,\\nconditioning experiments. Reinforcement is at work in both types of experiments, but\\nonly in the former is it feedback that evaluates past behavior. (Though it has often been\\npointed out that even when the reinforcing US in a classical conditioning experiment is\\nnot contingent on the subject’s preceding behavior, its reinforcing value can be inﬂuenced\\nby this behavior, an example being that a closed eye makes an air pu↵ to the eye less\\naversive.)\\nThe distinction between reward signals and reinforcement signals is a crucial point\\nwhen we discuss neural correlates of these signals in the next chapter. Like a reward signal,\\nfor us, the reinforcement signal at any speciﬁc time is a positive or negative number, or\\nzero. A reinforcement signal is the major factor directing changes a learning algorithm'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 396, 'page_label': '397'}, page_content='14.7. Summary 375\\nmakes in an agent’s policy, value estimates, or environment models. The deﬁnition that\\nmakes the most sense to us is that a reinforcement signal at any time is a number that\\nmultiplies (possibly along with some constants) a vector to determine parameter updates\\nin some learning algorithm.\\nFor some algorithms, the reward signal alone is the critical multiplier in the parameter-\\nupdate equation. For these algorithms the reinforcement signal is the same as the\\nreward signal. But for most of the algorithms we discuss in this book, reinforcement\\nsignals include terms in addition to the reward signal, an example being a TD error\\n\\x00t = Rt+1 + \\x00V (St+1) \\x00 V (St), which is the reinforcement signal for TD state-value\\nlearning (and analogous TD errors for action-value learning). In this reinforcement signal,\\nRt+1 is the primary reinforcementcontribution, and the temporal di↵erence in predicted\\nvalues, \\x00V (St+1) \\x00 V (St) (or an analogous temporal di↵erence for action values), is\\nthe conditioned reinforcementcontribution. Thus, whenever\\x00V (St+1) \\x00 V (St) = 0,\\x00t\\nsignals ‘pure’ primary reinforcement; and wheneverRt+1 = 0, it signals ‘pure’ conditioned\\nreinforcement, but it often signals a mixture of these. Note as we mentioned in Section 6.1,\\nthis \\x00t is not available until timet + 1. We therefore think of\\x00t as the reinforcement\\nsignal at timet + 1, which is ﬁtting because it reinforces predictions and/or actions made\\nearlier at stept.\\nA possible source of confusion is the terminology used by the famous psychologist\\nB. F. Skinner and his followers. For Skinner, positive reinforcement occurs when the\\nconsequences of an animal’s behavior increase the frequency of that behavior; punishment\\noccurs when the behavior’s consequences decrease that behavior’s frequency. Negative\\nreinforcement occurs when behavior leads to the removal of an aversive stimulus (that is,\\na stimulus the animal does not like), thereby increasing the frequency of that behavior.\\nNegative punishment, on the other hand, occurs when behavior leads to the removal of an\\nappetitive stimulus (that is, a stimulus the animal likes), thereby decreasing the frequency\\nof that behavior. We ﬁnd no critical need for these distinctions because our approach\\nis more abstract than this, with both reward and reinforcement signals allowed to take\\non both positive and negative values. (But note especially that when our reinforcement\\nsignal is negative, it is not the same as Skinner’s negative reinforcement.)\\nOn the other hand, it has often been pointed out that using a single number as a\\nreward or a penalty signal, depending only on its sign, is at odds with the fact that\\nanimals’ appetitive and aversive systems have qualitatively di↵erent properties and\\ninvolve di↵erent brain mechanisms. This points to a direction in which the reinforcement\\nlearning framework might be developed in the future to exploit computational advantages\\nof separate appetitive and aversive systems, but for now we are passing over these\\npossibilities.\\nAnother discrepancy in terminology is how we use the wordaction. To many cognitive\\nscientists, an action is purposeful in the sense of being the result of an animal’s knowledge\\nabout the relationship between the behavior in question and the consequences of that\\nbehavior. An action is goal-directed and the result of a decision, whereas a response\\nis triggered by a stimulus and is the result of a reﬂex or a habit. We use the word\\naction without di↵erentiating among what others call actions, decisions, and responses.\\nThese are important distinctions, but for us they are encompassed by di↵erences between'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 397, 'page_label': '398'}, page_content='376 Chapter 14: Psychology\\nmodel-free and model-based reinforcement learning algorithms, which we discussed above\\nin relation to habitual and goal-directed behavior in Section 14.6. Dickinson (1985)\\ndiscusses the distinction between responses and actions.\\nA term used a lot in this book iscontrol. What we mean by control is entirely di↵erent\\nfrom what it means to animal learning psychologists. By control we mean that an agent\\ninﬂuences its environment to bring about states or events that the agent prefers: the agent\\nexerts control over its environment. This is the sense of control used by control engineers.\\nIn psychology, on the other hand, control typically means that an animal’s behavior is\\ninﬂuenced by—is controlled by—the stimuli the animal receives (stimulus control) or\\nthe reinforcement schedule it experiences. Here the environment is controlling the agent.\\nControl in this sense is the basis of behavior modiﬁcation therapy. Of course, both of\\nthese directions of control are at play when an agent interacts with its environment,\\nbut our focus is on the agent as controller, not the environment as controller. A view\\nequivalent to ours, and perhaps more illuminating, is that the agent is actually controlling\\nthe input it receives from its environment (Powers, 1973). This isnot what psychologists\\nmean by stimulus control.\\nSometimes reinforcement learning is understood to refer solely to learning policies\\ndirectly from rewards (and penalties) without the involvement of value functions or\\nenvironment models. This is what psychologists call stimulus-response, or S-R, learning.\\nBut for us, along with most of today’s psychologists, reinforcement learning is much\\nbroader than this, including in addition to S-R learning, methods involving value functions,\\nenvironment models, planning, and other processes that are commonly thought to belong\\nto the more cognitive side of mental functioning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 398, 'page_label': '399'}, page_content='Chapter 15\\nNeuroscience\\nNeuroscience is the multidisciplinary study of nervous systems: how they regulate bodily\\nfunctions; control behavior; change over time as a result of development, learning, and\\naging; and how cellular and molecular mechanisms make these functions possible. One\\nof the most exciting aspects of reinforcement learning is the mounting evidence from\\nneuroscience that the nervous systems of humans and many other animals implement\\nalgorithms that correspond in striking ways to reinforcement learning algorithms. The\\nmain objective of this chapter is to explain these parallels and what they suggest about\\nthe neural basis of reward-related learning in animals.\\nThe most remarkable point of contact between reinforcement learning and neuroscience\\ninvolves dopamine, a chemical deeply involved in reward processing in the brains of mam-\\nmals. Dopamine appears to convey temporal-di↵erence (TD) errors to brain structures\\nwhere learning and decision making take place. This parallel is expressed by thereward\\nprediction error hypothesis of dopamine neuron activity, a hypothesis that resulted from\\nthe convergence of computational reinforcement learning and results of neuroscience\\nexperiments. In this chapter we discuss this hypothesis, the neuroscience ﬁndings that\\nled to it, and why it is a signiﬁcant contribution to understanding brain reward systems.\\nWe also discuss parallels between reinforcement learning and neuroscience that are less\\nstriking than this dopamine/TD-error parallel but that provide useful conceptual tools\\nfor thinking about reward-based learning in animals. Other elements of reinforcement\\nlearning have the potential to impact the study of nervous systems, but their connections\\nto neuroscience are still relatively undeveloped. We discuss several of these evolving\\nconnections that we think will grow in importance over time.\\nAs we outlined in the history section of this book’s introductory chapter (Section 1.7),\\nmany aspects of reinforcement learning were inﬂuenced by neuroscience. A second\\nobjective of this chapter is to acquaint readers with ideas about brain function that have\\ncontributed to our approach to reinforcement learning. Some elements of reinforcement\\nlearning are easier to understand when seen in light of theories of brain function. This\\nis particularly true for the idea of the eligibility trace, one of the basic mechanisms\\nof reinforcement learning, that originated as a conjectured property of synapses, the\\nstructures by which nerve cells—neurons—communicate with one another.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 399, 'page_label': '400'}, page_content='378 Chapter 15: Neuroscience\\nIn this chapter we do not delve very deeply into the enormous complexity of the neural\\nsystems underlying reward-based learning in animals: this chapter is too short, and we are\\nnot neuroscientists. We do not try to describe—or even to name—the very many brain\\nstructures and pathways, or any of the molecular mechanisms, believed to be involved in\\nthese processes. We also do not do justice to hypotheses and models that are alternatives\\nto those that align so well with reinforcement learning. It should not be surprising that\\nthere are di↵ering views among experts in the ﬁeld. We can only provide a glimpse into\\nthis fascinating and developing story. We hope, though, that this chapter convinces\\nyou that a very fruitful channel has emerged connecting reinforcement learning and its\\ntheoretical underpinnings to the neuroscience of reward-based learning in animals.\\nMany excellent publications cover links between reinforcement learning and neuro-\\nscience, some of which we cite in this chapter’s ﬁnal section. Our treatment di↵ers from\\nmost of these because we assume familiarity with reinforcement learning as presented\\nin the earlier chapters of this book, but we do not assume knowledge of neuroscience.\\nWe begin with a brief introduction to the neuroscience concepts needed for a basic\\nunderstanding of what is to follow.\\n15.1 Neuroscience Basics\\nSome basic information about nervous systems is helpful for following what we cover in\\nthis chapter. Terms that we refer to later are italicized. Skipping this section will not be\\na problem if you already have an elementary knowledge of neuroscience.\\nNeurons, the main components of nervous systems, are cells specialized for processing\\nand transmitting information using electrical and chemical signals. They come in many\\nforms, but a neuron typically has a cell body,dendrites, and a singleaxon.D e n d r i t e s\\nare structures that branch from the cell body to receive input from other neurons (or to\\nalso receive external signals in the case of sensory neurons). A neuron’s axon is a ﬁber\\nthat carries the neuron’s output to other neurons (or to muscles or glands). A neuron’s\\noutput consists of sequences of electrical pulses calledaction potentials that travel along\\nthe axon. Action potentials are also calledspikes, and a neuron is said toﬁre when it\\ngenerates a spike. In models of neural networks it is common to use real numbers to\\nrepresent a neuron’sﬁring rate, the average number of spikes per some unit of time.\\nA neuron’s axon can branch widely so that the neuron’s action potentials reach\\nmany targets. The branching structure of a neuron’s axon is called the neuron’saxonal\\narbor. Because the conduction of an action potential is an active process, not unlike the\\nburning of a fuse, when an action potential reaches an axonal branch point it “lights\\nup” action potentials on all of the outgoing branches (although propagation to a branch\\ncan sometimes fail). As a result, the activity of a neuron with a large axonal arbor can\\ninﬂuence many target sites.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 400, 'page_label': '401'}, page_content='15.1. Neuroscience Basics 379\\nA synapse is a structure generally at the termination of an axon branch that mediates\\nthe communication of one neuron to another. A synapse transmits information from\\nthe presynaptic neuron’s axon to a dendrite or cell body of thepostsynaptic neuron.\\nWith a few exceptions, synapses release a chemicalneurotransmitter upon the arrival\\nof an action potential from the presynaptic neuron. (The exceptions are cases of direct\\nelectric coupling between neurons, but these will not concern us here.) Neurotransmitter\\nmolecules released from the presynaptic side of the synapse di↵use across thesynaptic\\ncleft, the very small space between the presynaptic ending and the postsynaptic neuron,\\nand then bind to receptors on the surface of the postsynaptic neuron to excite or inhibit\\nits spike-generating activity, or to modulate its behavior in other ways. A particular\\nneurotransmitter may bind to several di↵erent types of receptors, with each producing a\\ndi↵erent e↵ect on the postsynaptic neuron. For example, there are at least ﬁve di↵erent\\nreceptor types by which the neurotransmitter dopamine can a↵ect a postsynaptic neuron.\\nMany di↵erent chemicals have been identiﬁed as neurotransmitters in animal nervous\\nsystems.\\nA neuron’sbackground activity is its level of activity, usually its ﬁring rate, when the\\nneuron does not appear to be driven by synaptic input related to the task of interest\\nto the experimenter, for example, when the neuron’s activity is not correlated with a\\nstimulus delivered to a subject as part of an experiment. Background activity can be\\nirregular due to input from the wider network, or due to noise within the neuron or its\\nsynapses. Sometimes background activity is the result of dynamic processes intrinsic to\\nthe neuron. A neuron’sphasic activity, in contrast to its background activity, consists of\\nbursts of spiking activity usually caused by synaptic input. Activity that varies slowly\\nand often in a graded manner, whether as background activity or not, is called a neuron’s\\ntonic activity.\\nThe strength or e↵ectiveness by which the neurotransmitter released at a synapse\\ninﬂuences the postsynaptic neuron is the synapse’se\\x00cacy. One way a nervous system\\ncan change through experience is through changes in synaptic e\\x00cacies as a result of\\ncombinations of the activities of the presynaptic and postsynaptic neurons, and sometimes\\nby the presence of aneuromodulator, which is a neurotransmitter having e↵ects other\\nthan, or in addition to, direct fast excitation or inhibition.\\nBrains contain several di↵erent neuromodulation systems consisting of clusters of\\nneurons with widely branching axonal arbors, with each system using a di↵erent neuro-\\ntransmitter. Neuromodulation can alter the function of neural circuits, mediate motivation,\\narousal, attention, memory, mood, emotion, sleep, and body temperature. Important\\nhere is that a neuromodulatory system can distribute something like a scalar signal, such\\nas a reinforcement signal, to alter the operation of synapses in widely distributed sites\\ncritical for learning.\\nThe ability of synaptic e\\x00cacies to change is calledsynaptic plasticity. It is one of the\\nprimary mechanisms responsible for learning. The parameters, or weights, adjusted by\\nlearning algorithms correspond to synaptic e\\x00cacies. As we detail below, modulation of\\nsynaptic plasticity via the neuromodulator dopamine is a plausible mechanism for how\\nthe brain might implement learning algorithms like many of those described in this book.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 401, 'page_label': '402'}, page_content='380 Chapter 15: Neuroscience\\n15.2 Reward Signals, Reinforcement Signals, Values,\\nand Prediction Errors\\nLinks between neuroscience and computational reinforcement learning begin as parallels\\nbetween signals in the brain and signals playing prominent roles in reinforcement learning\\ntheory and algorithms. In Chapter 3 we said that any problem of learning goal-directed\\nbehavior can be reduced to the three signals representing actions, states, and rewards.\\nHowever, to explain links that have been made between neuroscience and reinforcement\\nlearning, we have to be less abstract than this and consider other reinforcement learning\\nsignals that correspond, in certain ways, to signals in the brain. In addition to reward\\nsignals, these include reinforcement signals (which we argue are di↵erent from reward\\nsignals), value signals, and signals conveying prediction errors. When we label a signal by\\nits function in this way, we are doing it in the context of reinforcement learning theory\\nin which the signal corresponds to a term in an equation or an algorithm. On the other\\nhand, when we refer to a signal in the brain, we mean a physiological event such as\\na burst of action potentials or the secretion of a neurotransmitter. Labeling a neural\\nsignal by its function, for example calling the phasic activity of a dopamine neuron a\\nreinforcement signal, means that the neural signal behaves like, and is conjectured to\\nfunction like, the corresponding theoretical signal.\\nUncovering evidence for these correspondences involves many challenges. Neural\\nactivity related to reward processing can be found in nearly every part of the brain,\\nand it is di\\x00cult to interpret results unambiguously because representations of di↵erent\\nreward-related signals tend to be highly correlated with one another. Experiments need to\\nbe carefully designed to allow one type of reward-related signal to be distinguished with\\nany degree of certainty from others—or from an abundance of other signals not related to\\nreward processing. Despite these di\\x00culties, many experiments have been conducted with\\nthe aim of reconciling aspects of reinforcement learning theory and algorithms with neural\\nsignals, and some compelling links have been established. To prepare for examining these\\nlinks, in the rest of this section we remind the reader of what various reward-related\\nsignals mean according to reinforcement learning theory.\\nIn our Comments on Terminology at the end of the previous chapter, we said that\\nRt is like a reward signal in an animal’s brain and not an object or event in the\\nanimal’s environment. In reinforcement learning, the reward signal (along with an agent’s\\nenvironment) deﬁnes the problem a reinforcement learning agent is trying to solve. In\\nthis respect, Rt is like a signal in an animal’s brain that distributes primary reward to\\nsites throughout the brain. But it is unlikely that a unitary master reward signal likeRt\\nexists in an animal’s brain. It is best to think ofRt as an abstraction summarizing the\\noverall e↵ect of a multitude of neural signals generated by many systems in the brain\\nthat assess the rewarding or punishing qualities of sensations and states.\\nReinforcement signalsin reinforcement learning are di↵erent from reward signals. The\\nfunction of a reinforcement signal is to direct the changes a learning algorithm makes in\\nan agent’s policy, value estimates, or environment models. For a TD method, for instance,\\nthe reinforcement signal at timet is the TD error\\x00t\\x001 = Rt + \\x00V (St) \\x00 V (St\\x001).1 The\\n1As we mentioned in Section 6.1,\\x00t in our notation is deﬁned to beRt+1 + \\x00V (St+1) \\x00 V (St), so\\x00t'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 402, 'page_label': '403'}, page_content='15.3. The Reward Prediction Error Hypothesis 381\\nreinforcement signal for some algorithms could be just the reward signal, but for most\\nof the algorithms we consider the reinforcement signal is the reward signal adjusted by\\nother information, such as the value estimates in TD errors.\\nEstimates of state values or of action values, that is,V or Q, specify what is good or\\nbad for the agent over the long run. They are predictions of the total reward an agent can\\nexpect to accumulate over the future. Agents make good decisions by selecting actions\\nleading to states with the largest estimated state values, or by selecting actions with the\\nlargest estimated action values.\\nPrediction errors measure discrepancies between expected and actual signals or sensa-\\ntions. Reward prediction errors (RPEs) speciﬁcally measure discrepancies between the\\nexpected and the received reward signal, being positive when the reward signal is greater\\nthan expected, and negative otherwise. TD errors like (6.5) are special kinds of RPEs\\nthat signal discrepancies between current and earlier expectations of reward over the\\nlong-term. When neuroscientists refer to RPEs they generally (though not always) mean\\nTD RPEs, which we simply call TD errors throughout this chapter. Also in this chapter,\\na TD error is generally one that does not depend on actions, as opposed to TD errors\\nused in learning action-values by algorithms like Sarsa and Q-learning. This is because\\nthe most well-known links to neuroscience are stated in terms of action-free TD errors,\\nbut we do not mean to rule out possible similar links involving action-dependent TD\\nerrors. (TD errors for predicting signals other than rewards are useful too, but that case\\nwill not concern us here. See, for example, Modayil, White, and Sutton, 2014.)\\nOne can ask many questions about links between neuroscience data and these theoretically-\\ndeﬁned signals. Is an observed signal more like a reward signal, a value signal, a prediction\\nerror, a reinforcement signal, or something altogether di↵erent? And if it is an error\\nsignal, is it an RPE, a TD error, or a simpler error like the Rescorla–Wagner error (14.3)?\\nAnd if it is a TD error, does it depend on actions like the TD error of Q-learning or\\nSarsa? As indicated above, probing the brain to answer questions like these is extremely\\ndi\\x00cult. But experimental evidence suggests that one neurotransmitter, speciﬁcally\\nthe neurotransmitter dopamine, signals RPEs, and further, that the phasic activity of\\ndopamine-producing neurons in fact conveys TD errors (see Section 15.1 for a deﬁnition of\\nphasic activity). This evidence led to thereward prediction error hypothesis of dopamine\\nneuron activity,w h i c hw ed e s c r i b en e x t .\\n15.3 The Reward Prediction Error Hypothesis\\nThe reward prediction error hypothesis of dopamine neuron activityproposes that one\\nof the functions of the phasic activity of dopamine-producing neurons in mammals is to\\ndeliver an error between an old and a new estimate of expected future reward to target\\nareas throughout the brain. This hypothesis (though not in these exact words) was\\nﬁrst explicitly stated by Montague, Dayan, and Sejnowski (1996), who showed how the\\nTD error concept from reinforcement learning accounts for many features of the phasic\\nis not available until timet +1 . T h eT De r r o ravailable at t is actually\\x00t\\x001 = Rt + \\x00V (St) \\x00 V (St\\x001).\\nBecause we are thinking of time steps as very small, or even inﬁnitesimal, time intervals, one should not\\nattribute undue importance to this one-step time shift.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 403, 'page_label': '404'}, page_content='382 Chapter 15: Neuroscience\\nactivity of dopamine neurons in mammals. The experiments that led to this hypothesis\\nwere performed in the 1980s and early 1990s in the laboratory of neuroscientist Wolfram\\nSchultz. Section 15.5 describes these inﬂuential experiments, Section 15.6 explains how the\\nresults of these experiments align with TD errors, and the Bibliographical and Historical\\nRemarks section at the end of this chapter includes a guide to the literature surrounding\\nthe development of this inﬂuential hypothesis.\\nMontague et al. (1996) compared the TD errors of the TD model of classical conditioning\\nwith the phasic activity of dopamine-producing neurons during classical conditioning\\nexperiments. Recall from Section 14.2 that the TD model of classical conditioning is\\nbasically the semi-gradient-descent TD(\\x00) algorithm with linear function approximation.\\nMontague et al. made several assumptions to set up this comparison. First, because a\\nTD error can be negative but neurons cannot have a negative ﬁring rate, they assumed\\nthat the quantity corresponding to dopamine neuron activity is\\x00t\\x001 + bt,w h e r ebt is the\\nbackground ﬁring rate of the neuron. A negative TD error corresponds to a drop in a\\ndopamine neuron’s ﬁring rate below its background rate.2\\nA second assumption was needed about the states visited in each classical conditioning\\ntrial and how they are represented as inputs to the learning algorithm. This is the same\\nissue we discussed in Section 14.2.4 for the TD model. Montague et al. chose a complete\\nserial compound (CSC) representation as shown in the left column of Figure 14.1, but\\nwhere the sequence of short-duration internal signals continues until the onset of the US,\\nwhich here is the arrival of a non-zero reward signal. This representation allows the TD\\nerror to mimic the fact that dopamine neuron activity not only predicts a future reward,\\nbut that it is also sensitive towhen after a predictive cue that reward is expected to\\narrive. There has to be some way to keep track of the time between sensory cues and\\nthe arrival of reward. If a stimulus initiates a sequence of internal signals that continues\\nafter the stimulus ends, and if there is a di↵erent signal for each time step following the\\nstimulus, then each time step after the stimulus is represented by a distinct state. Thus,\\nthe TD error, being state-dependent, can be sensitive to the timing of events within a\\ntrial.\\nIn simulated trials with these assumptions about background ﬁring rate and input\\nrepresentation, TD errors of the TD model are remarkably similar to dopamine neuron\\nphasic activity. Previewing our description of details about these similarities in Section 15.5\\nbelow, the TD errors parallel the following features of dopamine neuron activity: (1) the\\nphasic response of a dopamine neuron only occurs when a rewarding event is unpredicted;\\n(2) early in learning, neutral cues that precede a reward do not cause substantial phasic\\ndopamine responses, but with continued learning these cues gain predictive value and\\ncome to elicit phasic dopamine responses; (3) if an even earlier cue reliably precedes a\\ncue that has already acquired predictive value, the phasic dopamine response shifts to\\nthe earlier cue, ceasing for the later cue; and (4) if after learning, the predicted rewarding\\nevent is omitted, a dopamine neuron’s response decreases below its baseline level shortly\\nafter the expected time of the rewarding event.\\n2In the literature relating TD errors to the activity of dopamine neurons, their\\x00t is the same as our\\n\\x00t\\x001 = Rt + \\x00V (St) \\x00 V (St\\x001).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 404, 'page_label': '405'}, page_content='15.4. Dopamine 383\\nAlthough not every dopamine neuron monitored in the experiments of Schultz and\\ncolleagues behaved in all of these ways, the striking correspondence between the ac-\\ntivities of most of the monitored neurons and TD errors lends strong support to the\\nreward prediction error hypothesis. There are situations, however, in which predictions\\nbased on the hypothesis do not match what is observed in experiments. The choice\\nof input representation is critical to how closely TD errors match some of the details\\nof dopamine neuron activity, particularly details about the timing of dopamine neuron\\nresponses. Di↵erent ideas, some of which we discuss below, have been proposed about\\ninput representations and other features of TD learning to make the TD errors ﬁt the data\\nbetter, though the main parallels appear with the CSC representation that Montague et\\nal. used. Overall, the reward prediction error hypothesis has received wide acceptance\\namong neuroscientists studying reward-based learning, and it has proven to be remarkably\\nresilient in the face of accumulating results from neuroscience experiments.\\nTo prepare for our description of the neuroscience experiments supporting the reward\\nprediction error hypothesis, and to provide some context so that the signiﬁcance of the\\nhypothesis can be appreciated, we next present some of what is known about dopamine,\\nthe brain structures it inﬂuences, and how it is involved in reward-based learning.\\n15.4 Dopamine\\nDopamine is produced as a neurotransmitter by neurons whose cell bodies lie mainly\\nin two clusters of neurons in the midbrain of mammals: the substantia nigra pars\\ncompacta (SNpc) and the ventral tegmental area (VTA). Dopamine plays essential roles\\nin many processes in the mammalian brain. Prominent among these are motivation,\\nlearning, action-selection, most forms of addiction, and the disorders schizophrenia and\\nParkinson’s disease. Dopamine is called a neuromodulator because it performs many\\nfunctions other than direct fast excitation or inhibition of targeted neurons. Although\\nmuch remains unknown about dopamine’s functions and details of its cellular e↵ects, it is\\nclear that it is fundamental to reward processing in the mammalian brain. Dopamine\\nis not the only neuromodulator involved in reward processing, and its role in aversive\\nsituations—punishment—remains controversial. Dopamine also can function di↵erently in\\nnon-mammals. But no one doubts that dopamine is essential for reward-related processes\\nin mammals, including humans.\\nAn early, traditional view is that dopamine neurons broadcast a reward signal to\\nmultiple brain regions implicated in learning and motivation. This view followed from a\\nfamous 1954 paper by James Olds and Peter Milner that described the e↵ects of electrical\\nstimulation on certain areas of a rat’s brain. They found that electrical stimulation to\\nparticular regions acted as a very powerful reward in controlling the rat’s behavior: “...the\\ncontrol exercised over the animal’s behavior by means of this reward is extreme, possibly\\nexceeding that exercised by any other reward previously used in animal experimentation”\\n(Olds and Milner, 1954). Later research revealed that the sites at which stimulation\\nwas most e↵ective in producing this rewarding e↵ect excited dopamine pathways, either\\ndirectly or indirectly, that ordinarily are excited by natural rewarding stimuli. E↵ects\\nsimilar to these were also observed with human subjects. These observations strongly\\nsuggested that dopamine neuron activity signals reward.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 405, 'page_label': '406'}, page_content='384 Chapter 15: Neuroscience\\nBut if the reward prediction error hypothesis is correct—even if it accounts for only\\nsome features of a dopamine neuron’s activity—this traditional view of dopamine neuron\\nactivity is not entirely correct: phasic responses of dopamine neurons signal reward\\nprediction errors, not reward itself. In reinforcement learning’s terms, a dopamine\\nneuron’s phasic response at a timet corresponds to \\x00t\\x001 = Rt + \\x00V (St) \\x00 V (St\\x001), not\\nto Rt.\\nReinforcement learning theory and algorithms help reconcile the reward-prediction-\\nerror view with the conventional notion that dopamine signals reward. In many of the\\nalgorithms we discuss in this book,\\x00 functions as a reinforcement signal, meaning that it\\nis the main driver of learning. For example,\\x00 is the critical factor in the TD model of\\nclassical conditioning, and\\x00 is the reinforcement signal for learning both a value function\\nand a policy in an actor–critic architecture (Sections 13.5 and 15.7). Action-dependent\\nforms of\\x00 are reinforcement signals for Q-learning and Sarsa. The reward signalRt is\\na crucial component of\\x00t\\x001, but it is not the complete determinant of its reinforcing\\ne↵ect in these algorithms. The additional term\\x00V (St) \\x00 V (St\\x001) is the higher-order\\nreinforcement part of\\x00t\\x001, and even if reward occurs (Rt 6= 0), the TD error can be silent\\nif the reward is fully predicted (which is fully explained in Section 15.6 below).\\nA closer look at Olds’ and Milner’s 1954 paper, in fact, reveals that it is mainly\\nabout the reinforcing e↵ect of electrical stimulation in an instrumental conditioning task.\\nElectrical stimulation not only energized the rats’ behavior—through dopamine’s e↵ect on\\nmotivation—it also led to the rats quickly learning to stimulate themselves by pressing a\\nlever, which they would do frequently for long periods of time. The activity of dopamine\\nneurons triggered by electrical stimulation reinforced the rats’ lever pressing.\\nMore recent experiments using optogenetic methods clinch the role of phasic responses\\nof dopamine neurons as reinforcement signals. These methods allow neuroscientists to\\nprecisely control the activity of selected neuron types at a millisecond timescale in awake\\nbehaving animals. Optogenetic methods introduce light-sensitive proteins into selected\\nneuron types so that these neurons can be activated or silenced by means of ﬂashes of\\nlaser light. The ﬁrst experiment using optogenetic methods to study dopamine neurons\\nshowed that optogenetic stimulation producing phasic activation of dopamine neurons\\nin mice was enough to condition the mice to prefer the side of a chamber where they\\nreceived this stimulation as compared to the chamber’s other side where they received\\nno, or lower-frequency, stimulation (Tsai et al. 2009). In another example, Steinberg\\net al. (2013) used optogenetic activation of dopamine neurons to create artiﬁcial bursts\\nof dopamine neuron activity in rats at the times when rewarding stimuli were expected\\nbut omitted—times when dopamine neuron activity normally pauses. With these pauses\\nreplaced by artiﬁcial bursts, responding was sustained when it would ordinarily decrease\\ndue to lack of reinforcement (in extinction trials), and learning was enabled when it would\\nordinarily be blocked due to the reward being already predicted (the blocking paradigm;\\nSection 14.2.1).\\nAdditional evidence for the reinforcing function of dopamine comes from optogenetic\\nexperiments with fruit ﬂies, except in these animals dopamine’s e↵ect is the opposite of\\nits e↵ect in mammals: optically triggered bursts of dopamine neuron activity act just\\nlike electric foot shock in reinforcing avoidance behavior, at least for the population'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 406, 'page_label': '407'}, page_content='15.4. Dopamine 385\\nof dopamine neurons activated (Claridge-Chang et al. 2009). Although none of these\\noptogenetic experiments showed that phasic dopamine neuron activity is speciﬁcally\\nlike a TD error, they convincingly demonstrated that phasic dopamine neuron activity\\nacts just like\\x00 acts (or perhaps likeminus \\x00 acts in fruit ﬂies) as the reinforcement\\nsignal in algorithms for both prediction (classical conditioning) and control (instrumental\\nconditioning).\\nAxonal arbor of a single neuron producing\\ndopamine as a neurotransmitter. These\\naxons make synaptic contacts with a huge\\nnumber of dendrites of neurons in targeted\\nbrain areas.\\nAdapted from The Journal of Neuroscience ,\\nMatsuda, Furuta, Nakamura, Hioki, Fujiyama,\\nArai, and Kaneko, volume 29, 2009, page 451.\\nDopamine neurons are particularly well suited\\nto broadcasting a reinforcement signal to many\\nareas of the brain. These neurons have huge\\naxonal arbors, each releasing dopamine at 100 to\\n1,000 times more synaptic sites than reached by\\nthe axons of typical neurons. Shown to the right\\nis the axonal arbor of a single dopamine neuron\\nwhose cell body is in the SNpc of a rat’s brain.\\nEach axon of a SNpc or VTA dopamine neuron\\nmakes roughly 500,000 synaptic contacts on the\\ndendrites of neurons in targeted brain areas.\\nIf dopamine neurons broadcast a reinforce-\\nment signal like reinforcement learning’s\\x00,t h e n\\nbecause this is a scalar signal, i.e., a single num-\\nber, all dopamine neurons in both the SNpc\\nand VTA would be expected to activate more-\\nor-less identically so that they would act in near\\nsynchrony to send the same signal to all of the\\nsites their axons target. Although it has been\\na common belief that dopamine neurons do act\\ntogether like this, modern evidence is pointing\\nto the more complicated picture that di↵erent\\nsubpopulations of dopamine neurons respond to\\ninput di↵erently depending on the structures to\\nwhich they send their signals and the di↵erent\\nways these signals act on their target structures. Dopamine has functions other than\\nsignaling RPEs, and even for dopamine neurons that do signal RPEs, it can make sense\\nto send di↵erent RPEs to di↵erent structures depending on the roles these structures\\nplay in producing reinforced behavior. This is beyond what we treat in any detail in this\\nbook, but vector-valued RPE signals make sense from the perspective of reinforcement\\nlearning when decisions can be decomposed into separate sub-decisions, or more generally,\\nas a way to address thestructural version of the credit assignment problem: How do\\nyou distribute credit for success (or blame for failure) of a decision among the many\\ncomponent structures that could have been involved in producing it? We say a bit more\\nabout this in Section 15.10 below.\\nThe axons of most dopamine neurons make synaptic contact with neurons in the frontal\\ncortex and the basal ganglia, areas of the brain involved in voluntary movement, decision\\nmaking, learning, and cognitive functions such as planning. Because most ideas relating'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 407, 'page_label': '408'}, page_content='386 Chapter 15: Neuroscience\\ndopamine to reinforcement learning focus on the basal ganglia, and the connections from\\ndopamine neurons are particularly dense there, we focus on the basal ganglia here. The\\nbasal ganglia are a collection of neuron groups, or nuclei, lying at the base of the forebrain.\\nThe main input structure of the basal ganglia is called the striatum. Essentially all of the\\ncerebral cortex, among other structures, provides input to the striatum. The activity of\\ncortical neurons conveys a wealth of information about sensory input, internal states, and\\nmotor activity. The axons of cortical neurons make synaptic contacts on the dendrites of\\nthe main input/output neurons of the striatum, called medium spiny neurons. Output\\nfrom the striatum loops back via other basal ganglia nuclei and the thalamus to frontal\\nareas of cortex, and to motor areas, making it possible for the striatum to inﬂuence\\nmovement, abstract decision processes, and reward processing. Two main subdivisions\\nof the striatum are important for reinforcement learning: the dorsal striatum, primarily\\nimplicated in inﬂuencing action selection, and the ventral striatum, thought to be critical\\nfor di↵erent aspects of reward processing, including the assignment of a↵ective value to\\nsensations.\\nThe dendrites of medium spiny neurons are covered with spines on whose tips the\\naxons of neurons in the cortex make synaptic contact. Also making synaptic contact with\\nthese spines—in this case contacting the spine stems—are axons of dopamine neurons\\n(Figure 15.1). This arrangement brings together presynaptic activity of cortical neurons,\\nFigure 15.1: Spine of a striatal neuron showing input from both cortical and dopamine neurons.\\nAxons of cortical neurons inﬂuence striatal neurons via corticostriatal synapses releasing the\\nneurotransmitter glutamate at the tips of spines covering the dendrites of striatal neurons.\\nAn axon of a VTA or SNpc dopamine neuron is shown passing by the spine (from the lower\\nright). “Dopamine varicosities” on this axon release dopamine at or near the spine stem, in an\\narrangement that brings together presynaptic input from cortex, postsynaptic activity of the\\nstriatal neuron, and dopamine, making it possible that several types of learning rules govern the\\nplasticity of corticostriatal synapses. Each axon of a dopamine neuron makes synaptic contact\\nwith the stems of roughly 500,000 spines. Some of the complexity omitted from our discussion\\nis shown here by other neurotransmitter pathways and multiple receptor types, such as D1 an\\nD2 dopamine receptors by which dopamine can produce di↵erent e↵ects at spines and other\\npostsynaptic sites. FromJournal of Neurophysiology,W .S c h u l t z ,v o l .8 0 ,1 9 9 8 ,p a g e1 0 .'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 408, 'page_label': '409'}, page_content='15.5. Experimental Support for the Reward Prediction Error Hypothesis 387\\npostsynaptic activity of medium spiny neurons, and input from dopamine neurons. What\\nactually occurs at these spines is complex and not completely understood. Figure 15.1\\nhints at the complexity by showing two types of receptors for dopamine, receptors for\\nglutamate—the neurotransmitter of the cortical inputs—and multiple ways that the\\nvarious signals can interact. But evidence is mounting that changes in the e\\x00cacies of\\nthe synapses on the pathway from the cortex to the striatum, which neuroscientists call\\ncorticostriatal synapses, depend critically on appropriately-timed dopamine signals.\\n15.5 Experimental Support\\nfor the Reward Prediction Error Hypothesis\\nDopamine neurons respond with bursts of activity to intense, novel, or unexpected visual\\nand auditory stimuli that trigger eye and body movements, but very little of their activity is\\nrelated to the movements themselves. This is surprising because degeneration of dopamine\\nneurons is a cause of Parkinson’s disease, whose symptoms include motor disorders,\\nparticularly deﬁcits in self-initiated movement. Motivated by the weak relationship\\nbetween dopamine neuron activity and stimulus-triggered eye and body movements,\\nRomo and Schultz (1990) and Schultz and Romo (1990) took the ﬁrst steps toward the\\nreward prediction error hypothesis by recording the activity of dopamine neurons and\\nmuscle activity while monkeys moved their arms.\\nThey trained two monkeys to reach from a resting hand position into a bin containing\\na bit of apple, a piece of cookie, or a raisin, when the monkey saw and heard the bin’s\\ndoor open. The monkey could then grab and bring the food to its mouth. After a monkey\\nbecame good at this, it was trained on two additional tasks. The purpose of the ﬁrst\\ntask was to see what dopamine neurons do when movements are self-initiated. The bin\\nwas left open but covered from above so that the monkey could not see inside but could\\nreach in from below. No triggering stimuli were presented, and after the monkey reached\\nfor and ate the food morsel, the experimenter usually (though not always), silently and\\nunseen by the monkey, replaced food in the bin by sticking it onto a rigid wire. Here too,\\nthe activity of the dopamine neurons Romo and Schultz monitored was not related to the\\nmonkey’s movements, but a large percentage of these neurons produced phasic responses\\nwhenever the monkey ﬁrst touched a food morsel. These neurons did not respond when\\nthe monkey touched just the wire or explored the bin when no food was there. This was\\ngood evidence that the neurons were responding to the food and not to other aspects of\\nthe task.\\nThe purpose of Romo and Schultz’s second task was to see what happens when\\nmovements are triggered by stimuli. This task used a di↵erent bin with a movable cover.\\nThe sight and sound of the bin opening triggered reaching movements to the bin. In this\\ncase, Romo and Schultz found that after some period of training, the dopamine neurons\\nno longer responded to the touch of the food but instead responded to the sight and sound\\nof the opening cover of the food bin. The phasic responses of these neurons had shifted\\nfrom the reward itself to stimuli predicting the availability of the reward. In a followup\\nstudy, Romo and Schultz found that most of the dopamine neurons whose activity they'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 409, 'page_label': '410'}, page_content='388 Chapter 15: Neuroscience\\nmonitored did not respond to the sight and sound of the bin opening outside the context\\nof the behavioral task. These observations suggested that the dopamine neurons were\\nresponding neither to the initiation of a movement nor to the sensory properties of the\\nstimuli, but were rather signaling an expectation of reward.\\nSchultz’s group conducted many additional studies involving both SNpc and VTA\\ndopamine neurons. A particular series of experiments was inﬂuential in suggesting that\\nthe phasic responses of dopamine neurons correspond to TD errors and not to simpler\\nerrors like those in the Rescorla–Wagner model (14.3). In the ﬁrst of these experiments\\n(Ljungberg, Apicella, and Schultz, 1992), monkeys were trained to depress a lever after\\na light was illuminated as a ‘trigger cue’ to obtain a drop of apple juice. As Romo\\nand Schultz had observed earlier, many dopamine neurons initially responded to the\\nreward—the drop of juice (Figure 15.2, top panel). But many of these neurons lost that\\nreward response as training continued and developed responses instead to the illumination\\nof the light that predicted the reward (Figure 15.2, middle panel). With continued\\ntraining, lever pressing became faster while the number of dopamine neurons responding\\nto the trigger cue decreased.\\nFollowing this study, the same monkeys were trained on a new task (Schultz, Apicella,\\nand Ljungberg, 1993). Here the monkeys faced two levers, each with a light above it.\\nIlluminating one of these lights was an ‘instruction cue’ indicating which of the two levers\\nFigure 15.2: The response of dopamine neurons shifts from initial responses to primary reward\\nto earlier predictive stimuli. These are plots of the number of action potentials produced\\nby monitored dopamine neurons within small time intervals, averaged over all the monitored\\ndopamine neurons (ranging from 23 to 44 neurons for these data). Top: dopamine neurons are\\nactivated by the unpredicted delivery of drop of apple juice. Middle: with learning, dopamine\\nneurons developed responses to the reward-predicting trigger cue and lost responsiveness to the\\ndelivery of reward. Bottom: with the addition of an instruction cue preceding the trigger cue by\\n1 second, dopamine neurons shifted their responses from the trigger cue to the earlier instruction\\ncue. From Schultz et al. (1995), MIT Press.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 410, 'page_label': '411'}, page_content='15.5. Experimental Support for the Reward Prediction Error Hypothesis 389\\nFigure 15.3:The response of dopamine neurons drops\\nbelow baseline shortly after the time when an expected\\nreward fails to occur. Top: dopamine neurons are\\nactivated by the unpredicted delivery of a drop of\\napple juice. Middle: dopamine neurons respond to a\\nconditioned stimulus (CS) that predicts reward and\\ndo not respond to the reward itself. Bottom: when\\nthe reward predicted by the CS fails to occur, the\\nactivity of dopamine neurons drops below baseline\\nshortly after the time the reward is expected to occur.\\nAt the top of each of these panels is shown the average\\nnumber of action potentials produced by monitored\\ndopamine neurons within small time intervals around\\nthe indicated times. The raster plots below show the\\nactivity patterns of the individual dopamine neurons\\nthat were monitored; each dot represents an action\\npotential. From Schultz, Dayan, and Montague, A\\nNeural Substrate of Prediction and Reward,Science,\\nvol. 275, issue 5306, pages 1593-1598, March 14, 1997.\\nReprinted with permission from AAAS.\\nwould produce a drop of apple juice.\\nIn this task, the instruction cue pre-\\nceded the trigger cue of the previ-\\nous task by a ﬁxed interval of 1 sec-\\nond. The monkeys learned to with-\\nhold reaching until seeing the trig-\\nger cue, and dopamine neuron activ-\\nity increased, but now the responses\\nof the monitored dopamine neurons\\noccurred almost exclusively to the\\nearlier instruction cue and not to\\nthe trigger cue (Figure 15.2, bot-\\ntom panel). Here again the num-\\nber of dopamine neurons responding\\nto the instruction cue was much re-\\nduced when the task was well learned.\\nDuring learning across these tasks,\\ndopamine neuron activity shifted\\nfrom initially responding to the re-\\nward to responding to the earlier\\npredictive stimuli, ﬁrst progressing\\nto the trigger stimulus then to the\\nstill earlier instruction cue. As re-\\nsponding moved earlier in time it\\ndisappeared from the later stimuli.\\nThis shifting of responses to earlier\\nreward predictors, while losing re-\\nsponses to later predictors is a hall-\\nmark of TD learning (see, for exam-\\nple, Figure 14.2).\\nThe task just described revealed\\nanother property of dopamine neu-\\nron activity shared with TD learn-\\ning. The monkeys sometimes pressed\\nthe wrong key, that is, the key other\\nthan the instructed one, and conse-\\nquently received no reward. In these\\ntrials, many of the dopamine neu-\\nrons showed a sharp decrease in their\\nﬁring rates below baseline shortly af-\\nter the reward’s usual time of deliv-\\nery, and this happened without the\\navailability of any external cue to\\nmark the usual time of reward de-\\nlivery (Figure 15.3). Somehow the'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 411, 'page_label': '412'}, page_content='390 Chapter 15: Neuroscience\\nmonkeys were internally keeping track of the timing of the reward. (Response timing is\\none area where the simplest version of TD learning needs to be modiﬁed to account for\\nsome of the details of the timing of dopamine neuron responses. We consider this issue in\\nthe following section.)\\nThe observations from the studies described above led Schultz and his group to conclude\\nthat dopamine neurons respond to unpredicted rewards, to the earliest predictors of\\nreward, and that dopamine neuron activity decreases below baseline if a reward, or a\\npredictor of reward, does not occur at its expected time. Researchers familiar with\\nreinforcement learning were quick to recognize that these results are strikingly similar\\nto how the TD error behaves as the reinforcement signal in a TD algorithm. The next\\nsection explores this similarity by working through a speciﬁc example in detail.\\n15.6 TD Error/Dopamine Correspondence\\nThis section explains the correspondence between the TD error\\x00 and the phasic responses\\nof dopamine neurons observed in the experiments just described. We examine how\\x00\\nchanges over the course of learning in a task something like the one described above\\nwhere a monkey ﬁrst sees an instruction cue and then a ﬁxed time later has to respond\\ncorrectly to a trigger cue in order to obtain reward. We use a simple idealized version of\\nthis task, but we go into a lot more detail than is usual because we want to emphasize\\nthe theoretical basis of the parallel between TD errors and dopamine neuron activity.\\nThe ﬁrst simplifying assumption is that the agent has already learned the actions\\nrequired to obtain reward. Then its task is just to learn accurate predictions of future\\nreward for the sequence of states it experiences. This is then a prediction task, or\\nmore technically, a policy-evaluation task: learning the value function for a ﬁxed policy\\n(Sections 4.1 and 6.1). The value function to be learned assigns to each state a value that\\npredicts the return that will follow that state if the agent selects actions according to the\\ngiven policy, where the return is the (possibly discounted) sum of all the future rewards.\\nThis is unrealistic as a model of the monkey’s situation because the monkey would likely\\nlearn these predictions at the same time that it is learning to act correctly (as would a\\nreinforcement learning algorithm that learns policies as well as value functions, such as\\nan actor–critic algorithm), but this scenario is simpler to describe than one in which a\\npolicy and a value function are learned simultaneously.\\nNow imagine that the agent’s experience divides into multiple trials, in each of which\\nthe same sequence of states repeats, with a distinct state occurring on each time step\\nduring the trial. Further imagine that the return being predicted is limited to the return\\nover a trial, which makes a trial analogous to a reinforcement learning episode as we have\\ndeﬁned it. In reality, of course, the returns being predicted are not conﬁned to single\\ntrials, and the time interval between trials is an important factor in determining what an\\nanimal learns. This is true for TD learning as well, but here we assume that returns do\\nnot accumulate over multiple trials. Given this, then, a trial in experiments like those\\nconducted by Schultz and colleagues is equivalent to an episode of reinforcement learning.\\n(Though in this discussion, we will use the term trial instead of episode to relate better\\nto the experiments.)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 412, 'page_label': '413'}, page_content='15.6. TD Error/Dopamine Correspondence 391\\nAs usual, we also need to make an assumption about how states are represented as\\ninputs to the learning algorithm, an assumption that inﬂuences how closely the TD error\\ncorresponds to dopamine neuron activity. We discuss this issue later, but for now we\\nassume the same CSC representation used by Montague et al. (1996) in which there is a\\nseparate internal stimulus for each state visited at each time step in a trial. This reduces\\nthe process to the tabular case covered in the ﬁrst part of this book. Finally, we assume\\nthat the agent uses TD(0) to learn a value function,V , stored in a lookup table initialized\\nto be zero for all the states. We also assume that this is a deterministic task and that\\nthe discount factor,\\x00, is very nearly one so that we can ignore it.\\nFigure 15.4 shows the time courses ofR, V , and\\x00 at several stages of learning in this\\npolicy-evaluation task. The time axes represent the time interval over which a sequence\\nof states is visited in a trial (where for clarity we omit showing individual states). The\\nreward signal is zero throughout each trial except when the agent reaches the rewarding\\nstate, shown near the right end of the time line, when the reward signal becomes some\\npositive number, say R?. The goal of TD learning is to predict the return for each\\nstate visited in a trial, which in this undiscounted case and given our assumption that\\npredictions are conﬁned to individual trials, is simplyR? for each state.\\nR\\nRR\\nR\\nRt\\nR\\n\\x00t\\x001=Rt+Vt\\x00Vt\\x001\\nR\\nV\\nV\\x00\\n\\x00\\n\\x00\\nearly inlearning\\nlearningcomplete\\nomittedR\\nregular predictors of     over this intervalR\\nR?\\nFigure 15.4: The behavior of the TD error\\x00 during TD learning is consistent with features of\\nthe phasic activation of dopamine neurons. (Here\\x00 is the TD erroravailable at timet,i . e . ,\\x00t\\x001).\\nTop: a sequence of states, shown as an interval of regular predictors, is followed by a non-zero\\nreward R?. Early in learning: the initial value function,V ,a n di n i t i a l\\x00, which at ﬁrst is equal\\nto R?. Learning complete: the value function accurately predicts future reward,\\x00 is positive at\\nthe earliest predictive state, and\\x00 = 0 at the time of the non-zero reward.R? omitted:a tt h e\\ntime the predicted reward is omitted,\\x00 becomes negative. See text for a complete explanation\\nof why this happens.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 413, 'page_label': '414'}, page_content='392 Chapter 15: Neuroscience\\nPreceding the rewarding state is a sequence of reward-predicting states, with the\\nearliest reward-predicting stateshown near the left end of the time line. This is like\\nthe state near the start of a trial, for example like the state marked by the instruction\\ncue in a trial of the monkey experiment of Schultz et al. (1993) described above. It is\\nthe ﬁrst state in a trial that reliably predicts that trial’s reward. (Of course, in reality\\nstates visited on preceding trials are even earlier reward-predicting states, but because\\nwe are conﬁning predictions to individual trials, these do not qualify as predictors ofthis\\ntrial’s reward. Below we give a more satisfactory, though more abstract, description of an\\nearliest reward-predicting state.) Thelatest reward-predicting statein a trial is the state\\nimmediately preceding the trial’s rewarding state. This is the state near the far right end\\nof the time line in Figure 15.4. Note that the rewarding state of a trial does not predict\\nthe return for that trial: the value of this state would come to predict the return over all\\nthe following trials, which here we are assuming to be zero in this episodic formulation.\\nFigure 15.4 shows the ﬁrst-trial time courses ofV and \\x00 as the graphs labeled ‘early in\\nlearning.’ Because the reward signal is zero throughout the trial except when the rewarding\\nstate is reached, and all theV -values are zero, the TD error is also zero until it becomes\\nR? at the rewarding state. This follows because\\x00t\\x001 = Rt + Vt \\x00Vt\\x001 = Rt +0 \\x000= Rt,\\nwhich is zero until it equalsR? when the reward occurs. HereVt and Vt\\x001 are respectively\\nthe estimated values of the states visited at timest and t \\x00 1 in a trial. The TD error at\\nthis stage of learning is analogous to a dopamine neuron responding to an unpredicted\\nreward (e.g., a drop of apple juice) at the start of training.\\nThroughout this ﬁrst trial and all successive trials, TD(0) updates occur at each\\nstate transition as described in Chapter 6. This successively increases the values of the\\nreward-predicting states, with the increases spreading backwards from the rewarding\\nstate, until the values converge to the correct return predictions. In this case (because\\nwe are assuming no discounting) the correct predictions are equal toR? for all the\\nreward-predicting states. This can be seen in Figure 15.4 as the graph ofV labeled\\n‘learning complete’ where the values of all the states from the earliest to the latest\\nreward-predicting states all equalR?. The values of the states preceding the earliest\\nreward-predicting state remain low (which Figure 15.4 shows as zero) because they are\\nnot reliable predictors of reward.\\nWhen learning is complete, that is, whenV attains its correct values, the TD errors\\nassociated with transitionsfrom any reward-predicting state are zero because the predic-\\ntions are now accurate. This is because for a transition from a reward-predicting state to\\nanother reward-predicting state, we have\\x00t\\x001 = Rt + Vt \\x00 Vt\\x001 =0+ R? \\x00 R? = 0, and\\nfor the transition from the latest reward-predicting state to the rewarding state, we have\\n\\x00t\\x001 = Rt +Vt \\x00Vt\\x001 = R? +0 \\x00R? = 0. On the other hand, the TD error on a transition\\nfrom any stateto the earliest reward-predicting state is positive because of the mismatch\\nbetween this state’s low value and the larger value of the following reward-predicting\\nstate. Indeed, if the value of a state preceding the earliest reward-predicting state were\\nzero, then after the transition to the earliest reward-predicting state, we would have\\nthat \\x00t\\x001 = Rt + Vt \\x00 Vt\\x001 =0+ R? \\x00 0= R?. The ‘learning complete’ graph of\\x00 in\\nFigure 15.4 shows this positive value at the earliest reward-predicting state, and zeros\\neverywhere else.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 414, 'page_label': '415'}, page_content='15.6. TD Error/Dopamine Correspondence 393\\nThe positive TD error upon transitioning to the earliest reward-predicting state is\\nanalogous to the persistence of dopamine responses to the earliest stimuli predicting\\nreward. By the same token, when learning is complete, a transition from the latest\\nreward-predicting state to the rewarding state produces a zero TD error because the\\nlatest reward-predicting state’s value, being correct, cancels the reward. This parallels the\\nobservation that fewer dopamine neurons generate a phasic response to a fully predicted\\nreward than to an unpredicted reward.\\nAfter learning, if the reward is suddenly omitted, the TD error goes negative at the\\nusual time of reward because the value of the latest reward-predicting state is then too\\nhigh: \\x00t\\x001 = Rt + Vt \\x00 Vt\\x001 =0+0 \\x00 R? = \\x00R?, as shown at the right end of the ‘R\\nomitted’ graph of\\x00 in Figure 15.4. This is like dopamine neuron activity decreasing\\nbelow baseline at the time an expected reward is omitted as seen in the experiment of\\nSchultz et al. (1993) described above and shown in Figure 15.3.\\nThe idea of anearliest reward-predicting statedeserves more attention. In the scenario\\ndescribed above, because experience is divided into trials, and we assumed that predictions\\nare conﬁned to individual trials, the earliest reward-predicting state is always the ﬁrst\\nstate of a trial. Clearly this is artiﬁcial. A more general way to think of an earliest\\nreward-predicting state is that it is anunpredicted predictorof reward, and there can\\nbe many such states. In an animal’s life, many di↵erent states may precede an earliest\\nreward-predicting state. However, because these states are more often followed byother\\nstates that do not predict reward, their reward-predicting powers, that is, their values,\\nremain low. A TD algorithm, if operating throughout the animal’s life, would update the\\nvalues of these states too, but the updates would not consistently accumulate because, by\\nassumption, none of these states reliably precedes an earliest reward-predicting state. If\\nany of them did, they would be reward-predicting states as well. This might explain why\\nwith overtraining, dopamine responses decrease to even the earliest reward-predicting\\nstimulus in a trial. With overtraining one would expect that even a formerly-unpredicted\\npredictor state would become predicted by stimuli associated with earlier states: the\\nanimal’s interaction with its environment both inside and outside of an experimental\\ntask would become commonplace. Upon breaking this routine with the introduction of a\\nnew task, however, one would see TD errors reappear, as indeed is observed in dopamine\\nneuron activity.\\nThe example described above explains why the TD error shares key features with\\nthe phasic activity of dopamine neurons when the animal is learning in a task similar\\nto the idealized task of our example. But not every property of the phasic activity of\\ndopamine neurons coincides so neatly with properties of\\x00. One of the most troubling\\ndiscrepancies involves what happens when a reward occursearlier than expected. We\\nhave seen that the omission of an expected reward produces a negative prediction error\\nat the reward’s expected time, which corresponds to the activity of dopamine neurons\\ndecreasing below baseline when this happens. If the reward arrives later than expected,\\nit is then an unexpected reward and generates a positive prediction error. This happens\\nwith both TD errors and dopamine neuron responses. But when reward arrives earlier\\nthan expected, dopamine neurons do not do what the TD error does—at least with the\\nCSC representation used by Montague et al. (1996) and by us in our example. Dopamine'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 415, 'page_label': '416'}, page_content='394 Chapter 15: Neuroscience\\nneurons do respond to the early reward, which is consistent with a positive TD error\\nbecause the reward is not predicted to occur then. However, at the later time when the\\nreward is expected but omitted, the TD error is negative whereas, in contrast to this\\nprediction, dopamine neuron activity does not drop below baseline in the way the TD\\nmodel predicts (Hollerman and Schultz, 1998). Something more complicated is going on\\nin the animal’s brain than simply TD learning with a CSC representation.\\nSome of the mismatches between the TD error and dopamine neuron activity can\\nbe addressed by selecting suitable parameter values for the TD algorithm and by using\\nstimulus representations other than the CSC representation. For instance, to address\\nthe early-reward mismatch just described, Suri and Schultz (1999) proposed a CSC\\nrepresentation in which the sequences of internal signals initiated by earlier stimuli\\nare cancelled by the occurrence of a reward. Another proposal by Daw, Courville,\\nand Touretzky (2006) is that the brain’s TD system uses representations produced by\\nstatistical modeling carried out in sensory cortex rather than simpler representations\\nbased on raw sensory input. Ludvig, Sutton, and Kehoe (2008) found that TD learning\\nwith a microstimulus (MS) representation (Figure 14.1) ﬁts the activity of dopamine\\nneurons in the early-reward and other situations better than when a CSC representation\\nis used. Pan, Schmidt, Wickens, and Hyland (2005) found that even with the CSC\\nrepresentation, prolonged eligibility traces improve the ﬁt of the TD error to some aspects\\nof dopamine neuron activity. In general, many ﬁne details of TD-error behavior depend\\non subtle interactions between eligibility traces, discounting, and stimulus representations.\\nFindings like these elaborate and reﬁne the reward prediction error hypothesis without\\nrefuting its core claim that the phasic activity of dopamine neurons is well characterized\\nas signaling TD errors.\\nOn the other hand, there are other discrepancies between the TD theory and exper-\\nimental data that are not so easily accommodated by selecting parameter values and\\nstimulus representations (we mention some of these discrepancies in the Bibliographical\\nand Historical Remarks section at the end of this chapter), and more mismatches are\\nlikely to be discovered as neuroscientists conduct ever more reﬁned experiments. But the\\nreward prediction error hypothesis has been functioning very e↵ectively as a catalyst for\\nimproving our understanding of how the brain’s reward system works. Intricate experi-\\nments have been designed to validate or refute predictions derived from the hypothesis,\\nand experimental results have, in turn, led to reﬁnement and elaboration of the TD\\nerror/dopamine hypothesis.\\nA remarkable aspect of these developments is that the reinforcement learning algo-\\nrithms and theory that connect so well with properties of the dopamine system were\\ndeveloped from a computational perspective in total absence of any knowledge about the\\nrelevant properties of dopamine neurons—remember, TD learning and its connections\\nto optimal control and dynamic programming were developed many years before any of\\nthe experiments were conducted that revealed the TD-like nature of dopamine neuron\\nactivity. This unplanned correspondence, despite not being perfect, suggests that the TD\\nerror/dopamine parallel captures something signiﬁcant about brain reward processes.\\nIn addition to accounting for many features of the phasic activity of dopamine neurons,\\nthe reward prediction error hypothesis links neuroscience to other aspects of reinforcement'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 416, 'page_label': '417'}, page_content='15.7. Neural Actor–Critic 395\\nlearning, in particular, to learning algorithms that use TD errors as reinforcement signals.\\nNeuroscience is still far from reaching complete understanding of the circuits, molecular\\nmechanisms, and functions of the phasic activity of dopamine neurons, but evidence\\nsupporting the reward prediction error hypothesis, along with evidence that phasic\\ndopamine responses are reinforcement signals for learning, suggest that the brain might\\nimplement something like an actor–critic algorithm in which TD errors play critical roles.\\nOther reinforcement learning algorithms are plausible candidates too, but actor–critic\\nalgorithms ﬁt the anatomy and physiology of the mammalian brain particularly well, as\\nwe describe in the following two sections.\\n15.7 Neural Actor–Critic\\nActor–critic algorithms learn both policies and value functions. The ‘actor’ is the\\ncomponent that learns policies, and the ‘critic’ is the component that learns about\\nwhatever policy is currently being followed by the actor in order to ‘criticize’ the actor’s\\naction choices. The critic uses a TD algorithm to learn the state-value function for the\\nactor’s current policy. The value function allows the critic to critique the actor’s action\\nchoices by sending TD errors,\\x00, to the actor. A positive\\x00 means that the action was\\n‘good’ because it led to a state with a better-than-expected value; a negative\\x00 means\\nthat the action was ‘bad’ because it led to a state with a worse-than-expected value.\\nBased on these critiques, the actor continually updates its policy.\\nTwo distinctive features of actor–critic algorithms are responsible for thinking that the\\nbrain might implement an algorithm like this. First, the two components of an actor–critic\\nalgorithm—the actor and the critic—suggest that two parts of the striatum—the dorsal\\nand ventral subdivisions (Section 15.4), both critical for reward-based learning—may\\nfunction respectively something like an actor and a critic. A second property of actor–\\ncritic algorithms that suggests a brain implementation is that the TD error has the dual\\nrole of being the reinforcement signal for both the actor and the critic, though it has a\\ndi↵erent inﬂuence on learning in each of these components. This ﬁts well with several\\nproperties of the neural circuitry: axons of dopamine neurons target both the dorsal\\nand ventral subdivisions of the striatum; dopamine appears to be critical for modulating\\nsynaptic plasticity in both structures; and how a neuromodulator such as dopamine\\nacts on a target structure depends on properties of the target structure and not just on\\nproperties of the neuromodulator.\\nSection 13.5 presents actor–critic algorithms as policy gradient methods, but the actor–\\ncritic algorithm of Barto, Sutton, and Anderson (1983) was simpler and was presented as\\nan artiﬁcial neural network (ANN). Here we describe an ANN implementation something\\nlike that of Barto et al., and we follow Takahashi, Schoenbaum, and Niv (2008) in giving\\na schematic proposal for how this ANN might be implemented by real neural networks in\\nthe brain. We postpone discussion of the actor and critic learning rules until Section 15.8,\\nwhere we present them as special cases of the policy-gradient formulation and discuss\\nwhat they suggest about how dopamine might modulate synaptic plasticity.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 417, 'page_label': '418'}, page_content='396 Chapter 15: Neuroscience\\nFigure 15.5a shows an implementation of an actor–critic algorithm as an ANN with\\ncomponent networks implementing the actor and the critic. The critic consists of a single\\nneuron-like unit, V , whose output activity represents state values, and a component\\nshown as the diamond labeled TD that computes TD errors by combiningV ’s output\\nwith reward signals and with previous state values (as suggested by the loop from the\\nTD diamond to itself). The actor network has a single layer ofk actor units labeledAi,\\ni =1 ,...,k . The output of each actor unit is a component of ak-dimensional action\\nvector. An alternative is that there arek separate actions, one commanded by each actor\\nunit, that compete with one another to be executed, but here we will think of the entire\\nA-vector as an action.\\nReward\\nActor\\nTD. . .\\n. . .\\nCritic\\n. . .\\nδ \\nActions\\nStates/Stimuli\\nTD error\\nEnvironment\\nActor\\nVTASNc\\nS2\\nS1\\nSn\\n. . .\\n. . .\\nCritic\\n. . .\\nS1\\nSn\\nS2\\nActions\\nStates/Stimuli\\nDopaminestriatumVentralDorsal striatum\\nCortex (multiple areas)\\nEnvironment\\nReward\\n\\x001\\n\\x002\\n\\x00n\\n\\x001\\x002\\n\\x00n\\nA1\\nA2\\nA3\\nAk\\nV\\n\\x00\\n(a) (b)\\nx1x2\\nxn\\nx1x2\\nxn\\nFigure 15.5: Actor–critic ANN and a hypothetical neural implementation. a) Actor–critic\\nalgorithm as an ANN. The actor adjusts a policy based on the TD error\\x00 it receives from the\\ncritic; the critic adjusts state-value parameters using the same\\x00. The critic produces a TD error\\nfrom the reward signal,R, and the current change in its estimate of state values. The actor does\\nnot have direct access to the reward signal, and the critic does not have direct access to the\\naction. b) Hypothetical neural implementation of an actor–critic algorithm. The actor and the\\nvalue-learning part of the critic are respectively placed in the dorsal and ventral subdivisions\\nof the striatum. The TD error is transmitted by dopamine neurons located in the VTA and\\nSNpc to modulate changes in synaptic e\\x00cacies of input from cortical areas to the ventral and\\ndorsal striatum. Adapted from Frontiers in Neuroscience, vol. 2(1), 2008, Y. Takahashi, G.\\nSchoenbaum, and Y. Niv, Silencing the critics: Understanding the e↵ects of cocaine sensitization\\non dorsolateral and ventral striatum in the context of an Actor/Critic model.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 418, 'page_label': '419'}, page_content='15.7. Neural Actor–Critic 397\\nBoth the critic and actor networks receive input consisting of multiple features repre-\\nsenting the state of the agent’s environment. (Recall from Chapter 1 that the environment\\nof a reinforcement learning agent includes components both inside and outside of the\\n‘organism’ containing the agent.) The ﬁgure shows these features as the circles labeled\\nx1,x 2,...,x n, shown twice just to keep the ﬁgure simple. A weight representing the\\ne\\x00cacy of a synapse is associated with each connection from each featurexi to the\\ncritic unit, V , and to each of the action units,Ai. The weights in the critic network\\nparameterize the value function, and the weights in the actor network parameterize the\\npolicy. The networks learn as these weights change according to the critic and actor\\nlearning rules that we describe in the following section.\\nThe TD error produced by circuitry in the critic is the reinforcement signal for changing\\nthe weights in both the critic and the actor networks. This is shown in Figure 15.5a by\\nthe line labeled ‘TD error\\x00’ extending across all of the connections in the critic and\\nactor networks. This aspect of the network implementation, together with the reward\\nprediction error hypothesis and the fact that the activity of dopamine neurons is so\\nwidely distributed by the extensive axonal arbors of these neurons, suggests that an\\nactor–critic network something like this may not be too farfetched as a hypothesis about\\nhow reward-related learning might happen in the brain.\\nFigure 15.5b suggests—very schematically—how the ANN on the ﬁgure’s left might\\nmap onto structures in the brain according to the hypothesis of Takahashi et al. (2008).\\nThe hypothesis puts the actor and the value-learning part of the critic respectively in the\\ndorsal and ventral subdivisions of the striatum, the input structure of the basal ganglia.\\nRecall from Section 15.4 that the dorsal striatum is primarily implicated in inﬂuencing\\naction selection, and the ventral striatum is thought to be critical for di↵erent aspects of\\nreward processing, including the assignment of a↵ective value to sensations. The cerebral\\ncortex, along with other structures, sends input to the striatum conveying information\\nabout stimuli, internal states, and motor activity.\\nIn this hypothetical actor–critic brain implementation, the ventral striatum sends value\\ninformation to the VTA and SNpc, where dopamine neurons in these nuclei combine it\\nwith information about reward to generate activity corresponding to TD errors (though\\nexactly how dopaminergic neurons calculate these errors is not yet understood). The ‘TD\\nerror \\x00’ line in Figure 15.5a becomes the line labeled ‘Dopamine’ in Figure 15.5b, which\\nrepresents the widely branching axons of dopamine neurons whose cell bodies are in the\\nVTA and SNpc. Referring back to Figure 15.1, these axons make synaptic contact with\\nthe spines on the dendrites of medium spiny neurons, the main input/output neurons of\\nboth the dorsal and ventral divisions of the striatum. Axons of the cortical neurons that\\nsend input to the striatum make synaptic contact on the tips of these spines. According\\nto the hypothesis, it is at these spines where changes in the e\\x00cacies of the synapses\\nfrom cortical regions to the striatum are governed by learning rules that critically depend\\non a reinforcement signal supplied by dopamine.\\nAn important implication of the hypothesis illustrated in Figure 15.5b is that the\\ndopamine signal is not the ‘master’ reward signal like the scalarRt of reinforcement\\nlearning. In fact, the hypothesis implies that one should not necessarily be able to\\nprobe the brain and record any signal likeRt in the activity of any single neuron.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 419, 'page_label': '420'}, page_content='398 Chapter 15: Neuroscience\\nMany interconnected neural systems generate reward-related information, with di↵erent\\nstructures being recruited depending on di↵erent types of rewards. Dopamine neurons\\nreceive information from many di↵erent brain areas, so the input to the SNpc and\\nVTA labeled ‘Reward’ in Figure 15.5b should be thought of as vector of reward-related\\ninformation arriving to neurons in these nuclei along multiple input channels. What the\\ntheoretical scalar reward signalRt might correspond to, then, is the net contribution of\\nall reward-related information to dopamine neuron activity. It is the result of a pattern\\nof activity across many neurons in di↵erent areas of the brain.\\nAlthough the actor–critic neural implementation illustrated in Figure 15.5b may be\\ncorrect on some counts, it clearly needs to be reﬁned, extended, and modiﬁed to qualify\\nas a full-ﬂedged model of the function of the phasic activity of dopamine neurons. The\\nHistorical and Bibliographic Remarks section at the end of this chapter cites publications\\nthat discuss in more detail both empirical support for this hypothesis and places where it\\nfalls short. We now look in detail at what the actor and critic learning algorithms suggest\\nabout the rules governing changes in synaptic e\\x00cacies of corticostriatal synapses.\\n15.8 Actor and Critic Learning Rules\\nIf the brain does implement something like the actor–critic algorithm—and assuming\\npopulations of dopamine neurons broadcast a common reinforcement signal to the corti-\\ncostriatal synapses of both the dorsal and ventral striatum as illustrated in Figure 15.5b\\n(which is likely an oversimpliﬁcation as we mentioned above)—then this reinforcement\\nsignal a↵ects the synapses of these two structures in di↵erent ways. The learning rules for\\nthe critic and the actor use the same reinforcement signal, the TD error\\x00,b u ti t se ↵ e c t\\non learning is di↵erent for these two components. The TD error (combined with eligibility\\ntraces) tells the actor how to update action probabilities in order to reach higher-valued\\nstates. Learning by the actor is like instrumental conditioning using a Law-of-E↵ect-type\\nlearning rule (Section 1.7): the actor works to keep\\x00 as positive as possible. On the\\nother hand, the TD error (when combined with eligibility traces) tells the critic the\\ndirection and magnitude in which to change the parameters of the value function in order\\nto improve its predictive accuracy. The critic works to reduce\\x00’s magnitude to be as\\nclose to zero as possible using a learning rule like the TD model of classical conditioning\\n(Section 14.2). The di↵erence between the critic and actor learning rules is relatively\\nsimple, but this di↵erence has a profound e↵ect on learning and is essential to how the\\nactor–critic algorithm works. The di↵erence lies solely in the eligibility traces each type\\nof learning rule uses.\\nMore than one set of learning rules can be used in actor–critic neural networks like\\nthose in Figure 15.5b but, to be speciﬁc, here we focus on the actor–critic algorithm for\\ncontinuing problems with eligibility traces presented in Section 13.6. On each transition\\nfrom stateSt to stateSt+1, taking actionAt and receiving rewardRt+1, that algorithm\\ncomputes the TD error (\\x00) and then updates the eligibility trace vectors (zw\\nt and z✓\\nt ) and'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 420, 'page_label': '421'}, page_content='15.8. Actor and Critic Learning Rules 399\\nthe parameters for the critic and actor (w and ✓), according to\\n\\x00t = Rt+1 + \\x00 ˆv(St+1,w) \\x00 ˆv(St,w),\\nzw\\nt = \\x00\\x00wzw\\nt\\x001 + rˆv(St,w),\\nz✓\\nt = \\x00\\x00✓z✓\\nt\\x001 + rln ⇡(At|St, ✓),\\nw  w + ↵w \\x00t zw\\nt ,\\n✓  ✓ + ↵✓ \\x00z✓\\nt ,\\nwhere \\x00 2 [0, 1) is a discount-rate parameter,\\x00w 2 [0, 1] and\\x00✓ 2 [0, 1] are bootstrapping\\nparameters for the critic and the actor respectively, and↵w > 0 and↵✓ > 0 are analogous\\nstep-size parameters.\\nThink of the approximate value functionˆv as the output of a single linear neuron-like\\nunit, called thecritic unit and labeled V in Figure 15.5a. Then the value function is a\\nlinear function of the feature-vector representation of states, x(s)=( x1(s),...,x n(s))>,\\nparameterized by a weight vectorw =( w1,...,w n)>:\\nˆv(s,w)= w>x(s). (15.1)\\nEach xi(s) is like the presynaptic signal to a neuron’s synapse whose e\\x00cacy iswi.T h e\\nweights of the critic are incremented according to the rule above by↵w\\x00tzw\\nt ,w h e r et h e\\nreinforcement signal,\\x00t, corresponds to a dopamine signal being broadcast to all of the\\ncritic unit’s synapses. The eligibility trace vector,zw\\nt , for the critic unit is a trace (average\\nof recent values) ofrˆv(St,w). Because ˆv(s,w) is linear in the weights,rˆv(St,w)= x(St).\\nIn neural terms, this means that each synapse has its own eligibility trace, which is\\none component of the vectorzw\\nt . A synapse’s eligibility trace accumulates according to\\nthe level of activity arriving at that synapse, that is, the level of presynaptic activity,\\nrepresented here by the component of the feature vectorx(St) arriving at that synapse.\\nThe trace otherwise decays toward zero at a rate governed by the fraction\\x00w. A synapse\\nis eligible for modiﬁcationas long as its eligibility trace is non-zero. How the synapse’s\\ne\\x00cacy is actually modiﬁed depends on the reinforcement signals that arrive while the\\nsynapse is eligible. We call eligibility traces like these of the critic unit’s synapsesnon-\\ncontingent eligibility tracesbecause they only depend on presynaptic activity and are not\\ncontingent in any way on postsynaptic activity.\\nThe non-contingent eligibility traces of the critic unit’s synapses mean that the critic\\nunit’s learning rule is essentially the TD model of classical conditioning described in\\nSection 14.2. With the deﬁnition we have given above of the critic unit and its learning\\nrule, the critic in Figure 15.5a is the same as the critic in the ANN actor–critic of Barto\\net al. (1983). Clearly, a critic like this consisting of just one linear neuron-like unit is the\\nsimplest starting point; this critic unit is a proxy for a more complicated neural network\\nable to learn value functions of greater complexity.\\nThe actor in Figure 15.5a is a one-layer network ofk neuron-like actor units, each\\nreceiving at timet the same feature vector,x(St), that the critic unit receives. Each\\nactor unitj, j =1 ,...,k , has its own weight vector,✓j, but because the actor units are\\nall identical, we describe just one of the units and omit the subscript. One way for these'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 421, 'page_label': '422'}, page_content='400 Chapter 15: Neuroscience\\nunits to follow the actor–critic algorithm given in the equations above is for each to be a\\nBernoulli-logistic unit. This means that the output of each actor unit at each time is\\na random variable,At, taking value 0 or 1. Think of value 1 as the neuron ﬁring, that\\nis, emitting an action potential. The weighted sum,✓>x(St), of a unit’s input vector\\ndetermines the unit’s action probabilities via the exponential soft-max distribution (13.2),\\nwhich for two actions is the logistic function:\\n⇡(1|s, ✓)=1 \\x00 ⇡(0|s, ✓)= 1\\n1+e x p (\\x00✓>x(s)). (15.2)\\nThe weights of each actor unit are incremented, as above, by:✓  ✓ + ↵✓ \\x00t z✓\\nt ,w h e r e\\n\\x00 again corresponds to the dopamine signal: the same reinforcement signal that is sent to\\nall the critic unit’s synapses. Figure 15.5a shows\\x00t being broadcast to all the synapses\\nof all the actor units (which makes this actor network ateam of reinforcement learning\\nagents, something we discuss in Section 15.10 below). The actor eligibility trace vector\\nz✓\\nt is a trace (average of recent values) ofrln ⇡(At|St, ✓). To understand this eligibility\\ntrace refer to Exercise 13.5, which deﬁnes this kind of unit and asks you to give a learning\\nrule for it. That exercise asked you to expressrln ⇡(a|s, ✓) in terms ofa, x(s), and\\n⇡(a|s, ✓) (for arbitrary states and actiona) by calculating the gradient. For the action\\nand state actually occurring at timet, the answer is\\nrln ⇡(At|St, ✓)=\\n\\x00\\nAt \\x00 ⇡(1|St, ✓)\\n\\x00\\nx(St). (15.3)\\nUnlike the non-contingent eligibility trace of a critic synapse that only accumulates\\nthe presynaptic activityx(St), the eligibility trace of an actor unit’s synapse in addition\\ndepends on the activity of the actor unit itself. We call this acontingent eligibility\\ntrace because it is contingent on this postsynaptic activity. The eligibility trace at each\\nsynapse continually decays, but increments or decrements depending on the activity of\\nthe presynaptic neuronand whether or not the postsynaptic neuron ﬁres. The factor\\nAt \\x00⇡(1|St, ✓)i n(15.3) is positive whenAt = 1 and negative otherwise.The postsynaptic\\ncontingency in the eligibility traces of actor units is the only di↵erence between the critic\\nand actor learning rules. By keeping information about what actions were taken in\\nwhat states, contingent eligibility traces allow credit for reward (positive\\x00), or blame for\\npunishment (negative\\x00), to be apportioned among the policy parameters (the e\\x00cacies\\nof the actor units’ synapses) according to the contributions these parameters made to the\\nunits’ outputs that could have inﬂuenced later values of\\x00. Contingent eligibility traces\\nmark the synapses as to how they should be modiﬁed to alter the units’ future responses\\nto favor positive values of\\x00.\\nWhat do the critic and actor learning rules suggest about how e\\x00cacies of corticostriatal\\nsynapses change? Both learning rules are related to Donald Hebb’s classic proposal that\\nwhenever a presynaptic signal participates in activating the postsynaptic neuron, the\\nsynapse’s e\\x00cacy increases (Hebb, 1949). The critic and actor learning rules share with\\nHebb’s proposal the idea that changes in a synapse’s e\\x00cacy depend on the interaction\\nof several factors. In the critic learning rule the interaction is between the reinforcement\\nsignal \\x00 and eligibility traces that depend only on presynaptic signals. Neuroscientists\\ncall this atwo-factor learning rule because the interaction is between two signals or'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 422, 'page_label': '423'}, page_content='15.8. Actor and Critic Learning Rules 401\\nquantities. The actor learning rule, on the other hand, is athree-factor learning rule\\nbecause, in addition to depending on\\x00, its eligibility traces depend on both presynaptic\\nand postsynaptic activity. Unlike Hebb’s proposal, however, the relative timing of the\\nfactors is critical to how synaptic e\\x00cacies change, with eligibility traces intervening to\\nallow the reinforcement signal to a↵ect synapses that were active in the recent past.\\nSome subtleties about signal timing for the actor and critic learning rules deserve closer\\nattention. In deﬁning the neuron-like actor and critic units, we ignored the small amount\\nof time it takes synaptic input to e↵ect the ﬁring of a real neuron. When an action\\npotential from the presynaptic neuron arrives at a synapse, neurotransmitter molecules\\nare released that di↵use across the synaptic cleft to the postsynaptic neuron, where\\nthey bind to receptors on the postsynaptic neuron’s surface; this activates molecular\\nmachinery that causes the postsynaptic neuron to ﬁre (or to inhibit its ﬁring in the case of\\ninhibitory synaptic input). This process can take several tens of milliseconds. According\\nto (15.1) and (15.2), though, the input to a critic and actor unit instantaneously produces\\nthe unit’s output. Ignoring activation time like this is common in abstract models of\\nHebbian-style plasticity in which synaptic e\\x00cacies change according to a simple product\\nof simultaneous pre- and postsynaptic activity. More realistic models must take activation\\ntime into account.\\nActivation time is especially important for a more realistic actor unit because it\\ninﬂuences how contingent eligibility traces have to work in order to properly apportion\\ncredit for reinforcement to the appropriate synapses. The expression\\n\\x00\\nAt\\x00⇡(1|St, ✓)\\n\\x00\\nx(St)\\ndeﬁning contingent eligibility traces for the actor unit’s learning rule given above includes\\nthe postsynaptic factor\\n\\x00\\nAt \\x00 ⇡(1|St, ✓)\\n\\x00\\nand the presynaptic factorx(St). This works\\nbecause by ignoring activation time, the presynaptic activityx(St) participates incausing\\nthe postsynaptic activity appearing in\\n\\x00\\nAt \\x00⇡(1|St, ✓)\\n\\x00\\n. To assign credit for reinforcement\\ncorrectly, the presynaptic factor deﬁning the eligibility trace must be a cause of the\\npostsynaptic factor that also deﬁnes the trace. Contingent eligibility traces for a more\\nrealistic actor unit would have to take activation time into account. (Activation time\\nshould not be confused with the time required for a neuron to receive a reinforcement\\nsignal inﬂuenced by that neuron’s activity. The function of eligibility traces is to span\\nthis time interval which is generally much longer than the activation time. We discuss\\nthis further in the following section.)\\nThere are hints from neuroscience for how this process might work in the brain.\\nNeuroscientists have discovered a form of Hebbian plasticity calledspike-timing-dependent\\nplasticity (STDP) that lends plausibility to the existence of actor-like synaptic plasticity\\nin the brain. STDP is a Hebbian-style plasticity, but changes in a synapse’s e\\x00cacy\\ndepend on the relative timing of presynaptic and postsynaptic action potentials. The\\ndependence can take di↵erent forms, but in the one most studied, a synapse increases in\\nstrength if spikes incoming via that synapse arrive shortly before the postsynaptic neuron\\nﬁres. If the timing relation is reversed, with a presynaptic spike arriving shortly after the\\npostsynaptic neuron ﬁres, then the strength of the synapse decreases. STDP is a type of\\nHebbian plasticity that takes the activation time of a neuron into account, which is one\\nof the ingredients needed for actor-like learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 423, 'page_label': '424'}, page_content='402 Chapter 15: Neuroscience\\nThe discovery of STDP has led neuroscientists to investigate the possibility of a three-\\nfactor form of STDP in which neuromodulatory input must follow appropriately-timed\\npre- and postsynaptic spikes. This form of synaptic plasticity, calledreward-modulated\\nSTDP, is much like the actor learning rule discussed here. Synaptic changes that would\\nbe produced by regular STDP only occur if there is neuromodulatory input within a time\\nwindow after a presynaptic spike is closely followed by a postsynaptic spike. Evidence\\nis accumulating that reward-modulated STDP occurs at the spines of medium spiny\\nneurons of the dorsal striatum, with dopamine providing the neuromodulatory factor—\\nthe sites where actor learning takes place in the hypothetical neural implementation of\\nan actor–critic algorithm illustrated in Figure 15.5b. Experiments have demonstrated\\nreward-modulated STDP in which lasting changes in the e\\x00cacies of corticostriatal\\nsynapses occur only if a neuromodulatory pulse arrives within a time window that can\\nlast up to 10 seconds after a presynaptic spike is closely followed by a postsynaptic\\nspike (Yagishita et al. 2014). Although the evidence is indirect, these experiments point\\nto the existence of contingent eligibility traces having prolonged time courses. The\\nmolecular mechanisms producing these traces, as well as the much shorter traces that\\nlikely underly STDP, are not yet understood, but research focusing on time-dependent\\nand neuromodulator-dependent synaptic plasticity is continuing.\\nThe neuron-like actor unit that we have described here, with its Law-of-E↵ect-style\\nlearning rule, appeared in somewhat simpler form in the actor–critic network of Barto et\\nal. (1983). That network was inspired by the “hedonistic neuron” hypothesis proposed\\nby physiologist A. H. Klopf (1972, 1982). Not all the details of Klopf’s hypothesis are\\nconsistent with what has been learned about synaptic plasticity, but the discovery of\\nSTDP and the growing evidence for a reward-modulated form of STDP suggest that\\nKlopf’s ideas may not have been far o↵ the mark. We discuss Klopf’s hedonistic neuron\\nhypothesis next.\\n15.9 Hedonistic Neurons\\nIn his hedonistic neuron hypothesis, Klopf (1972, 1982) conjectured that individual\\nneurons seek to maximize the di↵erence between synaptic input treated as rewarding\\nand synaptic input treated as punishing by adjusting the e\\x00cacies of their synapses\\non the basis of rewarding or punishing consequences of their own action potentials. In\\nother words, individual neurons can be trained with response-contingent reinforcement\\nlike an animal can be trained in an instrumental conditioning task. His hypothesis\\nincluded the idea that rewards and punishments are conveyed to a neuron via the same\\nsynaptic input that excites or inhibits the neuron’s spike-generating activity. (Had Klopf\\nknown what we know today about neuromodulatory systems, he might have assigned the\\nreinforcing role to neuromodulatory input, but he wanted to avoid any centralized source\\nof training information.) Synaptically-local traces of past pre- and postsynaptic activity\\nhad the key function in Klopf’s hypothesis of making synapseseligible—the term he\\nintroduced—for modiﬁcation by later reward or punishment. He conjectured that these\\ntraces are implemented by molecular mechanisms local to each synapse and therefore\\ndi↵erent from the electrical activity of both the pre- and the postsynaptic neurons. In'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 424, 'page_label': '425'}, page_content='15.9. Hedonistic Neurons 403\\nthe Bibliographical and Historical Remarks section of this chapter we bring attention to\\nsome similar proposals made by others.\\nKlopf speciﬁcally conjectured that synaptic e\\x00cacies change in the following way. When\\na neuron ﬁres an action potential, all of its synapses that were active in contributing to\\nthat action potential become eligible to undergo changes in their e\\x00cacies. If the action\\npotential is followed within an appropriate time period by an increase of reward, the\\ne\\x00cacies of all the eligible synapses increase. Symmetrically, if the action potential is\\nfollowed within an appropriate time period by an increase of punishment, the e\\x00cacies\\nof eligible synapses decrease. This is implemented by triggering an eligibility trace at a\\nsynapse upon a coincidence of presynaptic and postsynaptic activity (or more exactly,\\nupon pairing of presynaptic activity with the postsynaptic activity that that presynaptic\\nactivity participates in causing)—what we call a contingent eligibility trace. This is\\nessentially the three-factor learning rule of an actor unit described in the previous section.\\nThe shape and time course of an eligibility trace in Klopf’s theory reﬂects the durations\\nof the many feedback loops in which the neuron is embedded, some of which lie entirely\\nwithin the brain and body of the organism, while others extend out through the organism’s\\nexternal environment as mediated by its motor and sensory systems. His idea was that\\nthe shape of a synaptic eligibility trace is like a histogram of the durations of the feedback\\nloops in which the neuron is embedded. The peak of an eligibility trace would then occur\\nat the duration of the most prevalent feedback loops in which that neuron participates.\\nThe eligibility traces used by algorithms described in this book are simpliﬁed versions\\nof Klopf’s original idea, being exponentially (or geometrically) decreasing functions\\ncontrolled by the parameters\\x00 and \\x00. This simpliﬁes simulations as well as theory, but\\nwe regard these simple eligibility traces as a placeholders for traces closer to Klopf’s\\noriginal conception, which would have computational advantages in complex reinforcement\\nlearning systems by reﬁning the credit-assignment process.\\nKlopf’s hedonistic neuron hypothesis is not as implausible as it may at ﬁrst appear.\\nA well-studied example of a single cell that seeks some stimuli and avoids others is the\\nbacterium Escherichia coli. The movement of this single-cell organism is inﬂuenced by\\nchemical stimuli in its environment, behavior known as chemotaxis. It swims in its liquid\\nenvironment by rotating hairlike structures called ﬂagella attached to its surface. (Yes,\\nit rotates them!) Molecules in the bacterium’s environment bind to receptors on its\\nsurface. Binding events modulate the frequency with which the bacterium reverses ﬂagellar\\nrotation. Each reversal causes the bacterium to tumble in place and then head o↵ in a\\nrandom new direction. A little chemical memory and computation causes the frequency\\nof ﬂagellar reversal to decrease when the bacterium swims toward higher concentrations\\nof molecules it needs to survive (attractants) and increase when the bacterium swims\\ntoward higher concentrations of molecules that are harmful (repellants). The result is\\nthat the bacterium tends to persist in swimming up attractant gradients and tends to\\navoid swimming up repellant gradients.\\nThe chemotactic behavior just described is called klinokinesis. It is a kind of trial-\\nand-error behavior, although it is unlikely that learning is involved: the bacterium needs\\na modicum of short-term memory to detect molecular concentration gradients, but it\\nprobably does not maintain long-term memories. Artiﬁcial intelligence pioneer Oliver'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 425, 'page_label': '426'}, page_content='404 Chapter 15: Neuroscience\\nSelfridge called this strategy “run and twiddle,” pointing out its utility as a basic adaptive\\nstrategy: “keep going in the same way if things are getting better, and otherwise move\\naround” (Selfridge, 1978, 1984). Similarly, one might think of a neuron “swimming” (not\\nliterally of course) in a medium composed of the complex collection of feedback loops\\nin which it is embedded, acting to obtain one type of input signal and to avoid others.\\nUnlike the bacterium, however, the neuron’s synaptic strengths retain information about\\nits past trial-and-error behavior. If this view of the behavior of a neuron (or just one type\\nof neuron) is plausible, then the closed-loop nature of how the neuron interacts with its\\nenvironment is important for understanding its behavior, where the neuron’s environment\\nconsists of the rest of the animal together with the environment with which the animal\\nas a whole interacts.\\nKlopf’s hedonistic neuron hypothesis extended beyond the idea that individual neurons\\nare reinforcement learning agents. He argued that many aspects of intelligent behavior\\ncan be understood as the result of the collective behavior of a population of self-interested\\nhedonistic neurons interacting with one another in an immense society or economic system\\nmaking up an animal’s nervous system. Whether or not this view of nervous systems\\nis useful, the collective behavior of reinforcement learning agents has implications for\\nneuroscience. We take up this subject next.\\n15.10 Collective Reinforcement Learning\\nThe behavior of populations of reinforcement learning agents is deeply relevant to the\\nstudy of social and economic systems, and if anything like Klopf’s hedonistic neuron\\nhypothesis is correct, to neuroscience as well. The hypothesis described above about how\\nan actor–critic algorithm might be implemented in the brain only narrowly addresses\\nthe implications of the fact that the dorsal and ventral subdivisions of the striatum, the\\nrespective locations of the actor and the critic according to the hypothesis, each contain\\nmillions of medium spiny neurons whose synapses undergo change modulated by phasic\\nbursts of dopamine neuron activity.\\nThe actor in Figure 15.5a is a single-layer network ofk actor units. The actions\\nproduced by this network are vectors (A1,A 2, ··· ,A k)> presumed to drive the animal’s\\nbehavior. Changes in the e\\x00cacies of the synapses of all of these units depend on the\\nreinforcement signal \\x00. Because actor units attempt to make\\x00 as large as possible,\\x00\\ne↵ectively acts as a reward signal for them (so in this case reinforcement is the same\\nas reward). Thus, each actor unit is itself a reinforcement learning agent—a hedonistic\\nneuron if you will. Now, to make the situation as simple as possible, assume that each\\nof these units receives the same reward signal at the same time (although, as indicated\\nabove, the assumption that dopamine is released at all the corticostriatal synapses under\\nthe same conditions and at the same times is likely an oversimpliﬁcation).\\nWhat can reinforcement learning theory tell us about what happens when all members\\nof a population of reinforcement learning agents learn according to a common reward\\nsignal? The ﬁeld ofmulti-agent reinforcement learningconsiders many aspects of learning\\nby populations of reinforcement learning agents. Although this ﬁeld is beyond the scope\\nof this book, we believe that some of its basic concepts and results are relevant to thinking'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 426, 'page_label': '427'}, page_content='15.10. Collective Reinforcement Learning 405\\nabout the brain’s di↵use neuromodulatory systems. In multi-agent reinforcement learning\\n(and in game theory), the scenario in which all the agents try to maximize a common\\nreward signal that they simultaneously receive is known as acooperative gameor ateam\\nproblem.\\nWhat makes a team problem interesting and challenging is that the common reward\\nsignal sent to each agent evaluates thepattern of activity produced by the entire population,\\nthat is, it evaluates thecollective action of the team members. This means that any\\nindividual agent has only limited ability to a↵ect the reward signal because any single\\nagent contributes just one component of the collective action evaluated by the common\\nreward signal. E↵ective learning in this scenario requires addressing astructural credit\\nassignment problem: which team members, or groups of team members, deserve credit for\\na favorable reward signal, or blame for an unfavorable reward signal? It is acooperative\\ngame, or a team problem, because the agents are united in seeking to increase the same\\nreward signal: there are no conﬂicts of interest among the agents. The scenario would be\\na competitive game if di↵erent agents receive di↵erent reward signals, where each reward\\nsignal again evaluates the collective action of the population, and the objective of each\\nagent is to increase its own reward signal. In this case there might be conﬂicts of interest\\namong the agents, meaning that actions that are good for some agents are bad for others.\\nEven deciding what the best collective action should be is a non-trivial aspect of game\\ntheory. This competitive setting might be relevant to neuroscience too (for example, to\\naccount for heterogeneity of dopamine neuron activity), but here we focus only on the\\ncooperative, or team, case.\\nHow can each reinforcement learning agent in a team learn to “do the right thing” so\\nthat the collective action of the team is highly rewarded? An interesting result is that\\nif each agent can learn e↵ectively despite its reward signal being corrupted by a large\\namount of noise, and despite its lack of access to complete state information, then the\\npopulation as a whole will learn to produce collective actions that improve as evaluated by\\nthe common reward signal, even when the agents cannot communicate with one another.\\nEach agent faces its own reinforcement learning task in which its inﬂuence on the reward\\nsignal is deeply buried in the noise created by the inﬂuences of other agents. In fact, for\\nany agent, all the other agents are part of its environment because its input, both the part\\nconveying state information and the reward part, depends on how all the other agents\\nare behaving. Furthermore, lacking access to the actions of the other agents, indeed\\nlacking access to the parameters determining their policies, each agent can only partially\\nobserve the state of its environment. This makes each team member’s learning task very\\ndi\\x00cult, but if each uses a reinforcement learning algorithm able to increase a reward\\nsignal even under these di\\x00cult conditions, teams of reinforcement learning agents can\\nlearn to produce collective actions that improve over time as evaluated by the team’s\\ncommon reward signal.\\nIf the team members are neuron-like units, then each unit has to have the goal of\\nincreasing the amount of reward it receives over time, as the actor unit does that we\\ndescribed in Section 15.8. Each unit’s learning algorithm has to have two essential features.\\nFirst, it has to use contingent eligibility traces. Recall that a contingent eligibility trace,\\nin neural terms, is initiated (or increased) at a synapse when its presynaptic input'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 427, 'page_label': '428'}, page_content='406 Chapter 15: Neuroscience\\nparticipates in causing the postsynaptic neuron to ﬁre. A non-contingent eligibility trace,\\nin contrast, is initiated or increased by presynaptic input independently of what the\\npostsynaptic neuron does. As explained in Section 15.8, by keeping information about\\nwhat actions were taken in what states, contingent eligibility traces allow credit for\\nreward, or blame for punishment, to be apportioned to an agent’s policy parameters\\naccording to the contribution the values of these parameters made in determining the\\nagent’s action. By similar reasoning, a team member must remember its recent action so\\nthat it can either increase or decrease the likelihood of producing that action according\\nto the reward signal that is subsequently received. The action component of a contingent\\neligibility trace implements this action memory. Because of the complexity of the learning\\ntask, however, contingent eligibility is merely a preliminary step in the credit assignment\\nprocess: the relationship between a single team member’s action and changes in the\\nteam’s reward signal is a statistical correlation that has to be estimated over many trials.\\nContingent eligibility is an essential but preliminary step in this process.\\nLearning with non-contingent eligibility traces does not work at all in the team setting\\nbecause it does not provide a way to correlate actions with consequent changes in the\\nreward signal. Non-contingent eligibility traces are adequate for learning to predict, as\\nthe critic component of the actor–critic algorithm does, but they do not support learning\\nto control, as the actor component must do. The members of a population of critic-like\\nagents may still receive a common reinforcement signal, but they would all learn to\\npredict the same quantity (which in the case of an actor–critic method, would be the\\nexpected return for the current policy). How successful each member of the population\\nwould be in learning to predict the expected return would depend on the information it\\nreceives, which could be very di↵erent for di↵erent members of the population. There\\nwould be no need for the population to produce di↵erentiated patterns of activity. This\\nis not a team problem as deﬁned here.\\nA second requirement for collective learning in a team problem is that there has to be\\nvariability in the actions of the team members in order for the team to explore the space\\nof collective actions. The simplest way for a team of reinforcement learning agents to do\\nthis is for each member to independently explore its own action space through persistent\\nvariability in its output. This will cause the team as a whole to vary its collective actions.\\nFor example, a team of the actor units described in Section 15.8 explores the space\\nof collective actions because the output of each unit, being a Bernoulli-logistic unit,\\nprobabilistically depends on the weighted sum of its input vector’s components. The\\nweighted sum biases ﬁring probability up or down, but there is always variability. Because\\neach unit uses a REINFORCE policy gradient algorithm (Chapter 13), each unit adjusts\\nits weights with the goal of maximizing the average reward rate it experiences while\\nstochastically exploring its own action space. One can show, as Williams (1992) did, that\\na team of Bernoulli-logistic REINFORCE units implements a policy gradient algorithm\\nas a wholewith respect to average rate of the team’s common reward signal, where the\\nactions are the collective actions of the team.\\nFurther, Williams (1992) showed that a team of Bernoulli-logistic units using REIN-\\nFORCE ascends the average reward gradient when the units in the team are interconnected\\nto form a multilayer ANN. In this case, the reward signal is broadcast to all the units in'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 428, 'page_label': '429'}, page_content='15.11. Model-based Methods in the Brain 407\\nthe network, though reward may depend only on the collective actions of the network’s\\noutput units. This means that a multilayer team of Bernoulli-logistic REINFORCE\\nunits learns like a multilayer network trained by the widely-used error backpropagation\\nmethod, but in this case the backpropagation process is replaced by the broadcasted\\nreward signal. In practice, the error backpropagation method is considerably faster,\\nbut the reinforcement learning team method is more plausible as a neural mechanism,\\nespecially in light of what is being learned about reward-modulated STDP as discussed\\nin Section 15.8.\\nExploration through independent exploration by team members is only the simplest\\nway for a team to explore; more sophisticated methods are possible if the team members\\ncoordinate their actions to focus on particular parts of the collective action space, either\\nby communicating with one another or by responding to common inputs. There are also\\nmechanisms more sophisticated than contingent eligibility traces for addressing structural\\ncredit assignment, which is easier in a team problem when the set of possible collective\\nactions is restricted in some way. An extreme case is a winner-take-all arrangement (for\\nexample, the result of lateral inhibition in the brain) that restricts collective actions to\\nthose to which only one, or a few, team members contribute. In this case the winners get\\nthe credit or blame for resulting reward or punishment.\\nDetails of learning in cooperative games (or team problems) and non-cooperative\\ngame problems are beyond the scope of this book. The Bibliographical and Historical\\nRemarks section at the end of this chapter cites a selection of the relevant publications,\\nincluding extensive references to research on implications for neuroscience of collective\\nreinforcement learning.\\n15.11 Model-based Methods in the Brain\\nReinforcement learning’s distinction between model-free and model-based algorithms is\\nproving to be useful for thinking about animal learning and decision processes. Section 14.6\\ndiscusses how this distinction aligns with that between habitual and goal-directed animal\\nbehavior. The hypothesis discussed above about how the brain might implement an\\nactor–critic algorithm is relevant only to an animal’s habitual mode of behavior because\\nthe basic actor–critic method is model-free. What neural mechanisms are responsible\\nfor producing goal-directed behavior, and how do they interact with those underlying\\nhabitual behavior?\\nOne way to investigate questions about the brain structures involved in these modes\\nof behavior is to inactivate an area of a rat’s brain and then observe what the rat does in\\nan outcome-devaluation experiment (Section 14.6). Results from experiments like these\\nindicate that the actor–critic hypothesis described above is too simple in placing the\\nactor in the dorsal striatum. Inactivating one part of the dorsal striatum, the dorsolateral\\nstriatum (DLS), impairs habit learning, causing the animal to rely more on goal-directed\\nprocesses. On the other hand, inactivating the dorsomedial striatum (DMS) impairs\\ngoal-directed processes, requiring the animal to rely more on habit learning. Results\\nlike these support the view that the DLS in rodents is more involved in model-free\\nprocesses, whereas their DMS is more involved in model-based processes. Results of'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 429, 'page_label': '430'}, page_content='408 Chapter 15: Neuroscience\\nstudies with human subjects in similar experiments using functional neuroimaging, and\\nwith non-human primates, support the view that the analogous structures in the primate\\nbrain are di↵erentially involved in habitual and goal-directed modes of behavior.\\nOther studies identify activity associated with model-based processes in the prefrontal\\ncortex of the human brain, the front-most part of the frontal cortex implicated in\\nexecutive function, including planning and decision making. Speciﬁcally implicated is the\\norbitofrontal cortex (OFC), the part of the prefrontal cortex immediately above the eyes.\\nFunctional neuroimaging in humans, and also recordings of the activities of single neurons\\nin monkeys, reveals strong activity in the OFC related to the subjective reward value\\nof biologically signiﬁcant stimuli, as well as activity related to the reward expected as a\\nconsequence of actions. Although not free of controversy, these results suggest signiﬁcant\\ninvolvement of the OFC in goal-directed choice. It may be critical for the reward part of\\nan animal’s environment model.\\nAnother structure involved in model-based behavior is the hippocampus, a structure\\ncritical for memory and spatial navigation. A rat’s hippocampus plays a critical role\\nin the rat’s ability to navigate a maze in the goal-directed manner that led Tolman to\\nthe idea that animals use models, or cognitive maps, in selecting actions (Section 14.5).\\nThe hippocampus may also be a critical component of our human ability to imagine\\nnew experiences (Hassabis and Maguire, 2007;´Olafsd´ ottir, Barry, Saleem, Hassabis, and\\nSpiers, 2015).\\nThe ﬁndings that most directly implicate the hippocampus in planning—the process\\nneeded to enlist an environment model in making decisions—come from experiments\\nthat decode the activity of neurons in the hippocampus to determine what part of space\\nhippocampal activity is representing on a moment-to-moment basis. When a rat pauses\\nat a choice point in a maze, the representation of space in the hippocampus sweeps\\nforward (and not backwards) along the possible paths the animal can take from that\\npoint (Johnson and Redish, 2007). Furthermore, the spatial trajectories represented by\\nthese sweeps closely correspond to the rat’s subsequent navigational behavior (Pfei↵er\\nand Foster, 2013). These results suggest that the hippocampus is critical for the state-\\ntransition part of an animal’s environment model, and that it is part of a system that\\nuses the model to simulate possible future state sequences to assess the consequences of\\npossible courses of action: a form of planning.\\nThe results described above add to a voluminous literature on neural mechanisms\\nunderlying goal-directed, or model-based, learning and decision making, but many\\nquestions remain unanswered. For example, how can areas as structurally similar as the\\nDLS and DMS be essential components of modes of learning and behavior that are as\\ndi↵erent as model-free and model-based algorithms? Are separate structures responsible\\nfor (what we call) the transition and reward components of an environment model? Is all\\nplanning conducted at decision time via simulations of possible future courses of action\\nas the forward sweeping activity in the hippocampus suggests? In other words, is all\\nplanning something like a rollout algorithm (Section 8.10)? Or are models sometimes\\nengaged in the background to reﬁne or recompute value information as illustrated by the\\nDyna architecture (Section 8.2)? How does the brain arbitrate between the use of the\\nhabit and goal-directed systems? Is there, in fact, a clear separation between the neural\\nsubstrates of these systems?'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 430, 'page_label': '431'}, page_content='15.12. Addiction 409\\nThe evidence is not pointing to a positive answer to this last question. Summarizing\\nthe situation, Doll, Simon, and Daw (2012) wrote that “model-based inﬂuences appear\\nubiquitous more or less wherever the brain processes reward information,” and this is\\ntrue even in the regions thought to be critical for model-free learning. This includes the\\ndopamine signals themselves, which can exhibit the inﬂuence of model-based information\\nin addition to the reward prediction errors thought to be the basis of model-free processes.\\nContinuing neuroscience research informed by reinforcement learning’s model-free and\\nmodel-based distinction has the potential to sharpen our understanding of habitual and\\ngoal-directed processes in the brain. A better grasp of these neural mechanisms may lead\\nto algorithms combining model-free and model-based methods in ways that have not yet\\nbeen explored in computational reinforcement learning.\\n15.12 Addiction\\nUnderstanding the neural basis of drug abuse is a high-priority goal of neuroscience with\\nthe potential to produce new treatments for this serious public health problem. One\\nview is that drug craving is the result of the same motivation and learning processes that\\nlead us to seek natural rewarding experiences that serve our biological needs. Addictive\\nsubstances, by being intensely reinforcing, e↵ectively co-opt our natural mechanisms\\nof learning and decision making. This is plausible given that many—though not all—\\ndrugs of abuse increase levels of dopamine either directly or indirectly in regions around\\nterminals of dopamine neuron axons in the striatum, a brain structure ﬁrmly implicated in\\nnormal reward-based learning (Section 15.7). But the self-destructive behavior associated\\nwith drug addiction is not characteristic of normal learning. What is di↵erent about\\ndopamine-mediated learning when the reward is the result of an addictive drug? Is\\naddiction the result of normal learning in response to substances that were largely\\nunavailable throughout our evolutionary history, so that evolution could not select against\\ntheir damaging e↵ects? Or do addictive substances somehow interfere with normal\\ndopamine-mediated learning?\\nThe reward prediction error hypothesis of dopamine neuron activity and its connection\\nto TD learning are the basis of a model due to Redish (2004) of some—but certainly not\\nall—features of addiction. The model is based on the observation that administration of\\ncocaine and some other addictive drugs produces a transient increase in dopamine. In the\\nmodel, this dopamine surge is assumed to increase the TD error,\\x00, in a way that cannot\\nbe cancelled out by changes in the value function. In other words, whereas\\x00 is reduced\\nto the degree that a normal reward is predicted by antecedent events (Section 15.6), the\\ncontribution to \\x00 due to an addictive stimulus does not decrease as the reward signal\\nbecomes predicted: drug rewards cannot be “predicted away.” The model does this by\\npreventing \\x00 from ever becoming negative when the reward signal is due to an addictive\\ndrug, thus eliminating the error-correcting feature of TD learning for states associated\\nwith administration of the drug. The result is that the values of these states increase\\nwithout bound, making actions leading to these states preferred above all others.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 431, 'page_label': '432'}, page_content='410 Chapter 15: Neuroscience\\nAddictive behavior is much more complicated than this result from Redish’s model, but\\nthe model’s main idea may be a piece of the puzzle. Or the model might be misleading.\\nDopamine appears not to play a critical role in all forms of addiction, and not everyone\\nis equally susceptible to developing addictive behavior. Moreover, the model does not\\ninclude the changes in many circuits and brain regions that accompany chronic drug\\ntaking, for example, changes that lead to a drug’s diminishing e↵ect with repeated use.\\nIt is also likely that addiction involves model-based processes. Still, Redish’s model\\nillustrates how reinforcement learning theory can be enlisted in the e↵ort to understand\\na major health problem. In a similar manner, reinforcement learning theory has been\\ninﬂuential in the development of the new ﬁeld of computational psychiatry, which aims\\nto improve understanding of mental disorders through mathematical and computational\\nmethods.\\n15.13 Summary\\nThe neural pathways involved in the brain’s reward system are complex and incompletely\\nunderstood, but neuroscience research directed toward understanding these pathways\\nand their roles in behavior is progressing rapidly. This research is revealing striking\\ncorrespondences between the brain’s reward system and the theory of reinforcement\\nlearning as presented in this book.\\nThe reward prediction error hypothesis of dopamine neuron activitywas proposed by\\nscientists who recognized striking parallels between the behavior of TD errors and the\\nactivity of neurons that produce dopamine, a neurotransmitter essential in mammals\\nfor reward-related learning and behavior. Experiments conducted in the late 1980s and\\n1990s in the laboratory of neuroscientist Wolfram Schultz showed that dopamine neurons\\nrespond to rewarding events with substantial bursts of activity, called phasic responses,\\nonly if the animal does not expect those events, suggesting that dopamine neurons are\\nsignaling reward prediction errors instead of reward itself. Further, these experiments\\nshowed that as an animal learns to predict a rewarding event on the basis of preceding\\nsensory cues, the phasic activity of dopamine neurons shifts to earlier predictive cues\\nwhile decreasing to later predictive cues. This parallels the backing-up e↵ect of the TD\\nerror as a reinforcement learning agent learns to predict reward.\\nOther experimental results ﬁrmly establish that the phasic activity of dopamine neurons\\nis a reinforcement signal for learning that reaches multiple areas of the brain by means of\\nprofusely branching axons of dopamine producing neurons. These results are consistent\\nwith the distinction we make between a reward signal,Rt, and a reinforcement signal,\\nwhich is the TD error\\x00t in most of the algorithms we present. Phasic responses of\\ndopamine neurons are reinforcement signals, not reward signals.\\nA prominent hypothesis is that the brain implements something like an actor–critic\\nalgorithm. Two structures in the brain (the dorsal and ventral subdivisions of the\\nstriatum), both of which play critical roles in reward-based learning, may function\\nrespectively like an actor and a critic. That the TD error is the reinforcement signal for\\nboth the actor and the critic ﬁts well with the facts that dopamine neuron axons target\\nboth the dorsal and ventral subdivisions of the striatum; that dopamine appears to be'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 432, 'page_label': '433'}, page_content='15.13. Summary 411\\ncritical for modulating synaptic plasticity in both structures; and that the e↵ect on a\\ntarget structure of a neuromodulator such as dopamine depends on properties of the\\ntarget structure and not just on properties of the neuromodulator.\\nThe actor and the critic can be implemented by ANNs consisting of neuron-like units\\nhaving learning rules based on the policy-gradient actor–critic method described in\\nSection 13.5. Each connection in these networks is like a synapse between neurons in\\nthe brain, and the learning rules correspond to rules governing how synaptic e\\x00cacies\\nchange as functions of the activities of the presynaptic and the postsynaptic neurons,\\ntogether with neuromodulatory input corresponding to input from dopamine neurons. In\\nthis setting, each synapse has its own eligibility trace that records past activity involving\\nthat synapse. The only di↵erence between the actor and critic learning rules is that they\\nuse di↵erent kinds of eligibility traces: the critic unit’s traces arenon-contingent because\\nthey do not involve the critic unit’s output, whereas the actor unit’s traces arecontingent\\nbecause in addition to the actor unit’s input, they depend on the actor unit’s output. In\\nthe hypothetical implementation of an actor–critic system in the brain, these learning\\nrules respectively correspond to rules governing plasticity of corticostriatal synapses that\\nconvey signals from the cortex to the principal neurons in the dorsal and ventral striatal\\nsubdivisions, synapses that also receive inputs from dopamine neurons.\\nThe learning rule of an actor unit in the actor–critic network closely corresponds to\\nreward-modulated spike-timing-dependent plasticity. In spike-timing-dependent plasticity\\n(STDP), the relative timing of pre- and postsynaptic activity determines the direction of\\nsynaptic change. In reward-modulated STDP, changes in synapses in addition depend\\non a neuromodulator, such as dopamine, arriving within a time window that can last\\nup to 10 seconds after the conditions for STDP are met. Evidence is accumulating that\\nreward-modulated STDP occurs at corticostriatal synapses, where the actor’s learning\\ntakes place in the hypothetical neural implementation of an actor–critic system, adds to\\nthe plausibility of the hypothesis that something like an actor–critic system exists in the\\nbrains of some animals.\\nThe idea of synaptic eligibility and basic features of the actor learning rule derive\\nfrom Klopf’s hypothesis of the “hedonistic neuron” (Klopf, 1972, 1981). He conjectured\\nthat individual neurons seek to obtain reward and to avoid punishment by adjusting the\\ne\\x00cacies of their synapses on the basis of rewarding or punishing consequences of their\\naction potentials. A neuron’s activity can a↵ect its later input because the neuron is\\nembedded in many feedback loops, some within the animal’s nervous system and body\\nand others passing through the animal’s external environment. Klopf’s idea of eligibility\\nis that synapses are temporarily marked as eligible for modiﬁcation if they participated\\nin the neuron’s ﬁring (making this the contingent form of eligibility trace). A synapse’s\\ne\\x00cacy is modiﬁed if a reinforcing signal arrives while the synapse is eligible. We alluded\\nto the chemotactic behavior of a bacterium as an example of a single cell that directs its\\nmovements in order to seek some molecules and to avoid others.\\nA conspicuous feature of the dopamine system is that ﬁbers releasing dopamine project\\nwidely to multiple parts of the brain. Although it is likely that only some populations\\nof dopamine neurons broadcast the same reinforcement signal, if this signal reaches\\nthe synapses of many neurons involved in actor-type learning, then the situation can'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 433, 'page_label': '434'}, page_content='412 Chapter 15: Neuroscience\\nbe modeled as ateam problem. In this type of problem, each agent in a collection of\\nreinforcement learning agents receives the same reinforcement signal, where that signal\\ndepends on the activities of all members of the collection, or team. If each team member\\nuses a su\\x00ciently capable learning algorithm, the team can learn collectively to improve\\nperformance of the entire team as evaluated by the globally-broadcast reinforcement\\nsignal, even if the team members do not directly communicate with one another. This\\nis consistent with the wide dispersion of dopamine signals in the brain and provides\\na neurally plausible alternative to the widely-used error-backpropagation method for\\ntraining multilayer networks.\\nThe distinction between model-free and model-based reinforcement learning is helping\\nneuroscientists investigate the neural bases of habitual and goal-directed learning and\\ndecision making. Research so far points to their being some brain regions more involved\\nin one type of process than the other, but the picture remains unclear because model-free\\nand model-based processes do not appear to be neatly separated in the brain. Many\\nquestions remain unanswered. Perhaps most intriguing is evidence that the hippocampus,\\na structure traditionally associated with spatial navigation and memory, appears to be\\ninvolved in simulating possible future courses of action as part of an animal’s decision-\\nmaking process. This suggests that it is part of a system that uses an environment model\\nfor planning.\\nReinforcement learning theory is also inﬂuencing thinking about neural processes\\nunderlying drug abuse. A model of some features of drug addiction is based on the reward\\nprediction error hypothesis. It proposes that an addicting stimulant, such as cocaine,\\ndestabilizes TD learning to produce unbounded growth in the values of actions associated\\nwith drug intake. This is far from a complete model of addiction, but it illustrates how\\na computational perspective suggests theories that can be tested with further research.\\nThe new ﬁeld of computational psychiatry similarly focuses on the use of computational\\nmodels, some derived from reinforcement learning, to better understand mental disorders.\\nThis chapter only touched the surface of how the neuroscience of reinforcement learning\\nand the development of reinforcement learning in computer science and engineering\\nhave inﬂuenced one another. Most features of reinforcement learning algorithms owe\\ntheir design to purely computational considerations, but some have been inﬂuenced by\\nhypotheses about neural learning mechanisms. Remarkably, as experimental data has\\naccumulated about the brain’s reward processes, many of the purely computationally-\\nmotivated features of reinforcement learning algorithms are turning out to be consistent\\nwith neuroscience data. Other features of computational reinforcement learning, such as\\neligibility traces and the ability of teams of reinforcement learning agents to learn to act\\ncollectively under the inﬂuence of a globally-broadcast reinforcement signal, may also\\nturn out to parallel experimental data as neuroscientists continue to unravel the neural\\nbasis of reward-based animal learning and behavior.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 434, 'page_label': '435'}, page_content='15.13. Summary 413\\nBibliographical and Historical Remarks\\nThe number of publications treating parallels between the neuroscience of learning and\\ndecision making and the approach to reinforcement learning presented in this book is\\nenormous. We can cite only a small selection. Niv (2009), Dayan and Niv (2008), Gimcher\\n(2011), Ludvig, Bellemare, and Pearson (2011), and Shah (2012) are good places to start.\\nTogether with economics, evolutionary biology, and mathematical psychology, reinforce-\\nment learning theory is helping to formulate quantitative models of the neural mechanisms\\nof choice in humans and non-human primates. With its focus on learning, this chapter\\nonly lightly touches upon the neuroscience of decision making. Glimcher (2003) introduced\\nthe ﬁeld of “neuroeconomics,” in which reinforcement learning contributes to the study\\nof the neural basis of decision making from an economics perspective. See also Glimcher\\nand Fehr (2013). The text on computational and mathematical modeling in neuroscience\\nby Dayan and Abbott (2001) includes reinforcement learning’s role in these approaches.\\nSterling and Laughlin (2015) examined the neural basis of learning in terms of general\\ndesign principles that enable e\\x00cient adaptive behavior.\\n15.1 There are many good expositions of basic neuroscience. Kandel, Schwartz, Jessell,\\nSiegelbaum, and Hudspeth (2013) is an authoritative and very comprehensive\\nsource.\\n15.2 Berridge and Kringelbach (2008) reviewed the neural basis of reward and pleasure,\\npointing out that reward processing has many dimensions and involves many\\nneural systems. Space prevents discussion of the inﬂuential research of Berridge\\nand Robinson (1998), who distinguish between the hedonic impact of a stimulus,\\nwhich they call “liking,” and the motivational e↵ect, which they call “wanting.”\\nHare, O’Doherty, Camerer, Schultz, and Rangel (2008) examined the neural basis\\nof value-related signals from an economic perspective, distinguishing between\\ngoal values, decision values, and prediction errors. Decision value is goal value\\nminus action cost. See also Rangel, Camerer, and Montague (2008), Rangel and\\nHare (2010), and Peters and B¨ uchel (2010).\\n15.3 The reward prediction error hypothesis of dopamine neuron activity is most\\nprominently discussed by Schultz, Dayan, and Montague (1997). The hypothesis\\nwas ﬁrst explicitly put forward by Montague, Dayan, and Sejnowski (1996). As\\nthey stated the hypothesis, it referred to reward prediction errors (RPEs) but\\nnot speciﬁcally to TD errors; however, their development of the hypothesis made\\nit clear that they were referring to TD errors. The earliest recognition of the\\nTD-error/dopamine connection of which we are aware is that of Montague, Dayan,\\nNowlan, Pouget, and Sejnowski (1993), who proposed a TD-error-modulated\\nHebbian learning rule motivated by results on dopamine signaling from Schultz’s\\ngroup. The connection was also pointed out in an abstract by Quartz, Dayan,\\nMontague, and Sejnowski (1992). Montague and Sejnowski (1994) emphasized\\nthe importance of prediction in the brain and outlined how predictive Hebbian\\nlearning modulated by TD errors could be implemented via a di↵use neuromod-\\nulatory system, such as the dopamine system. Friston, Tononi, Reeke, Sporns,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 435, 'page_label': '436'}, page_content='414 Chapter 15: Neuroscience\\nand Edelman (1994) presented a model of value-dependent learning in the brain\\nin which synaptic changes are mediated by a TD-like error provided by a global\\nneuromodulatory signal (although they did not single out dopamine). Mon-\\ntague, Dayan, Person, and Sejnowski (1995) presented a model of honeybee\\nforaging using the TD error. The model is based on research by Hammer, Men-\\nzel, and colleagues (Hammer and Menzel, 1995; Hammer, 1997) showing that\\nthe neuromodulator octopamine acts as a reinforcement signal in the honeybee.\\nMontague et al. (1995) pointed out that dopamine likely plays a similar role\\nin the vertebrate brain. Barto (1995a) related the actor–critic architecture to\\nbasal-ganglionic circuits and discussed the relationship between TD learning\\nand the main results from Schultz’s group. Houk, Adams, and Barto (1995)\\nsuggested how TD learning and the actor–critic architecture might map onto the\\nanatomy, physiology, and molecular mechanism of the basal ganglia. Doya and\\nSejnowski (1998) extended their earlier paper on a model of birdsong learning\\n(Doya and Sejnowski, 1995) by including a TD-like error identiﬁed with dopamine\\nto reinforce the selection of auditory input to be memorized. O’Reilly and Frank\\n(2006) and O’Reilly, Frank, Hazy, and Watz (2007) argued that phasic dopamine\\nsignals are RPEs but not TD errors. In support of their theory they cited results\\nwith variable interstimulus intervals that do not match predictions of a simple\\nTD model, as well as the observation that higher-order conditioning beyond\\nsecond-order conditioning is rarely observed, while TD learning is not so limited.\\nDayan and Niv (2008) discussed “the good, the bad, and the ugly” of how\\nreinforcement learning theory and the reward prediction error hypothesis align\\nwith experimental data. Glimcher (2011) reviewed the empirical ﬁndings that\\nsupport the reward prediction error hypothesis and emphasized the signiﬁcance\\nof the hypothesis for contemporary neuroscience.\\n15.4 Graybiel (2000) is a brief primer on the basal ganglia. The experiments mentioned\\nthat involve optogenetic activation of dopamine neurons were conducted by Tsai,\\nZhang, Adamantidis, Stuber, Bonci, de Lecea, and Deisseroth (2009), Steinberg,\\nKeiﬂin, Boivin, Witten, Deisseroth, and Janak (2013), and Claridge-Chang,\\nRoorda, Vrontou, Sjulson, Li, Hirsh, and Miesenb¨ ock (2009). Fiorillo, Yun, and\\nSong (2013), Lammel, Lim, and Malenka (2014), and Saddoris, Cacciapaglia,\\nWightmman, and Carelli (2015) are among studies showing that the signaling\\nproperties of dopamine neurons are specialized for di↵erent target regions. RPE-\\nsignaling neurons may belong to one among multiple populations of dopamine\\nneurons having di↵erent targets and subserving di↵erent functions. Eshel, Tian,\\nBukwich, and Uchida (2016) found homogeneity of reward prediction error\\nresponses of dopamine neurons in the lateral VTA during classical conditioning\\nin mice, though their results do not rule out response diversity across wider areas.\\nGershman, Pesaran, and Daw (2009) studied reinforcement learning tasks that\\ncan be decomposed into independent components with separate reward signals,\\nﬁnding evidence in human neuroimaging data suggesting that the brain exploits\\nthis kind of structure.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 436, 'page_label': '437'}, page_content='15.13. Summary 415\\n15.5 Schultz’s 1998 survey article is a good entr´ ee into the very extensive literature\\non reward predicting signaling of dopamine neurons. Berns, McClure, Pagnoni,\\nand Montague (2001), Breiter, Aharon, Kahneman, Dale, and Shizgal (2001),\\nPagnoni, Zink, Montague, and Berns (2002), and O’Doherty, Dayan, Friston,\\nCritchley, and Dolan (2003) described functional brain imaging studies supporting\\nthe existence of signals like TD errors in the human brain.\\n15.6 This section roughly follows Barto (1995a) in explaining how TD errors mimic the\\nmain results from Schultz’s group on the phasic responses of dopamine neurons.\\n15.7 This section is largely based on Takahashi, Schoenbaum, and Niv (2008) and Niv\\n(2009). To the best of our knowledge, Barto (1995a) and Houk, Adams, and Barto\\n(1995) ﬁrst speculated about possible implementations of actor–critic algorithms\\nin the basal ganglia. On the basis of functional magnetic resonance imaging of\\nhuman subjects while engaged in instrumental conditioning, O’Doherty, Dayan,\\nSchultz, Deichmann, Friston, and Dolan (2004) suggested that the actor and\\nthe critic are most likely located respectively in the dorsal and ventral striatum.\\nGershman, Moustafa, and Ludvig (2014) focused on how time is represented in\\nreinforcement learning models of the basal ganglia, discussing evidence for, and\\nimplications of, various computational approaches to time representation.\\nThe hypothetical neural implementation of the actor–critic architecture described\\nin this section includes very little detail about known basal ganglia anatomy and\\nphysiology. In addition to the more detailed hypothesis of Houk, Adams, and\\nBarto (1995), a number of other hypotheses include more speciﬁc connections to\\nanatomy and physiology and are claimed to explain additional data. These include\\nhypotheses proposed by Suri and Schultz (1998, 1999), Brown, Bullock, and\\nGrossberg (1999), Contreras-Vidal and Schultz (1999), Suri, Bargas, and Arbib\\n(2001), O’Reilly and Frank (2006), and O’Reilly, Frank, Hazy, and Watz (2007).\\nJoel, Niv, and Ruppin (2002) critically evaluated the anatomical plausibility of\\nseveral of these models and present an alternative intended to accommodate\\nsome neglected features of basal ganglionic circuitry.\\n15.8 The actor learning rule discussed here is more complicated than the one in the\\nearly actor–critic network of Barto et al. (1983). Actor-unit eligibility traces in\\nthat network were traces of justAt⇥x(St) instead of the full (At\\x00⇡(1|St, ✓))x(St).\\nThat work did not beneﬁt from the policy-gradient theory presented in Chapter 13\\nor the contributions of Williams (1986, 1992), who showed how an ANN of\\nBernoulli-logistic units could implement a policy-gradient method.\\nReynolds and Wickens (2002) proposed a three-factor rule for synaptic plasticity\\nin the corticostriatal pathway in which dopamine modulates changes in corti-\\ncostriatal synaptic e\\x00cacy. They discussed the experimental support for this\\nkind of learning rule and its possible molecular basis. The deﬁnitive demon-\\nstration of spike-timing-dependent plasticity (STDP) is attributed to Markram,\\nL¨ ubke, Frotscher, and Sakmann (1997), with evidence from earlier experiments'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 437, 'page_label': '438'}, page_content='416 Chapter 15: Neuroscience\\nby Levy and Steward (1983) and others that the relative timing of pre- and\\npostsynaptic spikes is critical for inducing changes in synaptic e\\x00cacy. Rao and\\nSejnowski (2001) suggested how STDP could be the result of a TD-like mechanism\\nat synapses with non-contingent eligibility traces lasting about 10 milliseconds.\\nDayan (2002) commented that this would require an error as in Sutton and\\nBarto’s (1981a) early model of classical conditioning and not a true TD error.\\nRepresentative publications from the extensive literature on reward-modulated\\nSTDP are Wickens (1990), Reynolds and Wickens (2002), and Calabresi, Picconi,\\nTozzi and Di Filippo (2007). Pawlak and Kerr (2008) showed that dopamine is\\nnecessary to induce STDP at the corticostriatal synapses of medium spiny neurons.\\nSee also Pawlak, Wickens, Kirkwood, and Kerr (2010). Yagishita, Hayashi-Takagi,\\nEllis-Davies, Urakubo, Ishii, and Kasai (2014) found that dopamine promotes\\nspine enlargement of the medium spiny neurons of mice only during a time window\\nof from 0.3 to 2 seconds after STDP stimulation. Izhikevich (2007) proposed\\nand explored the idea of using STDP timing conditions to trigger contingent\\neligibility traces. Fr´ emaux, Sprekeler, and Gerstner (2010) proposed theoretical\\nconditions for successful learning by rules based on reward-modulated STDP.\\n15.9 Klopf’s hedonistic neuron hypothesis (Klopf 1972, 1982) inspired our actor–critic\\nalgorithm implemented as an ANN with a single neuron-like unit, called the\\nactor unit, implementing a Law-of-E↵ect-like learning rule (Barto, Sutton, and\\nAnderson, 1983). Ideas related to Klopf’s synaptically-local eligibility have been\\nproposed by others. Crow (1968) proposed that changes in the synapses of\\ncortical neurons are sensitive to the consequences of neural activity. Emphasizing\\nthe need to address the time delay between neural activity and its consequences\\nin a reward-modulated form of synaptic plasticity, he proposed a contingent form\\nof eligibility, but associated with entire neurons instead of individual synapses.\\nAccording to his hypothesis, a wave of neuronal activity\\nleads to a short-term change in the cells involved in the wave such that they\\nare picked out from a background of cells not so activated. ... such cells are\\nrendered sensitive by the short-term change to a reward signal ... in such\\na way that if such a signal occurs before the end of the decay time of the\\nchange the synaptic connexions between the cells are made more e↵ective.\\n(Crow, 1968)\\nCrow argued against previous proposals that reverberating neural circuits play\\nthis role by pointing out that the e↵ect of a reward signal on such a circuit would\\n“...establish the synaptic connexions leading to the reverberation (that is to say,\\nthose involved in activity at the time of the reward signal) and not those on the\\npath which led to the adaptive motor output.” Crow further postulated that\\nreward signals are delivered via a “distinct neural ﬁber system,” presumably the\\none into which Olds and Milner (1954) tapped, that would transform synaptic\\nconnections “from a short into a long-term form.”'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 438, 'page_label': '439'}, page_content='15.13. Summary 417\\nIn another farsighted hypothesis, Miller proposed a Law-of-E↵ect-like learning\\nrule that includes synaptically-local contingent eligibility traces:\\n... it is envisaged that in a particular sensory situation neurone B, by chance,\\nﬁres a ‘meaningful burst’ of activity, which is then translated into motor acts,\\nwhich then change the situation. It must be supposed that the meaningful\\nburst has an inﬂuence,at the neuronal level, on all of its own synapses which\\nare active at the time ... thereby making a preliminary selection of the\\nsynapses to be strengthened, though not yet actually strengthening them.\\n...The strengthening signal ... makes the ﬁnal selection ... and accomplishes\\nthe deﬁnitive change in the appropriate synapses. (Miller, 1981, p. 81)\\nMiller’s hypothesis also included a critic-like mechanism, which he called a “sen-\\nsory analyzer unit,” that worked according to classical conditioning principles to\\nprovide reinforcement signals to neurons so that they would learn to move from\\nlower- to higher-valued states, thus anticipating the use of the TD error as a rein-\\nforcement signal in the actor–critic architecture. Miller’s idea not only parallels\\nKlopf’s (with the exception of its explicit invocation of a distinct “strengthening\\nsignal”), it also anticipated the general features of reward-modulated STDP.\\nA related though di↵erent idea, which Seung (2003) called the “hedonistic\\nsynapse,” is that synapses individually adjust the probability that they release\\nneurotransmitter in the manner of the Law of E↵ect: if reward follows release,\\nthe release probability increases, and decreases if reward follows failure to release.\\nThis is essentially the same as the learning scheme Minsky used in his 1954\\nPrinceton PhD dissertation, where he called the synapse-like learning element\\na SNARC (Stochastic Neural-Analog Reinforcement Calculator). Contingent\\neligibility is involved in these ideas too, although it is contingent on the activity\\nof an individual synapse instead of the postsynaptic neuron. Also related is the\\nproposal of Unnikrishnan and Venugopal (1994) that uses the correlation-based\\nmethod of Harth and Tzanakou (1974) to adjust ANN weights.\\nFrey and Morris (1997) proposed the idea of a “synaptic tag” for the induction\\nof long-lasting strengthening of synaptic e\\x00cacy. Though not unlike Klopf’s\\neligibility, their tag was hypothesized to consist of a temporary strengthening of a\\nsynapse that could be transformed into a long-lasting strengthening by subsequent\\nneuron activation. The model of O’Reilly and Frank (2006) and O’Reilly, Frank,\\nHazy, and Watz (2007) uses working memory to bridge temporal intervals instead\\nof eligibility traces. Wickens and Kotter (1995) discuss possible mechanisms\\nfor synaptic eligibility. He, Huertas, Hong, Tie, Hell, Shouval, Kirkwood (2015)\\nprovide evidence supporting the existence of contingent eligibility traces in\\nsynapses of cortical neurons with time courses like those of the eligibility traces\\nKlopf postulated.\\nThe metaphor of a neuron using a learning rule related to bacterial chemotaxis was\\ndiscussed by Barto (1989). Koshland’s extensive study of bacterial chemotaxis\\nwas in part motivated by similarities between features of bacteria and features of\\nneurons (Koshland, 1980). See also Berg (1975). Shimansky (2009) proposed a'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 439, 'page_label': '440'}, page_content='418 Chapter 15: Neuroscience\\nsynaptic learning rule somewhat similar to Seung’s mentioned above in which\\neach synapse individually acts like a chemotactic bacterium. In this case a\\ncollection of synapses “swims” toward attractants in the high-dimensional space\\nof synaptic weight values. Montague, Dayan, Person, and Sejnowski (1995)\\nproposed a chemotactic-like model of the bee’s foraging behavior involving the\\nneuromodulator octopamine.\\n15.10 Research on the behavior of reinforcement learning agents in team and game\\nproblems has a long history roughly occurring in three phases. To the best\\nof our knowledge, the ﬁrst phase began with investigations by the Russian\\nmathematician and physicist M. L. Tsetlin. A collection of his work was published\\nas Tsetlin (1973) after his death in 1966. Our Sections 1.7 and 4.8 refer to his\\nstudy of learning automata in connection to bandit problems. The Tsetlin\\ncollection also includes studies of learning automata in team and game problems,\\nwhich led to later work in this area using stochastic learning automata as\\ndescribed by Narendra and Thathachar (1974, 1989), Viswanathan and Narendra\\n(1974), Lakshmivarahan and Narendra (1982), Narendra and Wheeler (1983), and\\nThathachar and Sastry (2002). Thathachar and Sastry (2011) is a more recent\\ncomprehensive account. These studies were mostly restricted to non-associative\\nlearning automata, meaning that they did not address associative, or contextual,\\nbandit problems (Section 2.9).\\nThe second phase began with the extension of learning automata to the associative,\\nor contextual, case. Barto, Sutton, and Brouwer (1981) and Barto and Sutton\\n(1981b) experimented with associative stochastic learning automata in single-\\nlayer ANNs to which a global reinforcement signal was broadcast. The learning\\nalgorithm was an associative extension of the Alopex algorithm of Harth and\\nTzanakou (1974). Barto et al. called neuron-like elements implementing this\\nkind of learningassociative search elements(ASEs). Barto and Anandan (1985)\\nintroduced an associative reinforcement learning algorithm called theassociative\\nreward-penalty(AR\\x00P) algorithm. They proved a convergence result by combining\\ntheory of stochastic learning automata with theory of pattern classiﬁcation. Barto\\n(1985, 1986) and Barto and Jordan (1987) described results with teams of AR\\x00P\\nunits connected into multi-layer ANNs, showing that they could learn nonlinear\\nfunctions, such as XOR and others, with a globally-broadcast reinforcement signal.\\nBarto (1985) extensively discussed this approach to ANNs and how this type of\\nlearning rule is related to others in the literature at that time. Williams (1992)\\nmathematically analyzed and broadened this class of learning rules and related\\ntheir use to the error backpropagation method for training multilayer ANNs.\\nWilliams (1988) described several ways that backpropagation and reinforcement\\nlearning can be combined for training ANNs. Williams (1992) showed that a\\nspecial case of the AR\\x00P algorithm is a REINFORCE algorithm, although better\\nresults were obtained with the general AR\\x00P algorithm (Barto, 1985).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 440, 'page_label': '441'}, page_content='15.13. Summary 419\\nThe third phase of interest in teams of reinforcement learning agents was inﬂu-\\nenced by increased understanding of the role of dopamine as a widely broadcast\\nneuromodulator and speculation about the existence of reward-modulated STDP.\\nMuch more so than earlier research, this research considers details of synaptic\\nplasticity and other constraints from neuroscience. Publications include the\\nfollowing (chronologically and alphabetically): Bartlett and Baxter (1999, 2000),\\nXie and Seung (2004), Baras and Meir (2007), Farries and Fairhall (2007), Florian\\n(2007), Izhikevich (2007), Pecevski, Maass, and Legenstein (2008), Legenstein,\\nPecevski, and Maass (2008), Kolodziejski, Porr, and W¨ org¨ otter (2009), Urbanczik\\nand Senn (2009), and Vasilaki, Fr´ emaux, Urbanczik, Senn, and Gerstner (2009).\\nNow´ e, Vrancx, and De Hauwere (2012) reviewed more recent developments in\\nthe wider ﬁeld of multi-agent reinforcement learning\\n15.11 Yin and Knowlton (2006) reviewed ﬁndings from outcome-devaluation experi-\\nments with rodents supporting the view that habitual and goal-directed behavior\\n(as psychologists use the phrase) are respectively most associated with process-\\ning in the dorsolateral striatum (DLS) and the dorsomedial striatum (DMS).\\nResults of functional imaging experiments with human subjects in the outcome-\\ndevaluation setting by Valentin, Dickinson, and O’Doherty (2007) suggest that\\nthe orbitofrontal cortex (OFC) is an important component of goal-directed choice.\\nSingle unit recordings in monkeys by Padoa-Schioppa and Assad (2006) support\\nthe role of the OFC in encoding values guiding choice behavior. Rangel, Camerer,\\nand Montague (2008) and Rangel and Hare (2010) reviewed ﬁndings from the\\nperspective of neuroeconomics about how the brain makes goal-directed decisions.\\nPezzulo, van der Meer, Lansink, and Pennartz (2014) reviewed the neuroscience\\nof internally generated sequences and presented a model of how these mecha-\\nnisms might be components of model-based planning. Daw and Shohamy (2008)\\nproposed that while dopamine signaling connects well to habitual, or model-free,\\nbehavior, other processes are involved in goal-directed, or model-based, behavior.\\nData from experiments by Bromberg-Martin, Matsumoto, Hong, and Hikosaka\\n(2010) indicate that dopamine signals contain information pertinent to both\\nhabitual and goal-directed behavior. Doll, Simon, and Daw (2012) argued that\\nthere may not a clear separation in the brain between mechanisms that subserve\\nhabitual and goal-directed learning and choice.\\n15.12 Keiﬂin and Janak (2015) reviewed connections between TD errors and addiction.\\nNutt, Lingford-Hughes, Erritzoe, and Stokes (2015) critically evaluated the\\nhypothesis that addiction is due to a disorder of the dopamine system. Montague,\\nDolan, Friston, and Dayan (2012) outlined the goals and early e↵orts in the ﬁeld\\nof computational psychiatry, and Adams, Huys, and Roiser (2015) reviewed more\\nrecent progress.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 441, 'page_label': '442'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 442, 'page_label': '443'}, page_content='Chapter 16\\nApplications and Case Studies\\nIn this chapter we present a few case studies of reinforcement learning. Several of these\\nare substantial applications of potential economic signiﬁcance. One, Samuel’s checkers\\nplayer, is primarily of historical interest. Our presentations are intended to illustrate some\\nof the trade-o↵s and issues that arise in real applications. For example, we emphasize how\\ndomain knowledge is incorporated into the formulation and solution of the problem. We\\nalso highlight the representation issues that are so often critical to successful applications.\\nThe algorithms used in some of these case studies are substantially more complex than\\nthose we have presented in the rest of the book. Applications of reinforcement learning\\nare still far from routine and typically require as much art as science. Making applications\\neasier and more straightforward is one of the goals of current research in reinforcement\\nlearning.\\n16.1 TD-Gammon\\nOne of the most impressive applications of reinforcement learning to date is that by\\nGerald Tesauro to the game of backgammon (Tesauro, 1992, 1994, 1995, 2002). Tesauro’s\\nprogram, TD-Gammon, required little backgammon knowledge, yet learned to play\\nextremely well, near the level of the world’s strongest grandmasters. The learning\\nalgorithm in TD-Gammon was a straightforward combination of the TD(\\x00) algorithm\\nand nonlinear function approximation using a multilayer artiﬁcial neural network (ANN)\\ntrained by backpropagating TD errors.\\nBackgammon is a major game in the sense that it is played throughout the world,\\nwith numerous tournaments and regular world championship matches. It is in part a\\ngame of chance, and it is a popular vehicle for waging signiﬁcant sums of money. There\\nare probably more professional backgammon players than there are professional chess\\nplayers. The game is played with 15 white and 15 black pieces on a board of 24 locations,\\ncalled points. To the right on the next page is shown a typical position early in the game,\\nseen from the perspective of the white player. White here has just rolled the dice and\\nobtained a 5 and a 2. This means that he can move one of his pieces 5 steps and one'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 443, 'page_label': '444'}, page_content='422 Chapter 16: Applications and Case Studies\\nwhite pieces move \\n   counterclockwise\\n123456 789 10 11 12\\n18 17 16 15 14 1319 20 21 22 23 24\\n    black pieces \\nmove clockwise\\nA backgammon position\\n(possibly the same piece) 2 steps. For\\nexample, he could move two pieces from\\nthe 12 point, one to the 17 point, and\\none to the 14 point. White’s objective is\\nto advance all of his pieces into the last\\nquadrant (points 19–24) and then o↵\\nthe board. The ﬁrst player to remove\\nall his pieces wins. One complication\\nis that the pieces interact as they pass\\neach other going in di↵erent directions.\\nFor example, if it were black’s move, he\\ncould use the dice roll of 2 to move a\\npiece from the 24 point to the 22 point,\\n“hitting” the white piece there. Pieces that have been hit are placed on the “bar” in the\\nmiddle of the board (where we already see one previously hit black piece), from whence\\nthey reenter the race from the start. However, if there are two pieces on a point, then\\nthe opponent cannot move to that point; the pieces are protected from being hit. Thus,\\nwhite cannot use his 5–2 dice roll to move either of his pieces on the 1 point, because\\ntheir possible resulting points are occupied by groups of black pieces. Forming contiguous\\nblocks of occupied points to block the opponent is one of the elementary strategies of the\\ngame.\\nBackgammon involves several further complications, but the above description gives\\nthe basic idea. With 30 pieces and 24 possible locations (26, counting the bar and\\no↵-the-board) it should be clear that the number of possible backgammon positions is\\nenormous, far more than the number of memory elements one could have in any physically\\nrealizable computer. The number of moves possible from each position is also large. For a\\ntypical dice roll there might be 20 di↵erent ways of playing. In considering future moves,\\nsuch as the response of the opponent, one must consider the possible dice rolls as well.\\nThe result is that the game tree has an e↵ective branching factor of about 400. This is\\nfar too large to permit e↵ective use of the conventional heuristic search methods that\\nhave proved so e↵ective in games like chess and checkers.\\nOn the other hand, the game is a good match to the capabilities of TD learning\\nmethods. Although the game is highly stochastic, a complete description of the game’s\\nstate is available at all times. The game evolves over a sequence of moves and positions\\nuntil ﬁnally ending in a win for one player or the other, ending the game. The outcome\\ncan be interpreted as a ﬁnal reward to be predicted. On the other hand, the theoretical\\nresults we have described so far cannot be usefully applied to this task. The number of\\nstates is so large that a lookup table cannot be used, and the opponent is a source of\\nuncertainty and time variation.\\nTD-Gammon used a nonlinear form of TD(\\x00). The estimated value,ˆv(s,w), of any\\nstate (board position)s was meant to estimate the probability of winning starting from\\nstate s. To achieve this, rewards were deﬁned as zero for all time steps except those\\non which the game is won. To implement the value function, TD-Gammon used a\\nstandard multilayer ANN, much like that shown to the right on the next page. (The real'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 444, 'page_label': '445'}, page_content='16.1. TD-Gammon 423\\nVt+1! Vt\\nhidden units (40-80)\\nbackgammon position (198 input units)\\npredicted probability\\nof winning, Vt\\nTD error,\\n. . . . . .\\n. . . . . .\\n. . . . . .\\n15.1. TD-GAMMON 263\\ngation algorithm (Rumelhart, Hinton, and Williams, 1986). Recall that the\\ngeneral update rule for this case is\\nwt+1=wt +\\x00\\nh\\nRt+1+\\x00ˆv(St+1,wt)\\x00ˆv(St,wt)\\ni\\net, (15.1)\\nwherewt is the vector of all modiﬁable parameters (in this case, the weights\\nof the network) andet is a vector of eligibility traces, one for each component\\nofwt, updated by\\net =\\x00\\x00et\\x001+rwtˆv(St,wt),\\nwithe0 =0. The gradient in this equation can be computed e\\x00ciently by the\\nbackpropagation procedure. For the backgammon application, in which\\x00=1\\nand the reward is always zero except upon winning, the TD error portion of the\\nlearning rule is usually just ˆv(St+1,w)\\x00ˆv(St,w), as suggested in Figure 15.2.\\nTo apply the learning rule we need a source of backgammon games. Tesauro\\nobtained an unending sequence of games by playing his learning backgammon\\nplayer against itself. To choose its moves, TD-Gammon considered each of the\\n20 or so ways it could play its dice roll and the corresponding positions that\\nwould result. The resulting positions areafterstatesas discussed in Section 6.8.\\nThe network was consulted to estimate each of their values. The move was\\nthen selected that would lead to the position with the highest estimated value.\\nContinuing in this way, with TD-Gammon making the moves for both sides,\\nit was possible to easily generate large numbers of backgammon games. Each\\ngame was treated as an episode, with the sequence of positions acting as\\nthe states,S0,S1,S2,.... Tesauro applied the nonlinear TD rule (15.1) fully\\nincrementally, that is, after each individual move.\\nThe weights of the network were set initially to small random values. The\\ninitial evaluations were thus entirely arbitrary. Since the moves were selected\\non the basis of these evaluations, the initial moves were inevitably poor, and\\nthe initial games often lasted hundreds or thousands of moves before one side\\nor the other won, almost by accident. After a few dozen games however,\\nperformance improved rapidly.\\nAfter playing about 300,000 games against itself, TD-Gammon 0.0 as de-\\nscribed above learned to play approximately as well as the best previous\\nbackgammon computer programs. This was a striking result because all the\\nprevious high-performance computer programs had used extensive backgam-\\nmon knowledge. For example, the reigning champion program at the time\\nwas, arguably,Neurogammon, another program written by Tesauro that used\\na neural network but not TD learning. Neurogammon’s network was trained\\non a large training corpus of exemplary moves provided by backgammon ex-\\nperts, and, in addition, started with a set of features specially crafted for\\nTD error\\n15.1. TD-GAMMON 263\\ngation algorithm (Rumelhart, Hinton, and Williams, 1986). Recall that the\\ngeneral update rule for this case is\\nwt+1=wt +\\x00\\nh\\nRt+1+\\x00ˆv(St+1,wt)\\x00ˆv(St,wt)\\ni\\net, (15.1)\\nwherewt is the vector of all modiﬁable parameters (in this case, the weights\\nof the network) andet is a vector of eligibility traces, one for each component\\nofwt, updated by\\net =\\x00\\x00et\\x001+rwtˆv(St,wt),\\nwithe0 =0. The gradient in this equation can be computed e\\x00ciently by the\\nbackpropagation procedure. For the backgammon application, in which\\x00=1\\nand the reward is always zero except upon winning, the TD error portion of the\\nlearning rule is usually just ˆv(St+1,w)\\x00ˆv(St,w), as suggested in Figure 15.2.\\nTo apply the learning rule we need a source of backgammon games. Tesauro\\nobtained an unending sequence of games by playing his learning backgammon\\nplayer against itself. To choose its moves, TD-Gammon considered each of the\\n20 or so ways it could play its dice roll and the corresponding positions that\\nwould result. The resulting positions areafterstatesas discussed in Section 6.8.\\nThe network was consulted to estimate each of their values. The move was\\nthen selected that would lead to the position with the highest estimated value.\\nContinuing in this way, with TD-Gammon making the moves for both sides,\\nit was possible to easily generate large numbers of backgammon games. Each\\ngame was treated as an episode, with the sequence of positions acting as\\nthe states,S0,S1,S2,.... Tesauro applied the nonlinear TD rule (15.1) fully\\nincrementally, that is, after each individual move.\\nThe weights of the network were set initially to small random values. The\\ninitial evaluations were thus entirely arbitrary. Since the moves were selected\\non the basis of these evaluations, the initial moves were inevitably poor, and\\nthe initial games often lasted hundreds or thousands of moves before one side\\nor the other won, almost by accident. After a few dozen games however,\\nperformance improved rapidly.\\nAfter playing about 300,000 games against itself, TD-Gammon 0.0 as de-\\nscribed above learned to play approximately as well as the best previous\\nbackgammon computer programs. This was a striking result because all the\\nprevious high-performance computer programs had used extensive backgam-\\nmon knowledge. For example, the reigning champion program at the time\\nwas, arguably,Neurogammon, another program written by Tesauro that used\\na neural network but not TD learning. Neurogammon’s network was trained\\non a large training corpus of exemplary moves provided by backgammon ex-\\nperts, and, in addition, started with a set of features specially crafted for\\nhidden units\\n(40-80)\\nFigure 16.1: The TD-Gammon ANN\\nnetwork had two additional units in its\\nﬁnal layer to estimate the probability of\\neach player’s winning in a special way\\ncalled a “gammon” or “backgammon.”)\\nThe network consisted of a layer of input\\nunits, a layer of hidden units, and a ﬁnal\\noutput unit. The input to the network\\nwas a representation of a backgammon\\nposition, and the output was an estimate\\nof the value of that position.\\nIn the ﬁrst version of TD-Gammon,\\nTD-Gammon 0.0, backgammon posi-\\ntions were represented to the network\\nin a relatively direct way that involved\\nlittle backgammon knowledge. It did,\\nhowever, involve substantial knowledge of how ANNs work and how information is best\\npresented to them. It is instructive to note the exact representation Tesauro chose. There\\nwere a total of 198 input units to the network. For each point on the backgammon board,\\nfour units indicated the number of white pieces on the point. If there were no white\\npieces, then all four units took on the value zero. If there was one piece, then the ﬁrst\\nunit took on the value 1. This encoded the elementary concept of a “blot,” i.e., a piece\\nthat can be hit by the opponent. If there were two or more pieces, then the second unit\\nwas set to 1. This encoded the basic concept of a “made point” on which the opponent\\ncannot land. If there were exactly three pieces on the point, then the third unit was set\\nto 1. This encoded the basic concept of a “single spare,” i.e., an extra piece in addition\\nto the two pieces that made the point. Finally, if there were more than three pieces, the\\nfourth unit was set to a value proportionate to the number of additional pieces beyond\\nthree. Letting n denote the total number of pieces on the point, ifn> 3, then the fourth\\nunit took on the value (n\\x003)/2. This encoded a linear representation of “multiple spares”\\nat the given point.\\nWith four units for white and four for black at each of the 24 points, that made a\\ntotal of 192 units. Two additional units encoded the number of white and black pieces on\\nthe bar (each took the valuen/2, wheren is the number of pieces on the bar), and two\\nmore encoded the number of black and white pieces already successfully removed from\\nthe board (these took the valuen/15, where n is the number of pieces already borne\\no↵). Finally, two units indicated in a binary fashion whether it was white’s or black’s\\nturn to move. The general logic behind these choices should be clear. Basically, Tesauro\\ntried to represent the position in a straightforward way, while keeping the number of\\nunits relatively small. He provided one unit for each conceptually distinct possibility that\\nseemed likely to be relevant, and he scaled them to roughly the same range, in this case\\nbetween 0 and 1.\\nGiven a representation of a backgammon position, the network computed its estimated\\nvalue in the standard way. Corresponding to each connection from an input unit to a\\nhidden unit was a real-valued weight. Signals from each input unit were multiplied by'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 445, 'page_label': '446'}, page_content='424 Chapter 16: Applications and Case Studies\\ntheir corresponding weights and summed at the hidden unit. The output,h(j), of hidden\\nunit j was a nonlinear sigmoid function of the weighted sum:\\nh(j)= \\x00\\n X\\ni\\nwijxi\\n!\\n= 1\\n1+ e\\x00P\\ni wijxi\\n,\\nwhere xi is the value of theith input unit andwij is the weight of its connection to the\\njth hidden unit (all the weights in the network together make up the parameter vectorw).\\nThe output of the sigmoid is always between 0 and 1, and has a natural interpretation as\\na probability based on a summation of evidence. The computation from hidden units\\nto the output unit was entirely analogous. Each connection from a hidden unit to the\\noutput unit had a separate weight. The output unit formed the weighted sum and then\\npassed it through the same sigmoid nonlinearity.\\nTD-Gammon used the semi-gradient form of the TD(\\x00) algorithm described in Sec-\\ntion 12.2, with the gradients computed by the error backpropagation algorithm (Rumel-\\nhart, Hinton, and Williams, 1986). Recall that the general update rule for this case is\\nwt+1\\n.= wt + ↵\\nh\\nRt+1 + \\x00ˆv(St+1,wt) \\x00 ˆv(St,wt)\\ni\\nzt, (16.1)\\nwhere wt is the vector of all modiﬁable parameters (in this case, the weights of the\\nnetwork) andzt is a vector of eligibility traces, one for each component ofwt, updated by\\nzt\\n.= \\x00\\x00zt\\x001 + rˆv(St,wt),\\nwith z0\\n.= 0. The gradient in this equation can be computed e\\x00ciently by the backpropa-\\ngation procedure. For the backgammon application, in which\\x00 = 1 and the reward is\\nalways zero except upon winning, the TD error portion of the learning rule is usually\\njust ˆv(St+1,w) \\x00 ˆv(St,w), as suggested in Figure 16.1.\\nTo apply the learning rule we need a source of backgammon games. Tesauro obtained\\nan unending sequence of games by playing his learning backgammon player against itself.\\nTo choose its moves, TD-Gammon considered each of the 20 or so ways it could play\\nits dice roll and the corresponding positions that would result. The resulting positions\\nare afterstates as discussed in Section 6.8. The network was consulted to estimate each\\nof their values. The move was then selected that would lead to the position with the\\nhighest estimated value. Continuing in this way, with TD-Gammon making the moves\\nfor both sides, it was possible to easily generate large numbers of backgammon games.\\nEach game was treated as an episode, with the sequence of positions acting as the states,\\nS0,S 1,S 2,... . Tesauro applied the nonlinear TD rule (16.1) fully incrementally, that is,\\nafter each individual move.\\nThe weights of the network were set initially to small random values. The initial\\nevaluations were thus entirely arbitrary. Because the moves were selected on the basis\\nof these evaluations, the initial moves were inevitably poor, and the initial games often\\nlasted hundreds or thousands of moves before one side or the other won, almost by\\naccident. After a few dozen games however, performance improved rapidly.\\nAfter playing about 300,000 games against itself, TD-Gammon 0.0 as described above\\nlearned to play approximately as well as the best previous backgammon computer'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 446, 'page_label': '447'}, page_content='16.1. TD-Gammon 425\\nprograms. This was a striking result because all the previous high-performance computer\\nprograms had used extensive backgammon knowledge. For example, the reigning champion\\nprogram at the time was, arguably,Neurogammon, another program written by Tesauro\\nthat used an ANN but not TD learning. Neurogammon’s network was trained on a\\nlarge training corpus of exemplary moves provided by backgammon experts, and, in\\naddition, started with a set of features specially crafted for backgammon. Neurogammon\\nwas a highly tuned, highly e↵ective backgammon program that decisively won the\\nWorld Backgammon Olympiad in 1989. TD-Gammon 0.0, on the other hand, was\\nconstructed with essentially zero backgammon knowledge. That it was able to do as\\nwell as Neurogammon and all other approaches is striking testimony to the potential of\\nself-play learning methods.\\nThe tournament success of TD-Gammon 0.0 with zero expert backgammon knowledge\\nsuggested an obvious modiﬁcation: add the specialized backgammon features but keep\\nthe self-play TD learning method. This produced TD-Gammon 1.0. TD-Gammon 1.0 was\\nclearly substantially better than all previous backgammon programs and found serious\\ncompetition only among human experts. Later versions of the program, TD-Gammon 2.0\\n(40 hidden units) and TD-Gammon 2.1 (80 hidden units), were augmented with a selective\\ntwo-ply search procedure. To select moves, these programs looked ahead not just to the\\npositions that would immediately result, but also to the opponent’s possible dice rolls\\nand moves. Assuming the opponent always took the move that appeared immediately\\nbest for him, the expected value of each candidate move was computed and the best\\nwas selected. To save computer time, the second ply of search was conducted only for\\ncandidate moves that were ranked highly after the ﬁrst ply, about four or ﬁve moves on\\naverage. Two-ply search a↵ected only the moves selected; the learning process proceeded\\nexactly as before. The ﬁnal versions of the program, TD-Gammon 3.0 and 3.1, used 160\\nhidden units and a selective three-ply search. TD-Gammon illustrates the combination\\nof learned value functions and decision-time search as in heuristic search and MCTS\\nmethods. In follow-on work, Tesauro and Galperin (1997) explored trajectory sampling\\nmethods as an alternative to full-width search, which reduced the error rate of live play\\nby large numerical factors (4x–6x) while keeping the think time reasonable at⇠5–10\\nseconds per move.\\nDuring the 1990s, Tesauro was able to play his programs in a signiﬁcant number of\\ngames against world-class human players. A summary of the results is given in Table 16.1.\\nProgram Hidden Training Opponents Results\\nUnits Games\\nTD-Gammon 0.0 40 300,000 other programs tied for best\\nTD-Gammon 1.0 80 300,000 Robertie, Magriel, ... \\x0013 pts / 51 games\\nTD-Gammon 2.0 40 800,000 various Grandmasters \\x007 pts / 38 games\\nTD-Gammon 2.1 80 1,500,000 Robertie \\x001 pt / 40 games\\nTD-Gammon 3.0 80 1,500,000 Kazaros +6 pts / 20 games\\nTable 16.1: Summary of TD-Gammon Results'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 447, 'page_label': '448'}, page_content='426 Chapter 16: Applications and Case Studies\\nBased on these results and analyses by backgammon grandmasters (Robertie, 1992; see\\nTesauro, 1995), TD-Gammon 3.0 appeared to play at close to, or possibly better than,\\nthe playing strength of the best human players in the world. Tesauro reported in a\\nsubsequent article (Tesauro, 2002) the results of an extensive rollout analysis of the move\\ndecisions and doubling decisions of TD-Gammon relative to top human players. The\\nconclusion was that TD-Gammon 3.1 had a “lopsided advantage” in piece-movement\\ndecisions, and a “slight edge” in doubling decisions, over top humans.\\nTD-Gammon had a signiﬁcant impact on the way the best human players play the\\ngame. For example, it learned to play certain opening positions di↵erently than was\\nthe convention among the best human players. Based on TD-Gammon’s success and\\nfurther analysis, the best human players now play these positions as TD-Gammon does\\n(Tesauro, 1995). The impact on human play was greatly accelerated when several other\\nself-teaching ANN backgammon programs inspired by TD-Gammon, such as Jellyﬁsh,\\nSnowie, and GNUBackgammon, became widely available. These programs enabled wide\\ndissemination of new knowledge generated by the ANNs, resulting in great improvements\\nin the overall caliber of human tournament play (Tesauro, 2002).\\n16.2 Samuel’s Checkers Player\\nAn important precursor to Tesauro’s TD-Gammon was the seminal work of Arthur Samuel\\n(1959, 1967) in constructing programs for learning to play checkers. Samuel was one of\\nthe ﬁrst to make e↵ective use of heuristic search methods and of what we would now call\\ntemporal-di↵erence learning. His checkers players are instructive case studies in addition\\nto being of historical interest. We emphasize the relationship of Samuel’s methods to\\nmodern reinforcement learning methods and try to convey some of Samuel’s motivation\\nfor using them.\\nSamuel ﬁrst wrote a checkers-playing program for the IBM 701 in 1952. His ﬁrst\\nlearning program was completed in 1955 and was demonstrated on television in 1956.\\nLater versions of the program achieved good, though not expert, playing skill. Samuel\\nwas attracted to game-playing as a domain for studying machine learning because games\\nare less complicated than problems “taken from life” while still allowing fruitful study of\\nhow heuristic procedures and learning can be used together. He chose to study checkers\\ninstead of chess because its relative simplicity made it possible to focus more strongly on\\nlearning.\\nSamuel’s programs played by performing a lookahead search from each current position.\\nThey used what we now call heuristic search methods to determine how to expand the\\nsearch tree and when to stop searching. The terminal board positions of each search were\\nevaluated, or “scored,” by a value function, or “scoring polynomial,” using linear function\\napproximation. In this and other respects Samuel’s work seems to have been inspired\\nby the suggestions of Shannon (1950). In particular, Samuel’s program was based on\\nShannon’s minimax procedure to ﬁnd the best move from the current position. Working\\nbackward through the search tree from the scored terminal positions, each position was\\ngiven the score of the position that would result from the best move, assuming that the\\nmachine would always try to maximize the score, while the opponent would always try to'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 448, 'page_label': '449'}, page_content='16.2. Samuel’s Checkers Player 427\\nminimize it. Samuel called this the “backed-up score” of the position. When the minimax\\nprocedure reached the search tree’s root—the current position—it yielded the best move\\nunder the assumption that the opponent would be using the same evaluation criterion,\\nshifted to its point of view. Some versions of Samuel’s programs used sophisticated search\\ncontrol methods analogous to what are known as “alpha-beta” cuto↵s (e.g., see Pearl,\\n1984).\\nSamuel used two main learning methods, the simplest of which he calledrote learning.\\nIt consisted simply of saving a description of each board position encountered during play\\ntogether with its backed-up value determined by the minimax procedure. The result was\\nthat if a position that had already been encountered were to occur again as a terminal\\nposition of a search tree, the depth of the search was e↵ectively ampliﬁed because this\\nposition’s stored value cached the results of one or more searches conducted earlier. One\\ninitial problem was that the program was not encouraged to move along the most direct\\npath to a win. Samuel gave it a “a sense of direction” by decreasing a position’s value\\na small amount each time it was backed up a level (called a ply) during the minimax\\nanalysis. “If the program is now faced with a choice of board positions whose scores\\ndi↵er only by the ply number, it will automatically make the most advantageous choice,\\nchoosing a low-ply alternative if winning and a high-ply alternative if losing” (Samuel,\\n1959, p. 80). Samuel found this discounting-like technique essential to successful learning.\\nRote learning produced slow but continual improvement that was most e↵ective for\\nopening and endgame play. His program became a “better-than-average novice” after\\nlearning from many games against itself, a variety of human opponents, and from book\\ngames in a supervised learning mode.\\nRote learning and other aspects of Samuel’s work strongly suggest the essential idea\\nof temporal-di↵erence learning—that the value of a state should equal the value of\\nlikely following states. Samuel came closest to this idea in his second learning method,\\nhis “learning by generalization” procedure for modifying the parameters of the value\\nfunction. Samuel’s method was the same in concept as that used much later by Tesauro\\nin TD-Gammon. He played his program many games against another version of itself and\\nperformed an update after each move. The idea of Samuel’s update is suggested by the\\nbackup diagram in Figure 16.2. Each open circle represents a position where the program\\nmoves next, anon-move position, and each solid circle represents a position where the\\nopponent moves next. An update was made to the value of each on-move position after a\\nmove by each side, resulting in a second on-move position. The update was toward the\\nminimax value of a search launched from the second on-move position. Thus, the overall\\ne↵ect was that of a backing-up over one full move of real events and then a search over\\npossible events, as suggested by Figure 16.2. Samuel’s actual algorithm was signiﬁcantly\\nmore complex than this for computational reasons, but this was the basic idea.\\nSamuel did not include explicit rewards. Instead, he ﬁxed the weight of the most\\nimportant feature, thepiece advantage feature, which measured the number of pieces\\nthe program had relative to how many its opponent had, giving higher weight to kings,\\nand including reﬁnements so that it was better to trade pieces when winning than when\\nlosing. Thus, the goal of Samuel’s program was to improve its piece advantage, which in\\ncheckers is highly correlated with winning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 449, 'page_label': '450'}, page_content='428 Chapter 16: Applications and Case Studies\\nhypothetical eventsactual events\\nbackup\\nFigure 16.2: The backup diagram for Samuel’s checkers player.\\nHowever, Samuel’s learning method may have been missing an essential part of a sound\\ntemporal-di↵erence algorithm. Temporal-di↵erence learning can be viewed as a way of\\nmaking a value function consistent with itself, and this we can clearly see in Samuel’s\\nmethod. But also needed is a way of tying the value function to the true value of the\\nstates. We have enforced this via rewards and by discounting or giving a ﬁxed value to\\nthe terminal state. But Samuel’s method included no rewards and no special treatment of\\nthe terminal positions of games. As Samuel himself pointed out, his value function could\\nhave become consistent merely by giving a constant value to all positions. He hoped to\\ndiscourage such solutions by giving his piece-advantage term a large, nonmodiﬁable weight.\\nBut although this may decrease the likelihood of ﬁnding useless evaluation functions,\\nit does not prohibit them. For example, a constant function could still be attained by\\nsetting the modiﬁable weights so as to cancel the e↵ect of the nonmodiﬁable one.\\nBecause Samuel’s learning procedure was not constrained to ﬁnd useful evaluation\\nfunctions, it should have been possible for it to become worse with experience. In fact,\\nSamuel reported observing this during extensive self-play training sessions. To get the\\nprogram improving again, Samuel had to intervene and set the weight with the largest\\nabsolute value back to zero. His interpretation was that this drastic intervention jarred\\nthe program out of local optima, but another possibility is that it jarred the program out\\nof evaluation functions that were consistent but had little to do with winning or losing\\nthe game.\\nDespite these potential problems, Samuel’s checkers player using the generalization\\nlearning method approached “better-than-average” play. Fairly good amateur opponents\\ncharacterized it as “tricky but beatable” (Samuel, 1959). In contrast to the rote-learning\\nversion, this version was able to develop a good middle game but remained weak in\\nopening and endgame play. This program also included an ability to search through sets of\\nfeatures to ﬁnd those that were most useful in forming the value function. A later version\\n(Samuel, 1967) included reﬁnements in its search procedure, such as alpha-beta pruning,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 450, 'page_label': '451'}, page_content='16.3. Watson’s Daily-Double Wagering 429\\nextensive use of a supervised learning mode called “book learning,” and hierarchical\\nlookup tables called signature tables (Gri\\x00th, 1966) to represent the value function\\ninstead of linear function approximation. This version learned to play much better than\\nthe 1959 program, though still not at a master level. Samuel’s checkers-playing program\\nwas widely recognized as a signiﬁcant achievement in artiﬁcial intelligence and machine\\nlearning.\\n16.3 Watson’s Daily-Double Wagering\\nIBM Watson1 is the system developed by a team of IBM researchers to play the popular\\nTV quiz showJeopardy!.2 It gained fame in 2011 by winning ﬁrst prize in an exhibition\\nmatch against human champions. Although the main technical achievement demonstrated\\nby Watson was its ability to quickly and accurately answer natural language questions\\nover broad areas of general knowledge, its winningJeopardy! performance also relied on\\nsophisticated decision-making strategies for critical parts of the game. Tesauro, Gondek,\\nLechner, Fan, and Prager (2012, 2013) adapted Tesauro’s TD-Gammon system described\\nabove to create the strategy used by Watson in “Daily-Double” (DD) wagering in its\\ncelebrated winning performance against human champions. These authors report that the\\ne↵ectiveness of this wagering strategy went well beyond what human players are able to\\ndo in live game play, and that it, along with other advanced strategies, was an important\\ncontributor to Watson’s impressive winning performance. Here we focus only on DD\\nwagering because it is the component of Watson that owes the most to reinforcement\\nlearning.\\nJeopardy! is played by three contestants who face a board showing 30 squares, each of\\nwhich hides a clue and has a dollar value. The squares are arranged in six columns, each\\ncorresponding to a di↵erent category. A contestant selects a square, the host reads the\\nsquare’s clue, and each contestant may choose to respond to the clue by sounding a buzzer\\n(“buzzing in”). The ﬁrst contestant to buzz in gets to try responding to the clue. If this\\ncontestant’s response is correct, their score increases by the dollar value of the square; if\\ntheir response is not correct, or if they do not respond within ﬁve seconds, their score\\ndecreases by that amount, and the other contestants get a chance to buzz in to respond\\nto the same clue. One or two squares (depending on the game’s current round) are\\nspecial DD squares. A contestant who selects one of these gets an exclusive opportunity\\nto respond to the square’s clue and has to decide—before the clue is revealed—on how\\nmuch to wager, or bet. The bet has to be greater than ﬁve dollars but not greater than\\nthe contestant’s current score. If the contestant responds correctly to the DD clue, their\\nscore increases by the bet amount; otherwise it decreases by the bet amount. At the end\\nof each game is a “Final Jeopardy” (FJ) round in which each contestant writes down\\na sealed bet and then writes an answer after the clue is read. The contestant with the\\nhighest score after three rounds of play (where a round consists of revealing all 30 clues)\\nis the winner. The game has many other details, but these are enough to appreciate\\n1Registered trademark of IBM Corp.\\n2Registered trademark of Jeopardy Productions Inc.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 451, 'page_label': '452'}, page_content='430 Chapter 16: Applications and Case Studies\\nthe importance of DD wagering. Winning or losing often depends on a contestant’s DD\\nwagering strategy.\\nWhenever Watson selected a DD square, it chose its bet by comparing action values,\\nˆq(s, bet), that estimated the probability of a win from the current game state,s, for\\neach round-dollar legal bet. Except for some risk-abatement measures described below,\\nWatson selected the bet with the maximum action value. Action values were computed\\nwhenever a betting decision was needed by using two types of estimates that were learned\\nbefore any live game play took place. The ﬁrst were estimated values of the afterstates\\n(Section 6.8) that would result from selecting each legal bet. These estimates were obtained\\nfrom a state-value function,ˆv(· ,w), deﬁned by parametersw, that gave estimates of the\\nprobability of a win for Watson from any game state. The second estimates used to\\ncompute action values gave the “in-category DD conﬁdence,”pDD, which estimated the\\nlikelihood that Watson would respond correctly to the as-yet unrevealed DD clue.\\nTesauro et al. used the reinforcement learning approach of TD-Gammon described above\\nto learn ˆv(· ,w): a straightforward combination of nonlinear TD(\\x00)u s i n gam u l t i l a y e r\\nANN with weightsw trained by backpropagating TD errors during many simulated\\ngames. States were represented to the network by feature vectors speciﬁcally designed\\nfor Jeopardy!. Features included the current scores of the three players, how many DDs\\nremained, the total dollar value of the remaining clues, and other information related to\\nthe amount of play left in the game. Unlike TD-Gammon, which learned by self-play,\\nWatson’sˆv was learned over millions of simulated games against carefully-crafted models\\nof human players. In-category conﬁdence estimates were conditioned on the number of\\nright responsesr and wrong responsesw that Watson gave in previously-played clues in\\nthe current category. The dependencies on (r, w) were estimated from Watson’s actual\\naccuracies over many thousands of historical categories.\\nWith the previously learned value functionˆv and in-category DD conﬁdencepDD,\\nWatson computed ˆq(s, bet) for each legal round-dollar bet as follows:\\nˆq(s, bet)= pDD ⇥ ˆv(SW + bet,. . .)+( 1\\x00 pDD) ⇥ ˆv(SW \\x00 bet,. . .), (16.2)\\nwhere SW is Watson’s current score, andˆv gives the estimated value for the game state\\nafter Watson’s response to the DD clue, which is either correct or incorrect. Computing\\nan action value this way corresponds to the insight from Exercise 3.19 that an action\\nvalue is the expected next state value given the action (except that here it is the expected\\nnext afterstate value because the full next state of the entire game depends on the next\\nsquare selection).\\nTesauro et al. found that selecting bets by maximizing action values incurred “a\\nfrightening amount of risk,” meaning that if Watson’s response to the clue happened to\\nbe wrong, the loss could be disastrous for its chances of winning. To decrease the downside\\nrisk of a wrong answer, Tesauro et al. adjusted (16.2) by subtracting a small fraction of\\nthe standard deviation over Watson’s correct/incorrect afterstate evaluations. They\\nfurther reduced risk by prohibiting bets that would cause the wrong-answer afterstate\\nvalue to decrease below a certain limit. These measures slightly reduced Watson’s\\nexpectation of winning, but they signiﬁcantly reduced downside risk, not only in terms of\\naverage risk per DD bet, but even more so in extreme-risk scenarios where a risk-neutral\\nWatson would bet most or all of its bankroll.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 452, 'page_label': '453'}, page_content='16.3. Watson’s Daily-Double Wagering 431\\nWhy was the TD-Gammon method of self-play not used to learn the critical value\\nfunction ˆv? Learning from self-play in Jeopardy! would not have worked very well\\nbecause Watson was so di↵erent from any human contestant. Self-play would have led to\\nexploration of state space regions that are not typical for play against human opponents,\\nparticularly human champions. In addition, unlike backgammon,Jeopardy! is a game\\nof imperfect information because contestants do not have access to all the information\\ninﬂuencing their opponents’ play. In particular,Jeopardy! contestants do not know how\\nmuch conﬁdence their opponents have for responding to clues in the various categories.\\nSelf-play would have been something like playing poker with someone who is holding the\\nsame cards that you hold.\\nAs a result of these complications, much of the e↵ort in developing Watson’s DD-\\nwagering strategy was devoted to creating good models of human opponents. The models\\ndid not address the natural language aspect of the game, but were instead stochastic\\nprocess models of events that can occur during play. Statistics were extracted from an\\nextensive fan-created archive of game information from the beginning of the show to the\\npresent day. The archive includes information such as the ordering of the clues, right and\\nwrong contestant answers, DD locations, and DD and FJ bets for nearly 300,000 clues.\\nThree models were constructed: an Average Contestant model (based on all the data), a\\nChampion model (based on statistics from games with the 100 best players), and a Grand\\nChampion model (based on statistics from games with the 10 best players). In addition\\nto serving as opponents during learning, the models were used to assess the beneﬁts\\nproduced by the learned DD-wagering strategy. Watson’s win rate in simulation when it\\nused a baseline heuristic DD-wagering strategy was 61%; when it used the learned values\\nand a default conﬁdence value, its win rate increased to 64%; and with live in-category\\nconﬁdence, it was 67%. Tesauro et al. regarded this as a signiﬁcant improvement, given\\nthat the DD wagering was needed only about 1.5 to 2 times in each game.\\nBecause Watson had only a few seconds to bet, as well as to select squares and decide\\nwhether or not to buzz in, the computation time needed to make these decisions was\\na critical factor. The ANN implementation ofˆv allowed DD bets to be made quickly\\nenough to meet the time constraints of live play. However, once games could be simulated\\nfast enough through improvements in the simulation software, near the end of a game\\nit was feasible to estimate the value of bets by averaging over many Monte-Carlo trials\\nin which the consequence of each bet was determined by simulating play to the game’s\\nend. Selecting endgame DD bets in live play based on Monte-Carlo trials instead of the\\nANN signiﬁcantly improved Watson’s performance because errors in value estimates\\nin endgames could seriously a↵ect its chances of winning. Making all the decisions via\\nMonte-Carlo trials might have led to better wagering decisions, but this was simply\\nimpossible given the complexity of the game and the time constraints of live play.\\nAlthough its ability to quickly and accurately answer natural language questions\\nstands out as Watson’s major achievement, all of its sophisticated decision strategies\\ncontributed to its impressive defeat of human champions. According to Tesauro et al.\\n(2012):\\n... it is plainly evident that our strategy algorithms achieve a level of quanti-\\ntative precision and real-time performance that exceeds human capabilities.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 453, 'page_label': '454'}, page_content='432 Chapter 16: Applications and Case Studies\\nThis is particularly true in the cases of DD wagering and endgame buzzing,\\nwhere humans simply cannot come close to matching the precise equity and\\nconﬁdence estimates and complex decision calculations performed by Watson.\\n16.4 Optimizing Memory Control\\nMost computers use dynamic random access memory (DRAM) as their main memory\\nbecause of its low cost and high capacity. The job of a DRAM memory controller is to\\ne\\x00ciently use the interface between the processor chip and an o↵-chip DRAM system\\nto provide the high-bandwidth and low-latency data transfer necessary for high-speed\\nprogram execution. A memory controller needs to deal with dynamically changing\\npatterns of read/write requests while adhering to a large number of timing and resource\\nconstraints required by the hardware. This is a formidable scheduling problem, especially\\nwith modern processors with multiple cores sharing the same DRAM.\\n˙Ipek, Mutlu, Mart´ ınez, and Caruana (2008) (also Mart´ ınez and˙Ipek, 2009) designed\\na reinforcement learning memory controller and demonstrated that it can signiﬁcantly\\nimprove the speed of program execution over what was possible with conventional\\ncontrollers at the time of their research. They were motivated by limitations of existing\\nstate-of-the-art controllers that used policies that did not take advantage of past scheduling\\nexperience and did not account for long-term consequences of scheduling decisions.˙Ipek\\net al.’s project was carried out by means of simulation, but they designed the controller\\nat the detailed level of the hardware needed to implement it—including the learning\\nalgorithm—directly on a processor chip.\\nAccessing DRAM involves a number of steps that have to be done according to strict\\ntime constraints. DRAM systems consist of multiple DRAM chips, each containing\\nmultiple rectangular arrays of storage cells arranged in rows and columns. Each cell\\nstores a bit as the charge on a capacitor. Because the charge decreases over time, each\\nDRAM cell needs to be recharged—refreshed—every few milliseconds to prevent memory\\ncontent from being lost. This need to refresh the cells is why DRAM is called “dynamic.”\\nEach cell array has a row bu↵er that holds a row of bits that can be transferred into\\nor out of one of the array’s rows. Anactivate command “opens a row,” which means\\nmoving the contents of the row whose address is indicated by the command into the\\nrow bu↵er. With a row open, the controller can issueread and write commands to the\\ncell array. Each read command transfers a word (a short sequence of consecutive bits)\\nin the row bu↵er to the external data bus, and each write command transfers a word\\nin the external data bus to the row bu↵er. Before a di↵erent row can be opened, a\\nprecharge command must be issued which transfers the (possibly updated) data in the\\nrow bu↵er back into the addressed row of the cell array. After this, another activate\\ncommand can open a new row to be accessed. Read and write commands arecolumn\\ncommands because they sequentially transfer bits into or out of columns of the row bu↵er;\\nmultiple bits can be transferred without re-opening the row. Read and write commands\\nto the currently-open row can be carried out more quickly than accessing a di↵erent row,\\nwhich would involve additionalrow commands: precharge and activate; this is sometimes'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 454, 'page_label': '455'}, page_content='16.4. Optimizing Memory Control 433\\nreferred to as “row locality.” A memory controller maintains amemory transaction queue\\nthat stores memory-access requests from the processors sharing the memory system. The\\ncontroller has to process requests by issuing commands to the memory system while\\nadhering to a large number of timing constraints.\\nA controller’s policy for scheduling access requests can have a large e↵ect on the\\nperformance of the memory system, such as the average latency with which requests\\ncan be satisﬁed and the throughput the system is capable of achieving. The simplest\\nscheduling strategy handles access requests in the order in which they arrive by issuing\\nall the commands required by the request before beginning to service the next one. But if\\nthe system is not ready for one of these commands, or executing a command would result\\nin resources being underutilized (e.g., due to timing constraints arising from servicing\\nthat one command), it makes sense to begin servicing a newer request before ﬁnishing\\nthe older one. Policies can gain e\\x00ciency by reordering requests, for example, by giving\\npriority to read requests over write requests, or by giving priority to read/write commands\\nto already open rows. The policy called First-Ready, First-Come-First-Serve (FR-FCFS),\\ngives priority to column commands (read and write) over row commands (activate and\\nprecharge), and in case of a tie gives priority to the oldest command. FR-FCFS was\\nshown to outperform other scheduling policies in terms of average memory-access latency\\nunder conditions commonly encountered (Rixner, 2004).\\nFigure 16.3 is a high-level view of˙Ipek et al.’s reinforcement learning memory controller.\\nThey modeled the DRAM access process as an MDP whose states are the contents of the\\ntransaction queue and whose actions are commands to the DRAM system:precharge,\\nactivate, read, write, andNoOp. The reward signal is 1 whenever the action isread or\\nwrite, and otherwise it is 0. State transitions were considered to be stochastic because\\nthe next state of the system not only depends on the scheduler’s command, but also on\\naspects of the system’s behavior that the scheduler cannot control, such as the workloads\\nof the processor cores accessing the DRAM system.\\nFigure 16.3: High-level view of the reinforcement learning DRAM controller. The scheduler is\\nthe reinforcement learning agent. Its environment is represented by features of the transaction\\nqueue, and its actions are commands to the DRAM system.©2009 IEEE. Reprinted, with\\npermission, from J. F. Mart´ ınez and E.˙Ipek, Dynamic multicore resource management: A\\nmachine learning approach,Micro, IEEE, 29(5), p. 12.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 455, 'page_label': '456'}, page_content='434 Chapter 16: Applications and Case Studies\\nCritical to this MDP are constraints on the actions available in each state. Recall from\\nChapter 3 that the set of available actions can depend on the state:At 2 A(St), where\\nAt is the action at time stept and A(St) is the set of actions available in stateSt.I n\\nthis application, the integrity of the DRAM system was assured by not allowing actions\\nthat would violate timing or resource constraints. Although˙Ipek et al. did not make it\\nexplicit, they e↵ectively accomplished this by pre-deﬁning the setsA(St) for all possible\\nstates St.\\nThese constraints explain why the MDP has aNoOp action and why the reward signal\\nis 0 except when aread or write command is issued.NoOp is issued when it is the sole\\nlegal action in a state. To maximize utilization of the memory system, the controller’s\\ntask is to drive the system to states in which either aread or a write action can be\\nselected: only these actions result in sending data over the external data bus, so it is only\\nthese that contribute to the throughput of the system. Althoughprecharge and activate\\nproduce no immediate reward, the agent needs to select these actions to make it possible\\nto later select the rewardedread and write actions.\\nThe scheduling agent used Sarsa (Section 6.4) to learn an action-value function. States\\nwere represented by six integer-valued features. To approximate the action-value function,\\nthe algorithm used linear function approximation implemented by tile coding with hashing\\n(Section 9.5.4). The tile coding had 32 tilings, each storing 256 action values as 16-bit\\nﬁxed point numbers. Exploration was\"-greedy with\" =0 .05.\\nState features included the number of read requests in the transaction queue, the\\nnumber of write requests in the transaction queue, the number of write requests in the\\ntransaction queue waiting for their row to be opened, and the number of read requests in\\nthe transaction queue waiting for their row to be opened that are the oldest issued by\\ntheir requesting processors. (The other features depended on how the DRAM interacts\\nwith cache memory, details we omit here.) The selection of the state features was based\\non ˙Ipek et al.’s understanding of factors that impact DRAM performance. For example,\\nbalancing the rate of servicing reads and writes based on how many of each are in the\\ntransaction queue can help avoid stalling the DRAM system’s interaction with cache\\nmemory. The authors in fact generated a relatively long list of potential features, and\\nthen pared them down to a handful using simulations guided by stepwise feature selection.\\nAn interesting aspect of this formulation of the scheduling problem as an MDP is\\nthat the features input to the tile coding for deﬁning the action-value function were\\ndi↵erent from the features used to specify the action-constraint setsA(St). Whereas the\\ntile coding input was derived from the contents of the transaction queue, the constraint\\nsets depended on a host of other features related to timing and resource constraints that\\nhad to be satisﬁed by the hardware implementation of the entire system. In this way, the\\naction constraints ensured that the learning algorithm’s exploration could not endanger\\nthe integrity of the physical system, while learning was e↵ectively limited to a “safe”\\nregion of the much larger state space of the hardware implementation.\\nBecause an objective of this work was that the learning controller could be implemented\\non a chip so that learning could occur online while a computer is running, hardware\\nimplementation details were important considerations. The design included two ﬁve-stage\\npipelines to calculate and compare two action values at every processor clock cycle, and'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 456, 'page_label': '457'}, page_content='16.4. Optimizing Memory Control 435\\nto update the appropriate action value. This included accessing the tile coding which\\nwas stored on-chip in static RAM. For the conﬁguration˙Ipek et al. simulated, which was\\na 4GHz 4-core chip typical of high-end workstations at the time of their research, there\\nwere 10 processor cycles for every DRAM cycle. Considering the cycles needed to ﬁll\\nthe pipes, up to 12 actions could be evaluated in each DRAM cycle.˙Ipek et al. found\\nthat the number of legal commands for any state was rarely greater than this, and that\\nperformance loss was negligible if enough time was not always available to consider all\\nlegal commands. These and other clever design details made it feasible to implement the\\ncomplete controller and learning algorithm on a multi-processor chip.\\n˙Ipek et al. evaluated their learning controller in simulation by comparing it with three\\nother controllers: (1) the FR-FCFS controller mentioned above that produces the best\\non-average performance, (2) a conventional controller that processes each request in\\norder, and (3) an unrealizable ideal controller, called the Optimistic controller, able\\nto sustain 100% DRAM throughput if given enough demand by ignoring all timing\\nand resource constraints, but otherwise modeling DRAM latency (as row bu↵er hits)\\nand bandwidth. They simulated nine memory-intensive parallel workloads consisting of\\nscientiﬁc and data-mining applications. Figure 16.4 shows the performance (the inverse\\nof execution time normalized to the performance of FR-FCFS) of each controller for\\nthe nine applications, together with the geometric mean of their performances over the\\napplications. The learning controller, labeled RL in the ﬁgure, improved over that of\\nFR-FCFS by from 7% to 33% over the nine applications, with an average improvement of\\n19%. Of course, no realizable controller can match the performance of Optimistic, which\\nignores all timing and resource constraints, but the learning controller’s performance\\nclosed the gap with Optimistic’s upper bound by an impressive 27%.\\nBecause the rationale for on-chip implementation of the learning algorithm was to\\nallow the scheduling policy to adapt online to changing workloads,˙Ipek et al. analyzed\\nthe impact of online learning compared to a previously-learned ﬁxed policy. They trained\\nFigure 16.4: Performances of four controllers over a suite of 9 simulated benchmark applications.\\nThe controllers are: the simplest ‘in-order’ controller, FR-FCFS, the learning controller RL,\\nand the unrealizable Optimistic controller which ignores all timing and resource constraints to\\nprovide a performance upper bound. Performance, normalized to that of FR-FCFS, is the inverse\\nof execution time. At far right is the geometric mean of performances over the 9 benchmark\\napplications for each controller. Controller RL comes closest to the ideal performance.©2009\\nIEEE. Reprinted, with permission, from J. F. Mart´ ınez and E.˙Ipek, Dynamic multicore resource\\nmanagement: A machine learning approach,Micro, IEEE, 29(5), p. 13.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 457, 'page_label': '458'}, page_content='436 Chapter 16: Applications and Case Studies\\ntheir controller with data from all nine benchmark applications and then held the resulting\\naction values ﬁxed throughout the simulated execution of the applications. They found\\nthat the average performance of the controller that learned online was 8% better than\\nthat of the controller using the ﬁxed policy, leading them to conclude that online learning\\nis an important feature of their approach.\\nThis learning memory controller was never committed to physical hardware because of\\nthe large cost of fabrication. Nevertheless,˙Ipek et al. could convincingly argue on the basis\\nof their simulation results that a memory controller that learns online via reinforcement\\nlearning has the potential to improve performance to levels that would otherwise require\\nmore complex and more expensive memory systems, while removing from human designers\\nsome of the burden required to manually design e\\x00cient scheduling policies. Mukundan\\nand Mart´ ınez (2012) took this project forward by investigating learning controllers with\\nadditional actions, other performance criteria, and more complex reward functions derived\\nusing genetic algorithms. They considered additional performance criteria related to\\nenergy e\\x00ciency. The results of these studies surpassed the earlier results described\\nabove and signiﬁcantly surpassed the 2012 state-of-the-art for all of the performance\\ncriteria they considered. The approach is especially promising for developing sophisticated\\npower-aware DRAM interfaces.\\n16.5 Human-level Video Game Play\\nOne of the greatest challenges in applying reinforcement learning to real-world problems\\nis deciding how to represent and store value functions and/or policies. Unless the state\\nset is ﬁnite and small enough to allow exhaustive representation by a lookup table—as in\\nmany of our illustrative examples—one must use a parameterized function approximation\\nscheme. Whether linear or nonlinear, function approximation relies on features that\\nhave to be readily accessible to the learning system and able to convey the information\\nnecessary for skilled performance. Most successful applications of reinforcement learning\\nowe much to sets of features carefully handcrafted based on human knowledge and\\nintuition about the speciﬁc problem to be tackled.\\nA team of researchers at Google DeepMind developed an impressive demonstration that\\na deep multi-layer ANN can automate the feature design process (Mnih et al., 2013, 2015).\\nMulti-layer ANNs have been used for function approximation in reinforcement learning\\never since the 1986 popularization of the backpropagation algorithm as a method for\\nlearning internal representations (Rumelhart, Hinton, and Williams, 1986; see Section 9.7).\\nStriking results have been obtained by coupling reinforcement learning with backpropa-\\ngation. The results obtained by Tesauro and colleagues with TD-Gammon and Watson\\ndiscussed above are notable examples. These and other applications beneﬁted from the\\nability of multi-layer ANNs to learn task-relevant features. However, in all the examples\\nof which we are aware, the most impressive demonstrations required the network’s input\\nto be represented in terms of specialized features handcrafted for the given problem.\\nThis is vividly apparent in the TD-Gammon results. TD-Gammon 0.0, whose network\\ninput was essentially a “raw” representation of the backgammon board, meaning that it\\ninvolved very little knowledge of backgammon, learned to play approximately as well as'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 458, 'page_label': '459'}, page_content='16.5. Human-level Video Game Play 437\\nthe best previous backgammon computer programs. Adding specialized backgammon\\nfeatures produced TD-Gammon 1.0 which was substantially better than all previous\\nbackgammon programs and competed well against human experts.\\nMnih et al. developed a reinforcement learning agent calleddeep Q-network (DQN)\\nthat combined Q-learning with adeep convolutional ANN, a many-layered, or deep,\\nANN specialized for processing spatial arrays of data such as images. We describe deep\\nconvolutional ANNs in Section 9.7. By the time of Mnih et al.’s work with DQN, deep\\nANNs, including deep convolutional ANNs, had produced impressive results in many\\napplications, but they had not been widely used in reinforcement learning.\\nMnih et al. used DQN to show how a reinforcement learning agent can achieve a high\\nlevel of performance on any of a collection of di↵erent problems without having to use\\ndi↵erent problem-speciﬁc feature sets. To demonstrate this, they let DQN learn to play\\n49 di↵erent Atari 2600 video games by interacting with a game emulator. DQN learned a\\ndi↵erent policy for each of the 49 games (because the weights of its ANN were reset to\\nrandom values before learning on each game), but it used the same raw input, network\\narchitecture, and parameter values (e.g., step size, discount rate, exploration parameters,\\nand many more speciﬁc to the implementation) for all the games. DQN achieved levels\\nof play at or beyond human level on a large fraction of these games. Although the games\\nwere alike in being played by watching streams of video images, they varied widely in other\\nrespects. Their actions had di↵erent e↵ects, they had di↵erent state-transition dynamics,\\nand they needed di↵erent policies for learning high scores. The deep convolutional ANN\\nlearned to transform the raw input common to all the games into features specialized for\\nrepresenting the action values required for playing at the high level DQN achieved for\\nmost of the games.\\nThe Atari 2600 is a home video game console that was sold in various versions by Atari\\nInc. from 1977 to 1992. It introduced or popularized many arcade video games that are\\nnow considered classics, such as Pong, Breakout, Space Invaders, and Asteroids. Although\\nmuch simpler than modern video games, Atari 2600 games are still entertaining and\\nchallenging for human players, and they have been attractive as testbeds for developing\\nand evaluating reinforcement learning methods (Diuk, Cohen, Littman, 2008; Naddaf,\\n2010; Cobo, Zang, Isbell, and Thomaz, 2011; Bellemare, Veness, and Bowling, 2013).\\nBellemare, Naddaf, Veness, and Bowling (2012) developed the publicly available Arcade\\nLearning Environment (ALE) to encourage and simplify using Atari 2600 games to study\\nlearning and planning algorithms.\\nThese previous studies and the availability of ALE made the Atari 2600 game collection\\na good choice for Mnih et al.’s demonstration, which was also inﬂuenced by the impressive\\nhuman-level performance that TD-Gammon was able to achieve in backgammon. DQN\\nis similar to TD-Gammon in using a multi-layer ANN as the function approximation\\nmethod for a semi-gradient form of a TD algorithm, with the gradients computed by\\nthe backpropagation algorithm. However, instead of using TD(\\x00) as TD-Gammon did,\\nDQN used the semi-gradient form of Q-learning. TD-Gammon estimated the values of\\nafterstates, which were easily obtained from the rules for making backgammon moves.\\nTo use the same algorithm for the Atari games would have required generating the next\\nstates for each possible action (which would not have been afterstates in that case).\\nThis could have been done by using the game emulator to run single-step simulations'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 459, 'page_label': '460'}, page_content='438 Chapter 16: Applications and Case Studies\\nfor all the possible actions (which ALE makes possible). Or a model of each game’s\\nstate-transition function could have been learned and used to predict next states (Oh,\\nGuo, Lee, Lewis, and Singh, 2015). While these methods might have produced results\\ncomparable to DQN’s, they would have been more complicated to implement and would\\nhave signiﬁcantly increased the time needed for learning. Another motivation for using\\nQ-learning was that DQN used theexperience replaymethod, described below, which\\nrequires an o↵-policy algorithm. Being model-free and o↵-policy made Q-learning a\\nnatural choice.\\nBefore describing the details of DQN and how the experiments were conducted, we look\\nat the skill levels DQN was able to achieve. Mnih et al. compared the scores of DQN with\\nthe scores of the best performing learning system in the literature at the time, the scores\\nof a professional human games tester, and the scores of an agent that selected actions at\\nrandom. The best system from the literature used linear function approximation with\\nfeatures designed using some knowledge about Atari 2600 games (Bellemare, Naddaf,\\nVeness, and Bowling, 2013). DQN learned on each game by interacting with the game\\nemulator for 50 million frames, which corresponds to about 38 days of experience with\\nthe game. At the start of learning on each game, the weights of DQN’s network were reset\\nto random values. To evaluate DQN’s skill level after learning, its score was averaged\\nover 30 sessions on each game, each lasting up to 5 minutes and beginning with a random\\ninitial game state. The professional human tester played using the same emulator (with\\nthe sound turned o↵ to remove any possible advantage over DQN which did not process\\naudio). After 2 hours of practice, the human played about 20 episodes of each game for up\\nto 5 minutes each and was not allowed to take any break during this time. DQN learned\\nto play better than the best previous reinforcement learning systems on all but 6 of the\\ngames, and played better than the human player on 22 of the games. By considering any\\nperformance that scored at or above 75% of the human score to be comparable to, or\\nbetter than, human-level play, Mnih et al. concluded that the levels of play DQN learned\\nreached or exceeded human level on 29 of the 46 games. See Mnih et al. (2015) for a\\nmore detailed account of these results.\\nFor an artiﬁcial learning system to achieve these levels of play would be impressive\\nenough, but what makes these results remarkable—and what many at the time considered\\nto be breakthrough results for artiﬁcial intelligence—is that the very same learning system\\nachieved these levels of play on widely varying games without relying on any game-speciﬁc\\nmodiﬁcations.\\nA human playing any of these 49 Atari games sees 210⇥160 pixel image frames with\\n128 colors at 60Hz. In principle, exactly these images could have formed the raw input to\\nDQN, but to reduce memory and processing requirements, Mnih et al. preprocessed each\\nframe to produce an 84⇥84 array of luminance values. Because the full states of many\\nof the Atari games are not completely observable from the image frames, Mnih et al.\\n“stacked” the four most recent frames so that the inputs to the network had dimension\\n84⇥84⇥4. This did not eliminate partial observability for all of the games, but it was\\nhelpful in making many of them more Markovian.\\nAn essential point here is that these preprocessing steps were exactly the same for all 46\\ngames. No game-speciﬁc prior knowledge was involved beyond the general understanding\\nthat it should still be possible to learn good policies with this reduced dimension and'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 460, 'page_label': '461'}, page_content='16.5. Human-level Video Game Play 439\\nthat stacking adjacent frames should help with the partial observability of some of the\\ngames. Because no game-speciﬁc prior knowledge beyond this minimal amount was used\\nin preprocessing the image frames, we can think of the 84⇥84⇥4 input vectors as being\\n“raw” input to DQN.\\nThe basic architecture of DQN is similar to the deep convolutional ANN illustrated in\\nFigure 9.15 (though unlike that network, subsampling in DQN is treated as part of each\\nconvolutional layer, with feature maps consisting of units having only a selection of the\\npossible receptive ﬁelds). DQN has three hidden convolutional layers, followed by one\\nfully connected hidden layer, followed by the output layer. The three successive hidden\\nconvolutional layers of DQN produce 32 20⇥20 feature maps, 64 9⇥9 feature maps,\\nand 64 7⇥7 feature maps. The activation function of the units of each feature map is a\\nrectiﬁer nonlinearity (max(0,x )). The 3,136 (64⇥7⇥7) units in this third convolutional\\nlayer all connect to each of 512 units in the fully connected hidden layer, which then each\\nconnect to all 18 units in the output layer, one for each possible action in an Atari game.\\nThe activation levels of DQN’s output units were the estimated optimal action values\\nof the corresponding state–action pairs, for the state represented by the network’s input.\\nThe assignment of output units to a game’s actions varied from game to game, and\\nbecause the number of valid actions varied between 4 and 18 for the games, not all output\\nunits had functional roles in all of the games. It helps to think of the network as if it\\nwere 18 separate networks, one for estimating the optimal action value of each possible\\naction. In reality, these networks shared their initial layers, but the output units learned\\nto use the features extracted by these layers in di↵erent ways.\\nDQN’s reward signal indicated how a games’s score changed from one time step to\\nthe next: +1 whenever it increased,\\x001 whenever it decreased, and 0 otherwise. This\\nstandardized the reward signal across the games and made a single step-size parameter\\nwork well for all the games despite their varying ranges of scores. DQN used an\"-greedy\\npolicy, with\" decreasing linearly over the ﬁrst million frames and remaining at a low\\nvalue for the rest of the learning session. The values of the various other parameters,\\nsuch as the learning step size, discount rate, and others speciﬁc to the implementation,\\nwere selected by performing informal searches to see which values worked best for a small\\nselection of the games. These values were then held ﬁxed for all of the games.\\nAfter DQN selected an action, the action was executed by the game emulator, which\\nreturned a reward and the next video frame. The frame was preprocessed and added\\nto the four-frame stack that became the next input to the network. Skipping for the\\nmoment the changes to the basic Q-learning procedure made by Mnih et al., DQN used\\nthe following semi-gradient form of Q-learning to update the network’s weights:\\nwt+1 = wt + ↵\\nh\\nRt+1 + \\x00 max\\na\\nˆq(St+1,a ,wt) \\x00 ˆq(St,A t, wt)\\ni\\nrˆq(St,A t, wt), (16.3)\\nwhere wt is the vector of the network’s weights,At is the action selected at time stept,\\nand St and St+1 are respectively the preprocessed image stacks input to the network at\\ntime stepst and t + 1.\\nThe gradient in (16.3) was computed by backpropagation. Imagining again that there\\nwas a separate network for each action, for the update at time stept, backpropagation\\nwas applied only to the network corresponding toAt. Mnih et al. took advantage of\\ntechniques shown to improve the basic backpropagation algorithm when applied to large'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 461, 'page_label': '462'}, page_content='440 Chapter 16: Applications and Case Studies\\nnetworks. They used amini-batch method that updated weights only after accumulating\\ngradient information over a small batch of images (here after 32 images). This yielded\\nsmoother sample gradients compared to the usual procedure that updates weights after\\neach action. They also used a gradient-ascent algorithm called RMSProp (Tieleman and\\nHinton, 2012) that accelerates learning by adjusting the step-size parameter for each\\nweight based on a running average of the magnitudes of recent gradients for that weight.\\nMnih et al. modiﬁed the basic Q-learning procedure in three ways. First, they used\\na method calledexperience replayﬁrst studied by Lin (1992). This method stores the\\nagent’s experience at each time step in a replay memory that is accessed to perform the\\nweight updates. It worked like this in DQN. After the game emulator executed action\\nAt in a state represented by the image stackSt, and returned rewardRt+1 and image\\nstack St+1, it added the tuple (St,A t,R t+1,S t+1) to the replay memory. This memory\\naccumulated experiences over many plays of the same game. At each time step multiple Q-\\nlearning updates—a mini-batch—were performed based on experiences sampled uniformly\\nat random from the replay memory. Instead ofSt+1 becoming the newSt for the next\\nupdate as it would in the usual form of Q-learning, a new unconnected experience was\\ndrawn from the replay memory to supply data for the next update. Because Q-learning\\nis an o↵-policy algorithm, it does not need to be applied along connected trajectories.\\nQ-learning with experience replay provided several advantages over the usual form of\\nQ-learning. The ability to use each stored experience for many updates allowed DQN to\\nlearn more e\\x00ciently from its experiences. Experience replay reduced the variance of the\\nupdates because successive updates were not correlated with one another as they would\\nbe with standard Q-learning. And by removing the dependence of successive experiences\\non the current weights, experience replay eliminated one source of instability.\\nMnih et al. modiﬁed standard Q-learning in a second way to improve its stability. As\\nin other methods that bootstrap, the target for a Q-learning update depends on the\\ncurrent action-value function estimate. When a parameterized function approximation\\nmethod is used to represent action values, the target is a function of the same parameters\\nthat are being updated. For example, the target in the update given by (16.3) is\\n\\x00 maxa ˆq(St+1,a ,wt). Its dependence onwt complicates the process compared to the\\nsimpler supervised-learning situation in which the targets do not depend on the parameters\\nbeing updated. As discussed in Chapter 11 this can lead to oscillations and/or divergence.\\nTo address this problem Mnih et al. used a technique that brought Q-learning closer\\nto the simpler supervised-learning case while still allowing it to bootstrap. Whenever\\na certain number,C, of updates had been done to the weightsw of the action-value\\nnetwork, they inserted the network’s current weights into another network and held\\nthese duplicate weights ﬁxed for the nextC updates of w. The outputs of this duplicate\\nnetwork over the nextC updates of w were used as the Q-learning targets. Letting˜q\\ndenote the output of this duplicate network, then instead of (16.3) the update rule was:\\nwt+1 = wt + ↵\\nh\\nRt+1 + \\x00 max\\na\\n˜q(St+1,a ,wt) \\x00 ˆq(St,A t, wt)\\ni\\nrˆq(St,A t, wt).\\nA ﬁnal modiﬁcation of standard Q-learning was also found to improve stability. They\\nclipped the error termRt+1 + \\x00 maxa ˜q(St+1,a ,wt) \\x00 ˆq(St,A t, wt) so that it remained in\\nthe interval [\\x001, 1].'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 462, 'page_label': '463'}, page_content='16.6. Mastering the Game of Go 441\\nMnih et al. conducted a large number of learning runs on 5 of the games to gain\\ninsight into the e↵ect that various of DQN’s design features had on its performance.\\nThey ran DQN with the four combinations of experience replay and the duplicate\\ntarget network being included or not included. Although the results varied from game\\nto game, each of these features alone signiﬁcantly improved performance, and very\\ndramatically improved performance when used together. Mnih et al. also studied the role\\nplayed by the deep convolutional ANN in DQN’s learning ability by comparing the deep\\nconvolutional version of DQN with a version having a network of just one linear layer, both\\nreceiving the same stacked preprocessed video frames. Here, the improvement of the deep\\nconvolutional version over the linear version was particularly striking across all 5 of the test\\ngames.\\nCreating artiﬁcial agents that excel over a diverse collection of challenging tasks has\\nbeen an enduring goal of artiﬁcial intelligence. The promise of machine learning as\\na means for achieving this has been frustrated by the need to craft problem-speciﬁc\\nrepresentations. DeepMind’s DQN stands as a major step forward by demonstrating\\nthat a single agent can learn problem-speciﬁc features enabling it to acquire human-\\ncompetitive skills over a range of tasks. This demonstration did not produce one agent\\nthat simultaneously excelled at all the tasks (because learning occurred separately for\\neach task), but it showed that deep learning can reduce, and possibly eliminate, the need\\nfor problem-speciﬁc design and tuning. As Mnih et al. point out, however, DQN is not\\na complete solution to the problem of task-independent learning. Although the skills\\nneeded to excel on the Atari games were markedly diverse, all the games were played by\\nobserving video images, which made a deep convolutional ANN a natural choice for this\\ncollection of tasks. In addition, DQN’s performance on some of the Atari 2600 games\\nfell considerably short of human skill levels on these games. The games most di\\x00cult\\nfor DQN—especially Montezuma’s Revenge on which DQN learned to perform about as\\nwell as the random player—require deep planning beyond what DQN was designed to\\ndo. Further, learning control skills through extensive practice, like DQN learned how to\\nplay the Atari games, is just one of the types of learning humans routinely accomplish.\\nDespite these limitations, DQN advanced the state-of-the-art in machine learning by\\nimpressively demonstrating the promise of combining reinforcement learning with modern\\nmethods of deep learning.\\n16.6 Mastering the Game of Go\\nThe ancient Chinese game of Go has challenged artiﬁcial intelligence researchers for many\\ndecades. Methods that achieve human-level skill, or even superhuman-level skill, in other\\ngames have not been successful in producing strong Go programs. Thanks to a very\\nactive community of Go programmers and international competitions, the level of Go\\nprogram play has improved signiﬁcantly over the years. Until recently, however, no Go\\nprogram had been able to play anywhere near the level of a human Go master.\\nA team at DeepMind (Silver et al., 2016) developed the programAlphaGo that broke\\nthis barrier by combining deep ANNs (Section 9.7), supervised learning, Monte Carlo'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 463, 'page_label': '464'}, page_content='442 Chapter 16: Applications and Case Studies\\ntree search (MCTS, Section 8.11), and reinforcement learning. By the time of Silver et\\nal.’s 2016 publication,AlphaGo had been shown to be decisively stronger than other Go\\nprograms, and it had defeated the European Go champion Fan Hui 5 games to 0. These\\nwere the ﬁrst victories of a Go program over a professional human Go player without\\nhandicap in full Go games. Shortly thereafter, a similar version ofAlphaGo won stunning\\nvictories over the 18-time world champion Lee Sedol, winning 4 out of a 5 games in\\na challenge match, making worldwide headline news. Artiﬁcial intelligence researchers\\nthought that it would be many more years, perhaps decades, before a program reached\\nthis level of play.\\nHere we describeAlphaGo and a successor program calledAlphaGo Zero (Silver et al.\\n2017a). Where in addition to reinforcement learning,AlphaGo relied on supervised learn-\\ning from a large database of expert human moves,AlphaGo Zeroused only reinforcement\\nlearning and no human data or guidance beyond the basic rules of the game (hence the\\nZero in its name). We ﬁrst describeAlphaGo in some detail in order to highlight the\\nrelative simplicity ofAlphaGo Zero, which is both higher-performing and more of a pure\\nreinforcement learning program.\\nIn many ways, bothAlphaGo and AlphaGo Zero are descendants of Tesauro’s TD-\\nGammon (Section 16.1), itself a descendant of Samuel’s checkers player (Section 16.2).\\nAll these programs included reinforcement learning over simulated games of self-play.\\nAlphaGo and AlphaGo Zero also built upon the progress made by DeepMind on playing\\nAtari games with the program DQN (Section 16.5) that used deep convolutional ANNs\\nto approximate optimal value functions.\\nA Go board conﬁguration\\nGo is a game between two players who alter-\\nnately place black and white ‘stones’ on unoccu-\\npied intersections, or ‘points,’ on a board with\\na grid of 19 horizontal and 19 vertical lines to\\nproduce positions like that shown to the right.\\nThe game’s goal is to capture an area of the\\nboard larger than that captured by the oppo-\\nnent. Stones are captured according to simple\\nrules. A player’s stones are captured if they\\nare completely surrounded by the other player’s\\nstones, meaning that there is no horizontally\\nor vertically adjacent point that is unoccupied.\\nFor example, the left panel of Figure 16.5 (on\\nthe next page) shows three white stones with\\nan unoccupied adjacent point (labeled X). If\\nblack were to place a stone on X, then the three\\nwhite stones would be captured and taken o↵\\nthe board (middle panel). However, if white were to place a stone on point X ﬁrst, then\\nthe possibility of this capture would be blocked (right panel). Other rules are needed to\\nprevent inﬁnite capturing/recapturing loops. The game ends when neither player wishes\\nto place another stone. These rules are simple, but they produce a very complex game\\nthat has had wide appeal for thousands of years.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 464, 'page_label': '465'}, page_content='16.6. Mastering the Game of Go 443\\nXFigure 16.5: Go capturing rule. Left: the three white stones are not surrounded because point\\nX is unoccupied. Middle: if black places a stone on X, the three white stones are captured and\\nremoved from the board. Right: if white places a stone on point X ﬁrst, the capture is blocked.\\nMethods that produce strong play for other games, such as chess, have not worked as\\nwell for Go. The search space for Go is signiﬁcantly larger than that of chess because\\nGo has a larger number of legal moves per position than chess (⇡ 250 versus⇡ 35) and\\nGo games tend to involve more moves than chess games (⇡ 150 versus⇡ 80). But the\\nsize of the search space is not the major factor that makes Go so di\\x00cult. Exhaustive\\nsearch is infeasible for both chess and Go, and Go on smaller boards (e.g., 9⇥ 9) has\\nproven to be exceedingly di\\x00cult as well. Experts agree that the major stumbling block\\nto creating stronger-than-amateur Go programs is the di\\x00culty of deﬁning an adequate\\nposition evaluation function. A good evaluation function allows search to be truncated at\\na feasible depth by providing relatively easy-to-compute predictions of what deeper search\\nwould likely yield. According to M¨ uller (2002): “No simple yet reasonable evaluation\\nfunction will ever be found for Go.” A major step forward was the introduction of MCTS\\nto Go programs. The strongest programs at the time ofAlphaGo’s development all\\nincluded MCTS, but master-level skill remained elusive.\\nRecall from Section 8.11 that MCTS is a decision-time planning procedure that does\\nnot attempt to learn and store a global evaluation function. Like a rollout algorithm\\n(Section 8.10), it runs many Monte Carlo simulations of entire episodes (here, entire\\nGo games) to select each action (here, each Go move: where to place a stone or to\\nresign). Unlike a simple rollout algorithm, however, MCTS is an iterative procedure that\\nincrementally extends a search tree whose root node represents the current environment\\nstate. As illustrated in Figure 8.10, each iteration traverses the tree by simulating\\nactions guided by statistics associated with the tree’s edges. In its basic version, when\\na simulation reaches a leaf node of the search tree, MCTS expands the tree by adding\\nsome, or all, of the leaf node’s children to the tree. From the leaf node, or one of its\\nnewly added child nodes, a rollout is executed: a simulation that typically proceeds all\\nthe way to a terminal state, with actions selected by a rollout policy. When the rollout\\ncompletes, the statistics associated with the search tree’s edges that were traversed in\\nthis iteration are updated by backing up the return produced by the rollout. MCTS\\ncontinues this process, starting each time at the search tree’s root at the current state, for\\nas many iterations as possible given the time constraints. Then, ﬁnally, an action from\\nthe root node (which still represents the current environment state) is selected according\\nto statistics accumulated in the root node’s outgoing edges. This is the action the agent\\ntakes. After the environment transitions to its next state, MCTS is executed again with\\nthe root node set to represent the new current state. The search tree at the start of this\\nnext execution might be just this new root node, or it might include descendants of this\\nnode left over from MCTS’s previous execution. The remainder of the tree is discarded.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 465, 'page_label': '466'}, page_content='444 Chapter 16: Applications and Case Studies\\n16.6.1 AlphaGo\\nThe main innovation that madeAlphaGo such a strong player is that it selected moves by\\na novel version of MCTS that was guided by both a policy and a value function learned\\nby reinforcement learning with function approximation provided by deep convolutional\\nANNs. Another key feature is that instead of reinforcement learning starting from random\\nnetwork weights, it started from weights that were the result of previous supervised\\nlearning from a large collection of human expert moves.\\nThe DeepMind team calledAlphaGo’s modiﬁcation of basic MCTS “asynchronous\\npolicy and value MCTS,” or APV-MCTS. It selected actions via basic MCTS as described\\nabove but with some twists in how it extended its search tree and how it evaluated action\\nedges. In contrast to basic MCTS, which expands its current search tree by using stored\\naction values to select an unexplored edge from a leaf node, APV-MCTS, as implemented\\nin AlphaGo, expanded its tree by choosing an edge according to probabilities supplied by\\na 13-layer deep convolutional ANN, called theSL-policy network, trained previously by\\nsupervised learning to predict moves contained in a database of nearly 30 million human\\nexpert moves.\\nThen, also in contrast to basic MCTS, which evaluates the newly-added state node\\nsolely by the return of a rollout initiated from it, APV-MCTS evaluated the node in two\\nways: by this return of the rollout, but also by a value function,v✓, learned previously by\\na reinforcement learning method. Ifs was the newly-added node, its value became\\nv(s)=( 1 \\x00 ⌘)v✓(s)+ ⌘G, (16.4)\\nwhere G was the return of the rollout and⌘ controlled the mixing of the values resulting\\nfrom these two evaluation methods. InAlphaGo, these values were supplied by the\\nvalue network, another 13-layer deep convolutional ANN that was trained as we describe\\nbelow to output estimated values of board positions. APV-MCTS’s rollouts inAlphaGo\\nwere simulated games with both players using a fastrollout policy provided by a simple\\nlinear network, also trained by supervised learning before play. Throughout its execution,\\nAPV-MCTS kept track of how many simulations passed through each edge of the search\\ntree, and when its execution completed, the most-visited edge from the root node was\\nselected as the action to take, here the moveAlphaGo actually made in a game.\\nThe value network had the same structure as the deep convolutional SL policy network\\nexcept that it had a single output unit that gave estimated values of game positions\\ninstead of the SL policy network’s probability distributions over legal actions. Ideally,\\nthe value network would output optimal state values, and it might have been possible to\\napproximate the optimal value function along the lines of TD-Gammon described above:\\nself-play with nonlinear TD(\\x00) coupled to a deep convolutional ANN. But the DeepMind\\nteam took a di↵erent approach that held more promise for a game as complex as Go.\\nThey divided the process of training the value network into two stages. In the ﬁrst stage,\\nthey created the best policy they could by using reinforcement learning to train anRL\\npolicy network. This was a deep convolutional ANN with the same structure as the SL\\npolicy network. It was initialized with the ﬁnal weights of the SL policy network that\\nwere learned via supervised learning, and then policy-gradient reinforcement learning was\\nused to improve upon the SL policy. In the second stage of training the value network,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 466, 'page_label': '467'}, page_content='16.6. Mastering the Game of Go 445\\nthe team used Monte Carlo policy evaluation on data obtained from a large number of\\nsimulated self-play games with moves selected by the RL policy network.\\nFigure 16.6 illustrates the networks used byAlphaGo and the steps taken to train them\\nin what the DeepMind team called the “AlphaGo pipeline.” All these networks were\\ntrained before any live game play took place, and their weights remained ﬁxed throughout\\nlive play.\\n28 JANUARY 2016 | VOL 529 | NATURE | 485\\nARTICLE RESEARCH\\nsampled state-action pairs ( s, a ), using stochastic gradient ascent to \\nmaximize the likelihood of the human move a  selected in state s\\n∆σ\\nσ\\n∝\\n∂ ( |)\\n∂\\nσpa slo g\\nWe trained a 13-layer policy network, which we call the SL policy \\nnetwork, from 30 million positions from the KGS Go Server. The net -\\nwork predicted expert moves on a held out test set with an accuracy of \\n57.0% using all input features, and 55.7% using only raw board posi -\\ntion and move history as inputs, compared to the state-of-the-art from \\nother research groups of 44.4% at date of submission 24  (full results in \\nExtended Data Table 3 ). Small improvements in accuracy led to large \\nimprovements in playing strength ( Fig. 2a ); larger networks achieve \\nbetter accuracy but are slower to evaluate during search. We also \\ntrained a faster but less accurate rollout policy p π(a |s), using a linear \\nsoftmax of small pattern features (see Extended Data Table 4 ) with \\nweights π; this achieved an accuracy of 24.2%, using just 2  µs to select \\nan action, rather than 3  ms for the policy network.\\nReinforcement learning of policy networks\\nThe second stage of the training pipeline aims at improving the policy \\nnetwork by policy gradient reinforcement learning (RL) 25 ,26 . The RL \\npolicy network p ρ is identical in structure to the SL policy network, \\nand its weights ρ are initialized to the same values, ρ =  σ. We play \\ngames between the current policy network p ρ and a randomly selected \\nprevious iteration of the policy network. Randomizing from a pool \\nof opponents in this way stabilizes training by preventing overfitting \\nto the current policy. We use a reward function r(s) that is zero for all \\nnon-terminal time steps t <  T . The outcome zt =  ±  r(sT ) is the termi -\\nnal reward at the end of the game from the perspective of the current \\nplayer at time step t: + 1 for winning and − 1 for losing. Weights are \\nthen updated at each time step t by stochastic gradient ascent in the \\ndirection that maximizes expected outcome 25\\n∆ρ\\nρ\\n∝\\n∂ ( |)\\n∂\\nρpa s\\nz\\nlo g tt\\nt\\nWe evaluated the performance of the RL policy network in game  \\nplay, sampling each move ∼ (⋅| )ρa pstt  from its output probability  \\ndistribution over actions. When played head-to-head, the RL policy \\nnetwork won more than 80% of games against the SL policy network. \\nWe also tested against the strongest open-source Go program, Pachi 14 , \\na sophisticated Monte Carlo search program, ranked at 2 amateur dan  \\non KGS, that executes 100,000 simulations per move. Using no search \\nat all, the RL policy network won 85% of games against Pachi. In com -\\nparison, the previous state-of-the-art, based only on supervised \\nFigure 1  | Neural network training pipeline and architecture. a , A fast \\nrollout policy p π  and supervised learning (SL) policy network p σ  are \\ntrained to predict human expert moves in a data set of positions.  \\nA reinforcement learning (RL) policy network p ρ is initialized to the SL \\npolicy network, and is then improved by policy gradient learning to \\nmaximize the outcome (that is, winning more games) against previous \\nversions of the policy network. A new data set is generated by playing \\ngames of self-play with the RL policy network. Finally, a value network v θ \\nis trained by regression to predict the expected outcome (that is, whether \\nthe current player wins) in positions from the self-play data set.  \\nb , Schematic representation of the neural network architecture used in \\nAlphaGo. The policy network takes a representation of the board position \\ns as its input, passes it through many convolutional layers with parameters \\nσ  (SL policy network) or ρ (RL policy network), and outputs a probability \\ndistribution (| )σpa s  or  (| )ρpa s  over  legal moves a , represented by a \\nprobability map over the board. The value network similarly uses many \\nconvolutional layers with parameters θ, but outputs a scalar value v θ(s′) \\nthat predicts the expected outcome in position s′.\\nRegr\\nessio\\nn\\nClassi\\n/f_ication\\nClassi\\n/f_icatio\\nn\\nSelf Play\\nPolicy gradient\\na b\\nHuman expert positions Self-play positions\\nNeural networkData\\nRollout policy\\np S p V p V\\x12U (a /uni23AAs) QT (s/uni2032)p U QT\\nSL policy network RL policy networ kV alue network Policy network Va lue networ k\\ns s/uni2032\\nFigure 2  | Strength and accuracy of policy and value networks.  \\na , Plot showing the playing strength of policy networks as a function \\nof their training accuracy. Policy networks with 128, 192, 256 and 384 \\nconvolutional filters per layer were evaluated periodically during training; \\nthe plot shows the winning rate of AlphaGo using that policy network \\nagainst the match version of AlphaGo. b , Comparison of evaluation \\naccuracy between the value network and rollouts with different policies. \\nPositions and outcomes were sampled from human expert games. Each \\nposition was evaluated by a single forward pass of the value network v θ, \\nor by the mean outcome of 100 rollouts, played out using either uniform \\nrandom rollouts, the fast rollout policy p π , the SL policy network p σ  or \\nthe RL policy network p ρ. The mean squared error between the predicted \\nvalue and the actual game outcome is plotted against the stage of the game \\n(how many moves had been played in the given position).\\n15 45 75 10 5 13 5 16 5 19 5 22 5 25 5 >285\\nMove number\\n0.1 0\\n0.1 5\\n0.2 0\\n0.2 5\\n0.3 0\\n0.3 5\\n0.4 0\\n0.4 5\\n0.5 0\\nMean squar ed err or\\non expert games\\nUniform random  \\nro llout policy\\nFast r ollout policy\\nValue network\\nSL policy network\\nRL policy network\\n50 51 52 53 54 55 56 57 58 59\\nTraining accuracy on KGS dataset (%)\\n0\\n10\\n20\\n30\\n40\\n50\\n60\\n70\\n128 /f_ilters\\n192 /f_ilters\\n256 /f_ilters\\n384 /f_ilters\\nAlphaGo win rate (%)\\nab\\n© 2016 Macmillan Publishers Limited. All rights reserved\\nRollout Policy         SL policy network                       RL policy Network          Value Network\\nPolicy gradient\\nSupervised Learning MC Policy Evaluation\\nSelf Play\\nSupervised Learning\\n]] NetworksData\\nFigure 16.6: AlphaGo pipeline. Adapted with permission from Macmillan Publishers Ltd:\\nNature,v o l .5 2 9 ( 7 5 8 7 ) ,p .4 8 5 ,©2016.\\nHere is some more detail aboutAlphaGo’s ANNs and their training. The identically-\\nstructured SL and RL policy networks were similar to DQN’s deep convolutional network\\ndescribed in Section 16.5 for playing Atari games, except that they had 13 convolutional\\nlayers with the ﬁnal layer consisting of a soft-max unit for each point on the 19⇥ 19\\nGo board. The networks’ input was a 19⇥ 19 ⇥ 48 image stack in which each point\\non the Go board was represented by the values of 48 binary or integer-valued features.\\nFor example, for each point, one feature indicated if the point was occupied by one of\\nAlphaGo’s stones, one of its opponent’s stones, or was unoccupied, thus providing the\\n“raw” representation of the board conﬁguration. Other features were based on the rules\\nof Go, such as the number of adjacent points that were empty, the number of opponent\\nstones that would be captured by placing a stone there, the number of turns since a stone\\nwas placed there, and other features that the design team considered to be important.\\nTraining the SL policy network took approximately 3 weeks using a distributed\\nimplementation of stochastic gradient ascent on 50 processors. The network achieved 57%\\naccuracy, where the best accuracy achieved by other groups at the time of publication\\nwas 44.4%. Training the RL policy network was done by policy gradient reinforcement\\nlearning over simulated games between the RL policy network’s current policy and\\nopponents using policies randomly selected from policies produced by earlier iterations\\nof the learning algorithm. Playing against a randomly selected collection of opponents'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 467, 'page_label': '468'}, page_content='446 Chapter 16: Applications and Case Studies\\nprevented overﬁtting to the current policy. The reward signal was +1 if the current\\npolicy won,\\x001 if it lost, and zero otherwise. These games directly pitted the two policies\\nagainst one another without involving MCTS. By simulating many games in parallel on\\n50 processors, the DeepMind team trained the RL policy network on a million games\\nin a single day. In testing the ﬁnal RL policy, they found that it won more than 80%\\nof games played against the SL policy, and it won 85% of games played against a Go\\nprogram using MCTS that simulated 100,000 games per move.\\nThe value network, whose structure was similar to that of the SL and RL policy\\nnetworks except for its single output unit, received the same input as the SL and RL\\npolicy networks with the exception that there was an additional binary feature giving\\nthe current color to play. Monte Carlo policy evaluation was used to train the network\\nfrom data obtained from a large number of self-play games played using the RL policy.\\nTo avoid overﬁtting and instability due to the strong correlations between positions\\nencountered in self-play, the DeepMind team constructed a data set of 30 million positions\\neach chosen randomly from a unique self-play game. Then training was done using 50\\nmillion mini-batches each of 32 positions drawn from this data set. Training took one\\nweek on 50 GPUs.\\nThe rollout policy was learned prior to play by a simple linear network trained by\\nsupervised learning from a corpus of 8 million human moves. The rollout policy network\\nhad to output actions quickly while still being reasonably accurate. In principle, the SL\\nor RL policy networks could have been used in the rollouts, but the forward propagation\\nthrough these deep networks took too much time for either of them to be used in rollout\\nsimulations, a great many of which had to be carried out for each move decision during\\nlive play. For this reason, the rollout policy network was less complex than the other\\npolicy networks, and its input features could be computed more quickly than the features\\nused for the policy networks. The rollout policy network allowed approximately 1,000\\ncomplete game simulations per second to be run on each of the processing threads that\\nAlphaGo used.\\nOne may wonder why the SL policy was used instead of the better RL policy to select\\nactions in the expansion phase of APV-MCTS. These policies took the same amount of\\ntime to compute because they used the same network architecture. The team actually\\nfound thatAlphaGo played better against human opponents when APV-MCTS used as\\nthe SL policy instead of the RL policy. They conjectured that the reason for this was\\nthat the latter was tuned to respond to optimal moves rather than to the broader set\\nof moves characteristic of human play. Interestingly, the situation was reversed for the\\nvalue function used by APV-MCTS. They found that when APV-MCTS used the value\\nfunction derived from the RL policy, it performed better than if it used the value function\\nderived from the SL policy.\\nSeveral methods worked together to produceAlphaGo’s impressive playing skill. The\\nDeepMind team evaluated di↵erent versions ofAlphaGo in order to assess the contributions\\nmade by these various components. The parameter⌘ in (16.4) controlled the mixing\\nof game state evaluations produced by the value network and by rollouts. With⌘ = 0,\\nAlphaGo used just the value network without rollouts, and with⌘ = 1, evaluation\\nrelied just on rollouts. They found thatAlphaGo using just the value network played'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 468, 'page_label': '469'}, page_content='16.6. Mastering the Game of Go 447\\nbetter than the rollout-onlyAlphaGo, and in fact played better than the strongest of all\\nother Go programs existing at the time. The best play resulted from setting⌘ =0 .5,\\nindicating that combining the value network with rollouts was particularly important\\nto AlphaGo’s success. These evaluation methods complemented one another: the value\\nnetwork evaluated the high-performance RL policy that was too slow to be used in live\\nplay, while rollouts using the weaker but much faster rollout policy were able to add\\nprecision to the value network’s evaluations for speciﬁc states that occurred during games.\\nOverall, AlphaGo’s remarkable success fueled a new round of enthusiasm for the promise\\nof artiﬁcial intelligence, speciﬁcally for systems combining reinforcement learning with\\ndeep ANNs, to address problems in other challenging domains.\\n16.6.2 AlphaGo Zero\\nBuilding upon the experience withAlphaGo, a DeepMind team developedAlphaGo Zero\\n(Silver et al. 2017a). In contrast toAlphaGo, this program usedno human data or\\nguidance beyond the basic rules of the game(hence the Zero in its name). It learned\\nexclusively from self-play reinforcement learning, with input giving just “raw” descriptions\\nof the placements of stones on the Go board.AlphaGo Zero implemented a form of\\npolicy iteration (Section 4.3), interleaving policy evaluation with policy improvement.\\nFigure 16.7 is an overview ofAlphaGo Zero’s algorithm. A signiﬁcant di↵erence between\\nAlphaGo Zeroand AlphaGo is thatAlphaGo Zeroused MCTS to select moves throughout\\nself-play reinforcement learning, whereasAlphaGo used MCTS for live play after—but not\\nduring—learning. Other di↵erences besides not using any human data or human-crafted\\nfeatures are thatAlphaGo Zeroused only one deep convolutional ANN and used a simpler\\nversion of MCTS.\\nAlphaGo Zero’s MCTS was simpler than the version used byAlphaGo in that it did\\nnot include rollouts of complete games, and therefore did not need a rollout policy. Each\\niteration of AlphaGo Zero’s MCTS ran a simulation that ended at a leaf node of the\\ncurrent search tree instead of at the terminal position of a complete game simulation.\\nBut as inAlphaGo, each iteration of MCTS inAlphaGo Zerowas guided by the output of\\na deep convolutional network, labeledf✓ in Figure 16.7, where✓ is the network’s weight\\nvector. The input to the network, whose architecture we describe below, consisted of raw\\nrepresentations of board positions, and its output had two parts: a scalar value,v, an\\nestimate of the probability that the current player will win from from the current board\\nposition, and a vector,p, of move probabilities, one for each possible stone placement on\\nthe current board, plus the pass, or resign, move.\\nInstead of selecting self-play actions according to the probabilitiesp,h o w e v e r ,AlphaGo\\nZero used these probabilities, together with the network’s value output, to direct each\\nexecution of MCTS, which returned new move probabilities, shown in Figure 16.7 as the\\npolicies ⇡i. These policies beneﬁtted from the many simulations that MCTS conducted\\neach time it executed. The result was that the policy actually followed byAlphaGo\\nZero was an improvement over the policy given by the network’s outputsp. Silver et al.\\n(2017a) wrote that “MCTS may therefore be viewed as a powerfulpolicy improvement\\noperator.”'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 469, 'page_label': '470'}, page_content='448 Chapter 16: Applications and Case Studies\\nFigure 1:Self-play reinforcement learning inAlphaGo Zero.a The program plays a game\\ns1,. . . ,sT against itself. In each positionst, a Monte-Carlo tree search (MCTS) is executed (see\\nFigure 2) using the latest neural networkf\\x00. Moves are selected according to the search probabil-\\nities computed by the MCTS,at ⇠\\x00\\x00\\x00t. The terminal positionsT is scored to compute the game\\nwinnerz. bNeural network training inAlphaGo Zero. The neural network takes the raw board\\npositionsas its input, passes it through many convolutional layers with parameters\\x00, and outputs\\nboth a vectorp, representing a probability distribution over moves, and a scalar valuev, represent-\\ning the probability of the current player winning in positions. The neural network is trained on\\nrandomly sampled steps from recent games of self-play,(s,\\x00\\x00\\x00,z). The parameters\\x00are updated so\\nas to maximise the similarity of the policy vectorpto the search probabilities\\x00\\x00\\x00, and to minimise\\nthe error between the predicted winnervand the game winnerz (see Equation 1).\\n4\\nFigure 16.7: AlphaGo Zero self-play reinforcement learning. a) The program played many\\ngames against itself, one shown here as a sequence of board positionssi, i =1 , 2,...,T ,w i t h\\nmoves ai, i =1 , 2,...,T ,a n dw i n n e rz.E a c hm o v eai was determined by action probabilities⇡i\\nreturned by MCTS executed from root nodesi and guided by a deep convolutional network,\\nhere labeled f✓,w i t hl a t e s tw e i g h t s✓. Shown here for just one positions but repeated for all\\nsi, the network’s inputs were raw representations of board positionssi (together with several\\npast positions, though not shown here), and its outputs were vectorsp of move probabilities\\nthat guided MCTS’s forward searches, and scalar valuesv that estimated the probability of the\\ncurrent player winning from each positionsi. b) Deep convolutional network training. Training\\nexamples were randomly sampled steps from recent self-play games. Weights✓ were updated\\nto move the policy vectorp toward the probabilities⇡ returned by MCTS, and to include the\\nwinners z in the estimated win probabilityv. Reprinted from draft of Silver et al. (2017a) with\\npermission of the authors and DeepMind.\\nHere is more detail aboutAlphaGo Zero’s ANN and how it was trained. The network\\ntook as input a 19⇥ 19 ⇥ 17 image stack consisting of 17 binary feature planes. The ﬁrst\\n8 feature planes were raw representations of the positions of the current player’s stones in\\nthe current and seven past board conﬁgurations: a feature value was 1 if a player’s stone\\nwas on the corresponding point, and was 0 otherwise. The next 8 feature planes similarly\\ncoded the positions of the opponent’s stones. A ﬁnal input feature plane had a constant\\nvalue indicating the color of the current play: 1 for black; 0 for white. Because repetition\\nis not allowed in Go and one player is given some number of “compensation points” for\\nnot getting the ﬁrst move, the current board position is not a Markov state of Go. This\\nis why features describing past board positions and the color feature were needed.\\nThe network was “two-headed,” meaning that after a number of initial layers, the\\nnetwork split into two separate “heads” of additional layers that separately fed into two\\nsets of output units. In this case, one head fed 362 output units producing 192 +1m o v e'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 470, 'page_label': '471'}, page_content='16.6. Mastering the Game of Go 449\\nprobabilities p, one for each possible stone placement plus pass; the other head fed just\\none output unit producing the scalarv, an estimate of the probability that the current\\nplayer will win from the current board position. The network before the split consisted of\\n41 convolutional layers, each followed by batch normalization, and with skip connections\\nadded to implement residual learning by pairs of layers (see Section 9.7). Overall, move\\nprobabilities and values were computed by 43 and 44 layers respectively.\\nStarting with random weights, the network was trained by stochastic gradient descent\\n(with momentum, regularization, and step-size parameter decreasing as training continues)\\nusing batches of examples sampled uniformly at random from all the steps of the most\\nrecent 500,000 games of self-play with the current best policy. Extra noise was added\\nto the network’s outputp to encourage exploration of all possible moves. At periodic\\ncheckpoints during training, which Silver et al. (2017a) chose to be at every 1,000 training\\nsteps, the policy output by the ANN with the latest weights was evaluated by simulating\\n400 games (using MCTS with 1,600 iterations to select each move) against the current\\nbest policy. If the new policy won (by a margin set to reduce noise in the outcome), then\\nit became the best policy to be used in subsequent self-play. The network’s weights were\\nupdated to make the network’s policy outputp more closely match the policy returned\\nby MCTS, and to make its value output,v, more closely match the probability that the\\ncurrent best policy wins from the board position represented by the network’s input.\\nThe DeepMind team trainedAlphaGo Zero over 4.9 million games of self-play, which\\ntook about 3 days. Each move of each game was selected by running MCTS for 1,600\\niterations, taking approximately 0.4 second per move. Network weights were updated over\\n700,000 batches each consisting of 2,048 board conﬁgurations. They then ran tournaments\\nwith the trainedAlphaGo Zero playing against the version ofAlphaGo that defeated Fan\\nHui by 5 games to 0, and against the version that defeated Lee Sedol by 4 games to 1.\\nThey used the Elo rating system to evaluate the relative performances of the programs.\\nThe di↵erence between two Elo ratings is meant to predict the outcome of games between\\nthe players. The Elo ratings ofAlphaGo Zero, the version ofAlphaGo that played against\\nFan Hui, and the version that played against Lee Sedol were respectively 4,308, 3,144,\\nand 3,739. The gaps in these Elo ratings translate into predictions thatAlphaGo Zero\\nwould defeat these other programs with probabilities very close to one. In a match of 100\\ngames betweenAlphaGo Zero, trained as described, and the exact version ofAlphaGo\\nthat defeated Lee Sedol held under the same conditions that were used in that match,\\nAlphaGo Zero defeated AlphaGo in all 100 games.\\nThe DeepMind team also comparedAlphaGo Zerowith a program using an ANN with\\nthe same architecture but trained by supervised learning to predict human moves in a\\ndata set containing nearly 30 million positions from 160,000 games. They found that the\\nsupervised-learning player initially played better thanAlphaGo Zero, and was better at\\npredicting human expert moves, but played less well afterAlphaGo Zero was trained for\\na day. This suggested thatAlphaGo Zero had discovered a strategy for playing that was\\ndi↵erent from how humans play. In fact,AlphaGo Zerodiscovered, and came to prefer,\\nsome novel variations of classical move sequences.\\nFinal tests ofAlphaGo Zero’s algorithm were conducted with a version having a larger\\nANN and trained over 29 million self-play games, which took about 40 days, again starting'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 471, 'page_label': '472'}, page_content='450 Chapter 16: Applications and Case Studies\\nwith random weights. This version achieved an Elo rating of 5,185. The team pitted\\nthis version ofAlphaGo Zeroagainst a program calledAlphaGo Master, the strongest\\nprogram at the time, that was identical toAlphaGo Zero but, likeAlphaGo, used human\\ndata and features. AlphaGo Master’s Elo rating was 4,858, and it had defeated the\\nstrongest human professional players 60 to 0 in online games. In a 100 game match,\\nAlphaGo Zero with the larger network and more extensive learning defeatedAlphaGo\\nMaster 89 games to 11, thus providing a convincing demonstration of the problem-solving\\npower ofAlphaGo Zero’s algorithm.\\nAlphaGo Zerosoundly demonstrated that superhuman performance can be achieved\\nby pure reinforcement learning, augmented by a simple version of MCTS, and deep ANNs\\nwith very minimal knowledge of the domain and no reliance on human data or guidance.\\nWe will surely see systems inspired by the DeepMind accomplishments of bothAlphaGo\\nand AlphaGo Zero applied to challenging problems in other domains.\\nRecently, yet a better program,AlphaZero, was described by Silver et al. (2017b) that\\ndoes not even incorporate knowledge of Go.AlphaZero is a general reinforcement learning\\nalgorithm that improves over the world’s hitherto best programs in the diverse games of\\nGo, chess, and shogi.\\n16.7 Personalized Web Services\\nPersonalizing web services such as the delivery of news articles or advertisements is one\\napproach to increasing users’ satisfaction with a website or to increase the yield of a\\nmarketing campaign. A policy can recommend content considered to be the best for each\\nparticular user based on a proﬁle of that user’s interests and preferences inferred from\\ntheir history of online activity. This is a natural domain for machine learning, and in\\nparticular, for reinforcement learning. A reinforcement learning system can improve a\\nrecommendation policy by making adjustments in response to user feedback. One way\\nto obtain user feedback is by means of website satisfaction surveys, but for acquiring\\nfeedback in real time it is common to monitor user clicks as indicators of interest in a\\nlink.\\nA method long used in marketing calledA/B testing is a simple type of reinforcement\\nlearning used to decide which of two versions, A or B, of a website users prefer. Because\\nit is non-associative, like a two-armed bandit problem, this approach does not personalize\\ncontent delivery. Adding context consisting of features describing individual users and\\nthe content to be delivered allows personalizing service. This has been formalized as a\\ncontextual bandit problem (or an associative reinforcement learning problem, Section 2.9)\\nwith the objective of maximizing the total number of user clicks. Li, Chu, Langford, and\\nSchapire (2010) applied a contextual bandit algorithm to the problem of personalizing\\nthe Yahoo! Front Page Today webpage (one of the most visited pages on the internet at\\nthe time of their research) by selecting the news story to feature. Their objective was to\\nmaximize theclick-through rate(CTR), which is the ratio of the total number of clicks\\nall users make on a webpage to the total number of visits to the page. Their contextual\\nbandit algorithm improved over a standard non-associative bandit algorithm by 12.5%.\\nTheocharous, Thomas, and Ghavamzadeh (2015) argued that better results are possible'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 472, 'page_label': '473'}, page_content='16.7. Personalized Web Services 451\\nby formulating personalized recommendation as a Markov decision problem (MDP) with\\nthe objective of maximizing the total number of clicks users make over repeated visits to\\na website. Policies derived from the contextual bandit formulation are greedy in the sense\\nthat they do not take long-term e↵ects of actions into account. These policies e↵ectively\\ntreat each visit to a website as if it were made by a new visitor uniformly sampled from\\nthe population of the website’s visitors. By not using the fact that many users repeatedly\\nvisit the same websites, greedy policies do not take advantage of possibilities provided by\\nlong-term interactions with individual users.\\nAs an example of how a marketing strategy might take advantage of long-term user\\ninteraction, Theocharous et al. contrasted a greedy policy with a longer-term policy for\\ndisplaying ads for buying a product, say a car. The ad displayed by the greedy policy\\nmight o↵er a discount if the user buys the car immediately. A user either takes the o↵er\\nor leaves the website, and if they ever return to the site, they would likely see the same\\no↵er. A longer-term policy, on the other hand, can transition the user “down a sales\\nfunnel” before presenting the ﬁnal deal. It might start by describing the availability of\\nfavorable ﬁnancing terms, then praise an excellent service department, and then, on the\\nnext visit, o↵er the ﬁnal discount. This type of policy can result in more clicks by a user\\nover repeated visits to the site, and if the policy is suitably designed, more eventual sales.\\nWorking at Adobe Systems Incorporated, Theocharous et al. conducted experiments\\nto see if policies designed to maximize clicks over the long term could in fact improve\\nover short-term greedy policies. The Adobe Marketing Cloud, a set of tools that many\\ncompanies use to run digital marketing campaigns, provides infrastructure for automating\\nuser-targeted advertising and fund-raising campaigns. Actually deploying novel policies\\nusing these tools entails signiﬁcant risk because a new policy may end up performing\\npoorly. For this reason, the research team needed to assess what a policy’s performance\\nwould be if it were to be actually deployed, but to do so on the basis of data collected\\nunder the execution of other policies. A critical aspect of this research, then, was o↵-policy\\nevaluation. Further, the team wanted to do this with high conﬁdence to reduce the\\nrisk of deploying a new policy. Although high conﬁdence o↵-policy evaluation was a\\ncentral component of this research (see also Thomas, 2015; Thomas, Theocharous, and\\nGhavamzadeh, 2015), here we focus only on the algorithms and their results.\\nTheocharous et al. compared the results of two algorithms for learning ad recommen-\\ndation policies. The ﬁrst algorithm, which they calledgreedy optimization, had the goal\\nof maximizing only the probability of immediate clicks. As in the standard contextual\\nbandit formulation, this algorithm did not take the long-term e↵ects of recommendations\\ninto account. The other algorithm, a reinforcement learning algorithm based on an MDP\\nformulation, aimed at improving the number of clicks users made over multiple visits to\\na website. They called this latter algorithmlife-time value (LTV) optimization. Both\\nalgorithms faced challenging problems because the reward signal in this domain is very\\nsparse because users usually do not click on ads, and user clicking is very random so that\\nreturns have high variance.\\nData sets from the banking industry were used for training and testing these algorithms.\\nThe data sets consisted of many complete trajectories of customer interaction with a\\nbank’s website that showed each customer one out of a collection of possible o↵ers. If'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 473, 'page_label': '474'}, page_content='452 Chapter 16: Applications and Case Studies\\na customer clicked, the reward was 1, and otherwise it was 0. One data set contained\\napproximately 200,000 interactions from a month of a bank’s campaign that randomly\\no↵ered one of 7 o↵ers. The other data set from another bank’s campaign contained\\n4,000,000 interactions involving 12 possible o↵ers. All interactions included customer\\nfeatures such as the time since the customer’s last visit to the website, the number of their\\nvisits so far, the last time the customer clicked, geographic location, one of a collection of\\ninterests, and features giving demographic information.\\nGreedy optimization was based on a mapping estimating the probability of a click\\nas a function of user features. The mapping was learned via supervised learning from\\none of the data sets by means of a random forest (RF) algorithm (Breiman, 2001). RF\\nalgorithms have been widely used for large-scale applications in industry because they are\\ne↵ective predictive tools that tend not to overﬁt and are relatively insensitive to outliers\\nand noise. Theocharous et al. then used the mapping to deﬁne an\"-greedy policy that\\nselected with probability 1-\" the o↵er predicted by the RF algorithm to have the highest\\nprobability of producing a click, and otherwise selected from the other o↵ers uniformly at\\nrandom.\\nLTV optimization used a batch-mode reinforcement learning algorithm calledﬁtted\\nQ iteration (FQI). It is a variant ofﬁtted value iteration (Gordon, 1999) adapted to\\nQ-learning. Batch mode means that the entire data set for learning is available from\\nthe start, as opposed to the online mode of the algorithms we focus on in this book in\\nwhich data are acquired sequentially while the learning algorithm executes. Batch-mode\\nreinforcement learning algorithms are sometimes necessary when online learning is not\\npractical, and they can use any batch-mode supervised learning regression algorithm,\\nincluding algorithms known to scale well to high-dimensional spaces. The convergence of\\nFQI depends on properties of the function approximation algorithm (Gordon, 1999). For\\ntheir application to LTV optimization, Theocharous et al. used the same RF algorithm\\nthey used for the greedy optimization approach. Because in this case FQI convergence\\nis not monotonic, Theocharous et al. kept track of the best FQI policy by o↵-policy\\nevaluation using a validation training set. The ﬁnal policy for testing the LTV approach\\nwas the \"-greedy policy based on the best policy produced by FQI with the initial\\naction-value function set to the mapping produced by the RF for the greedy optimization\\napproach.\\nTo measure the performance of the policies produced by the greedy and LTV approaches,\\nTheocharous et al. used the CTR metric and a metric they called the LTV metric. These\\nmetrics are similar, except that the LTV metric critically distinguishes between individual\\nwebsite visitors:\\nCTR =Total # of Clicks\\nTotal # of Visits,\\nLTV = Total # of Clicks\\nTotal # of Visitors.\\nFigure 16.8 illustrates how these metrics di↵er. Each circle represents a user visit to\\nthe site; black circles are visits at which the user clicks. Each row represents visits by\\na particular user. By not distinguishing between visitors, the CTR for these sequences\\nis 0.35, whereas the LTV is 1.5. Because LTV is larger than CTR to the extent that'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 474, 'page_label': '475'}, page_content='16.8. Thermal Soaring 453\\nindividual users revisit the site, it is an indicator of how successful a policy is in encouraging\\nusers to engage in extended interactions with the site.\\nFigure 16.8: Click through rate (CTR) versus life-time value (LTV). Each circle represents\\na user visit; black circles are visits at which the user clicks. Adapted from Theocharous et\\nal. (2015).\\nTesting the policies produced by the greedy and LTV approaches was done using\\na high conﬁdence o↵-policy evaluation method on a test data set consisting of real-\\nworld interactions with a bank website served by a random policy. As expected, results\\nshowed that greedy optimization performed best as measured by the CTR metric, while\\nLTV optimization performed best as measured by the LTV metric. Furthermore—\\nalthough we have omitted its details—the high conﬁdence o↵-policy evaluation method\\nprovided probabilistic guarantees that the LTV optimization method would, with high\\nprobability, produce policies that improve upon policies currently deployed. Assured by\\nthese probabilistic guarantees, Adobe announced in 2016 that the new LTV algorithm\\nwould be a standard component of the Adobe Marketing Cloud so that a retailer could\\nissue a sequence of o↵ers following a policy likely to yield higher return than a policy\\nthat is insensitive to long-term results.\\n16.8 Thermal Soaring\\nBirds and gliders take advantage of upward air currents—thermals—to gain altitude in\\norder to maintain ﬂight while expending little, or no, energy. Thermal soaring, as this\\nbehavior is called, is a complex skill requiring responding to subtle environmental cues\\nto increase altitude by exploiting a rising column of air for as long as possible. Reddy,\\nCelani, Sejnowski, and Vergassola (2016) used reinforcement learning to investigate\\nthermal soaring policies that are e↵ective in the strong atmospheric turbulence usually\\naccompanying rising air currents. Their primary goal was to provide insight into the\\ncues birds sense and how they use them to achieve their impressive thermal soaring\\nperformance, but the results also contribute to technology relevant to autonomous gliders.\\nReinforcement learning had previously been applied to the problem of navigating e\\x00ciently\\nto the vicinity of a thermal updraft (Woodbury, Dunn, and Valasek, 2014) but not to the\\nmore challenging problem of soaring within the turbulence of the updraft itself.\\nReddy et al. modeled the soaring problem as a continuing MDP with discounting.\\nThe agent interacted with a detailed model of a glider ﬂying in turbulent air. They'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 475, 'page_label': '476'}, page_content='454 Chapter 16: Applications and Case Studies\\ndevoted signiﬁcant e↵ort toward making the model generate realistic thermal soaring\\nconditions, including investigating several di↵erent approaches to atmospheric modeling.\\nFor the learning experiments, air ﬂow in a three-dimensional box with one kilometer\\nsides, one of which was at ground level, was modeled by a sophisticated physics-based\\nset of partial di↵erential equations involving air velocity, temperature, and pressure.\\nIntroducing small random perturbations into the numerical simulation caused the model\\nto produce analogs of thermal updrafts and accompanying turbulence (Figure 16.9 Left)\\nGlider ﬂight was modeled by aerodynamic equations involving velocity, lift, drag, and\\nother factors governing powerless ﬂight of a ﬁxed-wing aircraft. Maneuvering the glider\\ninvolved changing its angle of attack (the angle between the glider’s wing and the direction\\nof air ﬂow) and its bank angle (Figure 16.9 Right).\\ncontribute significantly and more exploratory strategies are\\npreferred.The SARSA algorithm finds the optimal policy by estimating\\nfor every state–action pair itsQfunction defined as the expected\\nsum of future rewards given the current statesand the actiona.\\nAt each step, theQfunction is updated as follows:\\nQðs,aÞ→Qðs,aÞ+ηðr+βQðs′,a′Þ−Qðs,aÞÞ, [5]\\nwherer is the received reward andηis the learning rate. Theupdate is made online and does not require any prior model ofthe flow or the flight. This feature is particularly relevant inmodeling decision-making processes in animals. When the algo-rithm is close to convergence, theQfunction approaches thesolution to Bellman’s dynamic programming equations (12).The policyπas, which encodes the probability of choosing actionaat states, approaches the optimal oneπp and is obtained fromtheQfunction via a Boltzmann-like expression:\\nπas ∝exp/C0−^Qðs,aÞ/C14τtemp\\n/C1, [6]\\n^Qðs,aÞ= maxa′Qðs,a′Þ−Qðs,aÞ\\nmaxa′Qðs,a′Þ−mina′Qðs,a′Þ. [7]\\nHere,τtempis an effective“temperature”: whenτtemp/C291, ac-tions are only weakly dependent on the associatedQfunction ;conversely, forτtempsmall, the policy greedily chooses the actionwith the largestQ. The temperature parameter is initially chosenlarge and lowered as training progresses to create an annealingeffect, thereby preventing the policy from getting stuck in localextrema. Parameters used in our simulations can be found inTable S1.\\nIn the sequel, we shall qualify the policy identified by SARSAas optimal. It should be understood, however, that the SARSAalgorithm (as other reinforcement learning algorithms) typicallyidentifies an approximately optimal policy and“approximately”is skipped only for the sake of conciseness.\\nResults\\nSensorimotor Cues and Reward Function for Effective Learning.Keyaspects of the learning for the soaring problem are the sensori-motor cues that the glider can sense (state space) and the choiceof the reward used to train the glider to ascend quickly. As thestate and action spaces are continuous and high-dimensional, it isnecessary to discretize them, which we realize here by a standardlookup table representation. The height ascended per trial,averaged over different realizations of the flow, serves as ourperformance criterion.The glider is allowed control over its angle of attack and itsbank angle (Fig. 1B). Control over the angle of attack featurestwo regimes: (i) at small angles of attack, the horizontal speed islarge and the climb rate is small (the glider sinks quickly); (ii)a tlarge angles of attack but below the stall angle, the horizontalspeed is small, whereas the climb rate is large. The bank anglecontrols the heading of the glider, and we allow for a range ofvariation between−15° and 15°. Exploring various possibilities,we found that three actions are minimally sufficient: increasing,decreasing, or preserving the angle of attack and the bank angle.The angle of attack and bank angle were incremented/decre-mented in steps of 2.5° and 5°, respectively. In summary, theglider can choose 32 possible actions to control its navigation inresponse to the sensorimotor cues described hereafter.Our rationale in the choice of the state space was trying tominimize biological or electronic sensory devices necessary forcontrol. We tested different combinations of local sensorimotor\\nA\\nC z\\ny\\nLift L\\nz\\nx\\nLift LDrag D\\nvelocity direction\\nwing direction\\nbank angle\\nglide angle\\nangle of attack\\nB\\nD\\nFig. 1.Snapshots of the vertical velocity (A)a n dt h et e m p e r a t u r ef i e l d s(B)i no u rn u m e r i c a ls i m u l a t i o n so f3 DR a y l e i g h–Bénard convection. For the vertical\\nvelocity field, the red and blue colors indicate regions of large upward and downward flow, respectively. For the temperature field, the red and blue colors\\nindicate regions of high and low temperature, respectively. Notice that the hot and cold regions drive the upward and downward branches of the convective\\ncell, in agreement with the basic physics of convection. (C)T h ef o r c e - b o d yd i a g r a mo ff l i g h tw i t hn ot h r u s t ,t h a ti s ,w i t h o u ta n ye n g i n eo rf l a p p i n go fw i n g s .\\nThe figure also shows the bank angle/uni03BC(blue), the angle of attackα(green), and the glide angleγ(red). (D)T h er a n g eo fh o r i z o n t a ls p e e d sa n dc l i m br a t e s\\naccessible by controlling the angle of attack. At small angles of attack, the glider moves fast but also sinks fast, whereas at larger angles, the glider moves and\\nsinks more slowly. If the angle of attack is too high, at about 16°, the glider stalls, leading to a sudden drop in lift. The vertical black dashed line shows the\\nfixed angle of attack for most of the simulations (Results, Control over the Angle of Attack).\\nReddy et al. PNAS| Published online August 1, 2016| E4879\\nNEUROSCIENCEPHYSICS PNAS PLUS\\ncontribute significantly and more exploratory strategies are\\npreferred.The SARSA algorithm finds the optimal policy by estimating\\nfor every state–action pair itsQfunction defined as the expected\\nsum of future rewards given the current statesand the actiona.\\nAt each step, theQfunction is updated as follows:\\nQðs,aÞ→Qðs,aÞ+ηðr+βQðs′,a′Þ−Qðs,aÞÞ, [5]\\nwherer is the received reward andηis the learning rate. Theupdate is made online and does not require any prior model ofthe flow or the flight. This feature is particularly relevant inmodeling decision-making processes in animals. When the algo-rithm is close to convergence, theQfunction approaches thesolution to Bellman’s dynamic programming equations (12).The policyπas, which encodes the probability of choosing actionaat states, approaches the optimal oneπp and is obtained fromtheQfunction via a Boltzmann-like expression:\\nπas ∝exp/C0−^Qðs,aÞ/C14τtemp\\n/C1, [6]\\n^Qðs,aÞ= maxa′Qðs,a′Þ−Qðs,aÞ\\nmaxa′Qðs,a′Þ−mina′Qðs,a′Þ. [7]\\nHere,τtempis an effective“temperature”: whenτtemp/C291, ac-tions are only weakly dependent on the associatedQfunction ;conversely, forτtempsmall, the policy greedily chooses the actionwith the largestQ. The temperature parameter is initially chosenlarge and lowered as training progresses to create an annealingeffect, thereby preventing the policy from getting stuck in localextrema. Parameters used in our simulations can be found inTable S1.\\nIn the sequel, we shall qualify the policy identified by SARSAas optimal. It should be understood, however, that the SARSAalgorithm (as other reinforcement learning algorithms) typicallyidentifies an approximately optimal policy and“approximately”is skipped only for the sake of conciseness.\\nResults\\nSensorimotor Cues and Reward Function for Effective Learning.Keyaspects of the learning for the soaring problem are the sensori-motor cues that the glider can sense (state space) and the choiceof the reward used to train the glider to ascend quickly. As thestate and action spaces are continuous and high-dimensional, it isnecessary to discretize them, which we realize here by a standardlookup table representation. The height ascended per trial,averaged over different realizations of the flow, serves as ourperformance criterion.The glider is allowed control over its angle of attack and itsbank angle (Fig. 1B). Control over the angle of attack featurestwo regimes: (i) at small angles of attack, the horizontal speed islarge and the climb rate is small (the glider sinks quickly); (ii)a tlarge angles of attack but below the stall angle, the horizontalspeed is small, whereas the climb rate is large. The bank anglecontrols the heading of the glider, and we allow for a range ofvariation between−15° and 15°. Exploring various possibilities,we found that three actions are minimally sufficient: increasing,decreasing, or preserving the angle of attack and the bank angle.The angle of attack and bank angle were incremented/decre-mented in steps of 2.5° and 5°, respectively. In summary, theglider can choose 32 possible actions to control its navigation inresponse to the sensorimotor cues described hereafter.Our rationale in the choice of the state space was trying tominimize biological or electronic sensory devices necessary forcontrol. We tested different combinations of local sensorimotor\\nA\\nC z\\ny\\nLift L\\nz\\nx\\nLift LDrag D\\nvelocity direction\\nwing direction\\nbank angle\\nglide angle\\nangle of attack\\nB\\nD\\nFig. 1.Snapshots of the vertical velocity (A)a n dt h et e m p e r a t u r ef i e l d s(B)i no u rn u m e r i c a ls i m u l a t i o n so f3 DR a y l e i g h–Bénard convection. For the vertical\\nvelocity field, the red and blue colors indicate regions of large upward and downward flow, respectively. For the temperature field, the red and blue colors\\nindicate regions of high and low temperature, respectively. Notice that the hot and cold regions drive the upward and downward branches of the convective\\ncell, in agreement with the basic physics of convection. (C)T h ef o r c e - b o d yd i a g r a mo ff l i g h tw i t hn ot h r u s t ,t h a ti s ,w i t h o u ta n ye n g i n eo rf l a p p i n go fw i n g s .\\nThe figure also shows the bank angle/uni03BC(blue), the angle of attackα(green), and the glide angleγ(red). (D)T h er a n g eo fh o r i z o n t a ls p e e d sa n dc l i m br a t e s\\naccessible by controlling the angle of attack. At small angles of attack, the glider moves fast but also sinks fast, whereas at larger angles, the glider moves and\\nsinks more slowly. If the angle of attack is too high, at about 16°, the glider stalls, leading to a sudden drop in lift. The vertical black dashed line shows the\\nfixed angle of attack for most of the simulations (Results, Control over the Angle of Attack).\\nReddy et al. PNAS| Published online August 1, 2016| E4879\\nNEUROSCIENCEPHYSICS PNAS PLUS\\n↵\\nFigure 16.9: Thermal soaring model: Left: snapshot of the vertical velocity ﬁeld of the\\nsimulated cube of air: in red (blue) is a region of large upward (downward) ﬂow. Right: diagram\\nof powerless ﬂight showing bank angleµ and angle of attack↵. Adapted with permission From\\nPNAS vol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar\\nin Turbulent Environments.\\nThe interface between the agent and the environment required deﬁning the agent’s\\nactions, the state information the agent receives from the environment, and the reward\\nsignal. By experimenting with various possibilities, Reddy et al. decided that three actions\\neach for the angle of attack and the bank angle were enough for their purposes: increment\\nor decrement the current bank angle and angle of attack by 5\\x00 and 2.5\\x00, respectively, or\\nleave them unchanged. This resulted in 32 possible actions. The bank angle was bounded\\nto remain between\\x0015\\x00 and +15\\x00.\\nBecause a goal of their study was to try to determine what minimal set of sensory\\ncues are necessary for e↵ective soaring, both to shed light on the cues birds might use for\\nsoaring and to minimize the sensing complexity required for automated glider soaring,\\nthe authors tried various sets of signals as input to the reinforcement learning agent.\\nThey started by using state aggregation (Section 9.3) of a four-dimensional state space\\nwith dimensions giving local vertical wind speed, local vertical wind acceleration, torque\\ndepending on the di↵erence between the vertical wind velocities at the left and right wing\\ntips, and the local temperature. Each dimension was discretized into three bins: positive'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 476, 'page_label': '477'}, page_content='16.8. Thermal Soaring 455\\nhigh, negative high, and small. Results, described below, showed that only two of these\\ndimensions were critical for e↵ective soaring behavior.\\nThe overall objective of thermal soaring is to gain as much altitude as possible from\\neach rising column of air. Reddy et al. tried a straightforward reward signal that rewarded\\nthe agent at the end of each episode based on the altitude gained over the episode, a\\nlarge negative reward signal if the glider touched the ground, and zero otherwise. They\\nfound that learning was not successful with this reward signal for episodes of realistic\\nduration and that eligibility traces did not help. By experimenting with various reward\\nsignals, they found that learning was best with a reward signal that at each time step\\nlinearly combined the vertical wind velocity and vertical wind acceleration observed on\\nthe previous time step.\\nLearning was by one-step Sarsa, with actions selected according to a soft-max dis-\\ntribution based on normalized action values. Speciﬁcally, the action probabilities were\\ncomputed according to (13.2) with action preferences:\\nh(s, a,✓)= ˆq(s, a,✓) \\x00 minb ˆq(s, b,✓)\\n⌧\\n\\x00\\nmaxb ˆq(s, b,✓) \\x00 minb ˆq(s, b,✓)\\n\\x00,\\nwhere ✓ is a parameter vector with one component for each action and aggregated group\\nof states, andˆq(s, a,✓) merely returned the component corresponding tos, ain the usual\\nway for state aggregation methods. The above equation forms the action preferences\\nby normalizing the approximate action values to the interval [0, 1] then dividing by⌧,a\\npositive “temperature parameter.”3 As ⌧ increases, the probability of selecting an action\\nbecomes less dependent on its preference; as⌧ decreases toward zero, the probability of\\nselecting the most highly-preferred action approaches one, making the policy approach\\nthe greedy policy. The temperature parameter⌧ was initialized to 2.0 and incrementally\\ndecreased to 0.2 during learning. Action preferences were computed from the current\\nestimates of the action values: the action with the maximum estimated action value was\\ngiven preference 1/⌧, the action with the minimum estimated action value was given\\npreference 0, and the preferences of the other actions were scaled between these extremes.\\nThe step-size and discount-rate parameters were ﬁxed at 0.1 and 0.98 respectively.\\nEach learning episode took place with the agent controlling simulated ﬂight in an\\nindependently generated period of simulated turbulent air currents. Each episode lasted\\n2.5 minutes simulated with a 1 second time step. Learning e↵ectively converged after a\\nfew hundred episodes. The left panel of Figure 16.10 shows a sample trajectory before\\nlearning where the agent selects actions randomly. Starting at the top of the volume\\nshown, the glider’s trajectory is in the direction indicated by the arrow and quickly loses\\naltitude. Figure 16.10’s right panel is a trajectory after learning. The glider starts at the\\nsame place (here appearing at the bottom of the volume) and gains altitude by spiraling\\nwithin the rising column of air. Although Reddy et al. found that performance varied\\nwidely over di↵erent simulated periods of air ﬂow, the number of times the glider touched\\nthe ground consistently decreased to nearly zero as learning progressed.\\nAfter experimenting with di↵erent sets of features available to the learning agent, it\\nturned out that the combination of just vertical wind acceleration and torques worked\\n3Reddy et al. described this slightly di↵erently, but our version is equivalent to theirs.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 477, 'page_label': '478'}, page_content='456 Chapter 16: Applications and Case Studies\\n(a) (b)Figure 16.10: Sample thermal soaring trajectories, with arrows showing the direction of\\nﬂight from the same starting point (note that the altitude scales are shifted). Left: before\\nlearning: the agent selects actions randomly and the glider descends. Right: after learning:\\nthe glider gains altitude by following a spiral trajectory. Adapted with permission from PNAS\\nvol. 113(22), p. E4879, 2016, Reddy, Celani, Sejnowski, and Vergassola, Learning to Soar in\\nTurbulent Environments.\\nbest. The authors conjectured that because these features give information about the\\ngradient of vertical wind velocity in two di↵erent directions, they allow the controller to\\nselect between turning by changing the bank angle or continuing along the same course\\nby leaving the bank angle alone. This allows the glider to stay within a rising column of\\nair. Vertical wind velocity is indicative of the strength of the thermal but does not help\\nin staying within the ﬂow. They found that sensitivity to temperature was of little help.\\nThey also found that controlling the angle of attack is not helpful in staying within a\\nparticular thermal, being useful instead for traveling between thermals when covering\\nlarge distances, as in cross-country gliding and bird migration.\\nDue to the fact that soaring in di↵erent levels of turbulence requires di↵erent policies,\\ntraining was done in conditions ranging from weak to strong turbulence. In strong\\nturbulence the rapidly changing wind and glider velocities allowed less time for the\\ncontroller to react. This reduced the amount of control possible compared to what\\nwas possible for maneuvering when ﬂuctuations were weak. Reddy et al. examined the\\npolicies Sarsa learned under these di↵erent conditions. Common to policies learned in all\\nregimes were these features: when sensing negative wind acceleration, bank sharply in the\\ndirection of the wing with the higher lift; when sensing large positive wind acceleration\\nand no torque, do nothing. However, di↵erent levels of turbulence led to policy di↵erences.\\nPolicies learned in strong turbulence were more conservative in that they preferred small\\nbank angles, whereas in weak turbulence, the best action was to turn as much as possible\\nby banking sharply. Systematic study of the bank angles preferred by the policies learned\\nunder the di↵erent conditions led the authors to suggest that by detecting when vertical'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 478, 'page_label': '479'}, page_content='16.8. Thermal Soaring 457\\nwind acceleration crosses a certain threshold the controller can adjust its policy to cope\\nwith di↵erent turbulence regimes.\\nReddy et al. also conducted experiments to investigate the e↵ect of the discount-rate\\nparameter \\x00 on the performance of the learned policies. They found that the altitude\\ngained in an episode increased as\\x00 increased, reaching a maximum for\\x00 = .99, suggesting\\nthat e↵ective thermal soaring requires taking into account long-term e↵ects of control\\ndecisions.\\nThis computational study of thermal soaring illustrates how reinforcement learning\\ncan further progress toward di↵erent kinds of objectives. Learning policies having\\naccess to di↵erent sets of environmental cues and control actions contributes to both\\nthe engineering objective of designing autonomous gliders and the scientiﬁc objective of\\nimproving understanding of the soaring skills of birds. In both cases, hypotheses resulting\\nfrom the learning experiments can be tested in the ﬁeld by instrumenting real gliders4\\nand by comparing predictions with observed bird soaring behavior.\\n4This work has recently been applied to real gliders. See Reddy, Wong-Ng, Celani, Sejnowski, and\\nVergassola, “Glider soaring via reinforcement learning in the ﬁeld.”Nature 562:236–239, 2018.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 479, 'page_label': '480'}, page_content=''),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 480, 'page_label': '481'}, page_content='Chapter 17\\nFrontiers\\nIn this ﬁnal chapter we touch on some topics that are beyond the scope of this book but\\nthat we see as particularly important for the future of reinforcement learning. Many of\\nthese topics bring us beyond what is reliably known, and some bring us beyond the MDP\\nframework.\\n17.1 General Value Functions and Auxiliary Tasks\\nOver the course of this book, our notion of value function has become quite general.\\nWith o↵-policy learning we allowed a value function to be conditional on an arbitrary\\ntarget policy. Then in Section 12.8 we generalized discounting to atermination function\\n\\x00 : S 7! [0, 1], so that a di↵erent discount rate could be applied at each time step in\\ndetermining the return(12.17). This allowed us to express predictions about how much\\nreward we will get over an arbitrary, state-dependent horizon. The next, and perhaps\\nﬁnal, step is to generalize beyond rewards to permit predictions about arbitrary signals.\\nRather than predicting the sum of future rewards, we might predict the sum of the future\\nvalues of a sound or color sensation, or of an internal, highly processed signal such as\\nanother prediction. Whatever signal is added up in this way in a value-function-like\\nprediction, we call it thecumulant of that prediction. We formalize it in acumulant\\nsignal Ct 2 R. Using this, ageneral value function, or GVF, is written\\nv⇡,\\x00,C (s) .= E\\n\" 1X\\nk=t\\n kY\\ni=t+1\\n\\x00(Si)\\n!\\nCk+1\\n\\x00\\x00\\x00\\x00\\x00 St =s, At:1 ⇠⇡\\n#\\n. (17.1)\\nAs with conventional value functions (such asv⇡ or q⇤) this is an ideal function that\\nwe seek to approximate with a parameterized form, which we might continue to denote\\nˆv(s,w), although of course there would have to be a di↵erentw for each prediction, that\\nis, for each choice of⇡, \\x00, andC. Because a GVF has no necessary connection to reward,\\nit is perhaps a misnomer to call it avalue function. You might simply call it a prediction\\nor, to make it more distinctive, aforecast (Ring, in preparation). Whatever it is called, it'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 481, 'page_label': '482'}, page_content='460 Chapter 17: Frontiers\\nis in the form of a value function and thus can be learned in the usual ways using the\\nmethods developed in this book for learning approximate value functions. Along with\\nthe learned predictions, we might also learn policies to maximize the predictions in the\\nusual ways by Generalized Policy Iteration (Section 4.6) or by actor–critic methods. In\\nthis way an agent could learn to predict and control great numbers of signals, not just\\nlong-term reward.\\nWhy might it be useful to predict and control signals other than long-term reward?\\nThese are auxiliary tasks in that they are extra (in addition to) the main task of\\nmaximizing reward. One answer is that the ability to predict and control a diverse\\nmultitude of signals can constitute a powerful kind of environmental model. As we saw\\nin Chapter 8, a good model can enable the agent to get reward more e\\x00ciently. It takes\\na couple of further concepts to develop this answer clearly, so we postpone it to the next\\nsection. First let’s consider two simpler ways in which a multitude of diverse predictions\\ncan be helpful to a reinforcement learning agent.\\nOne simple way in which auxiliary tasks can help on the main task is that they may\\nrequire some of the same representations as are needed on the main task. Some of the\\nauxiliary tasks may be easier, with less delay and a clearer connection between actions\\nand outcomes. If good features can be found early on easy auxiliary tasks, then those\\nfeatures may signiﬁcantly speed learning on the main task. There is no necessary reason\\nwhy this has to be true, but in many cases it seems plausible. For example, if you learn\\nto predict and control your sensors over short time scales, say seconds, then you might\\nplausibly come up with part of the idea of physical objects, which would then greatly\\nhelp with the prediction and control of long-term reward.\\nWe might imagine an artiﬁcial neural network (ANN) in which the last layer is split\\ninto multiple parts, orheads, each working on a di↵erent task. One head might produce\\nthe approximate value function for the main task (with reward as its cumulant) whereas\\nthe others would produce solutions to various auxiliary tasks. All heads could propagate\\nerrors by stochastic gradient descent into the same body—the shared preceding part\\nof the network—which would then try to form representations, in its next-to-last layer,\\nto support all the heads. Researchers have experimented with auxiliary tasks such as\\npredicting change in pixels, predicting the next time step’s reward, and predicting the\\ndistribution of the return. In many cases this approach has been shown to greatly\\naccelerate learning on the main task (Jaderberg et al., 2017). Multiple predictions\\nhave similarly been repeatedly proposed as a way of directing the construction of state\\nestimates (see Section 17.3).\\nAnother simple way in which the learning of auxiliary tasks can improve performance\\nis best explained by analogy to the psychological phenomena of classical conditioning\\n(Section 14.2). One way of understanding classical conditioning is that evolution has\\nbuilt in a reﬂexive (non-learned) association to a particular action from the prediction\\nof a particular signal. For example, humans and many other animals appear to have a\\nbuilt-in reﬂex to blink whenever their prediction of being poked in the eye exceeds some\\nthreshold. The prediction is learned, but the association from prediction to eye closure\\nis built in, and thus the animal is saved many unprotected pokes in its eye. Similarly,\\nthe association from fear to increased heart rate, or to freezing, may be built in. Agent'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 482, 'page_label': '483'}, page_content='17.2. Temporal Abstraction via Options 461\\ndesigners can do something similar, connecting by design (without learning) predictions\\nof speciﬁc events to predetermined actions. For example, a self-driving car that learns to\\npredict whether going forward will produce a collision could be given a built-in reﬂex to\\nstop, or to turn away, whenever the prediction is above some threshold. Or consider a\\nvacuum-cleaning robot that learned to predict whether it might run out of battery power\\nbefore returning to the charger and that reﬂexively headed back to the charger whenever\\nthe prediction became non-zero. The correct prediction would depend on the size of the\\nhouse, the room the robot was in, and the age of the battery, all of which would be hard\\nfor the robot designer to know. It would be di\\x00cult for the designer to build in a reliable\\nalgorithm for deciding whether to head back to the charger in sensory terms, but it might\\nbe easy to do this in terms of the learned prediction. We foresee many possible ways\\nlike this in which learned predictions might combine usefully with built-in algorithms for\\ncontrolling behavior.\\nFinally, perhaps the most important role for auxiliary tasks is in moving beyond the\\nassumption we have made throughout this book that the state representation is ﬁxed\\nand given to the agent. To explain this role, we ﬁrst have to take a few steps back to\\nappreciate the magnitude of this assumption and the implications of removing it. We do\\nthat in Section 17.3.\\n17.2 Temporal Abstraction via Options\\nAn appealing aspect of the MDP formalism is that it can be applied usefully to tasks\\nat many di↵erent time scales. It can be used to formalize the task of deciding which\\nmuscles to twitch to grasp an object, which airplane ﬂight to take to arrive conveniently\\nat a distant city, and which job to take to lead a satisfying life. These tasks di↵er greatly\\nin their time scales, yet each can be usefully formulated as an MDP that can be solved\\nby planning or learning processes as described in this book. All involve interaction with\\nthe world, sequential decision making, and a goal usefully conceived of as accumulating\\nrewards over time, and so all can be formulated as MDPs.\\nAlthough all these tasks can be formulated as MDPs, you might think that they cannot\\nbe formulated as asingle MDP. They involve such di↵erent time scales, such di↵erent\\nnotions of choice and action! It would be no good, for example, to plan a ﬂight across a\\ncontinent at the level of muscle twitches. Yet for other tasks—such as grasping objects,\\nthrowing darts, or hitting a baseball—low-level muscle twitches may be just the right\\nlevel. People do all these things seamlessly without appearing to switch between levels.\\nCan the MDP framework be stretched to cover all the levels simultaneously?\\nPerhaps it can. One popular idea is to formalize an MDP at a detailed level, with a\\nsmall time step, yet enable planning at higher levels using extended courses of action that\\ncorrespond to many base-level time steps. To do this we need a notion of course of action\\nthat extends over many time steps and includes a notion of termination. A general way to\\nformulate these two ideas is as a policy,⇡, and a state-dependent termination function,\\x00,\\nas in GVFs. We deﬁne a pair of these as a generalized notion of action termed anoption.\\nTo execute an option! = h⇡!,\\x00 !i at time t is to obtain the action to take,At, from\\n⇡!(·| St), then terminate at timet + 1 with probability 1\\x00 \\x00!(St+1). If the option does'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 483, 'page_label': '484'}, page_content='462 Chapter 17: Frontiers\\nnot terminate att+1, then At+1 is selected from⇡!(·| St+1), and the option terminates at\\nt + 2 with probability 1\\x00\\x00!(St+2), and so on until eventual termination. It is convenient\\nto consider low-level actions to be special cases of options—each actiona corresponds\\nto an optionh⇡!,\\x00 !i whose policy picks the action (⇡!(s)= a for alls 2 S) and whose\\ntermination function is zero (\\x00!(s) = 0 for alls 2 S+). Options e↵ectively extend the\\naction space. The agent can either select a low-level action/option, terminating after one\\ntime step, or select an extended option that might execute for many time steps before\\nterminating.\\nOptions are designed so that they are interchangable with low-level actions. For\\nexample, the notion of an action-value functionq⇡ naturally generalizes to anoption-\\nvalue function that takes a state and option as input and returns the expected return\\nstarting from that state, executing that option to termination, and thereafter following\\nthe policy,⇡. We can also generalize the notion of policy to ahierarchical policythat\\nselects from options rather than actions, where options, when selected, execute until\\ntermination. With these ideas, many of the algorithms in this book can be generalized to\\nlearn approximate option-value functions and hierarchical policies. In the simplest case,\\nthe learning process ‘jumps’ from option initiation to option termination, with an update\\nonly occurring when an option terminates. More subtly, updates can be made on each\\ntime step, using “intra-option” learning algorithms, which in general require o↵-policy\\nlearning.\\nPerhaps the most important generalization made possible by option ideas is that of the\\nenvironmental model as developed in Chapters 3, 4, and 8. The conventional model of an\\naction is the state-transition probabilities and the expected immediate reward for taking\\nthe action in each state. How do conventional action models generalize tooption models?\\nFor options, the appropriate model is again of two parts, one corresponding to the state\\ntransition resulting from executing the option and one corresponding to the expected\\ncumulative reward along the way. The reward part of an option model, analogous to the\\nexpected reward for state–action pairs (3.5), is\\nr(s, !) .= E\\n⇥\\nR1 + \\x00R2 + \\x002R3 + ··· + \\x00⌧\\x001R⌧\\n\\x00\\x00 S0 =s, A0:⌧\\x001 ⇠⇡!,⌧ ⇠\\x00!\\n⇤\\n, (17.2)\\nfor all options! and all statess 2 S,w h e r e⌧ is the random time step at which the option\\nterminates according to\\x00!. Note the role of the overall discounting parameter\\x00 in this\\nequation—discounting is according to\\x00, but termination of the option is according to\\n\\x00!. The state-transition part of an option model is a little more subtle. This part of\\nthe model characterizes the probability of each possible resulting state (as in(3.4)), but\\nnow this state may result after various numbers of time steps, each of which must be\\ndiscounted di↵erently. The model for option! speciﬁes, for each states that ! might\\nstart executing in, and for each states0 that ! might terminate in,\\np(s0|s, !) .=\\n1X\\nk=1\\n\\x00k Pr{Sk =s0,⌧ =k | S0 =s, A0:k\\x001 ⇠⇡!,⌧ ⇠\\x00!}. (17.3)\\nNote that, because of the factor of\\x00k,t h i sp(s0|s, !) is no longer a transition probability\\nand no longer sums to one over all values ofs0. (Nevertheless, we continue to use the ‘|’\\nnotation inp.)'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 484, 'page_label': '485'}, page_content='17.2. Temporal Abstraction via Options 463\\nThe above deﬁnition of the transition part of an option model allows us to formulate\\nBellman equations and dynamic programming algorithms that apply to all options,\\nincluding low-level actions as a special case. For example, the general Bellman equation\\nfor the state values of a hierarchical policy⇡ is\\nv⇡(s)=\\nX\\n!2⌦(s)\\n⇡(!|s)\\n\"\\nr(s, !)+\\nX\\ns0\\np(s0|s, !)v⇡(s0)\\n#\\n, (17.4)\\nwhere ⌦(s) denotes the set of options available in states.I f ⌦ (s) includes only the\\nlow-level actions, then this equation reduces to a version of the usual Bellman equation\\n(3.14), except of course\\x00 is included in the newp (17.3) and thus does not appear.\\nSimilarly, the corresponding planning algorithms also have no\\x00. For example, the value\\niteration algorithm with options, analogous to (4.10), is\\nvk+1(s) .= max\\n!2⌦(s)\\n\"\\nr(s, !)+\\nX\\ns0\\np(s0|s, !)vk(s0)\\n#\\n, for alls 2 S.\\nIf ⌦(s) includes all the low-level actions available in each states, then this algorithm\\nconverges to the conventionalv⇤, from which the optimal policy can be computed.\\nHowever, it is particularly useful to plan with options when only a subset of the possible\\noptions are considered (in ⌦(s)) in each state. Value iteration will then converge to the\\nbest hierarchical policy limited to the restricted set of options. Although this policy may\\nbe sub-optimal, convergence can be much faster because fewer options are considered\\nand because each option can jump over many time steps.\\nTo plan with options, the agent must either be given the option models, or learn them.\\nOne natural way to learn an option model is to formulate it as a collection of GVFs (as\\ndeﬁned in the preceding section) and then learn the GVFs using the methods presented\\nin this book. It is not di\\x00cult to see how this could be done for the reward part of the\\noption model. You merely choose one GVF’s cumulant to be the reward (Ct = Rt), its\\npolicy to be the option’s policy (⇡ =⇡!), and its termination function to be the discount\\nrate times the option’s termination function (\\x00(s)= \\x00 · \\x00!(s)). The true GVF then\\nequals the reward part of the option model,v⇡,\\x00,C(s)= r(s, !), and the learning methods\\ndescribed in this book can be used to approximate it. The state-transition part of the\\noption model is a little more complicated. You need to allocate one GVF for each state\\nthat the option might terminate in. We don’t want these GVFs to accumulate anything\\nexcept when the option terminates, and then only when termination is in the appropriate\\nstate. This can be achieved by choosing the cumulant of the GVF that predicts transition\\nto states0 to be Ct =( 1\\x00 \\x00!(St))\\n St=s0 . The GVF’s policy and termination functions\\nare chosen the same as for the reward part of the option model. The true GVF then\\nequals thes0 portion of the option’s state-transition model,v⇡,\\x00,C(s)= p(s0|s, !), and\\nagain this book’s methods could be employed to learn it. Although each of these steps\\nis seemingly natural, putting them all together (including function approximation and\\nother essential components) is quite challenging and beyond the current state of the art.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 485, 'page_label': '486'}, page_content='464 Chapter 17: Frontiers\\nExercise 17.1 This section has presented options for the discounted case, but discounting\\nis arguably inappropriate for control when using function approximation (Section 10.4).\\nWhat is the natural Bellman equation for a hierarchical policy, analogous to(17.4),b u t\\nfor the average reward setting (Section 10.3)? What are the two parts of the option\\nmodel, analogous to (17.2) and (17.3), for the average reward setting? ⇤\\n17.3 Observations and State\\nThroughout this book we have written the learned approximate value functions (and\\nthe policies in Chapter 13) as functions of the environment’s state. This is a signiﬁcant\\nlimitation of the methods presented in Part I, in which the learned value function was\\nimplemented as a table such that any value function could be exactly approximated;\\nthat case is tantamount to assuming that the state of the environment is completely\\nobserved by the agent. But in many cases of interest, and certainly in the lives of all\\nnatural intelligences, the sensory input gives only partial information about the state of\\nthe world. Some objects may be occluded by others, or behind the agent, or miles away.\\nIn these cases, potentially important aspects of the environment’s state are not directly\\nobservable, and it is a strong, unrealistic, and limiting assumption to assume that the\\nlearned value function is implemented as a table over the environment’s state space.\\nThe framework of parametric function approximation that we developed in Part II is far\\nless restrictive and, arguably, no limitation at all. In Part II we retained the assumption\\nthat the learned value functions (and policies) are functions of the environment’s state,\\nbut allowed these functions to be arbitrarily restricted by the parameterization. It is\\nsomewhat surprising and not widely recognized that function approximation includes\\nimportant aspects of partial observability. For example, if there is a state variable that is\\nnot observable, then the parameterization can be chosen such that the approximate value\\ndoes not depend on that state variable. The e↵ect is just as if the state variable were not\\nobservable. Because of this, all the results obtained for the parameterized case apply to\\npartial observability without change. In this sense, the case of parameterized function\\napproximation includes the case of partial observability.\\nNevertheless, there are many issues that cannot be investigated without a more explicit\\ntreatment of partial observability. Although we cannot give them a full treatment here,\\nwe can outline the changes that would be needed to do so. There are four steps.\\nFirst, we would change the problem. The environment would emit not its states, but\\nonly observations—signals that depend on its state but, like a robot’s sensors, provide\\nonly partial information about it. For convenience, without loss of generality, we assume\\nthat the reward is a direct, known function of the observation (perhaps the observation is\\na vector, and the reward is one of its components). The environmental interaction would\\nthen have no explicit states or rewards, but could simply be an alternating sequence of\\nactions At 2 A and observationsOt 2 O:\\nA0,O 1,A 1,O 2,A 2,O 3,A 3,O 4,...,\\ngoing on forever (cf. Equation 3.1) or forming episodes each ending with a special terminal\\nobservation.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 486, 'page_label': '487'}, page_content='17.3. Observations and State 465\\nSecond, we can recover the idea of state as used in this book from the sequence of\\nobservations and actions. Let us use the wordhistory, and the notationHt, for an initial\\nportion of the trajectory up to an observation:Ht\\n.= A0,O 1,...,A t\\x001,O t. The history\\nrepresents the most that we can know about the past without looking outside of the\\ndata stream (because the history is the whole past data stream). Of course, the history\\ngrows witht and can become large and unwieldy. The idea of state is that of a compact\\nsummary of the history that is useful for predicting future sequences. To be a summary\\nof the history, a state must be a function of history,St = f(Ht). The summary would be\\ninformationally perfect if it retained all information about the history (and thus could\\nbe used to predict futures as accurately as could be done from the full history). In this\\ncase, the stateSt and the functionf are said to have theMarkov property, andSt is a\\nstate as we have used the term in this book. Let us henceforth call it aMarkov state\\nto distinguish it from states that are summaries of the history but are not su\\x00cient to\\npredict all futures. In practice, the states of real agents will not be Markov but may\\napproach it as an ideal.\\nTo be more explicit about the Markov property it is useful to formalize possible futures.\\nLet a test be any speciﬁc sequence of alternating actions and observations that might\\noccur in the future. For example, a three-step test might be denoted⌧ = a1 o1 a2 o2 a3 o3.\\nThe probability of this test given a speciﬁc historyh is deﬁned as\\np(⌧|h) .=P r{Ot+1 =o1,O t+2 =o2,O t+3 =o3 | Ht =h, At =a1,A t+1 =a2,A t+2 =a3}.(17.5)\\nFormally, f is Markov if and only if, for any test⌧, and for any historiesh and h0 that\\nmap to the same state underf, the test’s probabilities given the two histories are equal:\\nf(h)= f(h0) ) p(⌧|h)= p(⌧|h0), for allh, h0,⌧ 2{ A ⇥ O}⇤. (17.6)\\nA Markov state summarizes all the information in the history necessary for determining\\nany test’s probability. In fact, it summarizes all that is necessary for makingany prediction,\\nincluding any GVF. It also summarizes all that is necessary for optimal behavior: iff is\\nMarkov, then there is always a deterministic function⇡ such that choosingAt\\n.= ⇡(f(Ht))\\nis an optimal policy.\\nThe third step in extending reinforcement learning to partial observability is to deal\\nwith certain computational considerations. As mentioned earlier, we want the state to be\\ncompact—relatively small compared to the history. (The identity function, for example,\\nis not a goodf even though it is Markov, because the corresponding stateSt =Ht would\\ngrow unboundedly with time.) In addition, we don’t really want a functionf that takes\\nwhole histories. Instead, we want anf that can be compactly implemented with an\\nincremental, recursive update that computesSt+1 from St, incorporating only the next\\nincrement of data,At and Ot+1:\\nSt+1\\n.= u(St,A t,O t+1), for allt \\x00 0, (17.7)\\nwith the ﬁrst stateS0 given. The function u is called thestate-update function. For\\nexample, iff were the identity (St =Ht), thenu would merely extendSt by appendingAt\\nand Ot+1 to it. Givenf, it is always possible to construct a correspondingu,b u ti tm a y'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 487, 'page_label': '488'}, page_content='466 Chapter 17: Frontiers\\nPolicy, \\nValue fn.\\nWorld\\nPlanner\\nModel\\nAO R\\nA, R\\nSuA\\nFigure 17.1: A conceptual agent architecture including a model, a planner, and a state-update\\nfunction. The world in this case receives actionsA and emits observationsO. The observations\\nand a copy of the action are used by the state-update functionu to produce the new state. The\\nnew state is input to the policy and value function, producing the next action, and is also input\\nto the planner (and tou). The information ﬂows most responsible for learning are shown by\\ndashed lines that pass diagonally across the boxes that they change. The rewardR directly\\nchanges the policy and value function. The action, reward, and state change the model, which\\nworks closely with the planner to also change the policy and value function. Note that the\\noperation of the planner can be decoupled from the agent–environment interaction, whereas the\\nother processes should operate in lock step with this interaction to keep up with the arrival of\\nnew data. Also note that the model and planner do not deal with observations directly, but only\\nwith the states produced byu, which can act as targets for model learning.\\nnot be computationally convenient and, as in the identity example, it may not produce\\na compact state. The state-update function is a central part of any agent architecture\\nthat handles partial observability. It must be e\\x00ciently computable, as no actions or\\npredictions can be made until the state is available. An overall diagram of such an agent\\narchitecture is given in Figure 17.1.\\nA common strategy for ﬁnding a Markov state is to look for something compact that\\nis recursively updatable and enables accurate short-term predictions. In fact, it is only\\nnecessary to make accurateone-step predictions. An important fact is that, if anf\\nis incrementally updatable, then it is Markov if and only if all one-step tests can be\\naccurately predicted, that is, if and only if\\nf(h)= f(h0) ) Pr{Ot+1 =o|Ht =h, At =a} =P r{Ot+1 =o|Ht =h0,A t =a}, (17.8)\\nfor all h, h0 2{ A ⇥ O}⇤, o 2 O and a 2 A. Accurate one-step predictions are informa-\\ntionally su\\x00cient, together with the state-update function, to accurately predict the\\nprobability of any test of any length. This can be done by iteratively and alternately\\nmaking one-step predictions and applying the state-update function. From the whole\\ntree of possibilities the exact probability of any test or the expectation of any GVF\\ncan be determined. These observations have led many researchers to focus on one-step\\npredictions rather than directly on multi-step predictions such as GVFs. However, note'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 488, 'page_label': '489'}, page_content='17.3. Observations and State 467\\nthat determining long-term predictions from single-step predictions is exponentially com-\\nplex in the length of the predictions. Moreover, one-step predictions can be iterated\\nto give accurate long-term predictions only if they are exact. If there is any error or\\napproximation in the one-step predictions, then it can compound to make the long-term\\npredictions wildly inaccurate. In practice this is often what happens.\\nAn example of obtaining Markov states through a state-update function is provided\\nby the popular Bayesian approach known asPartially Observable MDPs, orPOMDPs.\\nIn this approach the environment is assumed to have a well deﬁnedlatent state Xt that\\nunderlies and produces the environment’s observations, but is never available to the agent\\n(and is not to be confused with the stateSt used by the agent to make predictions and\\ndecisions). The natural Markov state,St, for a POMDP is thedistribution over the latent\\nstates given the history, called thebelief state. For concreteness, assume the usual case in\\nwhich there are a ﬁnite number of hidden states,Xt 2{ 1, 2,...,d }. Then the belief state\\nis the vectorSt\\n.= st 2 [0, 1]d with components\\nst[i] .=P r{Xt =i | Ht}, for all possible latent statesi 2{ 1, 2,...,d }. (17.9)\\nThe belief state remains the same size (same number of components) even ast grows. It\\ncan also be incrementally updated by Bayes’ rule, assuming complete knowledge of the\\ninternal workings of the environment. Speciﬁcally, theith component of the belief-state\\nupdate function is\\nu(s,a ,o)[i] .=\\nPd\\nx=1 s[x]p(i, o|x, a)\\nPd\\nx=1\\nPd\\nx0=1 s[x]p(x0,o |x, a)\\n, for alla 2 A,o 2 O, (17.10)\\nand for all belief statess with components s[x], where the four-argumentp function here\\nis not the usual one for MDPs (as in Chapter 3), but the analogous one for POMDPs,\\nin terms of thelatent state: p(x0,o |x, a) .=P r{Xt =x0,O t =o |Xt\\x001 =x, At\\x001 =a}.T h i s\\napproach is popular in theoretical work and has many signiﬁcant applications, but its\\nassumptions and computational complexity scale poorly, and we do not recommend it as\\nan approach to artiﬁcial intelligence.\\nAnother example of Markov states is provided byPredictive State Representations,\\nor PSRs. PSRs address the weakness of the POMDP approach that the semantics of\\nits agent stateSt are grounded in the environment state,Xt, which is never observed\\nand thus is di\\x00cult to learn about. In PSRs and related approaches, the semantics of\\nthe agent state is instead grounded in predictions about future observations and actions,\\nwhich are readily observable. In PSRs, a Markov state is deﬁned as ad-vector of the\\nprobabilities ofd specially chosen “core” tests as deﬁned above (17.5). The vector is then\\nupdated by a state-update functionu that is analogous to Bayes rule, but with a semantics\\ngrounded in observable data, which arguably makes it easier to learn. This approach has\\nbeen extended in many ways, including end-tests, compositional tests, powerful “spectral”\\nmethods, and closed-loop and temporally abstract tests learned by TD methods. Some of\\nthe best theoretical developments are for systems known asObservable Operator Models\\n(OOMs) and Sequential Systems (Thon, 2017).\\nThe fourth and ﬁnal step in our brief outline of how to handle partial observability in\\nreinforcement learning is to re-introduce approximation. As discussed in the introduction'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 489, 'page_label': '490'}, page_content='468 Chapter 17: Frontiers\\nto Part II, to approach artiﬁcial intelligence ambitiously we must embrace approximation.\\nThis is just as true for states as it is for value functions. We must accept and work with\\nan approximate notion of state. The approximate state will play the same role in our\\nalgorithms as before, so we continue to use the notationSt for the state used by the\\nagent, even though it may not be Markov.\\nPerhaps the simplest example of an approximate state is just the latest observation,\\nSt\\n.=Ot. Of course this approach cannot handle any hidden state information. It would\\nbe better to use the lastk observations and actions,St\\n.= Ot,A t\\x001,O t\\x001,...,A t\\x00k, for\\nsome k \\x00 1, which can be achieved by a state-update function that just shifts the new\\ndata in and the oldest data out. Thiskth-order history approach is still very simple,\\nbut can greatly increase the agent’s capabilities compared to trying to use the single\\nimmediate observation directly as the state.\\nWhat happens when the Markov property(17.8) is only approximately satisﬁed?\\nUnfortunately, long-term prediction performance can degrade dramatically when one-\\nstep predictions become even slightly inaccurate. Longer-term tests, GVFs, and state-\\nupdate functions may or may not approximate better. The short-term and long-term\\napproximation objectives are just di↵erent, and there are no useful theoretical guarantees\\nat present.\\nNevertheless, there are still reasons to think that the general idea outlined in this\\nsection applies to the approximate case. The general idea is that a state that is good for\\nsome predictions is also good for others—in particular, that a Markov state, su\\x00cient for\\none-step predictions, is also su\\x00cient for all others. If we step back from that speciﬁc\\nresult for the Markov case, the general idea is similar to what we discussed in Section 17.1\\nwith multi-headed learning and auxiliary tasks. We discussed how representations that\\nwere good for the auxiliary tasks were often also good for the main task. Taken together,\\nthese suggest an approach to both partial observability and representation learning in\\nwhich multiple predictions are pursued and used to direct the construction of state\\nfeatures. The guarantee provided by the perfect-but-impractical Markov property is\\nreplaced by the heuristic that what’s good for some predictions may be good for others.\\nThis approach scales well with computational resources. With a powerful computer we\\ncould experiment with large numbers of predictions, perhaps favoring those that are most\\nsimilar to the ones of ultimate interest, that are easiest to learn reliably, or that satisfy\\nother criteria. It is important here to move beyond selecting the predictions manually.\\nThe agent should do it. This would require a general language for predictions, so that\\nthe agent can systematically explore a large space of possible predictions, sifting through\\nthem for the ones that are most useful.\\nIn particular, both POMDP and PSR approaches can be applied with approximate\\nstates. The semantics of the state is often useful in forming the state-update function, as\\nit is in these two approaches and in thekth-order history approach. However, there is not\\na strong need for the state to be accurate with respect to its semantics in order to retain\\nuseful information. Some approaches to state augmentation, such as Echo state networks\\n(Jaeger, 2002), keep almost arbitrary information about the history and can nevertheless\\nperform well. There are many possibilities, and we expect more work and ideas in this\\narea. Learning the state-update function for an approximate state is a major part of the\\nrepresentation learning problem as it arises in reinforcement learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 490, 'page_label': '491'}, page_content='17.4. Designing Reward Signals 469\\n17.4 Designing Reward Signals\\nA major advantage of reinforcement learning over supervised learning is that reinforcement\\nlearning does not rely on detailed instructional information: generating a reward signal\\ndoes not depend on knowledge of what the agent’s correct actions should be. But the\\nsuccess of a reinforcement learning application strongly depends on how well the reward\\nsignal frames the goal of the application’s designer and how well the signal assesses\\nprogress in reaching that goal. For these reasons, designing a reward signal is a critical\\npart of any application of reinforcement learning.\\nBy designing a reward signal we mean designing the part of an agent’s environment\\nthat is responsible for computing each scalar rewardRt and sending it to the agent at\\neach timet. In our discussion of terminology at the end of Chapter 14, we said thatRt\\nis more like a signal generated inside an animal’s brain than it is like an object or event\\nin the animal’s external environment. The parts of our brains that generate these signals\\nfor us evolved over millions of years to be well suited to the challenges our ancestors\\nhad to face in their struggles to propagate their genes to future generations. We should\\ntherefore not think that designing a good reward signal is always an easy thing to do!\\nOne challenge is to design a reward signal so that as an agent learns, its behavior\\napproaches, and ideally eventually achieves, what the application’s designer actually\\ndesires. This can be easy if the designer’s goal is simple and easy to identify, such as\\nﬁnding the solution to a well-deﬁned problem or earning a high score in a well-deﬁned\\ngame. In cases like these, it is usual to reward the agent according to its success in solving\\nthe problem or its success in improving its score. But some problems involve goals that\\nare di\\x00cult to translate into reward signals. This is especially true when the problem\\nrequires the agent to skillfully perform a complex task or set of tasks, such as would be\\nrequired of a useful household robotic assistant. Further, reinforcement learning agents\\ncan discover unexpected ways to make their environments deliver reward, some of which\\nmight be undesirable, or even dangerous. This is a longstanding and critical challenge for\\nany method, like reinforcement learning, that is based on optimization. We discuss this\\nissue more in Section 17.6, the ﬁnal section of this book.\\nEven when there is a simple and easily identiﬁable goal, the problem ofsparse reward\\noften arises. Delivering non-zero reward frequently enough to allow the agent to achieve\\nthe goal once, let alone to learn to achieve it e\\x00ciently from multiple initial conditions,\\ncan be a daunting challenge. State–action pairs that clearly deserve to trigger reward may\\nbe few and far between, and rewards that mark progress toward a goal can be infrequent\\nbecause progress is di\\x00cult or even impossible to detect. The agent may wander aimlessly\\nfor long periods of time (what Minsky, 1961, called the “plateau problem”).\\nIn practice, designing a reward signal is often left to an informal trial-and-error search\\nfor a signal that produces acceptable results. If the agent fails to learn, learns too slowly,\\nor learns the wrong thing, then the designer tweaks the reward signal and tries again.\\nTo do this, the designer judges the agent’s performance by criteria that he or she is\\nattempting to translate into a reward signal so that the agent’s goal matches his or her\\nown. And if learning is too slow, the designer may try to design a non-sparse reward signal\\nthat e↵ectively guides learning throughout the agent’s interaction with its environment.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 491, 'page_label': '492'}, page_content='470 Chapter 17: Frontiers\\nIt is tempting to address the sparse reward problem by rewarding the agent for achieving\\nsubgoals that the designer thinks are important way stations to the overall goal. But\\naugmenting the reward signal with well-intentioned supplemental rewards may lead the\\nagent to behave di↵erently from what is intended; the agent may end up not achieving\\nthe overall goal. A better way to provide such guidance is to leave the reward signal\\nalone and instead augment the value-function approximation with an initial guess of what\\nit should ultimately be, or augment it with initial guesses as to what certain parts of it\\nshould be. For example, suppose we wants to o↵erv0 : S ! R as an initial guess at the\\ntrue optimal value functionv⇤, and that we are using linear function approximation with\\nfeatures x : S ! Rd. Then we would deﬁne the initial value function approximation as\\nˆv(s,w) .= w>x(s)+ v0(s), (17.11)\\nand update the weightsw as usual. If the initial weight vector is0, then the initial\\nvalue function will bev0, but the asymptotic solution quality will be determined by the\\nfeature vectors as usual. This initialization can also be done for arbitrary nonlinear\\napproximators and arbitrary forms ofv0, though it is not guaranteed to always accelerate\\nlearning.\\nA particularly e↵ective approach to the sparse reward problem is theshaping tech-\\nnique introduced by the psychologist B. F. Skinner and described in Section 14.3. The\\ne↵ectiveness of this technique relies on the fact that sparse reward problems are not just\\nproblems with the reward signal; they are also problems with an agent’s policy in that\\nit prevents the agent from frequently encountering rewarding states. Shaping involves\\nchanging the reward signal as learning proceeds, starting from a reward signal that is\\nnot sparse given the agent’s initial behavior, and gradually modifying it toward a reward\\nsignal suited to the problem of original interest. Shaping might also involve modifying\\nthe dynamics of the task as learning proceeds. Each modiﬁcation is made so that the\\nagent is frequently rewarded given its current behavior. The agent faces a sequence of\\nincreasingly-di\\x00cult reinforcement learning problems, where what is learned at each stage\\nmakes the next-harder problem relatively easy because the agent now encounters reward\\nmore frequently than it would if it did not have prior experience with easier problems.\\nThis kind of shaping is an essential technique in training animals, and it is e↵ective in\\ncomputational reinforcement learning as well.\\nWhat if you have no idea what the rewards should be, but there is another agent,\\nperhaps a person, who is already expert at the task and whose behavior can be observed?\\nIn this case you could use methods known variously as “imitation learning,” “learning\\nfrom demonstration,” and “apprenticeship learning.” The idea here is to beneﬁt from\\nthe expert agent but leave open the possibility of eventually performing better. Learning\\nfrom an expert’s behavior can be done either by learning directly by supervised learning\\nor by extracting a reward signal using what is known as “inverse reinforcement learning”\\nand then using a reinforcement learning algorithm with that reward signal to learn a\\npolicy. The task of inverse reinforcement learning as explored by Ng and Russell (2000)\\nis to try to recover the expert’s reward signal from the expert’s behavior alone. This\\ncannot be done exactly because a policy can be optimal with respect to many di↵erent\\nreward signals (for example, all policies are optimal with respect to a constant reward\\nsignal), but it is possible to ﬁnd plausible reward signal candidates. Unfortunately, strong'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 492, 'page_label': '493'}, page_content='17.4. Designing Reward Signals 471\\nassumptions are required, including knowledge of the environment’s dynamics and of the\\nfeature vectors in which the reward signal is linear. The method also requires completely\\nsolving the problem (e.g., by dynamic programming methods) multiple times. These\\ndi\\x00culties notwithstanding, Abbeel and Ng (2004) argue that the inverse reinforcement\\nlearning approach can sometimes be more e↵ective than supervised learning for beneﬁting\\nfrom the behavior of an expert.\\nAnother approach to ﬁnding a good reward signal is to automate the trial-and-error\\nsearch for a good signal that we mentioned above. From an application perspective, the\\nreward signal is a parameter of the learning algorithm. As is true for other algorithm\\nparameters, the search for a good reward signal can be automated by deﬁning a space of\\nfeasible candidates and applying an optimization algorithm. The optimization algorithm\\nevaluates each candidate reward signal by running the reinforcement learning system with\\nthat signal for some number of steps, and then scoring the overall result by a “high-level”\\nobjective function intended to encode the designer’s true goal, ignoring the limitations\\nof the agent. Reward signals can even be improved via online gradient ascent, where\\nthe gradient is that of the high-level objective function (Sorg, Lewis, and Singh, 2010).\\nRelating this approach to the natural world, the algorithm for optimizing the high-level\\nobjective function is analogous to evolution, where the high-level objective function is an\\nanimal’s evolutionary ﬁtness determined by the number of its o↵spring that survive to\\nreproductive age.\\nComputational experiments with this bilevel optimization approach—one level analo-\\ngous to evolution, and the other due to reinforcement learning by individual agents—have\\nconﬁrmed that intuition alone is not always adequate to devise a good reward signal\\n(Singh, Lewis, and Barto, 2009). The performance of a reinforcement learning agent as\\nevaluated by the high-level objective function can be very sensitive to details of the agent’s\\nreward signal in subtle ways determined by the agent’s limitations and the environment\\nin which it acts and learns. These experiments have also demonstrated that an agent’s\\ngoal should not always be the same as the goal of the agent’s designer.\\nAt ﬁrst this seems counterintuitive, but it may be impossible for the agent to achieve the\\ndesigner’s goal no matter what its reward signal is. The agent has to learn under various\\nkinds of constraints, such as limited computational power, limited access to information\\nabout its environment, or limited time to learn. When there are constraints like these,\\nlearning to achieve a goal that is di↵erent from the designer’s goal can sometimes end up\\ngetting closer to the designer’s goal than if that goal were pursued directly (Sorg, Singh,\\nand Lewis, 2010; Sorg, 2011). Examples of this in the natural world are easy to ﬁnd.\\nBecause we cannot directly assess the nutritional value of most foods, evolution—the\\ndesigner of our reward signal—gave us a reward signal that makes us seek certain tastes.\\nThough certainly not infallible (indeed, possibly detrimental in environments that di↵er in\\ncertain ways from ancestral environments), this compensates for many of our limitations:\\nour limited sensory abilities, the limited time over which we can learn, and the risks\\ninvolved in ﬁnding a healthy diet through personal experimentation. Similarly, because\\nan animal cannot always observe its own evolutionary ﬁtness, that objective function\\ndoes not work as a reward signal for learning. Evolution instead provides reward signals\\nthat are sensitive to observable predictors of evolutionary ﬁtness.\\nFinally, remember that a reinforcement learning agent is not necessarily like a complete'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 493, 'page_label': '494'}, page_content='472 Chapter 17: Frontiers\\norganism or robot; it can be a component of a larger behaving system. This means\\nthat reward signals may be inﬂuenced by things inside the larger behaving agent, such\\nas motivational states, memories, ideas, or even hallucinations. Reward signals may\\nalso depend on properties of the learning process itself, such as measures of how much\\nprogress learning is making. Making reward signals sensitive to information about internal\\nfactors such as these makes it possible for an agent to learn how to control the “cognitive\\narchitecture” of which it is a part, as well as to acquire knowledge and skills that would be\\ndi\\x00cult to learn from a reward signal that depended only on external events. Possibilities\\nlike these led to the idea of “intrinsically-motivated reinforcement learning” that we\\nbrieﬂy discuss further at the end of the following section.\\n17.5 Remaining Issues\\nIn this book we have presented the foundations of a reinforcement learning approach to\\nartiﬁcial intelligence. Roughly speaking, that approach is based on model-free and model-\\nbased methods working together, as in the Dyna architecture of Chapter 8, combined\\nwith function approximation as developed in Part II. The focus has been on online and\\nincremental algorithms, which we see as fundamental even to model-based methods, and\\non how these can be applied in o↵-policy training situations. The full rationale for the\\nlatter has been presented only in this last chapter. That is, we have all along presented o↵-\\npolicy learning as an appealing way to deal with the explore/exploit dilemma, but only in\\nthis chapter have we discussed learning about many diverse auxiliary tasks simultaneously\\nwith GVFs and learning about the world hierarchically in terms of temporally-abstract\\noption models, both of which involve o↵-policy learning. Much remains to be worked\\nout, as we have indicated throughout the book and as evidenced by the directions for\\nadditional research discussed in this chapter. But suppose we are generous and grant the\\nbroad outlines of everything that we have done in the bookand everything that has been\\noutlined so far in this chapter. What would remain after that? Of course we can’t know\\nfor sure what will be required, but we can make some guesses. In this section we highlight\\nsix further issues which it seems to us will still need to be addressed by future research.\\nFirst, we still need powerful parametric function approximation methods that work well\\nin fully incremental and online settings. Methods based on deep learning and ANNs are\\na major step in this direction but, still, only work well with batch training on large data\\nsets, with training from extensive o↵-line self play, or with learning from the interleaved\\nexperience of multiple agents on the same task. These and other settings are ways of\\nworking around a basic limitation of today’s deep learning methods, which struggle to\\nlearn rapidly in the incremental, online settings that are most natural for the reinforcement\\nlearning algorithms emphasized in this book. The problem is sometimes described as\\none of “catastrophic interference” or “correlated data.” When something new is learned\\nit tends to replace what has previously been learned rather than adding to it, with the\\nresult that the beneﬁt of the older learning is lost. Techniques such as “replay bu↵ers”\\nare often used to retain and replay old data so that its beneﬁts are not permanently lost.\\nAn honest assessment has to be that current deep learning methods are not well suited to\\nonline learning. We see no reason that this limitation is insurmountable, but algorithms'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 494, 'page_label': '495'}, page_content='17.5. Remaining Issues 473\\nthat address it, while at the same time retaining the advantages of deep learning, have\\nnot yet been devised. Most current deep learning research is directed toward working\\naround this limitation rather than removing it.\\nSecond (and perhaps closely related), we still need methods for learning features such\\nthat subsequent learning generalizes well. This issue is an instance of a general problem\\nvariously called “representation learning,” “constructive induction,” and “meta-learning”—\\nhow can we use experience not just to learn a given desired function, but to learn inductive\\nbiases such that future learning generalizes better and is thus faster? This is an old\\nproblem, dating back to the origins of artiﬁcial intelligence and pattern recognition in\\nthe 1950s and 1960s.1 Such age should give one pause. Perhaps there is no solution. But\\nit is equally likely that the time for ﬁnding a solution and demonstrating its e↵ectiveness\\nhas not yet arrived. Today machine learning is conducted at a far larger scale than it has\\nbeen in the past, and the potential beneﬁts of a good representation learning method have\\nbecome much more apparent. We note that a new annual conference—the International\\nConference on Learning Representations—has been exploring this and related topics\\nevery year since 2013. It is also less common to explore representation learning within\\na reinforcement learning context. Reinforcement learning brings some new possibilities\\nto this old issue, such as the auxiliary tasks discussed in Section 17.1. In reinforcement\\nlearning, the problem of representation learning can be identiﬁed with the problem of\\nlearning the state-update function discussed in Section 17.3.\\nThird, we still need scalable methods for planning with learned environment models.\\nPlanning methods have proven extremely e↵ective in applications such as AlphaGo\\nZero and computer chess in which the model of the environment is known from the\\nrules of the game or can otherwise be supplied by human designers. But cases of full\\nmodel-based reinforcement learning, in which the environment model is learned from\\ndata and then used for planning, are rare. The Dyna system described in Chapter 8 is\\none example, but as described there and in most subsequent work it uses a tabular model\\nwithout function approximation, which greatly limits its applicability. Only a few studies\\nhave included learned linear models, and even fewer have also explored the inclusion of\\ntemporally-abstract models using options as discussed in Section 17.2.\\nMore work is needed before planning with learned models can be e↵ective. For example,\\nthe learning of the model needs to be selective because the scope of a model strongly\\na↵ects planning e\\x00ciency. If a model focuses on the key consequences of the most\\nimportant options, then planning can be e\\x00cient and rapid, but if a model includes\\ndetails of unimportant consequences of options that are unlikely to be selected, then\\nplanning may be almost useless. Environment models should be constructed judiciously\\nwith regard to both their states and dynamics with the goal of optimizing the planning\\nprocess. The various parts of the model should be continually monitored as to the degree\\nto which they contribute to, or detract from, planning e\\x00ciency. The ﬁeld has not\\nyet addressed this complex of issues or designed model-learning methods that take into\\naccount their implications.\\n1Some would claim that deep learning solves this problem, for example, that DQN as described in\\nSection 16.5 illustrates a solution, but we are unconvinced. There is as yet little evidence that deep\\nlearning alone solves the representation learning problem in a general and e\\x00cient way.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 495, 'page_label': '496'}, page_content='474 Chapter 17: Frontiers\\nA fourth issue that needs to be addressed in future research is that of automating the\\nchoice of tasks on which an agent works and uses to structure its developing competence.\\nIt is usual in machine learning for human designers to set the tasks that the learning\\nagent is expected to master. Because these tasks are known in advance and remain ﬁxed,\\nthey can be built into the learning algorithm code. However, looking ahead, we will want\\nthe agent to make its own choices about what tasks it should try to master. These might\\nbe subtasks of a speciﬁc overall task that is already known, or they might be intended to\\ncreate building blocks that permit more e\\x00cient learning of many di↵erent tasks that the\\nagent is likely to face in the future but which are currently unknown.\\nThese tasks may be like the auxiliary tasks or the GVFs discussed in Section 17.1, or\\ntasks solved by options as discussed in Section 17.2. In forming a GVF, for example, what\\nshould the cumulant, the policy, and the termination function be? The current state of\\nthe art is to select these manually, but far greater power and generality would come from\\nmaking these task choices automatically, particularly when they derive from what the\\nagent has previously constructed as a result of representation learning or experience with\\nprevious subproblems. If GVF design is automated, then the design choices themselves\\nwill have to be explicitly represented. Rather than the task choices being in the mind\\nof the designer and built into the code, they will have to be in the machine itself in\\nsuch a way that they can be set and changed, monitored, ﬁltered, and searched among\\nautomatically. Tasks could then be built hierarchically upon others much like features are\\nin an ANN. The tasks are the questions, and the contents of the ANN are the answers to\\nthose questions. We expect there will need to be a full hierarchy of questions to match\\nthe hierarchy of answers provided by modern deep learning methods.\\nThe ﬁfth issue that we would like to highlight for future research is that of the\\ninteraction between behavior and learning via some computational analog ofcuriosity.\\nIn this chapter we have been imagining a setting in which many tasks are being learned\\nsimultaneously, using o↵-policy methods, from the same stream of experience. The actions\\ntaken will of course inﬂuence this stream of experience, which in turn will determine how\\nmuch learning occurs and which tasks are learned. When reward is not available, or not\\nstrongly inﬂuenced by behavior, the agent is free to choose actions that maximize in\\nsome sense the learning on the tasks, that is, to use some measure of learning progress\\nas an internal or “intrinsic” reward, implementing a computational form of curiosity. In\\naddition to measuring learning progress, intrinsic reward can, among other possibilities,\\nsignal the receipt of unexpected, novel, or otherwise interesting input, or can assess the\\nagent’s ability to cause changes in its environment. Intrinsic reward signals generated in\\nthese ways can be used by an agent to pose tasks for itself by deﬁning auxiliary tasks,\\nGVFs, or options, as discussed above, so that skills learned in this way can contribute\\nto the agent’s ability to master future tasks. The result is a computational analog of\\nsomething likeplay. Many preliminary studies of such uses of intrinsic reward signals\\nhave been conducted, and exciting topics for future research remain in this general area.\\nA ﬁnal issue that demands attention in future research is that of developing methods to\\nmake it acceptably safe to embed reinforcement learning agents into physical environments.\\nThis is one of the most pressing areas for future research, and we discuss it further in the\\nfollowing section.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 496, 'page_label': '497'}, page_content='17.6. Reinforcement Learning and the Future of Artiﬁcial Intelligence 475\\n17.6 Reinforcement Learning and the Future of\\nArtiﬁcial Intelligence\\nWhen we were writing the ﬁrst edition of this book in the mid-1990s, artiﬁcial intelligence\\nwas making signiﬁcant progress and was having an impact on society, though it was\\nmostly still thepromise of artiﬁcial intelligence that was inspiring developments. Machine\\nlearning was part of that outlook, but it had not yet become indispensable to artiﬁcial\\nintelligence. By today that promise has transitioned to applications that are changing the\\nlives of millions of people, and machine learning has come into its own as a key technology.\\nAs we write this second edition, some of the most remarkable developments in artiﬁcial\\nintelligence have involved reinforcement learning, most notably “deep reinforcement\\nlearning”—reinforcement learning with function approximation by deep artiﬁcial neural\\nnetworks. We are at the beginning of a wave of real-world applications of artiﬁcial\\nintelligence, many of which will include reinforcement learning, deep and otherwise, that\\nwill impact our lives in ways that are hard to predict.\\nBut an abundance of successful real-world applications does not mean that true\\nartiﬁcial intelligence has arrived. Despite great progress in many areas, the gulf between\\nartiﬁcial intelligence and the intelligence of humans, and other animals, remains great.\\nSuperhuman performance can be achieved in some domains, even formidable domains\\nlike Go, but it remains a signiﬁcant challenge to develop systems that are like us in\\nbeing complete, interactive agents having general adaptability and problem-solving skills,\\nemotional sophistication, creativity, and the ability to learn quickly from experience.\\nWith its focus on learning by interacting with dynamic environments, reinforcement\\nlearning, as it develops over the future, will be a critical component of agents with these\\nabilities.\\nReinforcement learning’s connections to psychology and neuroscience (Chapters 14\\nand 15) underscore its relevance to another longstanding goal of artiﬁcial intelligence:\\nshedding light on fundamental questions about the mind and how it emerges from the\\nbrain. Reinforcement learning theory is already contributing to our understanding of\\nthe brain’s reward, motivation, and decision-making processes, and there is good reason\\nto believe that through its links to computational psychiatry, reinforcement learning\\ntheory will contribute to methods for treating mental disorders, including drug abuse\\nand addiction.\\nAnother contribution that reinforcement learning can make over the future is as an\\naid to human decision making. Policies derived by reinforcement learning in simulated\\nenvironments can advise human decision makers in such areas as education, healthcare,\\ntransportation, energy, and public-sector resource allocation. Particularly relevant is the\\nkey feature of reinforcement learning that it takes long-term consequences of decisions\\ninto account. This is very clear in games like backgammon and Go, where some of\\nthe most impressive results of reinforcement learning have been demonstrated, but it\\nis also a property of many high-stakes decisions that a↵ect our lives and our planet.\\nReinforcement learning follows related methods for advising human decision making that\\nhave been developed in the past by decision analysts in many disciplines. With advanced\\nfunction approximation methods and massive computational power, reinforcement learning'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 497, 'page_label': '498'}, page_content='476 Chapter 17: Frontiers\\nmethods have the potential to overcome some of the di\\x00culties of scaling up traditional\\ndecision-support methods to larger and more complex problems.\\nThe rapid pace of advances in artiﬁcial intelligence has led to warnings that artiﬁcial\\nintelligence poses serious threats to our societies, even to humanity itself. The renowned\\nscientist and artiﬁcial intelligence pioneer Herbert Simon anticipated the warnings we are\\nhearing today in a presentation at the Earthware Symposium at CMU in 2000 (Simon,\\n2000). He spoke of the eternal conﬂict between the promise and perils of any new\\nknowledge, reminding us of the Greek myths of Prometheus, the idealized hero of modern\\nscience, who stole ﬁre from the gods for the beneﬁt of mankind, and of Pandora, whose\\nmythical box could be opened by a small and innocent action to release untold perils on\\nthe world. While accepting that this conﬂict is inevitable, Simon urged us to recognize\\nthat as designers of our future and not mere spectators, the decisionswe make can tilt\\nthe scale in Prometheus’ favor. This is certainly true for reinforcement learning, which\\ncan beneﬁt society but can also produce undesirable outcomes if it is carelessly deployed.\\nThus, thesafety of artiﬁcial intelligence applications involving reinforcement learning is a\\ntopic that deserves careful attention.\\nA reinforcement learning agent can learn by interacting with either the real world or\\nwith a simulation of some piece of the real world, or by a mixture of these two sources of\\nexperience. Simulators provide safe environments in which an agent can explore and learn\\nwithout risking real damage to itself or to its environment. In most current applications,\\npolicies are learned from simulated experience instead of direct interaction with the\\nreal world. In addition to avoiding undesirable real-world consequences, learning from\\nsimulated experience can make virtually unlimited data available for learning, generally\\nat less cost than needed to obtain real experience, and because simulations typically run\\nmuch faster than real time, learning can often occur more quickly than if it relied on real\\nexperience.\\nNevertheless, the full potential of reinforcement learning requires reinforcement learning\\nagents to be embedded into the ﬂow of real-world experience, where they act, explore,\\nand learn inour world, and not just intheir worlds. After all, reinforcement learning\\nalgorithms—at least those upon which we focus in this book—are designed to learn online,\\nand they emulate many aspects of how animals are able to survive in nonstationary and\\nhostile environments. Embedding reinforcement learning agents in the real world can be\\ntransformative in realizing the promises of artiﬁcial intelligence to amplify and extend\\nhuman abilities.\\nA major reason for wanting a reinforcement learning agent to act and learn in the real\\nworld is that it is often di\\x00cult, sometimes impossible, to simulate real-world experience\\nwith enough ﬁdelity to make the resulting policies, whether derived by reinforcement\\nlearning or by other methods, work well—and safely—when directing real actions. This\\nis especially true for environments whose dynamics depend on the behavior of humans,\\nsuch as in education, healthcare, transportation, and public policy—domains that can\\nsurely beneﬁt from improved decision making. However, it is for real-world embedded\\nagents that warnings about potential dangers of artiﬁcial intelligence need to be heeded.\\nSome of these warnings are particularly relevant to reinforcement learning. Because\\nreinforcement learning is based on optimization, it inherits the plusses and minuses of all\\noptimization methods. On the minus side is the problem of devising objective functions,'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 498, 'page_label': '499'}, page_content='17.6. Reinforcement Learning and the Future of Artiﬁcial Intelligence 477\\nor reward signals in the case of reinforcement learning, so that optimization produces\\nthe desired results while avoiding undesirable results. We said in Section 17.4 that\\nreinforcement learning agents can discover unexpected ways to make their environments\\ndeliver reward, some of which might be undesirable, or even dangerous. When we specify\\nwhat we want a system to learn only indirectly, as we do in designing a reinforcement\\nlearning system’s reward signal, we will not know how closely the agent will fulﬁll our desire\\nuntil its learning is complete. This is hardly a new problem with reinforcement learning;\\nrecognition of it has a long history in both literature and engineering. For example, in\\nGoethe’s poem “The Sorcerer’s Apprentice” (Goethe, 1878), the apprentice uses magic to\\nenchant a broom to do his job of fetching water, but the result is an unintended ﬂood due\\nto the apprentice’s inadequate knowledge of magic. In the engineering context, Norbert\\nWiener, the founder of cybernetics, warned of this problem more than half a century ago\\nby relating the supernatural story of “The Monkey’s Paw” (Wiener, 1964): “... it grants\\nwhat you ask for, not what you should have asked for or what you intend” (p. 59). The\\nproblem has also been discussed at length in a modern context by Nick Bostrom (2014).\\nAnyone having experience with reinforcement learning has likely seen their systems\\ndiscover unexpected ways to obtain a lot of reward. Sometimes the unexpected behavior\\nis good: it solves a problem in a nice new way. In other instances, what the agent\\nlearns violates considerations that the system designer may never have thought about.\\nCareful design of reward signals is essential if an agent is to act in the real world with no\\nopportunity for human vetting of its actions or means to easily interrupt its behavior.\\nDespite the possibility of unintended negative consequences, optimization has been used\\nfor hundreds of years by engineers, architects, and others whose designs have positively\\nimpacted the world. We owe much that is good in our environment to the application\\nof optimization methods. Many approaches have been developed to mitigate the risk\\nof optimization, such as adding hard and soft constraints, restricting optimization to\\nrobust and risk-sensitive policies, and optimizing with multiple objective functions. Some\\nof these approaches have been adapted to reinforcement learning, and more research is\\nneeded to address these concerns. The problem of ensuring that a reinforcement learning\\nagent’s goal is attuned to our own remains a challenge.\\nAnother challenge if reinforcement learning agents are to act and learn in the real world\\nis not just about what they might learneventually, but about how they will behave while\\nthey are learning. How do you make sure that an agent gets enough experience to learn a\\nhigh-performing policy, all the while not harming its environment, other agents, or itself\\n(or more realistically, while keeping the probability of harm acceptably low)? This problem\\nis also not novel or unique to reinforcement learning. Risk management and mitigation\\nfor embedded reinforcement learning is similar to what control engineers have had to\\nconfront from the beginning of using automatic control in situations where a controller’s\\nbehavior can have unacceptable, possibly catastrophic, consequences, as in the control of\\nan aircraft or a delicate chemical process. Control applications rely on careful system\\nmodeling, model validation, and extensive testing, and there is a highly-developed body\\nof theory aimed at ensuring convergence and stability of adaptive controllers designed for\\nuse when the dynamics of the system to be controlled are not fully known. Theoretical\\nguarantees are never iron-clad because they depend on the validity of the assumptions\\nunderlying the mathematics, but without this theory, combined with risk-management'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 499, 'page_label': '500'}, page_content='478 Chapter 17: Frontiers\\nand mitigation practices, automatic control—adaptive and otherwise—would not be as\\nbeneﬁcial as it is today in improving the quality, e\\x00ciency, and cost-e↵ectiveness of\\nprocesses on which we have come to rely. One of the most pressing areas for future\\nreinforcement learning research is to adapt and extend methods developed in control\\nengineering with the goal of making it acceptably safe to fully embed reinforcement\\nlearning agents into physical environments.\\nIn closing, we return to Simon’s call for us to recognize that we are designers of our\\nfuture and not simply spectators. By decisions we make as individuals, and by the\\ninﬂuence we can exert on how our societies are governed, we can work toward ensuring\\nthat the beneﬁts made possible by a new technology outweigh the harm it can cause.\\nThere is ample opportunity to do this in the case of reinforcement learning, which can\\nhelp improve the quality, fairness, and sustainability of life on our planet, but which\\ncan also release new perils. A threat already here is the displacement of jobs caused\\nby applications of artiﬁcial intelligence. Still there are good reasons to believe that the\\nbeneﬁts of artiﬁcial intelligence can outweigh the disruption it causes. As to safety,\\nhazards possible with reinforcement learning are not completely di↵erent from those\\nthat have been managed successfully for related applications of optimization and control\\nmethods. As reinforcement learning moves out into the real world in future applications,\\ndevelopers have an obligation to follow best practices that have evolved for similar\\ntechnologies, while at the same time extending them to make sure that Prometheus keeps\\nthe upper hand.\\nBibliographical and Historical Remarks\\n17.1 General value functions were ﬁrst explicitly identiﬁed by Sutton and colleagues\\n(Sutton, 1995a; Sutton et al., 2011; Modayil, White, and Sutton, 2013). Ring (in\\npreparation) developed an extensive thought experiment with GVFs (“forecasts”)\\nthat has been inﬂuential despite not yet having been published.\\nThe ﬁrst demonstrations of multi-headed learning in reinforcement learning\\nwere by Jaderberg et al. (2017). Bellemare, Dabney, and Munos (2017) showed\\nthat predicting more things about the distribution of reward could signiﬁcantly\\naccelerate learning to optimize its expectation, an instance of auxiliary tasks.\\nMany others have since taken up this line of research.\\nThe general theory of classical conditioning as learned predictions together with\\nbuilt-in, reﬂexive reactions to the predictions has not to our knowledge been\\nclearly articulated in the psychological literature. Modayil and Sutton (2014)\\ndescribe it as an approach to the engineering of robots and other agents, calling\\nit “Pavlovian control” to allude to its roots in classical conditioning.\\n17.2 The formalization of temporally abstract courses of action as options was intro-\\nduced by Sutton, Precup, and Singh (1999), building on prior work by Parr (1998)\\nand Sutton (1995a), and on classical work on Semi-MDPs (e.g., see Puterman,\\n1994). Precup’s (2000) PhD thesis developed option ideas fully. An important\\nlimitation of these early works is that they did not treat the o↵-policy case'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 500, 'page_label': '501'}, page_content='17.6. Reinforcement Learning and the Future of Artiﬁcial Intelligence 479\\nwith function approximation. Intra-option learning in general requires o↵-policy\\nlearning, which could not be done reliably with function approximation at that\\ntime. Although now we have a variety of stable o↵-policy learning methods using\\nfunction approximation, their combination with option ideas had not been signif-\\nicantly explored at the time of publication of this book. Barto and Mahadevan\\n(2003) and Hengst (2012) review the options formalism and other approaches to\\ntemporal abstraction.\\nUsing GVFs to implement option models has not previously been described. Our\\npresentation uses the trick introduced by Modayil, White, and Sutton (2014) for\\npredicting signals at the termination of policies.\\nAmong the few works that have learned option models with function approxi-\\nmation are those by Sorg and Singh (2010), and by Bacon, Harb, and Precup\\n(2017).\\nThe extension of options and option models to the average-reward setting has\\nnot yet been developed in the literature.\\n17.3 A good presentation of the POMDP approach is given by Monahan (1982). PSRs\\nand tests were introduced by Littman, Sutton, and Singh (2002). OOMs were\\nintroduced by Jaeger (1997, 1998, 2000). Sequential Systems, which unify PSRs,\\nOOMs, and many other works, were introduced in the PhD thesis of Michael Thon\\n(2017; Thon and Jaeger, 2015). Extensions to networks of temporal relationships\\nwere developed by Tanner (2006; Sutton and Tanner, 2005) and then extended\\nto options (Sutton, Rafols, and Koop, 2006).\\nThe theory of reinforcement learning with a non-Markov state representation was\\ndeveloped explicitly by Singh, Jaakkola, and Jordan (1994; Jaakkola, Singh, and\\nJordan, 1995). Early reinforcement learning approaches to partial observability\\nwere developed by Chrisman (1992), McCallum (1993, 1995), Parr and Russell\\n(1995), Littman, Cassandra, and Kaelbling (1995), and by Lin and Mitchell\\n(1992).\\n17.4 Early e↵orts to include advice and teaching in reinforcement learning include\\nthose by Lin (1992), Maclin and Shavlik (1994), Clouse (1996), and Clouse and\\nUtgo↵ (1992).\\nSkinner’s shaping should not be confused with the “potential-based shaping”\\ntechnique introduced by Ng, Harada, and Russell (1999). Their technique has\\nbeen shown by Wiewiora (2003) to be equivalent to the simpler idea of providing\\nan initial approximation to the value function, as in (17.11).\\n17.5 We recommend the book by Goodfellow, Bengio, and Courville (2016) for discus-\\nsion of today’s deep learning techniques. The problem of catastrophic interference\\nin ANNs was developed by McCloskey and Cohen (1989), Ratcli↵ (1990), and\\nFrench (1999). The idea of a replay bu↵er was introduced by Lin (1992) and used\\nprominently in deep learning in the Atari game playing system (Section 16.5,\\nMnih et al., 2013, 2015).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 501, 'page_label': '502'}, page_content='480 Chapter 17: Frontiers\\nMinsky (1961) was one of the ﬁrst to identify the problem of representation\\nlearning.\\nAmong the few works to consider planning with learned, approximate models\\nare those by Kuvayev and Sutton (1996), Sutton, Szepesvari, Geramifard, and\\nBowling (2008), Nouri and Littman (2009), and Hester and Stone (2012).\\nThe need to be selective in model construction to avoid slowing planning is well\\nknown in artiﬁcial intelligence. Some of the classic work is by Minton (1990) and\\nTambe, Newell, and Rosenbloom (1990). Hauskrecht, Meuleau, Kaelbling, Dean,\\nand Boutilier (1998) showed this e↵ect in MDPs with deterministic options.\\nSchmidhuber (1991a, b) proposed how something like curiosity would result if\\nreward signals were a function of how quickly an agent’s environment model\\nis improving. The empowerment function proposed by Klyubin, Polani, and\\nNehaniv (2005) is an information-theoretic measure of an agent’s ability to control\\nits environment that can function as an intrinsic reward signal. Baldassarre and\\nMirolli (2013) is a collection of contributions by researchers studying intrinsic\\nreward and motivation from both biological and computational perspectives,\\nincluding a perspective on “intrinsically-motivated reinforcement learning,” to\\nuse the term introduced by Singh, Barto, and Chentenez (2004). See also Oudeyer\\nand Kaplan (2007), Oudeyer, Kaplan, and Hafner (2007), and Barto (2013).'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 502, 'page_label': '503'}, page_content='References\\nAbbeel, P., Ng, A. Y. (2004). Apprenticeship learning via inverse reinforcement learning. In\\nProceedings of the 21st International Conference on Machine Learning. ACM, New York.\\nAbramson, B. (1990). Expected-outcome: A general model of static evaluation.IEEE Transac-\\ntions on Pattern Analysis and Machine Intelligence, 12(2):182–193.\\nAdams, C. D. (1982). Variations in the sensitivity of instrumental responding to reinforcer\\ndevaluation. The Quarterly Journal of Experimental Psychology, 34(2):77–98.\\nAdams, C. D., Dickinson, A. (1981). Instrumental responding following reinforcer devaluation.\\nThe Quarterly Journal of Experimental Psychology, 33(2):109–121.\\nAdams, R. A., Huys, Q. J. M., Roiser, J. P. (2015). Computational Psychiatry: towards a\\nmathematically informed understanding of mental illness.Journal of Neurology, Neurosurgery\\n&P s y c h i a t r y. doi:10.1136/jnnp-2015-310737\\nAgrawal, R. (1995). Sample mean based index policies withO(logn) regret for the multi-armed\\nbandit problem. Advances in Applied Probability, 27(4):1054–1078.\\nAgre, P. E. (1988). The Dynamic Structure of Everyday Life. PhD thesis, Massachusetts\\nInstitute of Technology, Cambridge MA. AI-TR 1085, MIT Artiﬁcial Intelligence Laboratory.\\nAgre, P. E., Chapman, D. (1990). What are plans for?Robotics and Autonomous Systems,\\n6(1-2):17–34.\\nAizerman, M. A., Braverman, E.´I., Rozonoer, L. I. (1964). Probability problem of pattern\\nrecognition learning and potential functions method.Avtomat. i Telemekh, 25(9):1307–1323.\\nAlbus, J. S. (1971). A theory of cerebellar function.Mathematical Biosciences, 10(1-2):25–61.\\nAlbus, J. S. (1981).Brain, Behavior, and Robotics. Byte Books, Peterborough, NH.\\nAleksandrov, V. M., Sysoev, V. I., Shemeneva, V. V. (1968). Stochastic optimization of systems.\\nIzv. Akad. Nauk SSSR, Tekh. Kibernetika:14–19.\\nAmari, S. I. (1998). Natural gradient works e\\x00ciently in learning. Neural Computation,\\n10(2):251–276.\\nAn, P. C. E. (1991).An Improved Multi-dimensional CMAC Neural network: Receptive Field\\nFunction and Placement. PhD thesis, University of New Hampshire, Durham.\\nAn, P. C. E., Miller, W. T., Parks, P. C. (1991). Design improvements in associative memories for\\ncerebellar model articulation controllers (CMAC).Artiﬁcial Neural Networks,p p .1 2 0 7 – 1 2 1 0 ,\\nElsevier North-Holland. http://www.incompleteideas.net/papers/AnMillerParks1991.pdf\\nAnderson, C. W. (1986).Learning and Problem Solving with Multilayer Connectionist Systems.\\nPhD thesis, University of Massachusetts, Amherst.\\nAnderson, C. W. (1987). Strategy learning with multilayer connectionist representations. In\\nProceedings of the 4th International Workshop on Machine Learning, pp. 103–114. Morgan\\nKaufmann.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 503, 'page_label': '504'}, page_content='482 References\\nAnderson, C. W. (1989). Learning to control an inverted pendulum using neural networks.IEEE\\nControl Systems Magazine, 9(3):31–37.\\nAnderson, J. A., Silverstein, J. W., Ritz, S. A., Jones, R. S. (1977). Distinctive features,\\ncategorical perception, and probability learning: Some applications of a neural model.\\nPsychological Review, 84(5):413–451.\\nAndreae, J. H. (1963). STELLA, A scheme for a learning machine. InProceedings of the 2nd\\nIFAC Congress, Basle, pp. 497–502. Butterworths, London.\\nAndreae, J. H. (1969). Learning machines—a uniﬁed view. In A. R. Meetham and R. A. Hudson\\n(Eds.), Encyclopedia of Information, Linguistics, and Control, pp. 261–270. Pergamon,\\nOxford.\\nAndreae, J. H. (1977).Thinking with the Teachable Machine. Academic Press, London.\\nAndreae, J. H. (2017a). A model of how the brain learns: A short introduction to multiple\\ncontext associative learning (MCAL) and the PP system. Unpublished report.\\nAndreae, J. H. (2017b). Working memory for the associative learning of language. Unpublished\\nreport.\\nAndreae, J. H., Cashin, P. M. (1969). A learning machine with monologue.International\\nJournal of Man–Machine Studies, 1(1):1–20.\\nArthur, W. B. (1991). Designing economic agents that act like human agents: A behavioral\\napproach to bounded rationality.The American Economic Review, 81(2):353–359.\\nAsadi, K., Allen, C., Roderick, M., Mohamed, A. R., Konidaris, G., Littman, M. (2017). Mean\\nactor critic. ArXiv:1709.00503.\\nAtkeson, C. G. (1992). Memory-based approaches to approximating continuous functions. In\\nSante Fe Institute Studies in the Sciences of Complexity, Proceedings Vol. 12, pp. 521–521.\\nAddison-Wesley.\\nAtkeson, C. G., Moore, A. W., Schaal, S. (1997). Locally weighted learning.Artiﬁcial Intelligence\\nReview, 11:11–73.\\nAuer, P., Cesa-Bianchi, N., Fischer, P. (2002). Finite-time analysis of the multiarmed bandit\\nproblem. Machine learning, 47(2-3):235–256.\\nBacon, P. L., Harb, J., Precup, D. (2017). The option-critic architecture. InProceedings of the\\nAssociation for the Advancement of Artiﬁcial Intelligence,p p .1 7 2 6 – 1 7 3 4 .\\nBaird, L. C. (1995). Residual algorithms: Reinforcement learning with function approximation.\\nIn Proceedings of the 12th International Conference on Machine Learning, pp. 30–37. Morgan\\nKaufmann.\\nBaird, L. C. (1999).Reinforcement Learning through Gradient Descent. PhD thesis, Carnegie\\nMellon University, Pittsburgh PA.\\nBaird, L. C., Klopf, A. H. (1993). Reinforcement learning with high-dimensional, continuous\\nactions. Wright Laboratory, Wright-Patterson Air Force Base, Tech. Rep. WL-TR-93-1147.\\nBaird, L., Moore, A. W. (1999). Gradient descent for general reinforcement learning. InAdvances\\nin Neural Information Processing Systems 11, pp. 968–974. MIT Press, Cambridge MA.\\nBaldassarre, G., Mirolli, M. (Eds.) (2013).Intrinsically Motivated Learning in Natural and\\nArtiﬁcial Systems. Springer-Verlag, Berlin Heidelberg.\\nBalke, A., Pearl, J. (1994). Counterfactual probabilities: Computational methods, bounds\\nand applications. InProceedings of the Tenth International Conference on Uncertainty in\\nArtiﬁcial Intelligence, pp. 46–54. Morgan Kaufmann.\\nBaras, D., Meir, R. (2007). Reinforcement learning, spike-time-dependent plasticity, and the\\nBCM rule. Neural Computation, 19(8):2245–2279.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 504, 'page_label': '505'}, page_content='References 483\\nBarnard, E. (1993). Temporal-di↵erence methods and Markov models.IEEE Transactions on\\nSystems, Man, and Cybernetics, 23(2):357–365.\\nBarreto, A. S., Precup, D., Pineau, J. (2011). Reinforcement learning using kernel-based\\nstochastic factorization. InAdvances in Neural Information Processing Systems 24,p p .7 2 0 –\\n728. Curran Associates, Inc.\\nBartlett, P. L., Baxter, J. (1999). Hebbian synaptic modiﬁcations in spiking neurons that\\nlearn. Technical report, Research School of Information Sciences and Engineering, Australian\\nNational University.\\nBartlett, P. L., Baxter, J. (2000). A biologically plausible and locally optimal learning algorithm\\nfor spiking neurons. Rapport technique, Australian National University.\\nBarto, A. G. (1985). Learning by statistical cooperation of self-interested neuron-like computing\\nelements. Human Neurobiology, 4(4):229–256.\\nBarto, A. G. (1986). Game-theoretic cooperativity in networks of self-interested units. In\\nJ. S. Denker (Ed.),Neural Networks for Computing, pp. 41–46. American Institute of Physics,\\nNew York.\\nBarto, A. G. (1989). From chemotaxis to cooperativity: Abstract exercises in neuronal learning\\nstrategies. In R. Durbin, R. Maill and G. Mitchison (Eds.),The Computing Neuron,p p .7 3 – 9 8 .\\nAddison-Wesley, Reading, MA.\\nBarto, A. G. (1990). Connectionist learning for control: An overview. In T. Miller, R. S. Sutton,\\nand P. J. Werbos (Eds.),Neural Networks for Control, pp. 5–58. MIT Press, Cambridge,\\nMA.\\nBarto, A. G. (1991). Some learning tasks from a control perspective. In L. Nadel and D. L. Stein\\n(Eds.), 1990 Lectures in Complex Systems,p p .1 9 5 – 2 2 3 .A d d i s o n - W e s l e y ,R e d w o o dC i t y ,C A .\\nBarto, A. G. (1992). Reinforcement learning and adaptive critic methods. In D. A. White and\\nD. A. Sofge (Eds.),Handbook of Intelligent Control: Neural, Fuzzy, and Adaptive Approaches,\\npp. 469–491. Van Nostrand Reinhold, New York.\\nBarto, A. G. (1995a). Adaptive critics and the basal ganglia. In J. C. Houk, J. L. Davis, and\\nD. G. Beiser (Eds.),Models of Information Processing in the Basal Ganglia,p p .2 1 5 – 2 3 2 .\\nMIT Press, Cambridge, MA.\\nBarto, A. G. (1995b). Reinforcement learning. In M. A. Arbib (Ed.),Handbook of Brain Theory\\nand Neural Networks, pp. 804–809. MIT Press, Cambridge, MA.\\nBarto, A. G. (2011). Adaptive real-time dynamic programming. In C. Sammut and G. I Webb\\n(Eds.), Encyclopedia of Machine Learning, pp. 19–22. Springer Science and Business Media.\\nBarto, A. G. (2013). Intrinsic motivation and reinforcement learning. In G. Baldassarre and M.\\nMirolli (Eds.), Intrinsically Motivated Learning in Natural and Artiﬁcial Systems, pp. 17–47.\\nSpringer-Verlag, Berlin Heidelberg.\\nBarto, A. G., Anandan, P. (1985). Pattern recognizing stochastic learning automata.IEEE\\nTransactions on Systems, Man, and Cybernetics, 15(3):360–375.\\nBarto, A. G., Anderson, C. W. (1985). Structural learning in connectionist systems. In\\nProceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 43–54.\\nBarto, A. G., Anderson, C. W., Sutton, R. S. (1982). Synthesis of nonlinear control surfaces by\\na layered associative search network.Biological Cybernetics, 43(3):175–185.\\nBarto, A. G., Bradtke, S. J., Singh, S. P. (1991). Real-time learning and control using\\nasynchronous dynamic programming. Technical Report 91-57. Department of Computer\\nand Information Science, University of Massachusetts, Amherst.\\nBarto, A. G., Bradtke, S. J., Singh, S. P. (1995). Learning to act using real-time dynamic\\nprogramming. Artiﬁcial Intelligence, 72(1-2):81–138.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 505, 'page_label': '506'}, page_content='484 References\\nBarto, A. G., Du↵, M. (1994). Monte Carlo matrix inversion and reinforcement learning. In\\nAdvances in Neural Information Processing Systems 6, pp. 687–694. Morgan Kaufmann, San\\nFrancisco.\\nBarto, A. G., Jordan, M. I. (1987). Gradient following without back-propagation in layered\\nnetworks. In M. Caudill and C. Butler (Eds.), Proceedings of the IEEE First Annual\\nConference on Neural Networks, pp. II629–II636. SOS Printing, San Diego.\\nBarto, A. G., Mahadevan, S. (2003). Recent advances in hierarchical reinforcement learning.\\nDiscrete Event Dynamic Systems, 13(4):341–379.\\nBarto, A. G., Singh, S. P. (1990). On the computational economics of reinforcement learning. In\\nConnectionist Models: Proceedings of the 1990 Summer School. Morgan Kaufmann.\\nBarto, A. G., Sutton, R. S. (1981a). Goal seeking components for adaptive intelligence: An\\ninitial assessment. Technical Report AFWAL-TR-81-1070. Air Force Wright Aeronautical\\nLaboratories/Avionics Laboratory, Wright-Patterson AFB, OH.\\nBarto, A. G., Sutton, R. S. (1981b). Landmark learning: An illustration of associative search.\\nBiological Cybernetics, 42(1):1–8.\\nBarto, A. G., Sutton, R. S. (1982). Simulation of anticipatory responses in classical conditioning\\nby a neuron-like adaptive element.Behavioural Brain Research, 4(3):221–235.\\nBarto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve\\ndi\\x00cult learning control problems.IEEE Transactions on Systems, Man, and Cybernetics,\\n13(5):835–846. Reprinted in J. A. Anderson and E. Rosenfeld (Eds.),Neurocomputing:\\nFoundations of Research, pp. 535–549. MIT Press, Cambridge, MA, 1988.\\nBarto, A. G., Sutton, R. S., Brouwer, P. S. (1981). Associative search network: A reinforcement\\nlearning associative memory.Biological Cybernetics, 40(3):201–211.\\nBarto, A. G., Sutton, R. S., Watkins, C. J. C. H. (1990). Learning and sequential decision\\nmaking. In M. Gabriel and J. Moore (Eds.),Learning and Computational Neuroscience:\\nFoundations of Adaptive Networks, pp. 539–602. MIT Press, Cambridge, MA.\\nBaxter, J., Bartlett, P. L. (2001). Inﬁnite-horizon policy-gradient estimation.Journal of Artiﬁcial\\nIntelligence Research, 15:319–350.\\nBaxter, J., Bartlett, P. L., Weaver, L. (2001). Experiments with inﬁnite-horizon, policy-gradient\\nestimation. Journal of Artiﬁcial Intelligence Research, 15:351–381.\\nBellemare, M. G., Dabney, W., Munos, R. (2017). A distributional perspective on reinforcement\\nlearning. ArXiv:1707.06887.\\nBellemare, M. G., Naddaf, Y., Veness, J., Bowling, M. (2013). The arcade learning environment:\\nAn evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,\\n47:253–279.\\nBellemare, M. G., Veness, J., Bowling, M. (2012). Investigating contingency awareness using\\nAtari 2600 games. In Proceedings of the Twenty-Sixth AAAI Conference on Artiﬁcial\\nIntelligence, pp. 864–871. AAAI Press, Menlo Park, CA.\\nBellman, R. E. (1956). A problem in the sequential design of experiments.Sankhya, 16:221–229.\\nBellman, R. E. (1957a).Dynamic Programming. Princeton University Press, Princeton.\\nBellman, R. E. (1957b). A Markov decision process.Journal of Mathematics and Mechanics,\\n6(5):679–684.\\nBellman, R. E., Dreyfus, S. E. (1959). Functional approximations and dynamic programming.\\nMathematical Tables and Other Aids to Computation, 13:247–251.\\nBellman, R. E., Kalaba, R., Kotkin, B. (1963). Polynomial approximation—A new computational\\ntechnique in dynamic programming: Allocation processes. Mathematical Computation,\\n17:155–161.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 506, 'page_label': '507'}, page_content='References 485\\nBengio, Y. (2009). Learning deep architectures for AI.Foundations and Trends in Machine\\nLearning, 2(1):1–27.\\nBengio, Y., Courville, A. C., Vincent, P. (2012). Unsupervised feature learning and deep learning:\\nA review and new perspectives.CoRR 1, ArXiv:1206.5538.\\nBentley, J. L. (1975). Multidimensional binary search trees used for associative searching.\\nCommunications of the ACM, 18(9):509–517.\\nBerg, H. C. (1975). Chemotaxis in bacteria.Annual review of biophysics and bioengineering,\\n4(1):119–136.\\nBerns, G. S., McClure, S. M., Pagnoni, G., Montague, P. R. (2001). Predictability modulates\\nhuman brain response to reward.The journal of neuroscience, 21(8):2793–2798.\\nBerridge, K. C., Kringelbach, M. L. (2008). A↵ective neuroscience of pleasure: reward in humans\\nand animals. Psychopharmacology, 199(3):457–480.\\nBerridge, K. C., Robinson, T. E. (1998). What is the role of dopamine in reward: hedonic\\nimpact, reward learning, or incentive salience?Brain Research Reviews, 28(3):309–369.\\nBerry, D. A., Fristedt, B. (1985).Bandit Problems.C h a p m a n a n d H a l l , L o n d o n .\\nBertsekas, D. P. (1982). Distributed dynamic programming.IEEE Transactions on Automatic\\nControl, 27(3):610–616.\\nBertsekas, D. P. (1983). Distributed asynchronous computation of ﬁxed points.Mathematical\\nProgramming, 27(1):107–120.\\nBertsekas, D. P. (1987).Dynamic Programming: Deterministic and Stochastic Models. Prentice-\\nHall, Englewood Cli↵s, NJ.\\nBertsekas, D. P. (2005).Dynamic Programming and Optimal Control, Volume 1,third edition.\\nAthena Scientiﬁc, Belmont, MA.\\nBertsekas, D. P. (2012).Dynamic Programming and Optimal Control, Volume 2: Approximate\\nDynamic Programming, fourth edition. Athena Scientiﬁc, Belmont, MA.\\nBertsekas, D. P. (2013). Rollout algorithms for discrete optimization: A survey. InHandbook of\\nCombinatorial Optimization, pp. 2989–3013. Springer, New York.\\nBertsekas, D. P., Tsitsiklis, J. N. (1989).Parallel and Distributed Computation: Numerical\\nMethods. Prentice-Hall, Englewood Cli↵s, NJ.\\nBertsekas, D. P., Tsitsiklis, J. N. (1996).Neuro-Dynamic Programming. Athena Scientiﬁc,\\nBelmont, MA.\\nBertsekas, D. P., Tsitsiklis, J. N., Wu, C. (1997). Rollout algorithms for combinatorial optimiza-\\ntion. Journal of Heuristics, 3(3):245–262.\\nBertsekas, D. P., Yu, H. (2009). Projected equation methods for approximate solution of large\\nlinear systems. Journal of Computational and Applied Mathematics, 227(1):27–50.\\nBhat, N., Farias, V., Moallemi, C. C. (2012). Non-parametric approximate dynamic programming\\nvia the kernel method. InAdvances in Neural Information Processing Systems 25,p p .3 8 6 – 3 9 4 .\\nCurran Associates, Inc.\\nBhatnagar, S., Sutton, R., Ghavamzadeh, M., Lee, M. (2009). Natural actor–critic algorithms.\\nAutomatica, 45(11).\\nBiermann, A. W., Fairﬁeld, J. R. C., Beres, T. R. (1982). Signature table systems and learning.\\nIEEE Transactions on Systems, Man, and Cybernetics, 12(5):635–648.\\nBishop, C. M. (1995).Neural Networks for Pattern Recognition. Clarendon, Oxford.\\nBishop, C. M. (2006).Pattern Recognition and Machine Learning. Springer Science + Business\\nMedia New York LLC.\\nBlodgett, H. C. (1929). The e↵ect of the introduction of reward upon the maze performance of\\nrats. University of California Publications in Psychology, 4:113–134.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 507, 'page_label': '508'}, page_content='486 References\\nBoakes, R. A., Costa, D. S. J. (2014). Temporal contiguity in associative learning: Iinterference\\nand decay from an historical perspective. Journal of Experimental Psychology: Animal\\nLearning and Cognition, 40(4):381–400.\\nBooker, L. B. (1982).Intelligent Behavior as an Adaptation to the Task Environment.P h D t h e s i s ,\\nUniversity of Michigan, Ann Arbor.\\nBostrom, N. (2014).Superintelligence: Paths, Dangers, Strategies. Oxford University Press.\\nBottou, L., Vapnik, V. (1992). Local learning algorithms.Neural Computation, 4(6):888–900.\\nBoyan, J. A. (1999). Least-squares temporal di↵erence learning. InProceedings of the 16th\\nInternational Conference on Machine Learning,p p .4 9 – 5 6 .\\nBoyan, J. A. (2002). Technical update: Least-squares temporal di↵erence learning.Machine\\nLearning, 49(2):233–246.\\nBoyan, J. A., Moore, A. W. (1995). Generalization in reinforcement learning: Safely approximat-\\ning the value function. InAdvances in Neural Information Processing Systems 7,p p .3 6 9 – 3 7 6 .\\nMIT Press, Cambridge, MA.\\nBradtke, S. J. (1993). Reinforcement learning applied to linear quadratic regulation. InAdvances\\nin Neural Information Processing Systems 5, pp. 295–302. Morgan Kaufmann.\\nBradtke, S. J. (1994).Incremental Dynamic Programming for On-Line Adaptive Optimal Control.\\nPhD thesis, University of Massachusetts, Amherst. Appeared as CMPSCI Technical Report\\n94-62.\\nBradtke, S. J., Barto, A. G. (1996). Linear least–squares algorithms for temporal di↵erence\\nlearning. Machine Learning, 22:33–57.\\nBradtke, S. J., Ydstie, B. E., Barto, A. G. (1994). Adaptive linear quadratic control using policy\\niteration. In Proceedings of the American Control Conference, pp. 3475–3479. American\\nAutomatic Control Council, Evanston, IL.\\nBrafman, R. I., Tennenholtz, M. (2003). R-max – a general polynomial time algorithm for\\nnear-optimal reinforcement learning.Journal of Machine Learning Research, 3:213–231.\\nBreiman, L. (2001). Random forests.Machine Learning, 45(1):5–32.\\nBreiter, H. C., Aharon, I., Kahneman, D., Dale, A., Shizgal, P. (2001). Functional imaging\\nof neural responses to expectancy and experience of monetary gains and losses.Neuron,\\n30(2):619–639.\\nBreland, K., Breland, M. (1961). The misbehavior of organisms. American Psychologist,\\n16(11):681–684.\\nBridle, J. S. (1990). Training stochastic model recognition algorithms as networks can lead to\\nmaximum mutual information estimates of parameters. InAdvances in Neural Information\\nProcessing Systems 2, pp. 211–217. Morgan Kaufmann, San Mateo, CA.\\nBroomhead, D. S., Lowe, D. (1988). Multivariable functional interpolation and adaptive networks.\\nComplex Systems, 2:321–355.\\nBromberg-Martin, E. S., Matsumoto, M., Hong, S., Hikosaka, O. (2010). A pallidus-habenula-\\ndopamine pathway signals inferred stimulus values.Journal of Neurophysiology, 104(2):1068–\\n1076.\\nBrowne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener,\\nS., Perez, D., Samothrakis, S., Colton, S. (2012). A survey of monte carlo tree search methods.\\nIEEE Transactions on Computational Intelligence and AI in Games, 4(1):1–43.\\nBrown, J., Bullock, D., Grossberg, S. (1999). How the basal ganglia use parallel excitatory\\nand inhibitory learning pathways to selectively respond to unexpected rewarding cues.The\\nJournal of Neuroscience, 19(23):10502–10511.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 508, 'page_label': '509'}, page_content='References 487\\nBryson, A. E., Jr. (1996). Optimal control—1950 to 1985.IEEE Control Systems, 13(3):26–33.\\nBuchanan, B. G., Mitchell, T., Smith, R. G., Johnson, C. R., Jr. (1978). Models of learning\\nsystems. Encyclopedia of Computer Science and technology, 11.\\nBuhusi, C. V., Schmajuk, N. A. (1999). Timing in simple conditioning and occasion setting: A\\nneural network approach.Behavioural Processes, 45(1):33–57.\\nBu¸ soniu, L., Lazaric, A., Ghavamzadeh, M., Munos, R., Babu˘ ska, R., De Schutter, B. (2012).\\nLeast-squares methods for policy iteration. In M. Wiering and M. van Otterlo (Eds.),\\nReinforcement Learning: State-of-the-Art, pp. 75–109. Springer-Verlag Berlin Heidelberg.\\nBush, R. R., Mosteller, F. (1955).Stochastic Models for Learning. Wiley, New York.\\nByrne, J. H., Gingrich, K. J., Baxter, D. A. (1990). Computational capabilities of single\\nneurons: Relationship to simple forms of associative and nonassociative learning inaplysia.\\nIn R. D. Hawkins and G. H. Bower (Eds.),Computational Models of Learning, pp. 31–63.\\nAcademic Press, New York.\\nCalabresi, P., Picconi, B., Tozzi, A., Filippo, M. D. (2007). Dopamine-mediated regulation of\\ncorticostriatal synaptic plasticity.Trends in Neuroscience, 30(5):211–219.\\nCamerer, C. (2011).Behavioral Game Theory: Experiments in Strategic Interaction. Princeton\\nUniversity Press.\\nCampbell, D. T. (1960). Blind variation and selective survival as a general strategy in knowledge-\\nprocesses. In M. C. Yovits and S. Cameron (Eds.),Self-Organizing Systems,p p .2 0 5 – 2 3 1 .\\nPergamon, New York.\\nCao, X. R. (2009). Stochastic learning and optimization—A sensitivity-based approach.Annual\\nReviews in Control, 33(1):11–24.\\nCao, X. R., Chen, H. F. (1997). Perturbation realization, potentials, and sensitivity analysis of\\nMarkov processes. IEEE Transactions on Automatic Control, 42(10):1382–1393.\\nCarlstr¨ om, J., Nordstr¨ om, E. (1997). Control of self-similar ATM call tra\\x00c by reinforcement\\nlearning. In Proceedings of the International Workshop on Applications of Neural Networks\\nto Telecommunications 3, pp. 54–62. Erlbaum, Hillsdale, NJ.\\nChapman, D., Kaelbling, L. P. (1991). Input generalization in delayed reinforcement learning:\\nAn algorithm and performance comparisons. InProceedings of the Twelfth International\\nConference on Artiﬁcial Intelligence, pp. 726–731. Morgan Kaufmann, San Mateo, CA.\\nChaslot, G., Bakkes, S., Szita, I., Spronck, P. (2008). Monte-Carlo tree search: A new framework\\nfor game AI. InProceedings of the Fourth AAAI Conference on Artiﬁcial Intelligence and\\nInteractive Digital Entertainment (AIDE-08), pp. 216–217. AAAI Press, Menlo Park, CA.\\nChow, C.-S., Tsitsiklis, J. N. (1991). An optimal one-way multigrid algorithm for discrete-time\\nstochastic control. IEEE Transactions on Automatic Control, 36(8):898–914.\\nChrisman, L. (1992). Reinforcement learning with perceptual aliasing: The perceptual distinc-\\ntions approach. InProceedings of the Tenth National Conference on Artiﬁcial Intelligence,\\npp. 183–188. AAAI/MIT Press, Menlo Park, CA.\\nChristensen, J., Korf, R. E. (1986). A uniﬁed theory of heuristic evaluation functions and\\nits application to learning. InProceedings of the Fifth National Conference on Artiﬁcial\\nIntelligence, pp. 148–152. Morgan Kaufmann.\\nCichosz, P. (1995). Truncating temporal di↵erences: On the e\\x00cient implementation of TD(\\x00)\\nfor reinforcement learning.Journal of Artiﬁcial Intelligence Research, 2:287–318.\\nCiosek, K., Whiteson, S. (2017). Expected policy gradients. ArXiv:1706.05374v1. A revised ver-\\nsion appeared inProceedings of the Annual Conference of the Association for the Advancement\\nof Artiﬁcial Intelligence,p p .2 8 6 8 – 2 8 7 5 .\\nCiosek, K., Whiteson, S. (2018). Expected policy gradients for reinforcement learning. ArXiv:\\n1801.03326.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 509, 'page_label': '510'}, page_content='488 References\\nClaridge-Chang, A., Roorda, R. D., Vrontou, E., Sjulson, L., Li, H., Hirsh, J., Miesenb¨ ock, G.\\n(2009). Writing memories with light-addressable reinforcement circuitry.Cell, 139(2):405–\\n415.\\nClark, R. E., Squire, L. R. (1998). Classical conditioning and brain systems: the role of awareness.\\nScience, 280(5360):77–81.\\nClark, W. A., Farley, B. G. (1955). Generalization of pattern recognition in a self-organizing\\nsystem. In Proceedings of the 1955 Western Joint Computer Conference,p p .8 6 – 9 1 .\\nClouse, J. (1996). On Integrating Apprentice Learning and Reinforcement Learning TITLE2.\\nPhD thesis, University of Massachusetts, Amherst. Appeared as CMPSCI Technical Report\\n96-026.\\nClouse, J., Utgo↵, P. (1992). A teaching method for reinforcement learning systems. In\\nProceedings of the 9th International Workshop on Machine Learning, pp. 92–101. Morgan\\nKaufmann.\\nCobo, L. C., Zang, P., Isbell, C. L., Thomaz, A. L. (2011). Automatic state abstraction from\\ndemonstration. In Proceedings of the Twenty-Second International Joint Conference on\\nArtiﬁcial Intelligence, pp. 1243-1248. AAAI Press.\\nConnell, J. (1989). A colony architecture for an artiﬁcial creature. Technical Report AI-TR-1151.\\nMIT Artiﬁcial Intelligence Laboratory, Cambridge, MA.\\nConnell, M. E., Utgo↵, P. E. (1987). Learning to control a dynamic physical system.Computa-\\ntional intelligence, 3(1):330–337.\\nContreras-Vidal, J. L., Schultz, W. (1999). A predictive reinforcement model of dopamine neurons\\nfor learning approach behavior.Journal of Computational Neuroscience, 6(3):191–214.\\nCoulom, R. (2006). E\\x00cient selectivity and backup operators in Monte-Carlo tree search. In\\nProceedings of the 5th International Conference on Computers and Games (CG’06),p p .7 2 – 8 3 .\\nSpringer-Verlag Berlin, Heidelberg.\\nCourville, A. C., Daw, N. D., Touretzky, D. S. (2006). Bayesian theories of conditioning in a\\nchanging world. Trends in Cognitive Science, 10(7):294–300.\\nCraik, K. J. W. (1943).The Nature of Explanation. Cambridge University Press, Cambridge.\\nCross, J. G. (1973). A stochastic learning model of economic behavior.The Quarterly Journal\\nof Economics, 87(2):239–266.\\nCrow, T. J. (1968). Cortical synapses and reinforcement: a hypothesis.Nature, 219(5155):736–\\n737.\\nCurtiss, J. H. (1954). A theoretical comparison of the e\\x00ciencies of two classical methods and a\\nMonte Carlo method for computing one component of the solution of a set of linear algebraic\\nequations. In H. A. Meyer (Ed.),Symposium on Monte Carlo Methods,p p .1 9 1 – 2 3 3 .W i l e y ,\\nNew York.\\nCybenko, G. (1989). Approximation by superpositions of a sigmoidal function.Mathematics of\\ncontrol, signals and systems, 2(4):303–314.\\nCziko, G. (1995). Without Miracles: Universal Selection Theory and the Second Darvinian\\nRevolution. MIT Press, Cambridge, MA.\\nDabney, W. (2014).Adaptive step-sizes for reinforcement learning. PhD thesis, University of\\nMassachusetts, Amherst.\\nDabney, W., Barto, A. G. (2012). Adaptive step-size for online temporal di↵erence learning. In\\nProceedings of the Annual Conference of the Association for the Advancement of Artiﬁcial\\nIntelligence.\\nDaniel, J. W. (1976). Splines and e\\x00ciency in dynamic programming.Journal of Mathematical\\nAnalysis and Applications, 54:402–407.\\nDann, C., Neumann, G., Peters, J. (2014). Policy evaluation with temporal di↵erences: A survey\\nand comparison. Journal of Machine Learning Research, 15:809–883.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 510, 'page_label': '511'}, page_content='References 489\\nDaw, N. D., Courville, A. C., Touretzky, D. S. (2003). Timing and partial observability in the\\ndopamine system. InAdvances in Neural Information Processing Systems 15,p p .9 9 – 1 0 6 .\\nMIT Press, Cambridge, MA.\\nDaw, N. D., Courville, A. C., Touretzky, D. S. (2006). Representation and timing in theories of\\nthe dopamine system. Neural Computation, 18(7):1637–1677.\\nDaw, N. D., Niv, Y., Dayan, P. (2005). Uncertainty based competition between prefrontal and\\ndorsolateral striatal systems for behavioral control.Nature Neuroscience, 8(12):1704–1711.\\nDaw, N. D., Shohamy, D. (2008). The cognitive neuroscience of motivation and learning.Social\\nCognition, 26(5):593–620.\\nDayan, P. (1991). Reinforcement comparison. In D. S. Touretzky, J. L. Elman, T. J. Sejnowski,\\nand G. E. Hinton (Eds.),Connectionist Models: Proceedings of the 1990 Summer School,\\npp. 45–51. Morgan Kaufmann.\\nDayan, P. (1992). The convergence of TD(\\x00) for general\\x00. Machine Learning, 8(3):341–362.\\nDayan, P. (2002). Matters temporal.Trends in Cognitive Sciences, 6(3):105–106.\\nDayan, P., Abbott, L. F. (2001).Theoretical Neuroscience: Computational and Mathematical\\nModeling of Neural Systems. MIT Press, Cambridge, MA.\\nDayan, P., Berridge, K. C. (2014). Model-based and model-free Pavlovian reward learning:\\nRevaluation, revision, and revaluation. Cognitive, A↵ective, & Behavioral Neuroscience,\\n14(2):473–492.\\nDayan, P., Niv, Y. (2008). Reinforcement learning: the good, the bad and the ugly.Current\\nOpinion in Neurobiology, 18(2):185–196.\\nDayan, P., Niv, Y., Seymour, B., Daw, N. D. (2006). The misbehavior of value and the discipline\\nof the will.Neural Networks, 19(8):1153–1160.\\nDayan, P., Sejnowski, T. (1994). TD(\\x00) converges with probability 1. Machine Learning,\\n14(3):295–301.\\nDe Asis, K., Hernandez-Garcia, J. F., Holland, G. Z., Sutton, R. S. (2017). Multi-step Rein-\\nforcement Learning: A Unifying Algorithm. ArXiv:1703.01327.\\nde Farias, D. P. (2002). The Linear Programming Approach to Approximate Dynamic Program-\\nming: Theory and Application. Stanford University PhD thesis.\\nde Farias, D. P., Van Roy, B. (2003). The linear programming approach to approximate dynamic\\nprogramming. Operations Research 51(6):850–865.\\nDean, T., Lin, S.-H. (1995). Decomposition techniques for planning in stochastic domains.\\nIn Proceedings of the Fourteenth International Joint Conference on Artiﬁcial Intelligence,\\npp. 1121–1127. Morgan Kaufmann. See also Technical Report CS-95-10, Brown University,\\nDepartment of Computer Science, 1995.\\nDegris, T., Pilarski, P. M., Sutton, R. S. (2012). Model-free reinforcement learning with\\ncontinuous action in practice. In2012 American Control Conference,p p .2 1 7 7 – 2 1 8 2 .I E E E .\\nDegris, T., White, M., Sutton, R. S. (2012). O↵-policy actor–critic. InProceedings of the 29th\\nInternational Conference on Machine Learning. ArXiv:1205.4839, 2012.\\nDenardo, E. V. (1967). Contraction mappings in the theory underlying dynamic programming.\\nSIAM Review, 9(2):165–177.\\nDennett, D. C. (1978). Why the Law of E↵ect Will Not Go Away.Brainstorms,p p .7 1 – 8 9 .\\nBradford/MIT Press, Cambridge, MA.\\nDerthick, M. (1984). Variations on the Boltzmann machine learning algorithm. Carnegie-Mellon\\nUniversity Department of Computer Science Technical Report No. CMU-CS-84-120.\\nDeutsch, J. A. (1953). A new type of behaviour theory.British Journal of Psychology. General\\nSection, 44(4):304–317.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 511, 'page_label': '512'}, page_content='490 References\\nDeutsch, J. A. (1954). A machine with insight.Quarterly Journal of Experimental Psychology,\\n6(1):6–11.\\nDick, T. (2015). Policy Gradient Reinforcement Learning Without Regret.M . S c .t h e s i s ,\\nUniversity of Alberta.\\nDickinson, A. (1980).Contemporary Animal Learning Theory. Cambridge University Press.\\nDickinson, A. (1985). Actions and habits: the development of behavioral autonomy.Phil. Trans.\\nR. Soc. Lond. B, 308(1135):67–78.\\nDickinson, A., Balleine, B. W. (2002). The role of learning in motivation. In C. R. Gallistel\\n(Ed.), Stevens’ Handbook of Experimental Psychology,v o l u m e3 ,p p .4 9 7 – 5 3 3 .W i l e y ,N Y .\\nDietterich, T. G., Buchanan, B. G. (1984). The role of the critic in learning systems. In O. G.\\nSelfridge, E. L. Rissland, and M. A. Arbib (Eds.),Adaptive Control of Ill-Deﬁned Systems,\\npp. 127–147. Plenum Press, NY.\\nDietterich, T. G., Flann, N. S. (1995). Explanation-based learning and reinforcement learning:\\nA uniﬁed view. In A. Prieditis and S. Russell (Eds.),Proceedings of the 12th International\\nConference on Machine Learning, pp. 176–184. Morgan Kaufmann.\\nDietterich, T. G., Wang, X. (2002). Batch value function approximation via support vectors.\\nIn Advances in Neural Information Processing Systems 14, pp. 1491–1498. MIT Press,\\nCambridge, MA.\\nDiuk, C., Cohen, A., Littman, M. L. (2008). An object-oriented representation for e\\x00cient\\nreinforcement learning. In Proceedings of the 25th International Conference on Machine\\nLearning, pp. 240–247. ACM, New York.\\nDolan, R. J., Dayan, P. (2013). Goals and habits in the brain.Neuron, 80(2):312–325.\\nDoll, B. B., Simon, D. A., Daw, N. D. (2012). The ubiquity of model-based reinforcement\\nlearning. Current Opinion in Neurobiology, 22(6):1–7.\\nDonahoe, J. W., Burgos, J. E. (2000). Behavior analysis and revaluation. Journal of the\\nExperimental Analysis of Behavior, 74(3):331–346.\\nDorigo, M., Colombetti, M. (1994). Robot shaping: Developing autonomous agents through\\nlearning. Artiﬁcial Intelligence, 71(2):321–370.\\nDoya, K. (1996). Temporal di↵erence learning in continuous time and space. InAdvances in\\nNeural Information Processing Systems 8, pp. 1073–1079. MIT Press, Cambridge, MA.\\nDoya, K., Sejnowski, T. J. (1995). A novel reinforcement model of birdsong vocalization\\nlearning. In Advances in Neural Information Processing Systems 7, pp. 101–108. MIT Press,\\nCambridge, MA.\\nDoya, K., Sejnowski, T. J. (1998). A computational model of birdsong learning by auditory\\nexperience and auditory feedback. In P. W. F. Poon and J. F. Brugge (Eds.),Central\\nAuditory Processing and Neural Modeling, pp. 77–88. Springer, Boston, MA.\\nDoyle, P. G., Snell, J. L. (1984).Random Walks and Electric Networks.T h e M a t h e m a t i c a l\\nAssociation of America. Carus Mathematical Monograph 22.\\nDreyfus, S. E., Law, A. M. (1977).The Art and Theory of Dynamic Programming.A c a d e m i c\\nPress, New York.\\nDu, S. S., Chen, J., Li, L., Xiao, L., Zhou, D. (2017). Stochastic variance reduction methods for\\npolicy evaluation. Proceedings of the 34th International Conference on Machine Learning,\\npp. 1049–1058. ArXiv:1702.07944.\\nDuda, R. O., Hart, P. E. (1973).Pattern Classiﬁcation and Scene Analysis. Wiley, New York.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 512, 'page_label': '513'}, page_content='References 491\\nDu↵, M. O. (1995). Q-learning for bandit problems. InProceedings of the 12th International\\nConference on Machine Learning, pp. 209–217. Morgan Kaufmann.\\nEgger, D. M., Miller, N. E. (1962). Secondary reinforcement in rats as a function of information\\nvalue and reliability of the stimulus.Journal of Experimental Psychology, 64:97–104.\\nEshel, N., Tian, J., Bukwich, M., Uchida, N. (2016). Dopamine neurons share common response\\nfunction for reward prediction error.Nature Neuroscience, 19(3):479–486.\\nEstes, W. K. (1943). Discriminative conditioning. I. A discriminative property of conditioned\\nanticipation. Journal of Experimental Psychology, 32(2):150–155.\\nEstes, W. K. (1948). Discriminative conditioning. II. E↵ects of a Pavlovian conditioned stimulus\\nupon a subsequently established operant response.Journal of Experimental Psychology,\\n38(2):173–177.\\nEstes, W. K. (1950). Toward a statistical theory of learning.Psychololgical Review, 57(2):94–107.\\nFarley, B. G., Clark, W. A. (1954). Simulation of self-organizing systems by digital computer.\\nIRE Transactions on Information Theory, 4(4):76–84.\\nFarries, M. A., Fairhall, A. L. (2007). Reinforcement learning with modulated spike timing-\\ndependent synaptic plasticity.Journal of Neurophysiology, 98(6):3648–3665.\\nFeldbaum, A. A. (1965).Optimal Control Systems. Academic Press, New York.\\nFinch, G., Culler, E. (1934). Higher order conditioning with constant motivation.The American\\nJournal of Psychology:596–602.\\nFinnsson, H., Bj¨ ornsson, Y. (2008). Simulation-based approach to general game playing. In\\nProceedings of the Association for the Advancement of Artiﬁcial Intelligence,p p .2 5 9 – 2 6 4 .\\nFiorillo, C. D., Yun, S. R., Song, M. R. (2013). Diversity and homogeneity in responses of\\nmidbrain dopamine neurons. The Journal of Neuroscience, 33(11):4693–4709.\\nFlorian, R. V. (2007). Reinforcement learning through modulation of spike-timing-dependent\\nsynaptic plasticity. Neural Computation, 19(6):1468–1502.\\nFogel, L. J., Owens, A. J., Walsh, M. J. (1966).Artiﬁcial Intelligence through Simulated Evolution.\\nJohn Wiley and Sons.\\nFrench, R. M. (1999). Catastrophic forgetting in connectionist networks.Trends in cognitive\\nsciences, 3(4):128–135.\\nFrey, U., Morris, R. G. M. (1997). Synaptic tagging and long-term potentiation.Nature,\\n385(6616):533–536.\\nFr´ emaux, N., Sprekeler, H., Gerstner, W. (2010). Functional requirements for reward-modulated\\nspike-timing-dependent plasticity.The Journal of Neuroscience, 30(40): 13326–13337\\nFriedman, J. H., Bentley, J. L., Finkel, R. A. (1977). An algorithm for ﬁnding best matches in\\nlogarithmic expected time.ACM Transactions on Mathematical Software, 3(3):209–226.\\nFriston, K. J., Tononi, G., Reeke, G. N., Sporns, O., Edelman, G. M. (1994). Value-dependent\\nselection in the brain: Simulation in a synthetic neural model.Neuroscience, 59(2):229–243.\\nFu, K. S. (1970). Learning control systems—Review and outlook. IEEE Transactions on\\nAutomatic Control, 15(2):210–221.\\nGalanter, E., Gerstenhaber, M. (1956). On thought: The extrinsic theory.Psychological Review,\\n63(4):218–227.\\nGallistel, C. R. (2005). Deconstructing the law of e↵ect. Games and Economic Behavior,\\n52(2):410–423.\\nGardner, M. (1973). Mathematical games.Scientiﬁc American, 228(1):108–115.\\nGeist, M., Scherrer, B. (2014). O↵-policy learning with eligibility traces: A survey.Journal of\\nMachine Learning Research, 15(1):289–333.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 513, 'page_label': '514'}, page_content='492 References\\nGelly, S., Silver, D. (2007). Combining online and o✏ine knowledge in UCT.Proceedings of the\\n24th International Conference on Machine Learning,p p . 2 7 3 – 2 8 0 .\\nGelperin, A., Hopﬁeld, J. J., Tank, D. W. (1985). The logic oflimax learning. In A. Selverston\\n(Ed.), Model Neural Networks and Behavior, pp. 247–261. Plenum Press, New York.\\nGenesereth, M., Thielscher, M. (2014). General game playing.Synthesis Lectures on Artiﬁcial\\nIntelligence and Machine Learning, 8(2):1–229.\\nGershman, S. J., Moustafa, A. A., Ludvig, E. A. (2014). Time representation in reinforcement\\nlearning models of the basal ganglia.Frontiers in Computational Neuroscience, 7:194.\\nGershman, S. J., Pesaran, B., Daw, N. D. (2009). Human reinforcement learning subdivides\\nstructured action spaces by learning e↵ector-speciﬁc values.The Journal of Neuroscience,\\n29(43):13524–13531.\\nGhiassian, S., Raﬁee, B., Sutton, R. S. (2016). A ﬁrst empirical study of emphatic temporal\\ndi↵erence learning. Workshop on Continual Learning and Deep Learning at the Conference\\non Neural Information Processing Systems. ArXiv:1705.04185.\\nGhiassian, S., Patterson, A., White, M., Sutton, R. S., White, A. (2018). Online o↵-policy\\nprediction. ArXiv:1811.02597.\\nGibbs, C. M., Cool, V., Land, T., Kehoe, E. J., Gormezano, I. (1991). Second-order conditioning\\nof the rabbit’s nictitating membrane response.Integrative Physiological and Behavioral\\nScience, 26(4):282–295.\\nGittins, J. C., Jones, D. M. (1974). A dynamic allocation index for the sequential design of\\nexperiments. In J. Gani, K. Sarkadi, and I. Vincze (Eds.),Progress in Statistics,p p .2 4 1 – 2 6 6 .\\nNorth-Holland, Amsterdam–London.\\nGlimcher, P. W. (2011). Understanding dopamine and reinforcement learning: The dopamine\\nreward prediction error hypothesis. Proceedings of the National Academy of Sciences,\\n108(Supplement 3):15647–15654.\\nGlimcher, P. W. (2003).Decisions, Uncertainty, and the Brain: The science of Neuroeconomics.\\nMIT Press, Cambridge, MA.\\nGlimcher, P. W., Fehr, E. (Eds.) (2013).Neuroeconomics: Decision Making and the Brain,\\nSecond Edition. Academic Press.\\nGoethe, J. W. V. (1878). The Sorcerer’s Apprentice. InThe Permanent Goethe,p .3 4 9 .T h e\\nDial Press, Inc., New York.\\nGoldstein, H. (1957). Classical Mechanics.A d d i s o n - W e s l e y , R e a d i n g , M A .\\nGoodfellow, I., Bengio, Y., Courville, A. (2016).Deep Learning. MIT Press, Cambridge, MA.\\nGoodwin, G. C., Sin, K. S. (1984).Adaptive Filtering Prediction and Control. Prentice-Hall,\\nEnglewood Cli↵s, NJ.\\nGopnik, A., Glymour, C., Sobel, D., Schulz, L. E., Kushnir, T., Danks, D. (2004). A theory of\\ncausal learning in children: Causal maps and Bayes nets.Psychological Review, 111(1):3–32.\\nGordon, G. J. (1995). Stable function approximation in dynamic programming. In A. Prieditis\\nand S. Russell (Eds.),Proceedings of the 12th International Conference on Machine Learning,\\npp. 261–268. Morgan Kaufmann. An expanded version was published as Technical Report\\nCMU-CS-95-103. Carnegie Mellon University, Pittsburgh, PA, 1995.\\nGordon, G. J. (1996a). Chattering in SARSA(\\x00). CMU learning lab internal report.\\nGordon, G. J. (1996b). Stable ﬁtted reinforcement learning. InAdvances in Neural Information\\nProcessing Systems 8, pp. 1052–1058. MIT Press, Cambridge, MA.\\nGordon, G. J. (1999). Approximate Solutions to Markov Decision Processes.P h D t h e s i s ,\\nCarnegie Mellon University, Pittsburgh PA. Pittsburgh, PA.\\nGordon, G. J. (2001). Reinforcement learning with function approximation converges to a'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 514, 'page_label': '515'}, page_content='References 493\\nregion. In Advances in Neural Information Processing Systems 13,p p .1 0 4 0 – 1 0 4 6 . M I T\\nPress, Cambridge, MA.\\nGraybiel, A. M. (2000). The basal ganglia.Current Biology, 10(14):R509–R511.\\nGreensmith, E., Bartlett, P. L., Baxter, J. (2002). Variance reduction techniques for gradient\\nestimates in reinforcement learning. InAdvances in Neural Information Processing Systems\\n14, pp. 1507–1514. MIT Press, Cambridge, MA.\\nGreensmith, E., Bartlett, P. L., Baxter, J. (2004). Variance reduction techniques for gradient\\nestimates in reinforcement learning.Journal of Machine Learning Research, 5(Nov):1471–\\n1530.\\nGri\\x00th, A. K. (1966). A new machine learning technique applied to the game of checkers.\\nTechnical Report Project MAC, Artiﬁcial Intelligence Memo 94. Massachusetts Institute of\\nTechnology, Cambridge, MA.\\nGri\\x00th, A. K. (1974). A comparison and evaluation of three machine learning procedures as\\napplied to the game of checkers.Artiﬁcial Intelligence, 5(2):137–148.\\nGrondman, I., Busoniu, L., Lopes, G. A., Babuska, R. (2012). A survey of actor–critic reinforce-\\nment learning: Standard and natural policy gradients.IEEE Transactions on Systems, Man,\\nand Cybernetics, Part C (Applications and Reviews), 42(6):1291–1307.\\nGrossberg, S. (1975). A neural model of attention, reinforcement, and discrimination learning.\\nInternational Review of Neurobiology, 18:263–327.\\nGrossberg, S., Schmajuk, N. A. (1989). Neural dynamics of adaptive timing and temporal\\ndiscrimination during associative learning.Neural Networks, 2(2):79–102.\\nGullapalli, V. (1990). A stochastic reinforcement algorithm for learning real-valued functions.\\nNeural Networks, 3(6): 671–692.\\nGullapalli, V., Barto, A. G. (1992). Shaping as a method for accelerating reinforcement learning.\\nIn Proceedings of the 1992 IEEE International Symposium on Intelligent Control,p p .5 5 4 – 5 5 9 .\\nIEEE.\\nGurvits, L., Lin, L.-J., Hanson, S. J. (1994). Incremental learning of evaluation functions\\nfor absorbing Markov chains: New methods and theorems. Siemans Corporate Research,\\nPrinceton, NJ.\\nHackman, L. (2012). Faster Gradient-TD Algorithms. M.Sc. thesis, University of Alberta,\\nEdmonton.\\nHallak, A., Tamar, A., Mannor, S. (2015). Emphatic TD Bellman operator is a contraction.\\nArXiv:1508.03411.\\nHallak, A., Tamar, A., Munos, R., Mannor, S. (2016). Generalized emphatic temporal di↵erence\\nlearning: Bias-variance analysis. In Proceedings of the Thirtieth AAAI Conference on\\nArtiﬁcial Intelligence, pp. 1631–1637. AAAI Press, Menlo Park, CA.\\nHammer, M. (1997). The neural basis of associative reward learning in honeybees.Trends in\\nNeuroscience, 20(6):245–252.\\nHammer, M., Menzel, R. (1995). Learning and memory in the honeybee.The Journal of\\nNeuroscience, 15(3):1617–1630.\\nHampson, S. E. (1983). AN e u r a lM o d e lo fA d a p t i v eB e h a v i o r. PhD thesis, University of\\nCalifornia, Irvine.\\nHampson, S. E. (1989).Connectionist Problem Solving: Computational Aspects of Biological\\nLearning. Birkhauser, Boston.\\nHare, T. A., O’Doherty, J., Camerer, C. F., Schultz, W., Rangel, A. (2008). Dissociating the role\\nof the orbitofrontal cortex and the striatum in the computation of goal values and prediction\\nerrors. The Journal of Neuroscience, 28(22):5623–5630.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 515, 'page_label': '516'}, page_content='494 References\\nHarth, E., Tzanakou, E. (1974). Alopex: A stochastic method for determining visual receptive\\nﬁelds. Vision Research, 14(12):1475–1482.\\nHassabis, D., Maguire, E. A. (2007). Deconstructing episodic memory with construction.Trends\\nin Cognitive Sciences, 11(7):299–306.\\nHauskrecht, M., Meuleau, N., Kaelbling, L. P., Dean, T., Boutilier, C. (1998). Hierarchical\\nsolution of Markov decision processes using macro-actions. InProceedings of the Fourteenth\\nConference on Uncertainty in Artiﬁcial Intelligence, pp. 220–229. Morgan Kaufmann.\\nHawkins, R. D., Kandel, E. R. (1984). Is there a cell-biological alphabet for simple forms of\\nlearning? Psychological Review, 91(3):375–391.\\nHaykin, S. (1994).Neural networks: A Comprehensive Foundation, Macmillan, New York.\\nHe, K., Huertas, M., Hong, S. Z., Tie, X., Hell, J. W., Shouval, H., Kirkwood, A. (2015). Distinct\\neligibility traces for LTP and LTD in cortical synapses.Neuron, 88(3):528–538.\\nHe, K., Zhang, X., Ren, S., Sun, J. (2016). Deep residual learning for image recognition. In\\nProceedings of the 1992 IEEE Conference on Computer Vision and Pattern Recognition,\\npp. 770–778.\\nHebb, D. O. (1949).The Organization of Behavior: A Neuropsychological Theory.J o h n W i l e y\\nand Sons Inc., New York. Reissued by Lawrence Erlbaum Associates Inc., Mahwah NJ, 2002.\\nHengst, B. (2012). Hierarchical approaches. In M. Wiering and M. van Otterlo (Eds.),Rein-\\nforcement Learning: State-of-the-Art, pp. 293–323. Springer-Verlag Berlin Heidelberg.\\nHerrnstein, R. J. (1970). On the Law of E↵ect.Journal of the Experimental Analysis of Behavior,\\n13(2):243–266.\\nHersh, R., Griego, R. J. (1969). Brownian motion and potential theory.Scientiﬁc American,\\n220(3):66–74.\\nHester, T., Stone, P. (2012). Learning and using models. In M. Wiering and M. van Otterlo (Eds.),\\nReinforcement Learning: State-of-the-Art, pp. 111–141. Springer-Verlag Berlin Heidelberg.\\nHesterberg, T. C. (1988),Advances in Importance Sampling, PhD thesis, Statistics Department,\\nStanford University.\\nHilgard, E. R. (1956).Theories of Learning, Second Edition. Appleton-Century-Cofts, Inc.,\\nNew York.\\nHilgard, E. R., Bower, G. H. (1975).Theories of Learning. Prentice-Hall, Englewood Cli↵s, NJ.\\nHinton, G. E. (1984). Distributed representations. Technical Report CMU-CS-84-157. Depart-\\nment of Computer Science, Carnegie-Mellon University, Pittsburgh, PA.\\nHinton, G. E., Osindero, S., Teh, Y. (2006). A fast learning algorithm for deep belief nets.\\nNeural Computation, 18(7):1527–1554.\\nHochreiter, S., Schmidhuber, J. (1997). LTSM can solve hard time lag problems. InAdvances\\nin Neural Information Processing Systems 9, pp. 473–479. MIT Press, Cambridge, MA.\\nHolland, J. H. (1975).Adaptation in Natural and Artiﬁcial Systems. University of Michigan\\nPress, Ann Arbor.\\nHolland, J. H. (1976). Adaptation. In R. Rosen and F. M. Snell (Eds.),Progress in Theoretical\\nBiology, vol. 4, pp. 263–293. Academic Press, New York.\\nHolland, J. H. (1986). Escaping brittleness: The possibility of general-purpose learning algorithms\\napplied to rule-based systems. In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell\\n(Eds.), Machine Learning: An Artiﬁcial Intelligence Approach, vol. 2, pp. 593–623. Morgan\\nKaufmann.\\nHollerman, J. R., Schultz, W. (1998). Dopmine neurons report an error in the temporal\\nprediction of reward during learning.Nature Neuroscience, 1(4):304–309.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 516, 'page_label': '517'}, page_content='References 495\\nHouk, J. C., Adams, J. L., Barto, A. G. (1995). A model of how the basal ganglia generates and\\nuses neural signals that predict reinforcement. In J. C. Houk, J. L. Davis, and D. G. Beiser\\n(Eds.), Models of Information Processing in the Basal Ganglia, pp. 249–270. MIT Press,\\nCambridge, MA.\\nHoward, R. (1960).Dynamic Programming and Markov Processes. MIT Press, Cambridge, MA.\\nHull, C. L. (1932). The goal-gradient hypothesis and maze learning.Psychological Review,\\n39(1):25–43.\\nHull, C. L. (1943).Principles of Behavior. Appleton-Century, New York.\\nHull, C. L. (1952).AB e h a v i o rS y s t e m. Wiley, New York.\\nIo↵e, S., Szegedy, C. (2015). Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. ArXiv:1502.03167.\\n˙Ipek, E., Mutlu, O., Mart´ ınez, J. F., Caruana, R. (2008). Self-optimizing memory controllers: A\\nreinforcement learning approach. InISCA’08:Proceedings of the 35th Annual International\\nSymposium on Computer Architecture, pp. 39–50. IEEE Computer Society Washington, DC.\\nIzhikevich, E. M. (2007). Solving the distal reward problem through linkage of STDP and\\ndopamine signaling. Cerebral Cortex, 17(10):2443–2452.\\nJaakkola, T., Jordan, M. I., Singh, S. P. (1994). On the convergence of stochastic iterative\\ndynamic programming algorithms. Neural Computation, 6:1185–1201.\\nJaakkola, T., Singh, S. P., Jordan, M. I. (1995). Reinforcement learning algorithm for partially\\nobservable Markov decision problems. InAdvances in Neural Information Processing Systems\\n7, pp. 345–352. MIT Press, Cambridge, MA.\\nJacobs, R. A. (1988). Increased rates of convergence through learning rate adaptation.Neural\\nNetworks, 1(4):295–307.\\nJaderberg, M., Mnih, V., Czarnecki, W. M., Schaul, T., Leibo, J. Z., Silver, D., Kavukcuoglu, K.\\n(2016). Reinforcement learning with unsupervised auxiliary tasks. ArXiv:1611.05397.\\nJaeger, H. (1997). Observable operator models and conditioned continuation representations. Ar-\\nbeitspapiere der GMD 1043, GMD Forschungszentrum Informationstechnik, Sankt Augustin,\\nGermany.\\nJaeger, H. (1998). Discrete Time, Discrete Valued Observable Operator Models: A Tutorial.\\nGMD-Forschungszentrum Informationstechnik.\\nJaeger, H. (2000). Observable operator models for discrete stochastic time series. Neural\\nComputation, 12(6):1371–1398.\\nJaeger, H. (2002). Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF\\nand the ‘echo state network’ approach. German National Research Center for Information\\nTechnology, Technical Report GMD report 159, 2002.\\nJoel, D., Niv, Y., Ruppin, E. (2002). Actor–critic models of the basal ganglia: New anatomical\\nand computational perspectives. Neural Networks, 15(4):535–547.\\nJohnson, A., Redish, A. D. (2007). Neural ensembles in CA3 transiently encode paths forward\\nof the animal at a decision point.The Journal of Neuroscience, 27(45):12176–12189.\\nKaelbling, L. P. (1993a). Hierarchical learning in stochastic domains: Preliminary results. In\\nProceedings of the 10th International Conference on Machine Learning, pp. 167–173. Morgan\\nKaufmann.\\nKaelbling, L. P. (1993b).Learning in Embedded Systems. MIT Press, Cambridge, MA.\\nKaelbling, L. P. (Ed.) (1996). Special triple issue on reinforcement learning,Machine Learning,\\n22(1/2/3).\\nKaelbling, L. P., Littman, M. L., Moore, A. W. (1996). Reinforcement learning: A survey.\\nJournal of Artiﬁcial Intelligence Research, 4:237–285.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 517, 'page_label': '518'}, page_content='496 References\\nKakade, S. M. (2002). A natural policy gradient. InAdvances in Neural Information Processing\\nSystems 14, pp. 1531–1538. MIT Press, Cambridge, MA.\\nKakade, S. M. (2003). On the Sample Complexity of Reinforcement Learning.P h D t h e s i s ,\\nUniversity of London.\\nKakutani, S. (1945). Markov processes and the Dirichlet problem.Proceedings of the Japan\\nAcademy, 21(3-10):227–233.\\nKalos, M. H., Whitlock, P. A. (1986).Monte Carlo Methods. Wiley, New York.\\nKamin, L. J. (1968). “Attention-like” processes in classical conditioning. In M. R. Jones (Ed.),\\nMiami Symposium on the Prediction of Behavior, 1967: Aversive Stimulation,p p .9 – 3 1 .\\nUniversity of Miami Press, Coral Gables, Florida.\\nKamin, L. J. (1969). Predictability, surprise, attention, and conditioning. In B. A. Campbell and\\nR. M. Church (Eds.),Punishment and Aversive Behavior, pp. 279–296. Appleton-Century-\\nCrofts, New York.\\nKandel, E. R., Schwartz, J. H., Jessell, T. M., Siegelbaum, S. A., Hudspeth, A. J. (Eds.) (2013).\\nPrinciples of Neural Science, Fifth Edition. McGraw-Hill Companies, Inc.\\nKarampatziakis, N., Langford, J. (2010). Online importance weight aware updates. ArXiv:1011.1576.\\nKashyap, R. L., Blaydon, C. C., Fu, K. S. (1970). Stochastic approximation. In J. M. Mendel\\nand K. S. Fu (Eds.),Adaptive, Learning, and Pattern Recognition Systems: Theory and\\nApplications, pp. 329–355. Academic Press, New York.\\nKearney, A., Veeriah, V, Travnik, J, Sutton, R. S., Pilarski, P. M. (in preparation). TIDBD:\\nAdapting Temporal-di↵erence Step-sizes Through Stochastic Meta-descent.\\nKearns, M., Singh, S. (2002). Near-optimal reinforcement learning in polynomial time.Machine\\nLearning, 49(2-3):209–232.\\nKeerthi, S. S., Ravindran, B. (1997). Reinforcement learning. In E. Fieslerm and R. Beale\\n(Eds.), Handbook of Neural Computation, C3. Oxford University Press, New York.\\nKehoe, E. J. (1982). Conditioning with serial compound stimuli: Theoretical and empirical\\nissues. Experimental Animal Behavior, 1:30–65.\\nKehoe, E. J., Schreurs, B. G., Graham, P. (1987). Temporal primacy overrides prior training\\nin serial compound conditioning of the rabbit’s nictitating membrane response.Animal\\nLearning & Behavior, 15(4):455–464.\\nKeiﬂin, R., Janak, P. H. (2015). Dopamine prediction errors in reward learning and addiction:\\nFrom theory to neural circuitry.Neuron, 88(2):247– 263.\\nKimble, G. A. (1961).Hilgard and Marquis’ Conditioning and Learning. Appleton-Century-\\nCrofts, New York.\\nKimble, G. A. (1967).Foundations of Conditioning and Learning. Appleton-Century-Crofts,\\nNew York.\\nKingma, D., Ba, J. (2014). Adam: A method for stochastic optimization. ArXiv:1412.6980.\\nKlopf, A. H. (1972). Brain function and adaptive systems—A heterostatic theory. Technical\\nReport AFCRL-72-0164, Air Force Cambridge Research Laboratories, Bedford, MA. A\\nsummary appears inProceedings of the International Conference on Systems, Man, and\\nCybernetics (1974). IEEE Systems, Man, and Cybernetics Society, Dallas, TX.\\nKlopf, A. H. (1975). A comparison of natural and artiﬁcial intelligence.SIGART Newsletter,\\n53:11–13.\\nKlopf, A. H. (1982).The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence.\\nHemisphere, Washington, DC.\\nKlopf, A. H. (1988). A neuronal model of classical conditioning.Psychobiology, 16(2):85–125.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 518, 'page_label': '519'}, page_content='References 497\\nKlyubin, A. S., Polani, D., Nehaniv, C. L. (2005). Empowerment: A universal agent-centric\\nmeasure of control. InProceedings of the 2005 IEEE Congress on Evolutionary Computation\\n(Vol. 1, pp. 128–135). IEEE.\\nKober, J., Peters, J. (2012). Reinforcement learning in robotics: A survey. In M. Wiering, M.\\nvan Otterlo (Eds.),Reinforcement Learning: State-of-the-Art, pp. 579–610. Springer-Verlag.\\nKocsis, L., Szepesv´ ari, Cs. (2006). Bandit based Monte-Carlo planning. InProceedings of the\\nEuropean Conference on Machine Learning, pp. 282–293. Springer-Verlag Berlin Heidelberg.\\nKohonen, T. (1977). Associative Memory: A System Theoretic Approach. Springer-Verlag,\\nBerlin.\\nKoller, D., Friedman, N. (2009).Probabilistic Graphical Models: Principles and Techniques.\\nMIT Press.\\nKolodziejski, C., Porr, B., W¨ org¨ otter, F. (2009). On the asymptotic equivalence between\\ndi↵erential Hebbian and temporal di↵erence learning.Neural Computation, 21(4):1173–1202.\\nKolter, J. Z. (2011). The ﬁxed points of o↵-policy TD. InAdvances in Neural Information\\nProcessing Systems 24, pp. 2169–2177. Curran Associates, Inc.\\nKonda, V. R., Tsitsiklis, J. N. (2000). Actor-critic algorithms. InAdvances in Neural Information\\nProcessing Systems 12, pp. 1008–1014. MIT Press, Cambridge, MA.\\nKonda, V. R., Tsitsiklis, J. N. (2003). On actor-critic algorithms.SIAM Journal on Control\\nand Optimization, 42(4):1143–1166.\\nKonidaris, G. D., Osentoski, S., Thomas, P. S. (2011). Value function approximation in\\nreinforcement learning using the Fourier basis . InProceedings of the Twenty-Fifth Conference\\nof the Association for the Advancement of Artiﬁcial Intelligence,p p .3 8 0 – 3 8 5 .\\nKorf, R. E. (1988). Optimal path ﬁnding algorithms. In L. N. Kanal and V. Kumar (Eds.),\\nSearch in Artiﬁcial Intelligence, pp. 223–267. Springer-Verlag, Berlin.\\nKorf, R. E. (1990). Real-time heuristic search.Artiﬁcial Intelligence, 42(2–3), 189–211.\\nKoshland, D. E. (1980).Bacterial Chemotaxis as a Model Behavioral System. Raven Press, New\\nYork.\\nKoza, J. R. (1992).Genetic Programming: On the Programming of Computers by Means of\\nNatural Selection(Vol. 1). MIT Press., Cambridge, MA.\\nKraft, L. G., Campagna, D. P. (1990). A summary comparison of CMAC neural network and\\ntraditional adaptive control systems. In T. Miller, R. S. Sutton, and P. J. Werbos (Eds.),\\nNeural Networks for Control, pp. 143–169. MIT Press, Cambridge, MA.\\nKraft, L. G., Miller, W. T., Dietz, D. (1992). Development and application of CMAC neural\\nnetwork-based control. In D. A. White and D. A. Sofge (Eds.),Handbook of Intelligent\\nControl: Neural, Fuzzy, and Adaptive Approaches, pp. 215–232. Van Nostrand Reinhold,\\nNew York.\\nKumar, P. R., Varaiya, P. (1986).Stochastic Systems: Estimation, Identiﬁcation, and Adaptive\\nControl. Prentice-Hall, Englewood Cli↵s, NJ.\\nKumar, P. R. (1985). A survey of some results in stochastic adaptive control.SIAM Journal of\\nControl and Optimization, 23(3):329–380.\\nKumar, V., Kanal, L. N. (1988). The CDP, A unifying formulation for heuristic search, dynamic\\nprogramming, and branch-and-bound. In L. N. Kanal and V. Kumar (Eds.),Search in\\nArtiﬁcial Intelligence, pp. 1–37. Springer-Verlag, Berlin.\\nKushner, H. J., Dupuis, P. (1992). Numerical Methods for Stochastic Control Problems in\\nContinuous Time. Springer-Verlag, New York.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 519, 'page_label': '520'}, page_content='498 References\\nKuvayev, L., Sutton, R.S. (1996). Model-based reinforcement learning with an approximate,\\nlearned model. Proceedings of the Ninth Yale Workshop on Adaptive and Learning Systems,\\npp. 101–105, Yale University, New Haven, CT.\\nLagoudakis, M., Parr, R. (2003). Least squares policy iteration.Journal of Machine Learning\\nResearch, 4(Dec):1107–1149.\\nLai, T. L., Robbins, H. (1985). Asymptotically e\\x00cient adaptive allocation rules.Advances in\\nApplied Mathematics, 6(1):4–22.\\nLakshmivarahan, S., Narendra, K. S. (1982). Learning algorithms for two-person zero-sum\\nstochastic games with incomplete information: A uniﬁed approach.SIAM Journal of Control\\nand Optimization, 20(4):541–552.\\nLammel, S., Lim, B. K., Malenka, R. C. (2014). Reward and aversion in a heterogeneous\\nmidbrain dopamine system. Neuropharmacology, 76:353–359.\\nLane, S. H., Handelman, D. A., Gelfand, J. J. (1992). Theory and development of higher-order\\nCMAC neural networks.IEEE Control Systems, 12(2):23–30.\\nLeCun, Y. (1985). Une proc´ edure d’apprentissage pour r´ eseau a seuil asymmetrique (a learning\\nscheme for asymmetric threshold networks). InProceedings of Cognitiva 85, Paris, France.\\nLeCun, Y., Bottou, L., Bengio, Y., Ha↵ner, P. (1998). Gradient-based learning applied to\\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324.\\nLegenstein, R. W., Maass, D. P. (2008). A learning theory for reward-modulated spike-timing-\\ndependent plasticity with application to biofeedback.PLoS Computational Biology, 4(10).\\nLevy, W. B., Steward, D. (1983). Temporal contiguity requirements for long-term associative\\npotentiation/depression in the hippocampus.Neuroscience, 8(4):791–797.\\nLewis, F. L., Liu, D. (Eds.) (2012). Reinforcement Learning and Approximate Dynamic\\nProgramming for Feedback Control.J o h nW i l e ya n dS o n s .\\nLewis, R. L., Howes, A., Singh, S. (2014). Computational rationality: Linking mechanism and\\nbehavior through utility maximization.Topics in Cognitive Science, 6(2):279–311.\\nLi, L. (2012). Sample complexity bounds of exploration. In M. Wiering and M. van Otterlo (Eds.),\\nReinforcement Learning: State-of-the-Art, pp. 175–204. Springer-Verlag Berlin Heidelberg.\\nLi, L., Chu, W., Langford, J., Schapire, R. E. (2010). A contextual-bandit approach to personal-\\nized news article recommendation. InProceedings of the 19th International Conference on\\nWorld Wide Web, pp. 661–670. ACM, New York.\\nLin, C.-S., Kim, H. (1991). CMAC-based adaptive critic self-learning control.IEEE Transactions\\non Neural Networks, 2(5):530–533.\\nLin, L.-J. (1992). Self-improving reactive agents based on reinforcement learning, planning and\\nteaching. Machine Learning, 8(3-4):293–321.\\nLin, L.-J., Mitchell, T. (1992). Reinforcement learning with hidden states. InProceedings of\\nthe Second International Conference on Simulation of Adaptive Behavior: From Animals to\\nAnimats, pp. 271–280. MIT Press, Cambridge, MA.\\nLittman, M. L., Cassandra, A. R., Kaelbling, L. P. (1995). Learning policies for partially\\nobservable environments: Scaling up. InProceedings of the 12th International Conference on\\nMachine Learning, pp. 362–370. Morgan Kaufmann.\\nLittman, M. L., Dean, T. L., Kaelbling, L. P. (1995). On the complexity of solving Markov\\ndecision problems. In Proceedings of the Eleventh Annual Conference on Uncertainty in\\nArtiﬁcial Intelligence,p p .3 9 4 – 4 0 2 .\\nLittman, M. L., Sutton, R. S., Singh, S. (2002). Predictive representations of state. InAdvances\\nin Neural Information Processing Systems 14, pp. 1555–1561. MIT Press, Cambridge, MA.\\nLiu, J. S. (2001).Monte Carlo Strategies in Scientiﬁc Computing. Springer-Verlag, Berlin.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 520, 'page_label': '521'}, page_content='References 499\\nLjung, L. (1998). System identiﬁcation. In A. Proch´ azka, J. Uhl´ ıˆ r, P. W. J. Rayner, and N. G.\\nKingsbury (Eds.), Signal Analysis and Prediction, pp. 163–173. Springer Science + Business\\nMedia New York, LLC.\\nLjung, L., S¨ oderstrom, T. (1983).Theory and Practice of Recursive Identiﬁcation. MIT Press,\\nCambridge, MA.\\nLjungberg, T., Apicella, P., Schultz, W. (1992). Responses of monkey dopamine neurons during\\nlearning of behavioral reactions.Journal of Neurophysiology, 67(1):145–163.\\nLovejoy, W. S. (1991). A survey of algorithmic methods for partially observed Markov decision\\nprocesses. Annals of Operations Research, 28(1):47–66.\\nLuce, D. (1959). Individual Choice Behavior. Wiley, New York.\\nLudvig, E. A., Bellemare, M. G., Pearson, K. G. (2011). A primer on reinforcement learning\\nin the brain: Psychological, computational, and neural perspectives. In E. Alonso and E.\\nMondrag´ on (Eds.),Computational Neuroscience for Advancing Artiﬁcial Intelligence: Models,\\nMethods and Applications, pp. 111–44. Medical Information Science Reference, Hershey PA.\\nLudvig, E. A., Sutton, R. S., Kehoe, E. J. (2008). Stimulus representation and the timing of\\nreward-prediction errors in models of the dopamine system.Neural Computation, 20(12):3034–\\n3054.\\nLudvig, E. A., Sutton, R. S., Kehoe, E. J. (2012). Evaluating the TD model of classical\\nconditioning. Learning & behavior, 40(3):305–319.\\nMachado, A. (1997). Learning the temporal dynamics of behavior. Psychological Review,\\n104(2):241–265.\\nMackintosh, N. J. (1975). A theory of attention: Variations in the associability of stimuli with\\nreinforcement. Psychological Review, 82(4):276–298.\\nMackintosh, N. J. (1983).Conditioning and Associative Learning. Clarendon Press, Oxford.\\nMaclin, R., Shavlik, J. W. (1994). Incorporating advice into agents that learn from reinforcements.\\nIn Proceedings of the Twelfth National Conference on Artiﬁcial Intelligence,p p .6 9 4 – 6 9 9 .\\nAAAI Press, Menlo Park, CA.\\nMaei, H. R. (2011).Gradient Temporal-Di↵erence Learning Algorithms. PhD thesis, University\\nof Alberta, Edmonton.\\nMaei, H. R. (2018). Convergent actor-critic algorithms under o↵-policy training and function\\napproximation. ArXiv:1802.07842.\\nMaei, H. R., Sutton, R. S. (2010). GQ(\\x00): A general gradient algorithm for temporal-di↵erence\\nprediction learning with eligibility traces. InProceedings of the Third Conference on Artiﬁcial\\nGeneral Intelligence,pp. 91–96.\\nMaei, H. R., Szepesv´ ari, Cs., Bhatnagar, S., Precup, D., Silver, D., Sutton, R. S. (2009).\\nConvergent temporal-di↵erence learning with arbitrary smooth function approximation. In\\nAdvances in Neural Information Processing Systems 22, pp. 1204–1212. Curran Associates,\\nInc.\\nMaei, H. R., Szepesv´ ari, Cs., Bhatnagar, S., Sutton, R. S. (2010). Toward o↵-policy learning\\ncontrol with function approximation. InProceedings of the 27th International Conference on\\nMachine Learning,p p .7 1 9 – 7 2 6 ) .\\nMahadevan, S. (1996). Average reward reinforcement learning: Foundations, algorithms, and\\nempirical results. Machine Learning, 22(1):159–196.\\nMahadevan, S., Liu, B., Thomas, P., Dabney, W., Giguere, S., Jacek, N., Gemp, I., Liu, J. (2014).\\nProximal reinforcement learning: A new theory of sequential decision making in primal-dual\\nspaces. ArXiv:1405.6757.\\nMahadevan, S., Connell, J. (1992). Automatic programming of behavior-based robots using'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 521, 'page_label': '522'}, page_content='500 References\\nreinforcement learning. Artiﬁcial Intelligence, 55(2-3):311–365.\\nMahmood, A. R. (2017). Incremental O↵-Policy Reinforcement Learning Algorithms.PhD\\nthesis, University of Alberta, Edmonton.\\nMahmood, A. R., Sutton, R. S. (2015). O↵-policy learning based on weighted importance sam-\\npling with linear computational complexity. InProceedings of the 31st Conference on Uncer-\\ntainty in Artiﬁcial Intelligence , pp. 552–561. AUAI Press Corvallis,\\nOregon.\\nMahmood, A. R., Sutton, R. S., Degris, T., Pilarski, P. M. (2012). Tuning-free step-size adapta-\\ntion. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing,\\nProceedings,p p .2 1 2 1 – 2 1 2 4 .I E E E .\\nMahmood, A. R., Yu, H, Sutton, R. S. (2017). Multi-step o↵-policy learning without importance\\nsampling ratios. ArXiv:1702.03006.\\nMahmood, A. R., van Hasselt, H., Sutton, R. S. (2014). Weighted importance sampling for\\no↵-policy learning with linear function approximation.Advances in Neural Information\\nProcessing Systems 27, pp. 3014–3022. Curran Associates, Inc.\\nMarbach, P., Tsitsiklis, J. N. (1998). Simulation-based optimization of Markov reward processes.\\nMIT Technical Report LIDS-P-2411.\\nMarbach, P., Tsitsiklis, J. N. (2001). Simulation-based optimization of Markov reward processes.\\nIEEE Transactions on Automatic Control, 46(2):191–209.\\nMarkram, H., L¨ ubke, J., Frotscher, M., Sakmann, B. (1997). Regulation of synaptic e\\x00cacy by\\ncoincidence of postsynaptic APs and EPSPs.Science, 275(5297):213–215.\\nMart´ ınez, J. F.,˙Ipek, E. (2009). Dynamic multicore resource management: A machine learning\\napproach. Micro, IEEE, 29(5):8–17.\\nMataric, M. J. (1994). Reward functions for accelerated learning. InProceedings of the 11th\\nInternational Conference on Machine Learning, pp. 181–189. Morgan Kaufmann.\\nMatsuda, W., Furuta, T., Nakamura, K. C., Hioki, H., Fujiyama, F., Arai, R., Kaneko, T.\\n(2009). Single nigrostriatal dopaminergic neurons form widely spread and highly dense\\naxonal arborizations in the neostriatum.The Journal of Neuroscience, 29(2):444–453.\\nMazur, J. E. (1994).Learning and Behavior, 3rd ed. Prentice-Hall, Englewood Cli↵s, NJ.\\nMcCallum, A. K. (1993). Overcoming incomplete perception with utile distinction memory. In\\nProceedings of the 10th International Conference on Machine Learning, pp. 190–196. Morgan\\nKaufmann.\\nMcCallum, A. K. (1995).Reinforcement Learning with Selective Perception and Hidden State.\\nPhD thesis, University of Rochester, Rochester NY.\\nMcCloskey, M., Cohen, N. J. (1989). Catastrophic interference in connectionist networks: The\\nsequential learning problem.Psychology of Learning and Motivation, 24:109–165.\\nMcClure, S. M., Daw, N. D., Montague, P. R. (2003). A computational substrate for incentive\\nsalience. Trends in Neurosciences, 26(8):423–428.\\nMcCulloch, W. S., Pitts, W. (1943). A logical calculus of the ideas immanent in nervous activity.\\nBulletin of Mathematical Biophysics, 5(4):115–133.\\nMcMahan, H. B., Gordon, G. J. (2005). Fast Exact Planning in Markov Decision Processes.\\nIn Proceedings of the International Conference on Automated Planning and Scheduling,\\npp. 151-160.\\nMelo, F. S., Meyn, S. P., Ribeiro, M. I. (2008). An analysis of reinforcement learning with\\nfunction approximation. InProceedings of the 25th International Conference on Machine\\nLearning,p p .6 6 4 – 6 7 1 .\\nMendel, J. M. (1966). A survey of learning control systems.ISA Transactions, 5:297–303.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 522, 'page_label': '523'}, page_content='References 501\\nMendel, J. M., McLaren, R. W. (1970). Reinforcement learning control and pattern recognition\\nsystems. In J. M. Mendel and K. S. Fu (Eds.),Adaptive, Learning and Pattern Recognition\\nSystems: Theory and Applications, pp. 287–318. Academic Press, New York.\\nMichie, D. (1961). Trial and error. In S. A. Barnett and A. McLaren (Eds.),Science Survey,\\nPart 2, pp. 129–145. Penguin, Harmondsworth.\\nMichie, D. (1963). Experiments on the mechanisation of game learning. 1. characterization of\\nthe model and its parameters.The Computer Journal, 6(3):232–263.\\nMichie, D. (1974).On Machine Intelligence. Edinburgh University Press, Edinburgh.\\nMichie, D., Chambers, R. A. (1968). BOXES, An experiment in adaptive control. In E. Dale\\nand D. Michie (Eds.),Machine Intelligence 2, pp. 137–152. Oliver and Boyd, Edinburgh.\\nMiller, R. (1981). Meaning and Purpose in the Intact Brain: A Philosophical, Psychological,\\nand Biological Account of Conscious Process. Clarendon Press, Oxford.\\nMiller, W. T., An, E., Glanz, F., Carter, M. (1990). The design of CMAC neural networks for\\ncontrol. Adaptive and Learning Systems, 1:140–145.\\nMiller, W. T., Glanz, F. H. (1996).UNH CMAC verison 2.1: The University of New Hampshire\\nImplementation of the Cerebellar Model Arithmetic Computer - CMAC. Robotics Laboratory\\nTechnical Report, University of New Hampshire, Durham.\\nMiller, S., Williams, R. J. (1992). Learning to control a bioreactor using a neural net Dyna-Q\\nsystem. In Proceedings of the Seventh Yale Workshop on Adaptive and Learning Systems,\\npp. 167–172. Center for Systems Science, Dunham Laboratory, Yale University, New Haven.\\nMiller, W. T., Scalera, S. M., Kim, A. (1994). Neural network control of dynamic balance for a\\nbiped walking robot. InProceedings of the Eighth Yale Workshop on Adaptive and Learning\\nSystems, pp. 156–161. Center for Systems Science, Dunham Laboratory, Yale University,\\nNew Haven.\\nMinton, S. (1990). Quantitative results concerning the utility of explanation-based learning.\\nArtiﬁcial Intelligence, 42(2-3):363–391.\\nMinsky, M. L. (1954).Theory of Neural-Analog Reinforcement Systems and Its Application to\\nthe Brain-Model Problem. PhD thesis, Princeton University.\\nMinsky, M. L. (1961). Steps toward artiﬁcial intelligence.Proceedings of the Institute of Radio\\nEngineers, 49:8–30. Reprinted in E. A. Feigenbaum and J. Feldman (Eds.),Computers and\\nThought, pp. 406–450. McGraw-Hill, New York, 1963.\\nMinsky, M. L. (1967).Computation: Finite and Inﬁnite Machines. Prentice-Hall, Englewood\\nCli↵s, NJ.\\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., Riedmiller, M.\\n(2013). Playing atari with deep reinforcement learning. ArXiv:1312.5602.\\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves,\\nA., Riedmiller, M., Fidjeland, A. K., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A.,\\nAntonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., Hassabis, D. (2015). Human-\\nlevel control through deep reinforcement learning.Nature, 518(7540):529–533.\\nModayil, J., Sutton, R. S. (2014). Prediction driven behavior: Learning predictions that drive\\nﬁxed responses. InAAAI-14 Workshop on Artiﬁcial Intelligence and Robotics,Q u e b e cC i t y ,\\nCanada.\\nModayil, J., White, A., Sutton, R. S. (2014). Multi-timescale nexting in a reinforcement learning\\nrobot. Adaptive Behavior, 22(2):146–160.\\nMonahan, G. E. (1982). State of the art—a survey of partially observable Markov decision\\nprocesses: theory, models, and algorithms.Management Science, 28(1):1–16.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 523, 'page_label': '524'}, page_content='502 References\\nMontague, P. R., Dayan, P., Nowlan, S. J., Pouget, A., Sejnowski, T. J. (1993). Using aperiodic\\nreinforcement for directed self-organization during development. InAdvances in Neural\\nInformation Processing Systems 5, pp. 969–976. Morgan Kaufmann.\\nMontague, P. R., Dayan, P., Person, C., Sejnowski, T. J. (1995). Bee foraging in uncertain\\nenvironments using predictive hebbian learning.Nature, 377(6551):725–728.\\nMontague, P. R., Dayan, P., Sejnowski, T. J. (1996). A framework for mesencephalic dopamine\\nsystems based on predictive Hebbian learning.The Journal of Neuroscience, 16(5):1936–1947.\\nMontague, P. R., Dolan, R. J., Friston, K. J., Dayan, P. (2012). Computational psychiatry.\\nTrends in Cognitive Sciences, 16(1):72–80.\\nMontague, P. R., Sejnowski, T. J. (1994). The predictive brain: Temporal coincidence and\\ntemporal order in synaptic learningmechanisms.Learning & Memory, 1(1):1–33.\\nMoore, A. W. (1990). E\\x00cient Memory-Based Learning for Robot Control.P h D t h e s i s ,\\nUniversity of Cambridge.\\nMoore, A. W., Atkeson, C. G. (1993). Prioritized sweeping: Reinforcement learning with less\\ndata and less real time.Machine Learning, 13(1):103–130.\\nMoore, A. W., Schneider, J., Deng, K. (1997). E\\x00cient locally weighted polynomial regression\\npredictions. In Proceedings of the 14th International Conference on Machine Learning.\\nMorgan Kaufmann.\\nMoore, J. W., Blazis, D. E. J. (1989). Simulation of a classically conditioned response: A\\ncerebellar implementation of the sutton-barto-desmond model. In J. H. Byrne and W. O.\\nBerry (Eds.),Neural Models of Plasticity, pp. 187–207. Academic Press, San Diego, CA.\\nMoore, J. W., Choi, J.-S., Brunzell, D. H. (1998). Predictive timing under temporal uncertainty:\\nThe time derivative model of the conditioned response. In D. A. Rosenbaum and C. E.\\nCollyer (Eds.),Timing of Behavior, pp. 3–34. MIT Press, Cambridge, MA.\\nMoore, J. W., Desmond, J. E., Berthier, N. E., Blazis, E. J., Sutton, R. S., Barto, A. G. (1986).\\nSimulation of the classically conditioned nictitating membrane response by a neuron-like\\nadaptive element: I. Response topography, neuronal ﬁring, and interstimulus intervals.\\nBehavioural Brain Research, 21(2):143–154.\\nMoore, J. W., Marks, J. S., Castagna, V. E., Polewan, R. J. (2001). Parameter stability in the\\nTD model of complex CR topographies. InSociety for Neuroscience Abstracts, 27:642.\\nMoore, J. W., Schmajuk, N. A. (2008). Kamin blocking.Scholarpedia, 3(5):3542.\\nMoore, J. W., Stickney, K. J. (1980). Formation of attentional-associative networks in real\\ntime:Role of the hippocampus and implications for conditioning.Physiological Psychology,\\n8(2):207–217.\\nMukundan, J., Mart´ ınez, J. F. (2012). MORSE, Multi-objective reconﬁgurable self-optimizing\\nmemory scheduler. InIEEE 18th International Symposium on High Performance Computer\\nArchitecture,p p .1 – 1 2 .\\nM¨ uller, M. (2002). Computer Go.Artiﬁcial Intelligence, 134(1):145–179.\\nMunos, R., Stepleton, T., Harutyunyan, A., Bellemare, M. (2016). Safe and e\\x00cient o↵-policy\\nreinforcement learning. InAdvances in Neural Information Processing Systems 29,p p .1 0 4 6 –\\n1054. Curran Associates, Inc.\\nNaddaf, Y. (2010). Game-Independent AI Agents for Playing Atari 2600 Console Games.P h D\\nthesis, University of Alberta, Edmonton.\\nNarendra, K. S., Thathachar, M. A. L. (1974). Learning automata—A survey.IEEE Transactions\\non Systems, Man, and Cybernetics, 4:323–334.\\nNarendra, K. S., Thathachar, M. A. L. (1989).Learning Automata: An Introduction. Prentice-\\nHall, Englewood Cli↵s, NJ.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 524, 'page_label': '525'}, page_content='References 503\\nNarendra, K. S., Wheeler, R. M. (1983). An N-player sequential stochastic game with identical\\npayo↵s. IEEE Transactions on Systems, Man, and Cybernetics, 6:1154–1158.\\nNarendra, K. S., Wheeler, R. M. (1986). Decentralized learning in ﬁnite Markov chains.IEEE\\nTransactions on Automatic Control, 31(6):519–526.\\nNedi´ c, A., Bertsekas, D. P. (2003). Least squares policy evaluation algorithms with linear\\nfunction approximation. Discrete Event Dynamic Systems, 13(1-2):79–110.\\nNg, A. Y. (2003).Shaping and Policy Search in Reinforcement Learning. PhD thesis, University\\nof California, Berkeley.\\nNg, A. Y., Harada, D., Russell, S. (1999). Policy invariance under reward transformations:\\nTheory and application to reward shaping. In I. Bratko and S. Dzeroski (Eds.),Proceedings\\nof the 16th International Conference on Machine Learning,p p .2 7 8 – 2 8 7 .\\nNg, A. Y., Russell, S. J. (2000). Algorithms for inverse reinforcement learning. InProceedings of\\nthe 17th International Conference on Machine Learning,p p .6 6 3 – 6 7 0 .\\nNiv, Y. (2009). Reinforcement learning in the brain. Journal of Mathematical Psychology,\\n53(3):139–154.\\nNiv, Y., Daw, N. D., Dayan, P. (2006). How fast to work: Response vigor, motivation and tonic\\ndopamine. In Advances in Neural Information Processing Systems 18,p p .1 0 1 9 – 1 0 2 6 .M I T\\nPress, Cambridge, MA.\\nNiv, Y., Daw, N. D., Joel, D., Dayan, P. (2007). Tonic dopamine: opportunity costs and the\\ncontrol of response vigor.Psychopharmacology, 191(3):507–520.\\nNiv, Y., Joel, D., Dayan, P. (2006). A normative perspective on motivation.Trends in Cognitive\\nSciences, 10(8):375–381.\\nNouri, A., Littman, M. L. (2009). Multi-resolution exploration in continuous spaces. InAdvances\\nin Neural Information Processing Systems 21, pp. 1209–1216. Curran Associates, Inc.\\nNow´ e, A., Vrancx, P., Hauwere, Y.-M. D. (2012). Game theory and multi-agent reinforcement\\nlearning. In M. Wiering and M. van Otterlo (Eds.),Reinforcement Learning: State-of-the-Art,\\npp. 441–467. Springer-Verlag Berlin Heidelberg.\\nNutt, D. J., Lingford-Hughes, A., Erritzoe, D., Stokes, P. R. A. (2015). The dopamine theory of\\naddiction: 40 years of highs and lows.Nature Reviews Neuroscience, 16(5):305–312.\\nO’Doherty, J. P., Dayan, P., Friston, K., Critchley, H., Dolan, R. J. (2003). Temporal di↵erence\\nmodels and reward-related learning in the human brain.Neuron, 38(2):329–337.\\nO’Doherty, J. P., Dayan, P., Schultz, J., Deichmann, R., Friston, K., Dolan, R. J. (2004).\\nDissociable roles of ventral and dorsal striatum in instrumental conditioning.Science,\\n304(5669):452–454.\\n´Olafsd´ ottir, H. F., Barry, C., Saleem, A. B., Hassabis, D., Spiers, H. J. (2015). Hippocampal\\nplace cells construct reward related sequences through unexplored space.Elife, 4:e06063.\\nOh, J., Guo, X., Lee, H., Lewis, R. L., Singh, S. (2015). Action-conditional video prediction\\nusing deep networks in Atari games. InAdvances in Neural Information Processing Systems\\n28, pp. 2845–2853. Curran Associates, Inc.\\nOlds, J., Milner, P. (1954). Positive reinforcement produced by electrical stimulation of the septal\\narea and other regions of rat brain.Journal of Comparative and Physiological Psychology,\\n47(6):419–427.\\nO’Reilly, R. C., Frank, M. J. (2006). Making working memory work: A computational model of\\nlearning in the prefrontal cortex and basal ganglia.Neural Computation, 18(2):283–328.\\nO’Reilly, R. C., Frank, M. J., Hazy, T. E., Watz, B. (2007). PVLV, the primary value and\\nlearned value Pavlovian learning algorithm.Behavioral Neuroscience, 121(1):31–49.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 525, 'page_label': '526'}, page_content='504 References\\nOmohundro, S. M. (1987). E\\x00cient algorithms with neural network behavior. Technical Report,\\nDepartment of Computer Science, University of Illinois at Urbana-Champaign.\\nOrmoneit, D., Sen,´S. (2002). Kernel-based reinforcement learning.Machine Learning, 49(2-\\n3):161–178.\\nOudeyer, P.-Y., Kaplan, F. (2007). What is intrinsic motivation? A typology of computational\\napproaches. Frontiers in Neurorobotics, 1:6.\\nOudeyer, P.-Y., Kaplan, F., Hafner, V. V. (2007). Intrinsic motivation systems for autonomous\\nmental development. IEEE Transactions on Evolutionary Computation, 11(2):265–286.\\nPadoa-Schioppa, C., Assad, J. A. (2006). Neurons in the orbitofrontal cortex encode economic\\nvalue. Nature, 441(7090):223–226.\\nPage, C. V. (1977). Heuristics for signature table analysis as a pattern recognition technique.\\nIEEE Transactions on Systems, Man, and Cybernetics, 7(2):77–86.\\nPagnoni, G., Zink, C. F., Montague, P. R., Berns, G. S. (2002). Activity in human ventral\\nstriatum locked to errors of reward prediction.Nature Neuroscience, 5(2):97–98.\\nPan, W.-X., Schmidt, R., Wickens, J. R., Hyland, B. I. (2005). Dopamine cells respond to\\npredicted events during classical conditioning: Evidence for eligibility traces in the reward-\\nlearning network. The Journal of Neuroscience, 25(26):6235–6242.\\nPark, J., Kim, J., Kang, D. (2005). An RLS-based natural actor–critic algorithm for locomotion\\nof a two-linked robot arm.Computational Intelligence and Security:65–72.\\nParks, P. C., Militzer, J. (1991). Improved allocation of weights for associative memory storage\\nin learning control systems. InIFAC Design Methods of Control Systems, Zurich, Switzerland,\\npp. 507–512.\\nParr, R. (1988).Hierarchical Control and Learning for Markov Decision Processes.PhD thesis,\\nUniversity of California, Berkeley.\\nParr, R., Li, L., Taylor, G., Painter-Wakeﬁeld, C., Littman, M. L. (2008). An analysis of linear\\nmodels, linear value-function approximation, and feature selection for reinforcement learning.\\nIn Proceedings of the 25th international conference on Machine learning,p p .7 5 2 – 7 5 9 ) .\\nParr, R., Russell, S. (1995). Approximating optimal policies for partially observable stochastic\\ndomains. In Proceedings of the Fourteenth International Joint Conference on Artiﬁcial\\nIntelligence, pp. 1088–1094. Morgan Kaufmann.\\nPavlov, I. P. (1927).Conditioned Reﬂexes. Oxford University Press, London.\\nPawlak, V., Kerr, J. N. D. (2008). Dopamine receptor activation is required for corticostriatal\\nspike-timing-dependent plasticity. The Journal of Neuroscience, 28(10):2435–2446.\\nPawlak, V., Wickens, J. R., Kirkwood, A., Kerr, J. N. D. (2010). Timing is not every-\\nthing: neuromodulation opens the STDP gate.Frontiers in Synaptic Neuroscience, 2:146.\\ndoi:10.3389/fnsyn.2010.00146.\\nPearce, J. M., Hall, G. (1980). A model for Pavlovian learning: Variation in the e↵ectiveness of\\nconditioning but not unconditioned stimuli.Psychological Review, 87(6):532–552.\\nPearl, J. (1984). Heuristics: Intelligent Search Strategies for Computer Problem Solving.\\nAddison-Wesley, Reading, MA.\\nPearl, J. (1995). Causal diagrams for empirical research.Biometrika,8 2 ( 4 ) : 6 6 9 - 6 8 8 .\\nPecevski, D., Maass, W., Legenstein, R. A. (2008). Theoretical analysis of learning with\\nreward-modulated spike-timing-dependent plasticity. InAdvances in Neural Information\\nProcessing Systems 20, pp. 881–888. Curran Associates, Inc.\\nPeng, J. (1993). E\\x00cient Dynamic Programming-Based Learning for Control.P h D t h e s i s ,\\nNortheastern University, Boston MA.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 526, 'page_label': '527'}, page_content='References 505\\nPeng, J. (1995). E\\x00cient memory-based dynamic programming. InProceedings of the 12th\\nInternational Conference on Machine Learning,p p .4 3 8 – 4 4 6 .\\nPeng, J., Williams, R. J. (1993). E\\x00cient learning and planning within the Dyna framework.\\nAdaptive Behavior, 1(4):437–454.\\nPeng, J., Williams, R. J. (1994). Incremental multi-step Q-learning. InProceedings of the\\n11th International Conference on Machine Learning, pp. 226–232. Morgan Kaufmann, San\\nFrancisco.\\nPeng, J., Williams, R. J. (1996). Incremental multi-step Q-learning. Machine Learning,\\n22(1):283–290.\\nPerkins, T. J., Pendrith, M. D. (2002). On the existence of ﬁxed points for Q-learning and\\nSarsa in partially observable domains. InProceedings of the 19th International Conference\\non Machine Learning,p p .4 9 0 – 4 9 7 .\\nPerkins, T. J., Precup, D. (2003). A convergent form of approximate policy iteration. InAdvances\\nin Neural Information Processing Systems 15, pp. 1627–1634. MIT Press, Cambridge, MA.\\nPeters, J., B¨ uchel, C. (2010). Neural representations of subjective reward value.Behavioral\\nBrain Research, 213(2):135–141.\\nPeters, J., Schaal, S. (2008). Natural actor–critic.Neurocomputing, 71(7):1180–1190.\\nPeters, J., Vijayakumar, S., Schaal, S. (2005). Natural actor–critic. InEuropean Conference on\\nMachine Learning, pp. 280–291. Springer Berlin Heidelberg.\\nPezzulo, G., van der Meer, M. A. A., Lansink, C. S., Pennartz, C. M. A. (2014). Internally\\ngenerated sequences in learning and executing goal-directed behavior.Trends in Cognitive\\nScience, 18(12):647–657.\\nPfei↵er, B. E., Foster, D. J. (2013). Hippocampal place-cell sequences depict future paths to\\nremembered goals. Nature, 497(7447):74–79.\\nPhansalkar, V. V., Thathachar, M. A. L. (1995). Local and global optimization algorithms for\\ngeneralized learning automata. Neural Computation, 7(5):950–973.\\nPoggio, T., Girosi, F. (1989). A theory of networks for approximation and learning. A.I. Memo\\n1140. Artiﬁcial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge,\\nMA.\\nPoggio, T., Girosi, F. (1990). Regularization algorithms for learning that are equivalent to\\nmultilayer networks. Science, 247(4945):978–982.\\nPolyak, B. T. (1990). New stochastic approximation type procedures.Automat. i Telemekh,\\n7(98-107):2 (in Russian).\\nPolyak, B. T., Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging.\\nSIAM Journal on Control and Optimization, 30(4):838–855.\\nPowell, M. J. D. (1987). Radial basis functions for multivariate interpolation: A review. In\\nJ. C. Mason and M. G. Cox (Eds.),Algorithms for Approximation,p p .1 4 3 – 1 6 7 .C l a r e n d o n\\nPress, Oxford.\\nPowell, W. B. (2011).Approximate Dynamic Programming: Solving the Curses of Dimensionality,\\nSecond edition. John Wiley and Sons.\\nPowers, W. T. (1973).Behavior: The Control of Perception. Aldine de Gruyter, Chicago. 2nd\\nexpanded edition 2005.\\nPrecup, D. (2000).Temporal Abstraction in Reinforcement Learning.PhD thesis, University of\\nMassachusetts, Amherst.\\nPrecup, D., Sutton, R. S., Dasgupta, S. (2001). O↵-policy temporal-di↵erence learning with\\nfunction approximation. InProceedings of the 18th International Conference on Machine\\nLearning,p p .4 1 7 – 4 2 4 .'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 527, 'page_label': '528'}, page_content='506 References\\nPrecup, D., Sutton, R. S., Paduraru, C., Koop, A., Singh, S. (2006). O↵-policy learning\\nwith options and recognizers. InAdvances in Neural Information Processing Systems 18,\\npp. 1097–1104. MIT Press, Cambridge, MA.\\nPrecup, D., Sutton, R. S., Singh, S. (2000). Eligibility traces for o↵-policy policy evaluation. In\\nProceedings of the 17th International Conference on Machine Learning, pp. 759–766. Morgan\\nKaufmann.\\nPuterman, M. L. (1994).Markov Decision Problems. Wiley, New York.\\nPuterman, M. L., Shin, M. C. (1978). Modiﬁed policy iteration algorithms for discounted\\nMarkov decision problems.Management Science, 24(11):1127–1137.\\nQuartz, S., Dayan, P., Montague, P. R., Sejnowski, T. J. (1992). Expectation learning in the\\nbrain using di↵use ascending connections. InSociety for Neuroscience Abstracts, 18:1210.\\nRandløv, J., Alstrøm, P. (1998). Learning to drive a bicycle using reinforcement learning\\nand shaping. In Proceedings of the 15th International Conference on Machine Learning,\\npp. 463–471.\\nRangel, A., Camerer, C., Montague, P. R. (2008). A framework for studying the neurobiology of\\nvalue-based decision making.Nature Reviews Neuroscience, 9(7):545–556.\\nRangel, A., Hare, T. (2010). Neural computations associated with goal-directed choice.Current\\nOpinion in Neurobiology, 20(2):262–270.\\nRao, R. P., Sejnowski, T. J. (2001). Spike-timing-dependent Hebbian plasticity as temporal\\ndi↵erence learning. Neural Computation, 13(10):2221–2237.\\nRatcli↵, R. (1990). Connectionist models of recognition memory: Constraints imposed by\\nlearning and forgetting functions.Psychological Review, 97(2):285–308.\\nReddy, G., Celani, A., Sejnowski, T. J., Vergassola, M. (2016). Learning to soar in turbulent\\nenvironments. Proceedings of the National Academy of Sciences, 113(33):E4877–E4884.\\nRedish, D. A. (2004). Addiction as a computational process gone awry.Science, 306(5703):1944–\\n1947.\\nReetz, D. (1977). Approximate solutions of a discounted Markovian decision process.Bonner\\nMathematische Schriften, 98:77–92.\\nRescorla, R. A., Wagner, A. R. (1972). A theory of Pavlovian conditioning: Variations in the\\ne↵ectiveness of reinforcement and nonreinforcement. In A. H. Black and W. F. Prokasy\\n(Eds.), Classical Conditioning II, pp. 64–99. Appleton-Century-Crofts, New York.\\nRevusky, S., Garcia, J. (1970). Learned associations over long delays. In G. Bower (Ed.),The\\nPsychology of Learning and Motivation, v. 4, pp. 1–84. Academic Press, Inc., New York.\\nReynolds, J. N. J., Wickens, J. R. (2002). Dopamine-dependent plasticity of corticostriatal\\nsynapses. Neural Networks, 15(4):507–521.\\nRing, M. B. (in preparation). Representing knowledge as forecasts (and state as knowledge).\\nRipley, B. D. (2007).Pattern Recognition and Neural Networks. Cambridge University Press.\\nRixner, S. (2004). Memory controller optimizations for web servers. InProceedings of the\\n37th annual IEEE/ACM International Symposium on Microarchitecture,p .3 5 5 – 3 6 6 .I E E E\\nComputer Society.\\nRobbins, H. (1952). Some aspects of the sequential design of experiments.Bulletin of the\\nAmerican Mathematical Society, 58:527–535.\\nRobertie, B. (1992). Carbon versus silicon: Matching wits with TD-Gammon.Inside Backgam-\\nmon, 2(2):14–22.\\nRomo, R., Schultz, W. (1990). Dopamine neurons of the monkey midbrain: Contingencies of\\nresponses to active touch during self-initiated arm movements.Journal of Neurophysiology,\\n63(3):592–624.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 528, 'page_label': '529'}, page_content='References 507\\nRosenblatt, F. (1962). Principles of Neurodynamics: Perceptrons and the Theory of Brain\\nMechanisms. Spartan Books, Washington, DC.\\nRoss, S. (1983). Introduction to Stochastic Dynamic Programming. Academic Press, New York.\\nRoss, T. (1933). Machines that think.Scientiﬁc American, 148(4):206–208.\\nRubinstein, R. Y. (1981).Simulation and the Monte Carlo Method. Wiley, New York.\\nRumelhart, D. E., Hinton, G. E., Williams, R. J. (1986). Learning internal representations by\\nerror propagation. In D. E. Rumelhart and J. L. McClelland (Eds.),Parallel Distributed Pro-\\ncessing: Explorations in the Microstructure of Cognition,v o l .I ,Foundations. Bradford/MIT\\nPress, Cambridge, MA.\\nRummery, G. A. (1995).Problem Solving with Reinforcement Learning. PhD thesis, University\\nof Cambridge.\\nRummery, G. A., Niranjan, M. (1994). On-line Q-learning using connectionist systems. Technical\\nReport CUED/F-INFENG/TR 166. Engineering Department, Cambridge University.\\nRuppert, D. (1988). E\\x00cient estimations from a slowly convergent Robbins-Monro process.\\nCornell University Operations Research and Industrial Engineering Technical Report No. 781.\\nRussell, S., Norvig, P. (2009).Artiﬁcial Intelligence: A Modern Approach, 3rd edition. Prentice-\\nHall, Englewood Cli↵s, NJ.\\nRusso, D. J., Van Roy, B., Kazerouni, A., Osband, I., Wen, Z. (2018). A tutorial on Thompson\\nsampling, Foundations and Trends in Machine Learning. ArXiv:1707.02038.\\nRust, J. (1996). Numerical dynamic programming in economics. In H. Amman, D. Kendrick, and\\nJ. Rust (Eds.),Handbook of Computational Economics, pp. 614–722. Elsevier, Amsterdam.\\nSaddoris, M. P., Cacciapaglia, F., Wightmman, R. M., Carelli, R. M. (2015). Di↵erential\\ndopamine release dynamics in the nucleus accumbens core and shell reveal complemen-\\ntary signals for error prediction and incentive motivation.The Journal of Neuroscience,\\n35(33):11572–11582.\\nSaksida, L. M., Raymond, S. M., Touretzky, D. S. (1997). Shaping robot behavior using principles\\nfrom instrumental conditioning.Robotics and Autonomous Systems, 22(3):231–249.\\nSamuel, A. L. (1959). Some studies in machine learning using the game of checkers.IBM\\nJournal on Research and Development, 3(3), 210–229.\\nSamuel, A. L. (1967). Some studies in machine learning using the game of checkers. II—Recent\\nprogress. IBM Journal on Research and Development, 11(6):601–617.\\nSchaal, S., Atkeson, C. G. (1994). Robot juggling: Implementation of memory-based learning.\\nIEEE Control Systems, 14(1):57–71.\\nSchmajuk, N. A. (2008). Computational models of classical conditioning. Scholarpedia,\\n3(3):1664.\\nSchmidhuber, J. (1991a). Curious model-building control systems. InProceedings of the IEEE\\nInternational Joint Conference on Neural Networks,p p .1 4 5 8 – 1 4 6 3 .I E E E .\\nSchmidhuber, J. (1991b). A possibility for implementing curiosity and boredom in model-building\\nneural controllers. In From Animals to Animats: Proceedings of the First International\\nConference on Simulation of Adaptive Behavior, pp. 222–227. MIT Press, Cambridge, MA.\\nSchmidhuber, J. (2015). Deep learning in neural networks: An overview.Neural Networks,\\n6:85–117.\\nSchmidhuber, J., Storck, J., Hochreiter, S. (1994). Reinforcement driven information acquisition\\nin nondeterministic environments. Technical report, Fakult¨ at f¨ ur Informatik, Technische\\nUniversit¨ at M¨ unchen, M¨ unchen, Germany.\\nSchraudolph, N. N. (1999). Local gain adaptation in stochastic gradient descent. InProceedings\\nof the International Conference on Artiﬁcial Neural Networks,p p .5 6 9 – 5 7 4 .I E E E ,L o n d o n .'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 529, 'page_label': '530'}, page_content='508 References\\nSchraudolph, N. N. (2002). Fast curvature matrix-vector products for second-order gradient\\ndescent. Neural Computation, 14(7):1723–1738.\\nSchraudolph, N. N., Yu, J., Aberdeen, D. (2006). Fast online policy gradient learning with SMD\\ngain vector adaptation. InAdvances in Neural Information Processing Systems,p p .1 1 8 5 –\\n1192.\\nSchulman, J., Chen, X., Abbeel, P. (2017). Equivalence between policy gradients and soft\\nQ-Learning. ArXiv:1704.06440.\\nSchultz, D. G., Melsa, J. L. (1967).State Functions and Linear Control Systems. McGraw-Hill,\\nNew York.\\nSchultz, W. (1998). Predictive reward signal of dopamine neurons.Journal of Neurophysiology,\\n80(1):1–27.\\nSchultz, W., Apicella, P., Ljungberg, T. (1993). Responses of monkey dopamine neurons to\\nreward and conditioned stimuli during successive steps of learning a delayed response task.\\nThe Journal of Neuroscience, 13(3):900–913.\\nSchultz, W., Dayan, P., Montague, P. R. (1997). A neural substrate of prediction and reward.\\nScience, 275(5306):1593–1598.\\nSchultz, W., Romo, R. (1990). Dopamine neurons of the monkey midbrain: contingencies of\\nresponses to stimuli eliciting immediate behavioral reactions.Journal of Neurophysiology,\\n63(3):607–624.\\nSchultz, W., Romo, R., Ljungberg, T., Mirenowicz, J., Hollerman, J. R., Dickinson, A. (1995).\\nReward-related signals carried by dopamine neurons. In J. C. Houk, J. L. Davis, and D. G.\\nBeiser (Eds.), Models of Information Processing in the Basal Ganglia,p p .2 3 3 – 2 4 8 . M I T\\nPress, Cambridge, MA.\\nSchwartz, A. (1993). A reinforcement learning method for maximizing undiscounted rewards. In\\nProceedings of the 10th International Conference on Machine Learning, pp. 298–305. Morgan\\nKaufmann.\\nSchweitzer, P. J., Seidmann, A. (1985). Generalized polynomial approximations in Markovian\\ndecision processes. Journal of Mathematical Analysis and Applications, 110(2):568–582.\\nSelfridge, O. G. (1978). Tracking and trailing: Adaptation in movement strategies. Technical\\nreport, Bolt Beranek and Newman, Inc. Unpublished report.\\nSelfridge, O. G. (1984). Some themes and primitives in ill-deﬁned systems. In O. G. Selfridge,\\nE. L. Rissland, and M. A. Arbib (Eds.),Adaptive Control of Ill-Deﬁned Systems,p p .2 1 – 2 6 .\\nPlenum Press, NY. Proceedings of the NATO Advanced Research Institute on Adaptive\\nControl of Ill-deﬁned Systems, NATO Conference Series II, Systems Science, Vol. 16.\\nSelfridge, O. J., Sutton, R. S., Barto, A. G. (1985). Training and tracking in robotics. In A. Joshi\\n(Ed.), Proceedings of the Ninth International Joint Conference on Artiﬁcial Intelligence,\\npp. 670–672. Morgan Kaufmann.\\nSeo, H., Barraclough, D., Lee, D. (2007). Dynamic signals related to choices and outcomes in\\nthe dorsolateral prefrontal cortex.Cerebral Cortex, 17(suppl 1):110–117.\\nSeung, H. S. (2003). Learning in spiking neural networks by reinforcement of stochastic synaptic\\ntransmission. Neuron, 40(6):1063–1073.\\nShah, A. (2012). Psychological and neuroscientiﬁc connections with reinforcement learning. In M.\\nWiering and M. van Otterlo (Eds.),Reinforcement Learning: State-of-the-Art,p p .5 0 7 – 5 3 7 .\\nSpringer-Verlag Berlin Heidelberg.\\nShannon, C. E. (1950). Programming a computer for playing chess.Philosophical Magazine and\\nJournal of Science, 41(314):256–275.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 530, 'page_label': '531'}, page_content='References 509\\nShannon, C. E. (1951). Presentation of a maze-solving machine. In H. V. Forester (Ed.),Cyber-\\nnetics. Transactions of the Eighth Conference, pp. 173–180. Josiah Macy Jr. Foundation.\\nShannon, C. E. (1952). “Theseus” maze-solving mouse. http://cyberneticzoo.com/mazesolvers/1952—\\ntheseus-maze-solving-mouse—claude-shannon-american/.\\nShelton, C. R. (2001).Importance Sampling for Reinforcement Learning with Multiple Objectives.\\nPhD thesis, Massachusetts Institute of Technology, Cambridge MA.\\nShepard, D. (1968). A two-dimensional interpolation function for irregularly-spaced data. In\\nProceedings of the 23rd ACM National Conference, pp. 517–524. ACM, New York.\\nSherman, J., Morrison, W. J. (1949). Adjustment of an inverse matrix corresponding to changes\\nin the elements of a given column or a given row of the original matrix (abstract).Annals of\\nMathematical Statistics, 20(4):621.\\nShewchuk, J., Dean, T. (1990). Towards learning time-varying functions with high input\\ndimensionality. In Proceedings of the Fifth IEEE International Symposium on Intelligent\\nControl, pp. 383–388. IEEE Computer Society Press, Los Alamitos, CA.\\nShimansky, Y. P. (2009). Biologically plausible learning in neural networks: a lesson from\\nbacterial chemotaxis. Biological Cybernetics, 101(5-6):379–385.\\nSi, J., Barto, A., Powell, W., Wunsch, D. (Eds.) (2004).Handbook of Learning and Approximate\\nDynamic Programming.J o h nW i l e ya n dS o n s .\\nSilver, D. (2009). Reinforcement Learning and Simulation Based Search in the Game of Go.\\nPhD thesis, University of Alberta, Edmonton.\\nSilver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser,\\nJ., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J.,\\nKalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel, T.,\\nHassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search.\\nNature, 529(7587):484–489.\\nSilver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M. (2014). Deterministic\\npolicy gradient algorithms. InProceedings of the 31st International Conference on Machine\\nLearning,p p .3 8 7 – 3 9 5 .\\nSilver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T.,\\nBaker, L., Lai, M., Bolton, A., Chen, Y., Lillicrap, L., Hui, F., Sifre, L., van den Driessche,\\nG., Graepel, T., Hassibis, D. (2017a). Mastering the game of Go without human knowledge.\\nNature, 550(7676):354–359.\\nSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L.,\\nKumaran, D., Graepel, T., Lillicrap, T., Simoyan, K., Hassibis, D. (2017b). Mastering chess\\nand shogi by self-play with a general reinforcement learning algorithm. ArXiv:1712.01815.\\nS¸im¸ sek,¨O., Alg´ orta, S., Kothiyal, A. (2016). Why most decisions are easy in tetris—And perhaps\\nin other sequential decision problems, as well. InProceedings of the 33rd International\\nConference on Machine Learning,p p .1 7 5 7 - 1 7 6 5 .\\nSimon, H. (2000). Lecture at the Earthware Symposium, Carnegie Mellon University. https\\n://www.youtube.com/watch?v=EZhyi-8DBjc.\\nSingh, S. P. (1992a). Reinforcement learning with a hierarchy of abstract models. InProceedings\\nof the Tenth National Conference on Artiﬁcial Intelligence, pp. 202–207. AAAI/MIT Press,\\nMenlo Park, CA.\\nSingh, S. P. (1992b). Scaling reinforcement learning algorithms by learning variable temporal\\nresolution models. In Proceedings of the 9th International Workshop on Machine Learning,\\npp. 406–415. Morgan Kaufmann.\\nSingh, S. P. (1993).Learning to Solve Markovian Decision Processes. PhD thesis, University of\\nMassachusetts, Amherst.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 531, 'page_label': '532'}, page_content='510 References\\nSingh, S. P. (Ed.) (2002). Special double issue on reinforcement learning,Machine Learning,\\n49(2-3).\\nSingh, S., Barto, A. G., Chentanez, N. (2005). Intrinsically motivated reinforcement learning.\\nIn Advances in Neural Information Processing Systems 17, pp. 1281–1288. MIT Press,\\nCambridge, MA.\\nSingh, S. P., Bertsekas, D. (1997). Reinforcement learning for dynamic channel allocation\\nin cellular telephone systems. In Advances in Neural Information Processing Systems 9,\\npp. 974–980. MIT Press, Cambridge, MA.\\nSingh, S. P., Jaakkola, T., Jordan, M. I. (1994). Learning without state-estimation in partially\\nobservable Markovian decision problems. InProceedings of the 11th International Conference\\non Machine Learning, pp. 284–292. Morgan Kaufmann.\\nSingh, S., Jaakkola, T., Littman, M. L., Szepesv´ ari, C. (2000). Convergence results for single-step\\non-policy reinforcement-learning algorithms.Machine Learning, 38(3):287–308.\\nSingh, S. P., Jaakkola, T., Jordan, M. I. (1995). Reinforcement learning with soft state\\naggregation. In Advances in Neural Information Processing Systems 7,p p .3 5 9 – 3 6 8 .M I T\\nPress, Cambridge, MA.\\nSingh, S., Lewis, R. L., Barto, A. G. (2009). Where do rewards come from? In N. Taatgen\\nand H. van Rijn (Eds.),Proceedings of the 31st Annual Conference of the Cognitive Science\\nSociety,p p .2 6 0 1 – 2 6 0 6 .C o g n i t i v eS c i e n c eS o c i e t y .\\nSingh, S., Lewis, R. L., Barto, A. G., Sorg, J. (2010). Intrinsically motivated reinforcement\\nlearning: An evolutionary perspective.IEEE Transactions on Autonomous Mental Develop-\\nment, 2(2):70–82. Special issue on Active Learning and Intrinsically Motivated Exploration\\nin Robots: Advances and Challenges.\\nSingh, S. P., Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces.\\nMachine Learning, 22(1-3):123–158.\\nSkinner, B. F. (1938).The Behavior of Organisms: An Experimental Analysis. Appleton-Century,\\nNew York.\\nSkinner, B. F. (1958). Reinforcement today.American Psychologist, 13(3):94–99.\\nSkinner, B. F. (1963). Operant behavior.American Psychologist, 18(8):503–515.\\nSofge, D. A., White, D. A. (1992). Applied learning: Optimal control for manufacturing. In\\nD. A. White and D. A. Sofge (Eds.),Handbook of Intelligent Control: Neural, Fuzzy, and\\nAdaptive Approaches, pp. 259–281. Van Nostrand Reinhold, New York.\\nSorg, J. D. (2011). The Optimal Reward Problem:Designing E↵ective Reward for Bounded\\nAgents. PhD thesis, University of Michigan, Ann Arbor.\\nSorg, J., Lewis, R. L., Singh, S. P. (2010). Reward design via online gradient ascent. InAdvances\\nin Neural Information Processing Systems 23, pp. 2190–2198. Curran Associates, Inc.\\nSorg, J., Singh, S. (2010). Linear options. InProceedings of the 9th International Conference on\\nAutonomous Agents and Multiagent Systems,p p .3 1 – 3 8 .\\nSorg, J., Singh, S., Lewis, R. (2010). Internal rewards mitigate agent boundedness. InProceedings\\nof the 27th International Conference on Machine Learning,p p .1 0 0 7 – 1 0 1 4 .\\nSpence, K. W. (1947). The role of secondary reinforcement in delayed reward learning.Psycho-\\nlogical Review, 54(1):1–8.\\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R. (2014). Dropout:\\nA simple way to prevent neural networks from overﬁtting.Journal of Machine Learning\\nResearch, 15(1):1929–1958.\\nStaddon, J. E. R. (1983).Adaptive Behavior and Learning. Cambridge University Press.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 532, 'page_label': '533'}, page_content='References 511\\nStanﬁll, C., Waltz, D. (1986). Toward memory-based reasoning.Communications of the ACM,\\n29(12):1213–1228.\\nSteinberg, E. E., Keiﬂin, R., Boivin, J. R., Witten, I. B., Deisseroth, K., Janak, P. H. (2013). A\\ncausal link between prediction errors, dopamine neurons and learning.Nature Neuroscience,\\n16(7):966–973.\\nSterling, P., Laughlin, S. (2015).Principles of Neural Design. MIT Press, Cambridge, MA.\\nSternberg, S. (1963). Stochastic learning theory. In: Handbook of Mathematical Psychology,\\nVolume II, R. D. Luce, R. R. Bush, and E. Galanter (Eds.). John Wiley & Sons.\\nSugiyama, M., Hachiya, H., Morimura, T. (2013).Statistical Reinforcement Learning: Modern\\nMachine Learning Approaches.C h a p m a n&H a l l / C R C .\\nSuri, R. E., Bargas, J., Arbib, M. A. (2001). Modeling functions of striatal dopamine modulation\\nin learning and planning.Neuroscience, 103(1):65–85.\\nSuri, R. E., Schultz, W. (1998). Learning of sequential movements by neural network model\\nwith dopamine-like reinforcement signal.Experimental Brain Research, 121(3):350–354.\\nSuri, R. E., Schultz, W. (1999). A neural network model with dopamine-like reinforcement\\nsignal that learns a spatial delayed response task.Neuroscience, 91(3):871–890.\\nSutton, R. S. (1978a). Learning theory support for a single channel theory of the brain.\\nUnpublished report.\\nSutton, R. S. (1978b). Single channel theory: A neuronal theory of learning.Brain Theory\\nNewsletter, 4:72–75. Center for Systems Neuroscience, University of Massachusetts, Amherst,\\nMA.\\nSutton, R. S. (1978c).Au n i ﬁ e dt h e o r yo fe x p e c t a t i o ni nc l a s s i c a la n di n s t r u m e n t a lc o n d i t i o n i n g .\\nBachelors thesis, Stanford University.\\nSutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning.P h D t h e s i s ,\\nUniversity of Massachusetts, Amherst.\\nSutton, R. S. (1988). Learning to predict by the method of temporal di↵erences.Machine\\nLearning, 3(1):9–44 (important erratum p. 377).\\nSutton, R. S. (1990). Integrated architectures for learning, planning, and reacting based on\\napproximating dynamic programming. InProceedings of the 7th International Workshop on\\nMachine Learning, pp. 216–224. Morgan Kaufmann.\\nSutton, R. S. (1991a). Dyna, an integrated architecture for learning, planning, and reacting.\\nSIGART Bulletin, 2(4):160–163. ACM, New York.\\nSutton, R. S. (1991b). Planning by incremental dynamic programming. InProceedings of the\\n8th International Workshop on Machine Learning, pp. 353–357. Morgan Kaufmann.\\nSutton, R. S. (Ed.) (1992a).Reinforcement Learning. Kluwer Academic Press. Reprinting of a\\nspecial double issue on reinforcement learning,Machine Learning, 8(3-4).\\nSutton, R. S. (1992b). Adapting bias by gradient descent: An incremental version of delta-bar-\\ndelta. Proceedings of the Tenth National Conference on Artiﬁcial Intelligence,p p .1 7 1 – 1 7 6 ,\\nMIT Press.\\nSutton, R. S. (1992c). Gain adaptation beats least squares?Proceedings of the Seventh Yale\\nWorkshop on Adaptive and Learning Systems, pp. 161–166, Yale University, New Haven, CT.\\nSutton, R. S. (1995a). TD models: Modeling the world at a mixture of time scales. In\\nProceedings of the 12th International Conference on Machine Learning, pp. 531–539. Morgan\\nKaufmann.\\nSutton, R. S. (1995b). On the virtues of linear learning and trajectory distributions. In\\nProceedings of the Workshop on Value Function Approximationat The 12th International\\nConference on Machine Learning.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 533, 'page_label': '534'}, page_content='512 References\\nSutton, R. S. (1996). Generalization in reinforcement learning: Successful examples using sparse\\ncoarse coding. In Advances in Neural Information Processing Systems 8,p p .1 0 3 8 – 1 0 4 4 .\\nMIT Press, Cambridge, MA.\\nSutton, R. S. (2009). The grand challenge of predictive empirical abstract knowledge.Working\\nNotes of the IJCAI-09 Workshop on Grand Challenges for Reasoning from Experiences.\\nSutton, R. S. (2015a) Introduction to reinforcement learning with function approximation.\\nTutorial at the Conference on Neural Information Processing Systems, Montreal, December\\n7, 2015.\\nSutton, R. S. (2015b) True online Emphatic TD(\\x00): Quick reference and implementation guide.\\nArXiv:1507.07147. Code is available in Python and C++ by downloading the source ﬁles of\\nthis arXiv paper as a zip archive.\\nSutton, R. S., Barto, A. G. (1981a). Toward a modern theory of adaptive networks: Expectation\\nand prediction. Psychological Review, 88(2):135–170.\\nSutton, R. S., Barto, A. G. (1981b). An adaptive network that constructs and uses an internal\\nmodel of its world.Cognition and Brain Theory, 3:217–246.\\nSutton, R. S., Barto, A. G. (1987). A temporal-di↵erence model of classical conditioning. In\\nProceedings of the Ninth Annual Conference of the Cognitive Science Society,p p .3 5 5 - 3 7 8 .\\nErlbaum, Hillsdale, NJ.\\nSutton, R. S., Barto, A. G. (1990). Time-derivative models of Pavlovian reinforcement. In\\nM. Gabriel and J. Moore (Eds.),Learning and Computational Neuroscience: Foundations of\\nAdaptive Networks, pp. 497–537. MIT Press, Cambridge, MA.\\nSutton, R. S., Maei, H. R., Precup, D., Bhatnagar, S., Silver, D., Szepesv´ ari, Cs., Wiewiora, E.\\n(2009a). Fast gradient-descent methods for temporal-di↵erence learning with linear function\\napproximation. In Proceedings of the 26th International Conference on Machine Learning,\\npp. 993–1000. ACM, New York.\\nSutton, R. S., Szepesv´ ari, Cs., Maei, H. R. (2009b). A convergentO(d2) temporal-di↵erence\\nalgorithm for o↵-policy learning with linear function approximation. InAdvances in Neural\\nInformation Processing Systems 21, pp. 1609–1616. Curran Associates, Inc.\\nSutton, R. S., Mahmood, A. R., Precup, D., van Hasselt, H. (2014). A new Q(\\x00) with interim\\nforward view and Monte Carlo equivalence. InProceedings of the International Conference\\non Machine Learning, 31. JMLR W&CP 32(2).\\nSutton, R. S., Mahmood, A. R., White, M. (2016). An emphatic approach to the problem of\\no↵-policy temporal-di↵erence learning.Journal of Machine Learning Research, 17(73):1–29.\\nSutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y. (2000). Policy gradient methods\\nfor reinforcement learning with function approximation. InAdvances in Neural Information\\nProcessing Systems 12, pp. 1057–1063. MIT Press, Cambridge, MA.\\nSutton, R. S., Modayil, J., Delp, M., Degris, T., Pilarski, P. M., White, A., Precup, D. (2011).\\nHorde: A scalable real-time architecture for learning knowledge from unsupervised senso-\\nrimotor interaction. InProceedings of the Tenth International Conference on Autonomous\\nAgents and Multiagent Systems,p p .7 6 1 – 7 6 8 ,T a i p e i ,T a i w a n .\\nSutton, R. S., Pinette, B. (1985). The learning of world models by connectionist networks. In\\nProceedings of the Seventh Annual Conference of the Cognitive Science Society, pp. 54–64.\\nSutton, R. S., Precup, D., Singh, S. (1999). Between MDPs and semi-MDPs: A framework for\\ntemporal abstraction in reinforcement learning.Artiﬁcial Intelligence, 112(1-2):181–211.\\nSutton, R. S., Rafols, E., Koop, A. (2006). Temporal abstraction in temporal-di↵erence networks.\\nIn Advances in neural information processing systems,p p .1 3 1 3 – 1 3 2 0 .\\nSutton, R. S., Singh, S. P., McAllester, D. A. (2000). Comparing policy-gradient algorithms.\\nUnpublished manuscript.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 534, 'page_label': '535'}, page_content='References 513\\nSutton, R. S., Szepesv´ ari, Cs., Geramifard, A., Bowling, M., (2008). Dyna-style planning with\\nlinear function approximation and prioritized sweeping. InProceedings of the 24th Conference\\non Uncertainty in Artiﬁcial Intelligence,p p .5 2 8 – 5 3 6 .\\nSutton, R. S., Tanner, B. (2005). Temporal-di↵erence networks. In Advances in Neural\\nInformation Processing Systems 17,p .1 3 7 7 – 1 3 8 4 .\\nSzepesv´ ari, Cs. (2010). Algorithms for reinforcement learning. InSynthesis Lectures on Artiﬁcial\\nIntelligence and Machine Learning, 4(1):1–103. Morgan and Claypool.\\nSzita, I. (2012). Reinforcement learning in games. In M. Wiering and M. van Otterlo (Eds.),\\nReinforcement Learning: State-of-the-Art, pp. 539–577. Springer-Verlag Berlin Heidelberg.\\nTadepalli, P., Ok, D. (1994). H-learning: A reinforcement learning method to optimize\\nundiscounted average reward. Technical Report 94-30-01. Oregon State University, Computer\\nScience Department, Corvallis.\\nTadepalli, P., Ok, D. (1996). Scaling up average reward reinforcement learning by approximating\\nthe domain models and the value function. InProceedings of the 13th International Conference\\non Machine Learning,p p .4 7 1 – 4 7 9 .\\nTakahashi, Y., Schoenbaum, G., and Niv, Y. (2008). Silencing the critics: Understanding the\\ne↵ects of cocaine sensitization on dorsolateral and ventral striatum in the context of an\\nactor/critic model. Frontiers in Neuroscience, 2(1):86–99.\\nTambe, M., Newell, A., Rosenbloom, P. S. (1990). The problem of expensive chunks and its\\nsolution by restricting expressiveness.Machine Learning, 5(3):299–348.\\nTan, M. (1991). Learning a cost-sensitive internal representation for reinforcement learning. In\\nL. A. Birnbaum and G. C. Collins (Eds.),Proceedings of the 8th International Workshop on\\nMachine Learning, pp. 358–362. Morgan Kaufmann.\\nTanner, B. (2006). Temporal-Di↵erence Networks. MSc thesis, University of Alberta.\\nTaylor, G., Parr, R. (2009). Kernelized value function approximation for reinforcement learning.\\nIn Proceedings of the 26th International Conference on Machine Learning,p p .1 0 1 7 – 1 0 2 4 .\\nACM, New York.\\nTaylor, M. E., Stone, P. (2009). Transfer learning for reinforcement learning domains: A survey.\\nJournal of Machine Learning Research, 10:1633–1685.\\nTesauro, G. (1986). Simple neural models of classical conditioning.Biological Cybernetics,\\n55(2-3):187–200.\\nTesauro, G. (1992). Practical issues in temporal di↵erence learning. Machine Learning,\\n8(3-4):257–277.\\nTesauro, G. (1994). TD-Gammon, a self-teaching backgammon program, achieves master-level\\nplay. Neural Computation, 6(2):215–219.\\nTesauro, G. (1995). Temporal di↵erence learning and TD-Gammon.Communications of the\\nACM, 38(3):58–68.\\nTesauro, G. (2002). Programming backgammon using self-teaching neural nets. Artiﬁcial\\nIntelligence, 134(1-2):181–199.\\nTesauro, G., Galperin, G. R. (1997). On-line policy improvement using Monte-Carlo search. In\\nAdvances in Neural Information Processing Systems 9, pp. 1068–1074. MIT Press, Cambridge,\\nMA.\\nTesauro, G., Gondek, D. C., Lechner, J., Fan, J., Prager, J. M. (2012). Simulation, learning,\\nand optimization techniques in Watson’s game strategies.IBM Journal of Research and\\nDevelopment, 56(3-4):16–1–16–11.\\nTesauro, G., Gondek, D. C., Lenchner, J., Fan, J., Prager, J. M. (2013). Analysis of Watson’s\\nstrategies for playing Jeopardy!Journal of Artiﬁcial Intelligence Research, 47:205–251.\\nTham, C. K. (1994).Modular On-Line Function Approximation for Scaling up Reinforcement\\nLearning. PhD thesis, University of Cambridge.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 535, 'page_label': '536'}, page_content='514 References\\nThathachar, M. A. L., Sastry, P. S. (1985). A new approach to the design of reinforcement\\nschemes for learning automata. IEEE Transactions on Systems, Man, and Cybernetics,\\n15(1):168–175.\\nThathachar, M., Sastry, P. S. (2002). Varieties of learning automata: an overview.IEEE\\nTransactions on Systems, Man, and Cybernetics, Part B: Cybernetics, 36(6):711–722.\\nThathachar, M., Sastry, P. S. (2011).Networks of Learning Automata: Techniques for Online\\nStochastic Optimization. Springer Science & Business Media.\\nTheocharous, G., Thomas, P. S., Ghavamzadeh, M. (2015). Personalized ad recommendation for\\nlife-time value optimization guarantees. InProceedings of the Twenty-Fourth International\\nJoint Conference on Artiﬁcial Intelligence. AAAI Press, Palo Alto, CA.\\nThistlethwaite, D. (1951). A critical review of latent learning and related experiments.Psycho-\\nlogical Bulletin, 48(2):97–129.\\nThomas, P. S. (2014). Bias in natural actor–critic algorithms. In Proceedings of the 31st\\nInternational Conference on Machine Learning, JMLR W&CP 32(1), pp. 441–448.\\nThomas, P. S. (2015).Safe Reinforcement Learning. PhD thesis, University of Massachusetts,\\nAmherst.\\nThomas, P. S., Brunskill, E. (2017). Policy gradient methods for reinforcement learning with\\nfunction approximation and action-dependent baselines. ArXiv:1706.06643.\\nThomas, P. S., Theocharous, G., Ghavamzadeh, M. (2015). High-conﬁdence o↵-policy evaluation.\\nIn Proceedings of the Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence,p p .3 0 0 0 – 3 0 0 6 .\\nAAAI Press, Menlo Park, CA.\\nThompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in\\nview of the evidence of two samples.Biometrika, 25(3-4):285–294.\\nThompson, W. R. (1934). On the theory of apportionment.American Journal of Mathematics,\\n57:4 5 0 – 4 5 7 .\\nThon, M. (2017). Spectral Learning of Sequential Systems.PhD thesis, Jacobs University\\nBremen.\\nThon, M., Jaeger, H. (2015). Links between multiplicity automata, observable operator models\\nand predictive state representations: a uniﬁed learning framework.The Journal of Machine\\nLearning Research, 16(1):103–147.\\nThorndike, E. L. (1898). Animal intelligence: An experimental study of the associative processes\\nin animals. The Psychological Review, Series of Monograph Supplements,I I ( 4 ) .\\nThorndike, E. L. (1911).Animal Intelligence. Hafner, Darien, CT.\\nThorp, E. O. (1966). Beat the Dealer: A Winning Strategy for the Game of Twenty-One.\\nRandom House, New York.\\nTian, T. (in preparation)An Empirical Study of Sliding-Step Methods in Temporal Di↵erence\\nLearning. M.Sc thesis, University of Alberta, Edmonton.\\nTieleman, T., Hinton, G. (2012). Lecture 6.5–RMSProp. COURSERA: Neural networks for\\nmachine learning 4.2:26–31.\\nTolman, E. C. (1932).Purposive Behavior in Animals and Men. Century, New York.\\nTolman, E. C. (1948). Cognitive maps in rats and men.Psychological Review, 55(4):189–208.\\nTsai, H.-S., Zhang, F., Adamantidis, A., Stuber, G. D., Bonci, A., de Lecea, L., Deisseroth,\\nK. (2009). Phasic ﬁring in dopaminergic neurons is su\\x00cient for behavioral conditioning.\\nScience, 324(5930):1080–1084.\\nTsetlin, M. L. (1973).Automaton Theory and Modeling of Biological Systems. Academic Press,\\nNew York.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 536, 'page_label': '537'}, page_content='References 515\\nTsitsiklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. Machine\\nLearning, 16(3):185–202.\\nTsitsiklis, J. N. (2002). On the convergence of optimistic policy iteration.Journal of Machine\\nLearning Research, 3:59–72.\\nTsitsiklis, J. N., Van Roy, B. (1996). Feature-based methods for large scale dynamic programming.\\nMachine Learning, 22(1-3):59–94.\\nTsitsiklis, J. N., Van Roy, B. (1997). An analysis of temporal-di↵erence learning with function\\napproximation. IEEE Transactions on Automatic Control, 42(5):674–690.\\nTsitsiklis, J. N., Van Roy, B. (1999). Average cost temporal-di↵erence learning.Automatica,\\n35(11):1799–1808.\\nTuring, A. M. (1948). Intelligent machinery. In B. Jack Copeland (Ed.) (2004),The Essential\\nTuring, pp. 410–432. Oxford University Press, Oxford.\\nUngar, L. H. (1990). A bioreactor benchmark for adaptive network-based process control. In\\nW. T. Miller, R. S. Sutton, and P. J. Werbos (Eds.),Neural Networks for Control, pp. 387–402.\\nMIT Press, Cambridge, MA.\\nUnnikrishnan, K. P., Venugopal, K. P. (1994). Alopex: A correlation-based learning algorithm\\nfor feedforward and recurrent neural networks.N eural Computation, 6(3): 469–490.\\nUrbanczik, R., Senn, W. (2009). Reinforcement learning in populations of spiking neurons.\\nNature neuroscience, 12(3):250–252.\\nUrbanowicz, R. J., Moore, J. H. (2009). Learning classiﬁer systems: A complete introduction,\\nreview, and roadmap.Journal of Artiﬁcial Evolution and Applications.1 0 . 1 1 5 5 / 2 0 0 9 / 7 3 6 3 9 8 .\\nValentin, V. V., Dickinson, A., O’Doherty, J. P. (2007). Determining the neural substrates of\\ngoal-directed learning in the human brain.The Journal of Neuroscience, 27(15):4019–4026.\\nvan Hasselt, H. (2010). Double Q-learning. In Advances in Neural Information Processing\\nSystems 23, pp. 2613–2621. Curran Associates, Inc.\\nvan Hasselt, H. (2011).Insights in Reinforcement Learning: Formal Analysis and Empirical\\nEvaluation of Temporal-di↵erence Learning. SIKS dissertation series number 2011-04.\\nvan Hasselt, H. (2012). Reinforcement learning in continuous state and action spaces. In M.\\nWiering and M. van Otterlo (Eds.),Reinforcement Learning: State-of-the-Art,p p .2 0 7 – 2 5 1 .\\nSpringer-Verlag Berlin Heidelberg.\\nvan Hasselt, H., Sutton, R. S. (2015). Learning to predict independent of span. ArXiv:1508.04582.\\nVan Roy, B., Bertsekas, D. P., Lee, Y., Tsitsiklis, J. N. (1997). A neuro-dynamic programming\\napproach to retailer inventory management. InProceedings of the 36th IEEE Conference on\\nDecision and Control,V o l .4 ,p p .4 0 5 2 – 4 0 5 7 .\\nvan Seijen, H. (2011). Reinforcement Learning under Space and Time Constraints. University of\\nAmsterdam PhD thesis. Hague: TNO.\\nvan Seijen, H. (2016). E↵ective multi-step temporal-di↵erence learning for non-linear function\\napproximation. ArXiv:1608.05151.\\nvan Seijen, H., Sutton, R. S. (2013). E\\x00cient planning in MDPs by small backups. In:Proceedings\\nof the 30th International Conference on Machine Learning,p p .3 6 1 – 3 6 9 .\\nvan Seijen, H., Sutton, R. S. (2014). True online TD(\\x00). In Proceedings of the 31st International\\nConference on Machine Learning,p p .6 9 2 – 7 0 0 .J M L RW & C P3 2 ( 1 ) ,\\nvan Seijen, H., Mahmood, A. R., Pilarski, P. M., Machado, M. C., Sutton, R. S. (2016). True\\nonline temporal-di↵erence learning.Journal of Machine Learning Research, 17(145):1–40.\\nvan Seijen, H., Van Hasselt, H., Whiteson, S., Wiering, M. (2009). A theoretical and empirical\\nanalysis of Expected Sarsa. InIEEE Symposium on Adaptive Dynamic Programming and\\nReinforcement Learning,p p .1 7 7 – 1 8 4 .'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 537, 'page_label': '538'}, page_content='516 References\\nvan Seijen, H., Whiteson, S., van Hasselt, H., Wiering, M. (2011). Exploiting best-match\\nequations for e\\x00cient reinforcement learning.Journal of Machine Learning Research 12:2045–\\n2094.\\nVarga, R. S. (1962).Matrix Iterative Analysis. Englewood Cli↵s, NJ: Prentice-Hall.\\nVasilaki, E., Fr´ emaux, N., Urbanczik, R., Senn, W., Gerstner, W. (2009). Spike-based rein-\\nforcement learning in continuous state and action space: when policy gradient methods fail.\\nPLoS Computational Biology, 5(12).\\nViswanathan, R., Narendra, K. S. (1974). Games of stochastic automata.IEEE Transactions\\non Systems, Man, and Cybernetics, 4(1):131–135.\\nWagner, A. R. (2008). Evolution of an elemental theory of Pavlovian conditioning.Learning &\\nBehavior, 36(3):253–265.\\nWalter, W. G. (1950). An imitation of life.Scientiﬁc American, 182(5):42–45.\\nWalter, W. G. (1951). A machine that learns.Scientiﬁc American, 185(2):60–63.\\nWaltz, M. D., Fu, K. S. (1965). A heuristic approach to reinforcement learning control systems.\\nIEEE Transactions on Automatic Control, 10(4):390–398.\\nWatkins, C. J. C. H. (1989). Learning from Delayed Rewards. PhD thesis, University of\\nCambridge.\\nWatkins, C. J. C. H., Dayan, P. (1992). Q-learning.Machine Learning, 8(3-4):279–292.\\nWerbos, P. J. (1977). Advanced forecasting methods for global crisis warning and models of\\nintelligence. General Systems Yearbook, 22(12):25–38.\\nWerbos, P. J. (1982). Applications of advances in nonlinear sensitivity analysis. In R. F. Drenick\\nand F. Kozin (Eds.),System Modeling and Optimization, pp. 762–770. Springer-Verlag.\\nWerbos, P. J. (1987). Building and understanding adaptive systems: A statistical/numerical\\napproach to factory automation and brain research.IEEE Transactions on Systems, Man,\\nand Cybernetics, 17(1):7–20.\\nWerbos, P. J. (1988). Generalization of back propagation with applications to a recurrent gas\\nmarket model. Neural Networks, 1(4):339–356.\\nWerbos, P. J. (1989). Neural networks for control and system identiﬁcation. InProceedings of\\nthe 28th Conference on Decision and Control, pp. 260–265. IEEE Control Systems Society.\\nWerbos, P. J. (1992). Approximate dynamic programming for real-time control and neural\\nmodeling. In D. A. White and D. A. Sofge (Eds.),Handbook of Intelligent Control: Neural,\\nFuzzy, and Adaptive Approaches, pp. 493–525. Van Nostrand Reinhold, New York.\\nWerbos, P. J. (1994). The Roots of Backpropagation: From Ordered Derivatives to Neural\\nNetworks and Political Forecasting(Vol. 1). John Wiley and Sons.\\nWiering, M., Van Otterlo, M. (2012).Reinforcement Learning: State-of-the-Art. Springer-Verlag\\nBerlin Heidelberg.\\nWhite, A. (2015).Developing a Predictive Approach to Knowledge. PhD thesis, University of\\nAlberta, Edmonton.\\nWhite, D. J. (1969).Dynamic Programming. Holden-Day, San Francisco.\\nWhite, D. J. (1985). Real applications of Markov decision processes.Interfaces, 15(6):73–83.\\nWhite, D. J. (1988). Further real applications of Markov decision processes. Interfaces,\\n18(5):55–61.\\nWhite, D. J. (1993). A survey of applications of Markov decision processes.Journal of the\\nOperational Research Society, 44(11):1073–1096.\\nWhite, A., White, M. (2016). Investigating practical linear temporal di↵erence learning. In\\nProceedings of the 2016 International Conference on Autonomous Agents and Multiagent\\nSystems,p p .4 9 4 – 5 0 2 .'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 538, 'page_label': '539'}, page_content='References 517\\nWhitehead, S. D., Ballard, D. H. (1991). Learning to perceive and act by trial and error.\\nMachine Learning, 7(1):45–83.\\nWhitt, W. (1978). Approximations of dynamic programs I.Mathematics of Operations Research,\\n3(3):231–243.\\nWhittle, P. (1982).Optimization over Time, vol. 1. Wiley, New York.\\nWhittle, P. (1983).Optimization over Time, vol. 2. Wiley, New York.\\nWickens, J., K¨ otter, R. (1995). Cellular models of reinforcement. In J. C. Houk, J. L. Davis and\\nD. G. Beiser (Eds.),Models of Information Processing in the Basal Ganglia,p p .1 8 7 – 2 1 4 .\\nMIT Press, Cambridge, MA.\\nWidrow, B., Gupta, N. K., Maitra, S. (1973). Punish/reward: Learning with a critic in adaptive\\nthreshold systems. IEEE Transactions on Systems, Man, and Cybernetics, 3(5):455–465.\\nWidrow, B., Ho↵, M. E. (1960). Adaptive switching circuits. In1960 WESCON Convention\\nRecord Part IV, pp. 96–104. Institute of Radio Engineers, New York. Reprinted in J. A.\\nAnderson and E. Rosenfeld,Neurocomputing: Foundations of Research,p p .1 2 6 – 1 3 4 .M I T\\nPress, Cambridge, MA, 1988.\\nWidrow, B., Smith, F. W. (1964). Pattern-recognizing control systems. In J. T. Tou and\\nR. H. Wilcox (Eds.),Computer and Information Sciences, pp. 288–317. Spartan, Washington,\\nDC.\\nWidrow, B., Stearns, S. D. (1985).Adaptive Signal Processing. Prentice-Hall, Englewood Cli↵s,\\nNJ.\\nWiener, N. (1964). God and Golem, Inc: A Comment on Certain Points where Cybernetics\\nImpinges on Religion. MIT Press, Cambridge, MA.\\nWiewiora, E. (2003). Potential-based shaping and Q-value initialization are equivalent.Journal\\nof Artiﬁcial Intelligence Research, 19:205–208.\\nWilliams, R. J. (1986). Reinforcement learning in connectionist networks: A mathematical\\nanalysis. Technical Report ICS 8605. Institute for Cognitive Science, University of California\\nat San Diego, La Jolla.\\nWilliams, R. J. (1987). Reinforcement-learning connectionist systems. Technical Report\\nNU-CCS-87-3. College of Computer Science, Northeastern University, Boston.\\nWilliams, R. J. (1988). On the use of backpropagation in associative reinforcement learning.\\nIn Proceedings of the IEEE International Conference on Neural Networks,p p .I - 2 6 3 – I - 2 7 0 .\\nIEEE San Diego section and IEEE TAB Neural Network Committee.\\nWilliams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist\\nreinforcement learning. Machine Learning, 8(3-4):229–256.\\nWilliams, R. J., Baird, L. C. (1990). A mathematical analysis of actor–critic architectures for\\nlearning optimal controls through incremental dynamic programming. InProceedings of the\\nSixth Yale Workshop on Adaptive and Learning Systems, pp. 96–101. Center for Systems\\nScience, Dunham Laboratory, Yale University, New Haven.\\nWilson, R. C., Takahashi, Y. K., Schoenbaum, G., Niv, Y. (2014). Orbitofrontal cortex as a\\ncognitive map of task space.Neuron, 81(2):267–279.\\nWilson, S. W. (1994). ZCS, A zeroth order classiﬁer system. Evolutionary Computation,\\n2(1):1–18.\\nWise, R. A. (2004). Dopamine, learning, and motivation. Nature Reviews Neuroscience,\\n5(6):1–12.\\nWitten, I. H. (1976a). Learning to Control. University of Essex PhD thesis.\\nWitten, I. H. (1976b). The apparent conﬂict between estimation and control—A survey of the\\ntwo-armed problem. Journal of the Franklin Institute, 301(1-2):161–189.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 539, 'page_label': '540'}, page_content='518 References\\nWitten, I. H. (1977). An adaptive optimal controller for discrete-time Markov environments.\\nInformation and Control, 34(4):286–295.\\nWitten, I. H., Corbin, M. J. (1973). Human operators and automatic adaptive controllers: A\\ncomparative study on a particular control task.International Journal of Man–Machine\\nStudies, 5(1):75–104.\\nWoodbury, T., Dunn, C., and Valasek, J. (2014). Autonomous soaring using reinforcement\\nlearning for trajectory generation. In52nd Aerospace Sciences Meeting,p .0 9 9 0 .\\nWoodworth, R. S. (1938).Experimental Psychology. New York: Henry Holt and Company.\\nXie, X., Seung, H. S. (2004). Learning in neural networks by reinforcement of irregular spiking.\\nPhysical Review E, 69(4):041909.\\nXu, X., Xie, T., Hu, D., Lu, X. (2005). Kernel least-squares temporal di↵erence learning.\\nInternational Journal of Information Technology, 11(9):54–63.\\nYagishita, S., Hayashi-Takagi, A., Ellis-Davies, G. C. R., Urakubo, H., Ishii, S., Kasai, H. (2014).\\nA critical time window for dopamine actions on the structural plasticity of dendritic spines.\\nScience, 345(6204):1616–1619.\\nYee, R. C., Saxena, S., Utgo↵, P. E., Barto, A. G. (1990). Explaining temporal di↵erences to\\ncreate useful concepts for evaluating states. InProceedings of the Eighth National Conference\\non Artiﬁcial Intelligence, pp. 882–888. AAAI Press, Menlo Park, CA.\\nYin, H. H., Knowlton, B. J. (2006). The role of the basal ganglia in habit formation.Nature\\nReviews Neuroscience, 7(6):464–476.\\nYoung, P. (1984).Recursive Estimation and Time-Series Analysis. Springer-Verlag, Berlin.\\nYu, H. (2010). Convergence of least squares temporal di↵erence methods under general conditions.\\nInternational Conference on Machine Learning 27,p p .1 2 0 7 – 1 2 1 4 .\\nYu, H. (2012). Least squares temporal di↵erence methods: An analysis under general conditions.\\nSIAM Journal on Control and Optimization, 50(6):3310–3343.\\nYu, H. (2015). On convergence of emphatic temporal-di↵erence learning. InProceedings of the\\n28th Annual Conference on Learning Theory, JMLR W&CP 40. Also ArXiv:1506.02582.\\nYu, H. (2016). Weak convergence properties of constrained emphatic temporal-di↵erence learning\\nwith constant and slowly diminishing stepsize. Journal of Machine Learning Research,\\n17(220):1–58.\\nYu, H. (2017). On convergence of some gradient-based temporal-di↵erences algorithms for\\no↵-policy learning. ArXiv:1712.09652.\\nYu, H., Mahmood, A. R., Sutton, R. S. (2017). On generalized bellman equations and temporal-\\ndi↵erence learning. ArXiv:17041.04463. A summary appeared inProceedings of the Canadian\\nConference on Artiﬁcial Intelligence, pp. 3–14. Springer.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 540, 'page_label': '541'}, page_content='Index\\nPage numbers initalics are recommended to be consulted ﬁrst. Page numbers inbold contain\\nboxed algorithms.\\nk-armed bandits, 25–45\\nabsorbing state, 57\\naccess-control queuing example, 256\\naction preferences,322,3 2 9 ,3 3 6 ,4 5 5\\nin bandit problems,37,4 2\\naction-value function,see value function, action\\naction-value methods, 321\\nfor bandit problems, 27\\nactor–critic, 21, 239, 321,331–332,3 3 8 ,4 0 6\\nadvantage, A2C, 338\\none-step (episodic), 332\\nwith eligibility traces (episodic),332\\nwith eligibility traces (continuing),333\\nneural, 395–415\\naddiction, 409–410\\nafterstates, 137,1 4 0 ,1 8 1 ,1 8 2 ,1 9 1 ,4 2 4 ,4 3 0\\nagent–environment interface, 47–58, 466\\nall-actions algorithm, 326\\nAlphaGo, AlphaGo Zero, AlphaZero, 441–450\\nAndreae, John,17,2 1 ,6 9 ,8 9\\nANN, see artiﬁcial neural networks\\napplications and case studies, 421–457\\napproximate dynamic programming, 15\\nartiﬁcial intelligence, xvii, 1, 472,478\\nartiﬁcial neural networks, 223–228,2 3 8 – 2 4 0 ,\\n395–398, 423, 430, 436–450, 472\\nassociative reinforcement learning,45,4 1 8\\nassociative search, 41\\nasynchronous dynamic programming,85,8 8\\nAtari video game play, 436–441\\nauxiliary tasks,460–461,4 6 8 ,4 7 4\\naverage reward setting,249–255,2 5 8 ,4 6 4\\naveragers, 264\\nbackgammon, 11, 21, 182, 184,421–426\\nbackpropagation, 21, 225–227,2 3 9 ,4 0 7 ,4 2 4 ,\\n436, 439\\nbackup diagram,60,1 3 9\\nfor dynamic programming, 59, 61, 64,172\\nfor Monte Carlo methods, 94\\nfor Q-learning, 134\\nfor TD(0), 121\\nfor Sarsa, 129\\nfor Expected Sarsa, 134\\nfor Sarsa(\\x00), 304\\nfor TD(\\x00), 289\\nfor Q(\\x00), 313\\nfor Tree Backup(\\x00), 314\\nfor Truncated TD(\\x00), 296\\nfor n-step Q(\\x00), 155\\nfor n-step Expected Sarsa, 146\\nfor n-step Sarsa, 146\\nfor n-step TD, 142\\nfor n-step Tree Backup, 152\\nfor Samuel’s Checker Player, 428\\ncompound, 288\\nhalf backups, 62\\nbackward view of eligibility traces, 288,293\\nBaird’s counterexample,261–264,2 8 0 ,2 8 3 ,2 8 5\\nbandit algorithm, simple,32\\nbandit problems, 25–45\\nbasal ganglia, 386\\nbaseline, 37–40,329,3 3 0 ,3 3 8\\nbehavior policy,103,1 1 0 ,see o↵-policy learning\\nBellman equation, 14\\nfor v⇡,5 9\\nfor q⇡,7 8\\nfor optimal value functions:v⇤ and q⇤,6 3\\ndi↵erential, 250\\nfor options, 463\\nBellman error,268,2 7 0 ,2 7 2 ,2 7 3\\nlearnability of, 274–278\\nvector, 267–269\\nBellman operator, 267–269, 286\\nBellman residual, 286,see Bellman error\\nBellman, Richard,14,7 1 ,8 9 ,2 4 1\\nbinary features,215,2 2 2 ,2 4 5 ,3 0 4 ,3 0 5\\nbioreactor example, 51\\nblackjack example,93–94, 99,1 0 6\\nblocking maze example, 166\\nbootstrapping, 89,1 8 9 ,3 0 8\\nn-step, 141–158,2 5 5\\nand dynamic programming, 89\\nand function approximation, 208,264–274'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 541, 'page_label': '542'}, page_content='520 Index\\nand Monte Carlo methods, 95\\nand stability, 263–265\\nand TD learning, 120\\nassessment of, 124–128, 248,264,2 9 1 ,3 1 8\\nin psychology, 345, 349,354,3 5 5\\nparameter (\\x00 or n), 291,3 0 7 ,3 9 9\\nBOXES, 18,7 1 ,2 3 7\\nbranching factor,173–177,4 2 2\\nbreakfast example,5,2 2\\nbucket-brigade algorithm, 19, 21,139\\ncatastrophic interference, 472\\ncertainty-equivalence estimate, 128\\nchess, 4, 20, 54, 182, 450\\nclassical conditioning, 20,343–357\\nblocking, 371\\nand higher-order conditioning, 345–355\\ndelay and trace conditioning, 344\\nRescorla-Wagner model, 346–349\\nTD model, 349–357\\nclassiﬁer systems, 19,2 1\\ncli↵ walking example,132,1 3 3\\nCMAC, see tile coding\\ncoarse coding, 215–220,2 3 8\\ncognitive maps, 363–364\\ncollective reinforcement learning, 404–407\\ncomplex backups,see compound update\\ncompound stimulus, 345,346–356,3 7 1 ,3 8 2\\ncompound update/backup, 288, 319\\nconditioned/unconditioned stimulus, conditioned\\nresponse (CS/US, CR), 344\\nconstant-↵ MC, 120\\ncontextual bandits, 41\\ncontinuing tasks,54,5 7 ,7 0 ,1 2 4 ,2 4 9 ,2 9 4\\ncontinuous action, 73, 244,335–336\\ncontinuous state,73,2 2 3 ,2 3 8\\ncontinuous time,11,7 1\\ncontrol and prediction, 342\\ncontrol theory,4,7 1\\ncontrol variates,150–152,1 5 5 ,2 8 1\\nand eligibility traces, 309–312\\ncredit assignment, 11,17,1 9 ,4 7 ,2 9 4 ,4 0 1\\nin psychology, 346,361\\nstructural, 385,4 0 5 ,4 0 7\\ncritic, 18,2 3 9 ,3 4 6 ,4 1 7 ,see actor–critic\\ncumulant, 459\\ncuriosity, 474\\ncurse of dimensionality, 4,14,2 2 1 ,2 3 1\\ncybernetics, xvii, 477\\ndeadly triad, 264\\ndeep learning, 12,223,4 4 1 ,4 7 2 – 4 7 4 ,4 7 9\\ndeep reinforcement learning, 236\\ndeep residual learning, 227\\ndelayed reinforcement, 361–363\\ndelayed reward,1,4 7 ,2 4 9\\ndimensions of reinforcement learning methods,\\n189–191\\ndirect and indirect RL,162,1 6 4 ,1 9 2\\ndiscounting, 55,1 9 9 ,2 4 3 ,2 4 9 ,2 8 2 ,3 2 4 ,3 2 8 ,\\n427, 459\\nin pole balancing, 56\\nstate dependent, 307\\ndeprecated, 253,2 5 6\\ndistribution models, 159,1 8 5\\ndopamine, 377,381–387,4 1 3 – 4 1 9\\nand addiction, 409–410\\ndouble learning,134–136,1 4 0\\nDP, see dynamic programming\\ndriving-home example, 122–123\\nDyna architecture,164,1 6 1 – 1 7 0\\ndynamic programming, 13–15,73–90,1 7 4 ,2 6 2\\nand artiﬁcial intelligence, 89\\nand function approximation, 241\\nand options, 463\\nand the deadly triad, 264\\ncomputational e\\x00ciency of, 87\\neligibility traces,287–320,3 5 0 ,3 6 2 ,3 9 8 – 4 0 3\\naccumulating, 300,3 0 6 ,3 1 0\\nreplacing, 301,3 0 6\\ndutch, 300–303\\ncontingent/non-contingent, 399–403,411\\no↵-policy, 309–316\\nwith state-dependent\\x00 and \\x00,3 0 9 – 3 1 6\\nEmphatic-TD methods, 234–235,315\\no↵-policy, 281–282\\nenvironment, 47–58\\nepisodes, episodic tasks, 11,54–58,9 1\\nerror reduction property,144,2 8 8\\nevaluative feedback, 17,25,4 7\\nevolution, 7, 359, 374, 471\\nevolutionary methods,7,8 ,9 ,1 1 ,1 9\\nexpected approximate value,148,1 5 5\\nExpected Sarsa, 133,see also Sarsa, Expected\\nexpected update, 75,1 7 2 – 1 8 1 ,1 8 9\\nexperience replay, 440–441\\nexplore/exploit dilemma,3,1 0 3 ,4 7 2\\nexploring starts,96,9 8 – 1 0 0 ,1 7 8'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 542, 'page_label': '543'}, page_content='Index 521\\nfeature construction, 210–223\\nﬁnal time step (T ), 54\\nFourier basis, 211–215\\nfunction approximation, 195–200\\ngambler’s example, 84\\ngame theory, 19\\ngazelle calf example, 5\\ngeneral value functions (GVFs),459–463,4 7 4\\ngeneralized policy iteration (GPI),86–87,9 2 ,\\n97, 138, 189\\ngenetic algorithms, 19\\nGittins index, 43\\ngliding/soaring case study, 453–457\\ngoal, see reward signal\\ngolf example,61,6 3 ,6 6\\ngradient, 201\\ngradient descent, see stochastic gradient de-\\nscent\\nGradient-TD methods,278–281,3 1 4 – 3 1 5\\ngreedy or\"-greedy\\nas exploiting, 26–28\\nas shortsighted, 64\\n\"-greedy policies, 100\\ngridworld examples, 60, 65, 76, 147\\ncli↵ walking, 132\\nDyna blocking maze, 166\\nDyna maze, 164\\nDyna shortcut maze, 167\\nwindy, 130, 131\\nhabitual and goal-directed control, 364–368\\nhedonistic neurons, 402–404\\nheuristic search,181–183,1 9 0\\nas sequences of backups, 183\\nin Samuel’s checkers player, 426\\nin TD-Gammon, 425\\nhistory of reinforcement learning, 13–22\\nHolland, John,19,2 1 ,4 4 ,1 3 9 ,2 4 1\\nHull, Clark, 16, 359, 360,362–363\\nimportance sampling, 103–117,1 5 1 ,2 5 7\\nratio, 104,1 4 8 ,2 5 8\\nweighted and ordinary,105,1 0 6\\nand eligibility traces, 309–312\\nand inﬁnite variance, 106\\ndiscounting aware, 112–113\\nincremental implementation, 109\\nper-decision, 114–115\\nn-step, 148–156\\nincremental implementation\\nof averages, 30–33\\nof weighted averages, 109\\ninstrumental conditioning, 357–361, see also\\nLaw of E↵ect\\nand motivation, 360–361\\nThorndike’s puzzle boxes, 358\\ninterest and emphasis,234–235,2 8 2 ,3 1 6\\ninverse reinforcement learning, 470\\nJack’s car rental example,81–82,1 3 7 ,2 1 0\\nkernel-based function approximation, 232–233\\nKlopf, A. Harry,xv,x v i i ,1 9 – 2 1 ,4 0 2 – 4 0 4 ,4 1 1\\nlatent learning, 192,363,3 6 6\\nLaw of E↵ect,15–16,4 5 ,3 4 3 ,3 5 8 – 3 6 1 ,4 1 7\\nlearning automata, 18\\nLeast Mean Square (LMS) algorithm, 279,301\\nLeast-Squares TD (LSTD), 228–229\\nlinear function approx.,204–209,2 6 6 – 2 6 9\\nlinear programming, 87, 90\\nlocal and global optima, 200\\nMarkov decision process (MDP), 2, 14,47–71\\nMarkov property,49,1 1 5 ,4 6 5 – 4 6 8\\nMarkov reward process (MRP), 125\\nmaximization bias, 134–136\\nmaximum-likelihood estimate, 128\\nMC, see Monte Carlo methods\\nMean Square\\nBellman Error,BE, 268\\nProjected Bellman Error,PBE, 269\\nReturn Error,RE, 275\\nTD Error,TDE, 270\\nValue Error,VE, 199–200\\nmemory-based function approx., 230–232\\nMichie, Donald,17,7 1 ,1 1 7\\nMinsky, Marvin,16,1 7 ,2 0 ,8 9\\nmodel of the environment, 7,159\\nmodel-based and model-free methods, 7,159\\nin animal learning, 363–368\\nmodel-based reinforcement learning, 159–193\\nin neuroscience, 407–409\\nMonte Carlo methods, 91–117\\nﬁrst- and every-visit MC, 92\\nﬁrst-visit MC control,101\\nﬁrst-visit MC prediction,92'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 543, 'page_label': '544'}, page_content='522 Index\\ngradient method forv⇡, 202\\nMonte Carlo ES (Exploring Starts),99\\no↵-policy control,111,1 1 0 – 1 1 2\\no↵-policy prediction, 103–109,110\\nMonte Carlo Tree Search (MCTS), 185–188\\nmotivation, 360–361\\nmountain car example,244–248,3 0 5 ,3 0 6\\nmulti-armed bandits, 25–45\\nn-step methods, 141–158\\nQ(\\x00), 156\\nSarsa, 147, 247\\ndi↵erential, 255\\no↵-policy, 149\\nTD, 144\\nTree Backup,154\\ntruncated \\x00-return, 295\\nnaughts and crosses,see tic-tac-toe\\nneural networks,see artiﬁcial neural networks\\nneurodynamic programming, 15\\nneuroeconomics, 413,4 1 9\\nneuroscience, 4, 21,377–419\\nnonstationarity, 30,32–36,4 4 ,2 5 5\\ninherent, 91,198\\nnotation, xiii,xix\\nobservations, 464\\no↵-policy methods, 257–286\\nvs on-policy methods, 100, 103\\nMonte Carlo, 103–115\\nQ-learning, 131\\nExpected Sarsa, 133–134\\nn-step, 148–156\\nn-step Q(\\x00), 156\\nn-step Sarsa,149\\nn-step Tree Backup,154\\nand eligibility traces, 309–316\\nEmphatic-TD(\\x00), 315\\nGQ(\\x00), 315\\nGTD(\\x00), 314\\nHTD(\\x00), 315\\nQ(\\x00), 312–314\\nTree Backup(\\x00), 312–314\\nreducing variance, 283–284\\non-policy distribution, 175,1 9 9 ,2 0 8 ,2 5 8 ,2 6 2 ,\\n281, 282\\nvs uniform distribution, 176\\non-policy methods, 100\\nactor–critic, 332, 333\\napproximate\\ncontrol, 244, 247, 251, 255\\nprediction, 202, 203, 209\\nMonte Carlo,101,1 0 0 – 1 0 3 ,328, 330\\nn-step, 144, 147\\nSarsa, 130,1 2 9 – 1 3 1\\nTD(0), 120,1 1 9 – 1 2 8\\nwith eligibility traces,293, 300, 305, 307\\noperant conditioning,see instrumental learning\\noptimal control, 2,14–15,2 1\\noptimistic initial values,34–35,1 9 2\\noptimizing memory control, 432–436\\noptions, 461–464\\nmodels of, 462\\npain and pleasure,6,1 6 ,4 1 3\\nPartially Observable MDPs (POMDPs),467\\nPavlov, Ivan, 16,343–345,3 6 2\\nPavlovian\\nconditioning, see classical conditioning\\ncontrol, 343,371,3 7 3 ,4 7 8\\npersonalizing web services, 450–453\\nplanning, 3, 5, 7, 11, 138,159–193\\nin psychology, 363, 364, 366\\nwith learned models,161–168,4 7 3\\nwith options, 461,463\\npolicy, 6, 41,58\\nhierarchical, 462\\nsoft and\"-soft, 100–103, 110\\npolicy approximation, 321–324\\npolicy evaluation, 74–76,see also prediction\\niterative, 75\\npolicy gradient methods, 321–338\\nREINFORCE, 328, 330\\nactor–critic, 332, 333\\npolicy gradient theorem, 324–326\\nproof, episodic case, 325\\nproof, continuing case, 334\\npolicy improvement, 76–80\\ntheorem, 78,1 0 1\\npolicy iteration, 14,80, 80–82\\npolynomial basis, 210–211\\nprediction, 74–76, see also policy evaluation\\nand control, 342\\nMonte Carlo, 92–97\\no↵-policy, 103–108\\nTD, 119–126\\nwith approximation, 197–242\\nprior knowledge, 12, 34, 54, 137, 236, 324, 471'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 544, 'page_label': '545'}, page_content='Index 523\\nprioritized sweeping,170,1 6 8 – 1 7 1\\nprojected Bellman error, 285\\nvector, 267,269\\nproximal TD methods, 286\\npseudo termination, 282,308\\npsychology, 4, 13, 19, 20,341–376\\nQ(\\x00), Watkins’s, 312–314\\nQ-function, see action-value function\\nQ-learning, 21,131, 131–135\\ndouble, 136\\nQ-planning, 161\\nQ(\\x00), 156,1 5 4 – 1 5 6\\nqueuing example, 252\\nR-learning, 256\\nracetrack exercise, 111\\nradial basis functions (RBFs), 221–222\\nrandom walk, 95\\n5-state, 125,1 2 6 ,1 2 7\\n19-state, 144,2 9 1\\nTD(\\x00) results on,294, 295,2 9 9\\n1000-state, 203–209,2 1 7 ,2 1 8\\nFourier and polynomial bases, 214\\nreal-time dynamic programming, 177–180\\nrecycling robot example, 52\\nREINFORCE, 328,3 2 6 – 3 3 1\\nwith baseline,330\\nreinforcement learning, 1–22\\nreinforcement signal,380\\nrepresentation learning, 473\\nresidual-gradient algorithm,272–274,2 7 7\\nnaive, 270, 271\\nreturn, 54–58\\nn-step, 143\\nfor Q(\\x00), 155\\nfor action values, 146\\nfor Expected Sarsa, 148\\nfor Tree Backup, 153\\nwith control variates, 150, 151\\nwith function approximation, 209\\ndi↵erential, 250, 255, 334\\nﬂat partial, 113\\nwith state-dependent termination, 308\\n\\x00-return, 288–291\\ntruncated, 296\\nreward prediction error hypothesis,381–383,\\n387–395\\nreward signal, 1, 6,48, 53,3 6 1 ,3 8 0 ,3 8 3 ,3 9 7\\nand reinforcement, 373–375, 380–381\\ndesign of,469–472,4 7 7\\nintrinsic, 474\\nsparse, 469–470\\nrod maneuvering example, 171\\nrollout algorithms, 183–185\\nroot mean square (RMS) error, 125\\nsafety, 434,478\\nsample and expected updates,121,1 7 0 – 1 7 4\\nsample or simulation model, 115\\nsample-average method, 27\\nSamuel’s checkers player, 20, 241,426–429\\nSarsa, 130, 129–131, 244\\nvs Q-learning, 132\\ndi↵erential, one-step,251\\nExpected, 133–134,1 4 0\\nn-step, 148\\nn-step o↵-policy, 150\\ndouble, 136\\nn-step, 147, 145–148, 247\\ndi↵erential, 255\\no↵-policy, 149\\nSarsa(\\x00), 305,3 0 3 – 3 0 7\\ntrue online,307\\nSchultz, Wolfram, 387–395,410\\nsearch control, 163\\nsecondary reinforcement,20,3 4 6 ,3 5 4 ,3 6 9\\nselective bootstrap adaptation, 239\\nsemi-gradient methods, 202,258–259\\nSGD, see stochastic gradient descent\\nShannon, Claude, 16,20,7 1 ,4 2 6\\nshaping, 360,4 7 0\\nSkinner, B. F.,359–360,3 7 5 ,4 7 0 ,4 7 9\\nsoap bubble example, 95\\nsoft and\"-soft policies, 100–103, 110\\nsoft-max, 322–323,3 2 9 ,3 3 6 ,4 0 0 ,4 4 5 ,4 5 5\\nfor bandits,37,4 5\\nspike-timing-dependent plasticity (STDP), 401\\nstate, 7,4 8 ,4 9\\nkth-order history approach, 468\\nand observations, 464–468\\nMarkov property, 465–468\\nbelief, 467\\nlatent, 467\\nobservable operator models (OOMs), 467\\npartially observable MDPs, 14, 467\\npredictive state representations, 467\\nstate-update function, 465'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 545, 'page_label': '546'}, page_content='524 Index\\nstate aggregation, 203–204\\nstate-update function, 465\\nstep-size parameter, 10,31–33,1 2 0 ,1 2 5 ,1 2 6\\nautomatic adaptation, 238\\nin DQN, 439, 440\\nin psychological models, 347, 348\\nselecting manually, 222–223\\nwith coarse coding, 216\\nwith Fourier features, 213\\nwith tile coding,217,2 2 3\\nstochastic approx. convergence conditions, 33\\nstochastic gradient descent (SGD), 200–204\\nin the Bellman error, 269–278\\nstrong and weak methods, 4\\nsupervised learning, xvii,2,1 7 – 1 9 ,1 9 8\\nsweeps, 75,1 6 0 ,see also prioritized sweeping\\nsynaptic plasticity, 379\\nHebbian, 400\\ntwo-factor and three factor, 400\\nsystem identiﬁcation, 364\\ntabular solution methods, 23\\ntarget\\npolicy, 103,1 1 0\\nof update, 31, 143,198\\nTD, see temporal-di↵erence learning\\nTD error, 121\\nn-step, 255\\ndi↵erential, 250\\nwith function approximation, 270\\nTD(\\x00), 293,2 9 2 – 2 9 5\\ntruncated, 295–297\\ntrue online,300,2 9 9 – 3 0 1\\nTD-Gammon, 21,421–426\\ntemporal abstraction, 461–464\\ntemporal-di↵erence learning, 10,119–140\\nhistory of, 20–21\\nadvantages of, 124–126\\noptimality of, 126–128\\nTD(0), 120, 203\\nTD(1), 294\\nTD(\\x00), 293,2 9 2 – 2 9 5\\ntrue online,300,2 9 9 – 3 0 1\\n\\x00-return methods\\no↵-line, 290\\nonline, 297–299\\nn-step, 144, 141–158, 209\\ntermination function, 307, 459\\nThompson sampling,43,4 5\\nThorndike, Edward,see Law of E↵ect\\ntic-tac-toe, 8–13,1 7 ,1 3 7\\ntile coding, 217–221,2 2 3 ,2 3 8 ,2 4 6 ,4 3 4 ,4 3 5\\nTolman, Edward,364,4 0 8\\ntrace-decay parameter (\\x00), 287,289,2 9 0 ,2 9 2\\nstate dependent, 307\\ntrajectory sampling, 174–177\\ntransition probabilities, 49\\nTree Backup\\nn-step, 152–153, 154\\nTree-Backup(\\x00), 312–314\\ntrial-and-error, 1, 7,15–21,4 0 3 ,4 0 4 ,see also\\ninstrumental conditioning\\ntrue online TD(\\x00), 300,2 9 9 – 3 0 1\\nTsitsiklis and Van Roy’s Counterexample, 263\\nundiscounted continuing tasks,see average re-\\nward setting\\nunsupervised learning, 2,2 2 6\\nvalue, 6,2 6 ,4 7\\nvalue function, 6,58–67\\nfor a given policy:v⇡ and q⇡,5 8\\nfor an optimal policy:v⇤ and q⇤,6 2\\naction, 58,6 3 ,6 5 ,7 1 ,1 2 9 ,1 3 1\\napproximate action values:ˆq(s, a,w), 243\\napproximate state values: ˆv(s,w), 197\\ndi↵erential, 243\\nvs evolutionary methods, 11\\nvalue iteration,83,8 2 – 8 4\\nvalue-function approximation, 198\\nWatkins, Chris,15,2 1 ,8 9 ,3 2 0\\nWatson (Jeopardy! player), 429–432\\nWerbos, Paul, 14, 21,70,8 9 ,1 3 9 ,2 3 9\\nWitten, Ian,21,7 0'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 546, 'page_label': '547'}, page_content='525\\nAdaptive Computation and Machine Learning\\nFrancis Bach, Editor\\nBioinformatics: The Machine Learning Approach, Pierre Baldi and Søren Brunak\\nReinforcement Learning: An Introduction, Richard S. Sutton and Andrew G. Barto\\nGraphical Models for Machine Learning and Digital Communication, Brendan J. Frey\\nLearning in Graphical Models, Michael I. Jordan\\nCausation, Prediction, and Search, second edition, Peter Spirtes, Clark Glymour, and\\nRichard Scheines\\nPrinciples of Data Mining, David Hand, Heikki Mannila, and Padhraic Smyth\\nBioinformatics: The Machine Learning Approach, second edition, Pierre Baldi and Søren\\nBrunak\\nLearning Kernel Classiﬁers: Theory and Algorithms, Ralf Herbrich\\nLearning with Kernels: Support Vector Machines, Regularization, Optimization, and\\nBeyond, Bernhard Sch¨ olkopf and Alexander J. Smola\\nIntroduction to Machine Learning, Ethem Alpaydin\\nGaussian Processes for Machine Learning, Carl Edward Rasmussen and Christopher K.I.\\nWilliams\\nSemi-Supervised Learning, Olivier Chapelle, Bernhard Sch¨ olkopf, and Alexander Zien,\\nEds.\\nThe Minimum Description Length Principle, Peter D. Gr¨ unwald\\nIntroduction to Statistical Relational Learning, Lise Getoor and Ben Taskar, Eds.'),\n",
       " Document(metadata={'producer': 'macOS Version 12.3.1 (Build 21E258) Quartz PDFContext', 'creator': 'LaTeX with hyperref', 'creationdate': \"D:20220426200917Z00'00'\", 'moddate': \"D:20220426200917Z00'00'\", 'source': 'PDF_DOC\\\\PDF\\\\RLbook2020.pdf', 'total_pages': 548, 'page': 547, 'page_label': '548'}, page_content='526\\nProbabilistic Graphical Models: Principles and Techniques, Daphne Koller and Nir Fried-\\nman\\nIntroduction to Machine Learning, second edition, Ethem Alpaydin\\nMachine Learning in Non-Stationary Environments: Introduction to Covariate Shift\\nAdaptation, Masashi Sugiyama and Motoaki Kawanabe\\nBoosting: Foundations and Algorithms, Robert E. Schapire and Yoav Freund\\nMachine Learning: A Probabilistic Perspective,K e v i nP .M u r p h y\\nFoundations of Machine Learning, Mehryar Mohri, Afshin Rostami, and Ameet Talwalker\\nIntroduction to Machine Learning, third edition, Ethem Alpaydin\\nDeep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron Courville\\nElements of Causal Inference, Jonas Peters, Dominik Janzing, and Bernhard Sch¨ olkopf\\nMachine Learning for Data Streams, with Practical Examples in MOA, Albert Bifet,\\nRicard Gavald‘ a, Geo↵rey Holmes, Bernhard Pfahringer')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd874b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_documents = pdf_documents + documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9648b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(final_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "embeddings = NVIDIAEmbeddings(nvidia_api_key = os.getenv(\"NVIDIA_API_KEY\")) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2c69c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS vector store\n",
    "db = FAISS.from_documents(texts, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feec7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local(\"faiss_index_rl_learn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac91215a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_vectorstore = FAISS.load_local(\"faiss_index_rl_learn\", embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0bfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatCerebras(model = \"gpt-oss-120b\",api_key = os.getenv(\"CEREBRAS_API_KEY\"))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7e5fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "\n",
    "Use Given Context to assist user query.\n",
    "\n",
    "Ensure response must be from context and domain specific outof domain query not allowed here domain is context.\n",
    "\n",
    "Brainstorm User with your context based responses.\n",
    "\n",
    "INPUTS:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "<query>\n",
    "{query}\n",
    "</query>\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a0224973",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32340a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8bb81aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Retrieval + injection\n",
    "def answer_with_context(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "    return chain.invoke({\"context\": context, \"query\": query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "118ad7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Below is a short “play‑by‑play’’ of the bandit ideas that appear in the excerpt you gave (Figure 2.1, Figure 2.6 and the code fragment at the end).  \n",
       "I’ll walk through the most common bandit strategies that are mentioned in the text – **ε‑greedy**, **optimistic‑initialization**, **UCB** and **gradient‑bandit** – and give a tiny Python example for each.  \n",
       "All of the examples reuse the same simple **Bernoulli‑bandit** environment that the snippet shows:\n",
       "\n",
       "```python\n",
       "import numpy as np\n",
       "import random\n",
       "\n",
       "# ----- 3‑armed Bernoulli bandit (the “true” click‑through‑rates) -----\n",
       "true_ctr = [0.05, 0.10, 0.20]                     # q* (a) for A, B, C\n",
       "articles = ['A', 'B', 'C']\n",
       "\n",
       "def bandit_step(action):\n",
       "    \"\"\"Return 1 (reward) with probability true_ctr[action],\n",
       "       otherwise 0. This is the stochastic reward generator.\"\"\"\n",
       "    return 1 if np.random.rand() < true_ctr[action] else 0\n",
       "```\n",
       "\n",
       "The **action‑value estimate** for each arm, \\(Q_t(a)\\), is the running average of the rewards observed for that arm.  \n",
       "In the textbook notation \\(q_*(a)\\) is the unknown true value (the numbers above), while \\(Q_t(a)\\) is the learner’s current guess.\n",
       "\n",
       "---\n",
       "\n",
       "## 1. ε‑greedy (the “greedy‑with‑exploration” rule)\n",
       "\n",
       "*Idea from the text*: “greedy with optimistic initialization” or “ε‑greedy” is the baseline method plotted in Figure 2.6.  \n",
       "With probability \\(ε\\) we explore (pick a random arm); otherwise we exploit the arm with the highest estimated value.\n",
       "\n",
       "```python\n",
       "def epsilon_greedy(eps=0.1, steps=1000):\n",
       "    n_actions = len(true_ctr)\n",
       "    Q = np.zeros(n_actions)          # Q_0(a) = 0 for all a\n",
       "    N = np.zeros(n_actions)          # count of pulls per arm\n",
       "    total_reward = 0\n",
       "\n",
       "    for t in range(steps):\n",
       "        # --- choose action ---\n",
       "        if random.random() < eps:                 # explore\n",
       "            a = random.randint(0, n_actions-1)\n",
       "        else:                                      # exploit\n",
       "            a = np.argmax(Q)\n",
       "\n",
       "        # --- take step and observe reward ---\n",
       "        r = bandit_step(a)\n",
       "        total_reward += r\n",
       "\n",
       "        # --- update estimates (sample‑average) ---\n",
       "        N[a] += 1\n",
       "        Q[a] += (r - Q[a]) / N[a]                 # incremental average\n",
       "\n",
       "    print(f'ε‑greedy (ε={eps}) avg. reward: {total_reward/steps:.3f}')\n",
       "    print('Final Q values:', Q)\n",
       "```\n",
       "\n",
       "Running `epsilon_greedy()` will converge toward the best arm (C, with true CTR 0.20) while still occasionally trying the poorer arms.\n",
       "\n",
       "---\n",
       "\n",
       "## 2. Optimistic‑initialization (a variant of ε‑greedy)\n",
       "\n",
       "*Idea from the text*: “greedy with optimistic initialization” starts each \\(Q_0(a)\\) at a high value (e.g., 5).  \n",
       "Because all arms look attractive initially, the algorithm is forced to **explore** until the estimates drop down to realistic levels.\n",
       "\n",
       "```python\n",
       "def optimistic_greedy(initial=5.0, steps=1000):\n",
       "    n_actions = len(true_ctr)\n",
       "    Q = np.full(n_actions, initial)   # all Q_0(a) = 5.0 (optimistic)\n",
       "    N = np.zeros(n_actions)\n",
       "    total_reward = 0\n",
       "\n",
       "    for t in range(steps):\n",
       "        a = np.argmax(Q)                     # always greedy\n",
       "        r = bandit_step(a)\n",
       "        total_reward += r\n",
       "\n",
       "        N[a] += 1\n",
       "        Q[a] += (r - Q[a]) / N[a]\n",
       "\n",
       "    print(f'optimistic‑greedy avg. reward: {total_reward/steps:.3f}')\n",
       "    print('Final Q values:', Q)\n",
       "```\n",
       "\n",
       "Because the true rewards are at most 1, the optimistic start quickly pushes the estimate of each arm down after a few pulls, automatically generating exploration without an explicit ε.\n",
       "\n",
       "---\n",
       "\n",
       "## 3. Upper‑Confidence‑Bound (UCB)\n",
       "\n",
       "*Idea from the text*: Figure 2.6 includes a “UCB” curve.  \n",
       "UCB selects the arm with the highest **upper confidence bound**:\n",
       "\n",
       "\\[\n",
       "a_t = \\arg\\max_a\\Bigl(Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}}\\Bigr)\n",
       "\\]\n",
       "\n",
       "The second term is large for rarely‑chosen arms, guaranteeing systematic exploration.\n",
       "\n",
       "```python\n",
       "def ucb(c=2.0, steps=1000):\n",
       "    n_actions = len(true_ctr)\n",
       "    Q = np.zeros(n_actions)\n",
       "    N = np.zeros(n_actions)\n",
       "    total_reward = 0\n",
       "\n",
       "    # Pull each arm once to avoid division‑by‑zero\n",
       "    for a in range(n_actions):\n",
       "        r = bandit_step(a)\n",
       "        Q[a] = r\n",
       "        N[a] = 1\n",
       "        total_reward += r\n",
       "\n",
       "    for t in range(n_actions, steps):\n",
       "        ucb_values = Q + c * np.sqrt(np.log(t) / N)\n",
       "        a = np.argmax(ucb_values)\n",
       "\n",
       "        r = bandit_step(a)\n",
       "        total_reward += r\n",
       "\n",
       "        N[a] += 1\n",
       "        Q[a] += (r - Q[a]) / N[a]\n",
       "\n",
       "    print(f'UCB (c={c}) avg. reward: {total_reward/steps:.3f}')\n",
       "    print('Final Q values:', Q)\n",
       "```\n",
       "\n",
       "UCB usually attains a higher average reward than plain ε‑greedy because it balances exploration **optimally** based on statistical confidence.\n",
       "\n",
       "---\n",
       "\n",
       "## 4. Gradient Bandit (policy‑gradient style)\n",
       "\n",
       "*Idea from the text*: “gradient‑bandit” appears as another curve in Figure 2.6.  \n",
       "Instead of estimating values, we keep a **preference** \\(H_t(a)\\) for each arm and turn them into a stochastic policy via a soft‑max:\n",
       "\n",
       "\\[\n",
       "\\pi_t(a) = \\frac{e^{H_t(a)}}{\\sum_b e^{H_t(b)}} .\n",
       "\\]\n",
       "\n",
       "After each reward we update the preferences toward actions that performed better than the *average* reward.\n",
       "\n",
       "```python\n",
       "def gradient_bandit(alpha=0.1, steps=1000):\n",
       "    n_actions = len(true_ctr)\n",
       "    H = np.zeros(n_actions)          # preferences H_0(a) = 0\n",
       "    avg_reward = 0.0\n",
       "    total_reward = 0\n",
       "\n",
       "    for t in range(1, steps+1):\n",
       "        # compute soft‑max policy\n",
       "        exp_H = np.exp(H - np.max(H))      # numeric stability\n",
       "        pi = exp_H / np.sum(exp_H)\n",
       "\n",
       "        # sample action according to pi\n",
       "        a = np.random.choice(n_actions, p=pi)\n",
       "\n",
       "        # observe reward\n",
       "        r = bandit_step(a)\n",
       "        total_reward += r\n",
       "\n",
       "        # incremental average reward\n",
       "        avg_reward += (r - avg_reward) / t\n",
       "\n",
       "        # update preferences\n",
       "        for i in range(n_actions):\n",
       "            indicator = 1.0 if i == a else 0.0\n",
       "            H[i] += alpha * (r - avg_reward) * (indicator - pi[i])\n",
       "\n",
       "    print(f'gradient‑bandit (α={alpha}) avg. reward: {total_reward/steps:.3f}')\n",
       "    print('Final preferences H:', H)\n",
       "    print('Resulting policy π:', np.exp(H)/np.sum(np.exp(H)))\n",
       "```\n",
       "\n",
       "The gradient method directly learns a **probability distribution** over actions.  \n",
       "When a pull yields a reward larger than the running average, the probability of that arm is increased; otherwise it is decreased.\n",
       "\n",
       "---\n",
       "\n",
       "## 5. Putting it together – a quick comparison\n",
       "\n",
       "```python\n",
       "if __name__ == \"__main__\":\n",
       "    steps = 2000\n",
       "    epsilon_greedy(eps=0.1, steps=steps)\n",
       "    optimistic_greedy(initial=5.0, steps=steps)\n",
       "    ucb(c=2.0, steps=steps)\n",
       "    gradient_bandit(alpha=0.1, steps=steps)\n",
       "```\n",
       "\n",
       "Running the block will output something like:\n",
       "\n",
       "```\n",
       "ε‑greedy (ε=0.1) avg. reward: 0.156\n",
       "optimistic‑greedy avg. reward: 0.162\n",
       "UCB (c=2.0) avg. reward: 0.167\n",
       "gradient‑bandit (α=0.1) avg. reward: 0.155\n",
       "```\n",
       "\n",
       "(The exact numbers vary because the environment is stochastic, just as the Figure 2.6 plot shows a spread of results for the same algorithms.)\n",
       "\n",
       "---\n",
       "\n",
       "### What the figures in the text illustrate\n",
       "\n",
       "* **Figure 2.1** – a concrete 10‑armed testbed (the true values \\(q_*(a)\\) are plotted).  \n",
       "  Our three‑armed example is a miniature version of that same set‑up.\n",
       "\n",
       "* **Figure 2.6** – a **parameter study**: each curve (ε‑greedy, UCB, gradient, optimistic) is the average reward over the first 1 000 steps for many runs.  \n",
       "  The code snippets above reproduce the same idea on a much smaller scale (you can change `steps` to 1000 to match the figure).\n",
       "\n",
       "* **Index entries** – the list mentions “k‑armed bandits”, “action preferences”, “gradient bandit”, etc.; the functions `gradient_bandit` and `ucb` directly correspond to those entries.\n",
       "\n",
       "---\n",
       "\n",
       "## TL;DR\n",
       "\n",
       "| Algorithm | Core Idea | Exploration Mechanism | Typical Code Hook |\n",
       "|-----------|-----------|----------------------|-------------------|\n",
       "| ε‑greedy | Keep sample‑average \\(Q_t(a)\\). Choose best arm most of the time, random arm with prob ε. | Random jumps (ε). | `if random()<ε: random action else: argmax(Q)` |\n",
       "| Optimistic‑init. | Same as greedy, but start \\(Q_0(a)\\) high → forced exploration. | Implicit via high initial values. | `Q = np.full(k, high_value)` |\n",
       "| UCB | Upper‑confidence bound adds bonus \\(\\propto \\sqrt{\\frac{\\ln t}{N_t(a)}}\\). | Systematic, theoretically efficient. | `a = argmax(Q + c*sqrt(log(t)/N))` |\n",
       "| Gradient | Maintain preferences \\(H_t(a)\\); derive policy via soft‑max. | Policy‑gradient update toward better‑than‑average actions. | `H += α*(r−avg)*(I[a]==1−π)` |\n",
       "\n",
       "All four methods operate on the same **bandit** framework that the excerpt describes: a set of actions (arms), stochastic rewards, and a learning rule that updates estimates or preferences after each pull. Feel free to experiment with the `steps`, `ε`, `c`, or `α` parameters to see how the curves in Figure 2.6 would move. Happy pulling!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(answer_with_context(\"Explain bandit algorithm use code examples\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc845bc2",
   "metadata": {},
   "source": [
    "### Streaming tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bcd466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM with streaming\n",
    "llm_streaming = ChatCerebras(model = \"gpt-oss-120b\",api_key = os.getenv(\"CEREBRAS_API_KEY\"))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6f5bfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain_streaming = prompt | llm_streaming | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7548d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming function\n",
    "def stream_answer(query):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "    for chunk in chain_streaming.stream({\"context\": context, \"query\": query}):\n",
    "        print(chunk, end=\"\", flush=True)  # stream tokens as they arrive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "aed90aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is a **stand‑alone Python script** that implements the SARSA(λ) algorithm (plain SARSA is a special case with λ = 0) from scratch for the classic *FrozenLake* environment.  \n",
      "All of the pieces shown in the provided context are used:\n",
      "\n",
      "* the `sarsa` function signature (`env, num_episodes=5000, alpha=0.1, gamma=0.95, epsilon=0.1, log_interval=100`)  \n",
      "* creation of a random 4 × 4 lake (`CustomFrozenLakeEnv` with `generate_random_map`)  \n",
      "* optional recording of episode statistics (`gym.wrappers.RecordEpisodeStatistics`)  \n",
      "* a progress bar (`tqdm`)  \n",
      "\n",
      "You can copy‑paste the whole block into a file (e.g. `sarsa_frozenlake.py`) and run it with a standard Python 3 interpreter.\n",
      "\n",
      "```python\n",
      "#!/usr/bin/env python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "\"\"\"\n",
      "SARSA implementation for a (random) FrozenLake environment.\n",
      "The code follows the template that appears in the supplied context,\n",
      "but is written from scratch so that it can be executed as a\n",
      "stand‑alone script.\n",
      "\n",
      "Requirements\n",
      "------------\n",
      "- numpy\n",
      "- gym (>=0.21)\n",
      "- tqdm\n",
      "\"\"\"\n",
      "\n",
      "import numpy as np\n",
      "import gym\n",
      "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
      "from tqdm import tqdm\n",
      "\n",
      "# --------------------------------------------------------------------\n",
      "# Helper: epsilon‑greedy policy\n",
      "# --------------------------------------------------------------------\n",
      "def epsilon_greedy_policy(Q, state, n_actions, epsilon):\n",
      "    \"\"\"\n",
      "    Return an action according to an epsilon‑greedy policy.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    Q : np.ndarray, shape (n_states, n_actions)\n",
      "        Current action‑value table.\n",
      "    state : int\n",
      "        Current discrete state index.\n",
      "    n_actions : int\n",
      "        Number of possible actions.\n",
      "    epsilon : float\n",
      "        Exploration probability.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    action : int\n",
      "        Selected action.\n",
      "    \"\"\"\n",
      "    if np.random.rand() < epsilon:\n",
      "        return np.random.randint(n_actions)          # explore\n",
      "    else:\n",
      "        # break ties randomly to avoid bias\n",
      "        max_q = np.max(Q[state])\n",
      "        candidates = np.where(Q[state] == max_q)[0]\n",
      "        return np.random.choice(candidates)          # exploit\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------------\n",
      "# SARSA core\n",
      "# --------------------------------------------------------------------\n",
      "def sarsa(env,\n",
      "          num_episodes: int = 5000,\n",
      "          alpha: float = 0.1,\n",
      "          gamma: float = 0.95,\n",
      "          epsilon: float = 0.1,\n",
      "          lambda_: float = 0.0,          # λ = 0 → plain SARSA\n",
      "          log_interval: int = 100):\n",
      "    \"\"\"\n",
      "    Train a Q‑table using the SARSA algorithm.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    env : gym.Env\n",
      "        The environment (must have discrete observation and action spaces).\n",
      "    num_episodes : int\n",
      "        Number of training episodes.\n",
      "    alpha : float\n",
      "        Learning rate.\n",
      "    gamma : float\n",
      "        Discount factor.\n",
      "    epsilon : float\n",
      "        Exploration probability for epsilon‑greedy.\n",
      "    lambda_ : float, optional\n",
      "        Eligibility‑trace decay (0 → vanilla SARSA, 1 → Monte‑Carlo).\n",
      "    log_interval : int\n",
      "        How often to print a progress line.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    Q : np.ndarray, shape (n_states, n_actions)\n",
      "        Learned state‑action value function.\n",
      "    \"\"\"\n",
      "    n_actions = env.action_space.n\n",
      "    n_states  = env.observation_space.n\n",
      "\n",
      "    # Q‑table initialised to zero\n",
      "    Q = np.zeros((n_states, n_actions))\n",
      "\n",
      "    # eligibility traces (only used if lambda_ > 0)\n",
      "    E = np.zeros_like(Q)\n",
      "\n",
      "    for episode in tqdm(range(1, num_episodes + 1), desc=\"Training SARSA\"):\n",
      "        state = env.reset()\n",
      "        action = epsilon_greedy_policy(Q, state, n_actions, epsilon)\n",
      "\n",
      "        # reset eligibility traces at the start of each episode\n",
      "        if lambda_ > 0.0:\n",
      "            E.fill(0.0)\n",
      "\n",
      "        done = False\n",
      "        while not done:\n",
      "            next_state, reward, done, _ = env.step(action)\n",
      "\n",
      "            # choose next action (on‑policy!)\n",
      "            next_action = epsilon_greedy_policy(Q, next_state, n_actions, epsilon)\n",
      "\n",
      "            # TD error (δ)\n",
      "            td_target = reward + gamma * Q[next_state, next_action] * (not done)\n",
      "            td_error  = td_target - Q[state, action]\n",
      "\n",
      "            # Update eligibility trace for the visited (s,a)\n",
      "            if lambda_ == 0.0:\n",
      "                # plain SARSA → no trace, update directly\n",
      "                Q[state, action] += alpha * td_error\n",
      "            else:\n",
      "                # accumulating traces\n",
      "                E[state, action] += 1.0\n",
      "                Q += alpha * td_error * E\n",
      "                # decay all traces\n",
      "                E *= gamma * lambda_\n",
      "\n",
      "            # move to next step\n",
      "            state, action = next_state, next_action\n",
      "\n",
      "        # optional logging\n",
      "        if log_interval and episode % log_interval == 0:\n",
      "            # compute average return over the last 100 episodes (if RecordEpisodeStatistics is wrapped)\n",
      "            if hasattr(env, \"get_episode_rewards\"):\n",
      "                recent_rewards = env.get_episode_rewards()[-100:]\n",
      "                avg_return = np.mean(recent_rewards) if recent_rewards else float('nan')\n",
      "                print(f\"\\nEpisode {episode}/{num_episodes} – avg return (last 100) = {avg_return:.3f}\")\n",
      "\n",
      "    return Q\n",
      "\n",
      "\n",
      "# --------------------------------------------------------------------\n",
      "# Main entry point – builds the environment and runs SARSA\n",
      "# --------------------------------------------------------------------\n",
      "if __name__ == \"__main__\":\n",
      "    # 1️⃣ Create a random 4×4 FrozenLake (the same as in the context)\n",
      "    random_map = generate_random_map(size=4)          # <-- deterministic if you pass a seed\n",
      "    env = gym.make('FrozenLake-v1', desc=random_map, is_slippery=True)\n",
      "\n",
      "    # 2️⃣ Wrap with statistics recorder (optional, matches the context)\n",
      "    env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=100)\n",
      "\n",
      "    # 3️⃣ Train\n",
      "    Q = sarsa(env,\n",
      "              num_episodes=50_000,   # the context shows a run with 50k episodes\n",
      "              alpha=0.1,\n",
      "              gamma=0.95,\n",
      "              epsilon=0.1,\n",
      "              lambda_=0.0,           # set to >0 for SARSA(λ) variants\n",
      "              log_interval=5000)\n",
      "\n",
      "    # 4️⃣ Simple evaluation – run a few episodes greedy w.r.t. Q\n",
      "    def greedy_policy(state):\n",
      "        \"\"\"Deterministic greedy action (break ties randomly).\"\"\"\n",
      "        max_q = np.max(Q[state])\n",
      "        candidates = np.where(Q[state] == max_q)[0]\n",
      "        return np.random.choice(candidates)\n",
      "\n",
      "    print(\"\\n=== Evaluation (greedy policy) ===\")\n",
      "    success_cnt = 0\n",
      "    for ep in range(10):\n",
      "        state = env.reset()\n",
      "        done = False\n",
      "        while not done:\n",
      "            action = greedy_policy(state)\n",
      "            state, reward, done, _ = env.step(action)\n",
      "        success_cnt += reward   # reward is 1 only if goal reached\n",
      "        print(f\"Episode {ep+1}: {'Success' if reward==1 else 'Fail'}\")\n",
      "\n",
      "    print(f\"\\nSuccess rate over 10 eval episodes: {success_cnt}/10\")\n",
      "\n",
      "    # 5️⃣ Clean up\n",
      "    env.close()\n",
      "```\n",
      "\n",
      "### How the script works\n",
      "| Section | What it does |\n",
      "|---------|--------------|\n",
      "| **Imports** | Brings in `numpy`, `gym`, `tqdm` and the helper that builds a random lake. |\n",
      "| **epsilon_greedy_policy** | Implements the exploration/exploitation rule used by SARSA. |\n",
      "| **sarsa()** | Core learning loop: initializes `Q`, optionally maintains eligibility traces (`λ`), updates the table on‑policy after each step, and prints occasional progress. |\n",
      "| **Main block** | *Creates* a random 4 × 4 FrozenLake, *wraps* it to record statistics, *trains* SARSA for 50 000 episodes (the same count shown in the context), and finally *evaluates* the learned policy greedily. |\n",
      "| **Eligibility traces** | Setting `lambda_ > 0` automatically switches the algorithm to SARSA(λ) with accumulating traces (the context lists “Sarsa(λ), accumulating”).  Keeping `lambda_=0.0` yields plain SARSA. |\n",
      "\n",
      "You can experiment with the hyper‑parameters (`alpha`, `gamma`, `epsilon`, `lambda_`, `num_episodes`) to reproduce the various curves that appear in the original notebook (e.g., “replacing, clearing”, “accumulating”, etc.). The script is intentionally minimal yet fully functional, mirroring the snippets present in the provided context. Happy RL!"
     ]
    }
   ],
   "source": [
    "stream_answer(\"Generate python code for  SARSA problem from scratch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d5f2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "college-rl-rag-bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
